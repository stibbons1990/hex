{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About This Place","text":""},{"location":"#whats-it-all-about","title":"What's It All About?","text":"<p>This site is mostly a rambling journal about Unexpected Linux Adventures, potentially wahooney-shapped Raspberry Pies and some other stuff along the way.</p> <p>Sometimes, some of the stuff coalesces into something somewhat useful. Sometimes, after some time refining it so that it works more than it sucks, it may end up among the Projects.</p>"},{"location":"blog/","title":"Unexpected Linux Adventures","text":""},{"location":"blog/#unexpected-linux-adventures","title":"Unexpected Linux Adventures","text":""},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/","title":"Optimized AAX-to-MP3 conversion","text":"<p>Audible is great, and the app is not bad, but I find Plex and other apps more attractive to use for audiobooks. Now, if only I could take my books home...</p> <p></p> <p>Added Bonus: pick your own choice of book covers!</p>"},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#audible-aaxc-formats","title":"Audible AAX/C formats","text":"<p>I didn't know much about this, until I found the KrumpetPirate/AAXtoMP3 tool that taught me the basics, so I quote that source here:</p> <p>Audible uses the AAX file format to maintain DRM restrictions on their audio books and if you download your book through your library it will be stored in this format.</p> <p>Downloading audible.com/library provides the same AAX files, and nothing else. There used to be an option to download lower quality files in a non-DRM format, which would only show when visiting the library from Linux, but that appears to have beem removed.</p> <p>Worse yet, newer (upcoming?) AAXC files will be even harder to use in Linux:</p> <p>The AAXC format is a new Audible encryption format, meant to replace the old AAX. The encryption has been updated, and now to decrypt the file the authcode is not sufficient, we need two \"keys\" which are unique for each audiobook. Since getting those keys is not simple, for now the method used to get them is handled by the package audible-cli, that stores them in a file when downloading the aaxc file. This means that in order to decrypt the aaxc files, they must be downloaded with audible-cli. Note that you need at least ffmpeg 4.4.</p>"},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#prerequiste-activation_bytes","title":"Prerequiste: <code>activation_bytes</code>","text":"<p>To decode the AAX files you need your own authentication code, that comes from Audible. To obtain this string, I used inAudible-NG/audible-activator and saved the bytes in <code>~/.authcode</code> to use later.</p>"},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#performance-comparison-for-short-and-long-books","title":"Performance comparison for short and long books","text":"<p>KrumpetPirate/AAXtoMP3 works very well so, for a while, this was the tool I used.</p> <p>After a while, it started to bother me that the conversion took a long time and only used 1 CPU core, all the time. Having to convert a boatload of books and having a 12-core CPU, I was motivated to create my <code>aax2mp3</code> script to parallelize the encoding of chapters.</p> <p>The performance diffences is thus most notable with long books, but there is quite a visible difference even with short books, e.g.</p> <ul> <li>How to Talk So Teens Will Listen and Listen So Teens Will Talk:     3 hrs and 18 mins is converted in half the time.</li> <li>Elantris, Tenth Anniversary Special Edition:     28 hrs and 42 mins is converted in a third of the time.</li> </ul>"},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#performance-of-aaxtomp3","title":"Performance of AAXtoMP3","text":""},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#short-book-3h-18m-converted-in-3m-50s","title":"Short book: 3h 18m converted in 3m 50s","text":"<pre><code>$ time AAXtoMP3 \\\n  HowtoTalkSoTeensWillListenandListenSoTeensWillTalk_ep7.aax\n...\nreal    3m50.693s\nuser    3m58.307s\nsys     0m13.704s\n\n$ id3v2 -l Audiobook/Adele\\ Faber\\,\\ Elaine\\ Mazlish/How\\ to\\ Talk\\ So\\ Teens\\ Will\\ Listen\\ and\\ Listen\\ So\\ Teens\\ Will\\ Talk/How\\ to\\ Talk\\ So\\ Teens\\ Will\\ Listen\\ and\\ Listen\\ So\\ Teens\\ Will\\ Talk-01\\ Chapter\\ 1.mp3 \nid3v2 tag info for Audiobook/Adele Faber, Elaine Mazlish/How to Talk So Teens Will Listen and Listen So Teens Will Talk/How to Talk So Teens Will Listen and Listen So Teens Will Talk-01 Chapter 1.mp3:\nTRCK (Track number/Position in set): 1\nTPE1 (Lead performer(s)/Soloist(s)): Adele Faber, Elaine Mazlish\nTPE2 (Band/orchestra/accompaniment): Adele Faber, Elaine Mazlish\nTALB (Album/Movie/Show title): How to Talk So Teens Will Listen and Listen So Teens Will Talk\nTYER (Year): 2005\nTIT2 (Title/songname/content description): How to Talk So Teens Will Listen and Listen So Teens Will Talk-01 Chapter 1\nTCON (Content type): Audiobook (255)\nTCOP (Copyright message): \u00a92005 Adele Faber and Elaine Mazlish (P)2005 HarperCollins Publishers\nTSSE (Software/Hardware and settings used for encoding): Lavf58.76.100\nCTOC ():  frame\nCHAP ():  frame\nCHAP ():  frame\nAPIC (Attached picture): (Album cover)[, 3]: image/png, 272335 bytes\nAudiobook/Adele Faber, Elaine Mazlish/How to Talk So Teens Will Listen and Listen So Teens Will Talk/How to Talk So Teens Will Listen and Listen So Teens Will Talk-01 Chapter 1.mp3: No ID3v1 tag\n</code></pre>"},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#long-book-28h-42m-converted-in-m-s","title":"Long book: 28h 42m converted in m s","text":"<pre><code>$ time AAXtoMP3 \\\n  ElantrisTenthAnniversarySpecialEdition_ep6.aax \n...\nreal    47m11.226s\nuser    52m53.673s\nsys     4m37.468s\n\n$ id3v2 -l Audiobook/Brandon\\ Sanderson/Elantris-Tenth\\ Anniversary\\ Special\\ Edition/Elantris-Tenth\\ Anniversary\\ Special\\ Edition*\\ 1.mp3\nid3v2 tag info for Audiobook/Brandon Sanderson/Elantris-Tenth Anniversary Special Edition/Elantris-Tenth Anniversary Special Edition-01 Chapter 1.mp3:\nTRCK (Track number/Position in set): 1\nTPE1 (Lead performer(s)/Soloist(s)): Brandon Sanderson\nTPE2 (Band/orchestra/accompaniment): Brandon Sanderson\nTALB (Album/Movie/Show title): Elantris: Tenth Anniversary Special Edition\nTYER (Year): 2015\nTIT2 (Title/songname/content description): Elantris-Tenth Anniversary Special Edition-01 Chapter 1\nTCON (Content type): Audiobook (255)\nTCOP (Copyright message): \u00a92005, 2015 Dragonsteel Entertainment, LLC (P)2015 Recorded Books\nTSSE (Software/Hardware and settings used for encoding): Lavf58.76.100\nCTOC ():  frame\nCHAP ():  frame\nCHAP ():  frame\nAPIC (Attached picture): (Album cover)[, 3]: image/png, 615956 bytes\nAudiobook/Brandon Sanderson/Elantris-Tenth Anniversary Special Edition/Elantris-Tenth Anniversary Special Edition-01 Chapter 1.mp3: No ID3v1 tag\n</code></pre>"},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#performance-of-aax2mp3","title":"Performance of aax2mp3","text":""},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#short-book-3h-18m-converted-in-3m-50s_1","title":"Short book: 3h 18m converted in 3m 50s","text":"<pre><code>$ wget -O cover.jpg https://m.media-amazon.com/images/...jpg\n$ time aax2mp3.sh \\\n  HowtoTalkSoTeensWillListenandListenSoTeensWillTalk_ep7.aax\n...\nreal    2m0.036s\nuser    4m21.804s\nsys     0m4.618s\n\n$ id3v2 -l 1.\\ Chapter\\ 1.mp3 \nid3v1 tag info for 1. Chapter 1.mp3:\nTitle  : Chapter 1                       Artist: Adele Faber, Elaine Mazlish   \nAlbum  : How to Talk So Teens Will List  Year: 2005, Genre: Vocal (28)\nComment:                                 Track: 1\nid3v2 tag info for 1. Chapter 1.mp3:\nTSSE (Software/Hardware and settings used for encoding): LAME 64bits version 3.100 (http://lame.sf.net)\nTIT2 (Title/songname/content description): Chapter 1\nTPE1 (Lead performer(s)/Soloist(s)): Adele Faber, Elaine Mazlish\nTALB (Album/Movie/Show title): How to Talk So Teens Will Listen and Listen So Teens Will Talk\nTYER (Year): 2005\nTRCK (Track number/Position in set): 1/10\nTCON (Content type): Vocal (28)\nTLEN (Length): 526419\nAPIC (Attached picture): ()[, 0]: image/jpeg, 51854 bytes\n</code></pre>"},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#long-book-28h-42m-converted-in-m-s_1","title":"Long book: 28h 42m converted in m s","text":"<pre><code>$ wget -O cover.jpg https://m.media-amazon.com/images/...jpg\n$ time aax2mp3.sh \\\n  ElantrisTenthAnniversarySpecialEdition_ep6.aax \n...\nreal    14m37.037s\nuser    62m19.688s\nsys     0m46.445s\n\n\n$ id3v2 -l 1.\\ Chapter\\ 1.mp3 \nid3v1 tag info for 1. Chapter 1.mp3:\nTitle  : Chapter 1                       Artist: Brandon Sanderson             \nAlbum  : Elantris: Tenth Anniversary Sp  Year: 2015, Genre: Vocal (28)\nComment:                                 Track: 1\nid3v2 tag info for 1. Chapter 1.mp3:\nTSSE (Software/Hardware and settings used for encoding): LAME 64bits version 3.100 (http://lame.sf.net)\nTIT2 (Title/songname/content description): Chapter 1\nTPE1 (Lead performer(s)/Soloist(s)): Brandon Sanderson\nTALB (Album/Movie/Show title): Elantris: Tenth Anniversary Special Edition\nTYER (Year): 2015\nTRCK (Track number/Position in set): 1/79\nTCON (Content type): Vocal (28)\nTLEN (Length): 195651\nAPIC (Attached picture): ()[, 0]: image/jpeg, 89607 bytes\n</code></pre>"},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#cpu-usage-of-aaxtomp3-and-aax2mp3","title":"CPU usage of AAXtoMP3 and aax2mp3","text":""},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#short-book-3h-18m-converted-in-3m-50s_2","title":"Short book: 3h 18m converted in 3m 50s","text":""},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#long-book-28h-42m-converted-in-m-s_2","title":"Long book: 28h 42m converted in m s","text":""},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#code","title":"Code","text":"<p>The gist of this is using <code>xargs</code> to parallelize the extraction and encoding of individual chapters. This would probably be better using GNU parallel, but I learned about that one a few months too late.</p>"},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#aax2mp3sh","title":"<code>aax2mp3.sh</code>","text":"<p>The main script <code>aax2mp3.sh</code> does most of the work.</p> <p>Note</p> <p>This script takes the activation codes from a different file: <code>~/audible_activation_bytes</code></p> aax2mp3.sh<pre><code>#!/bin/bash\n#\n# Convert DRM'ed AAX audibooks (e.g. Audible) to DRMless MP3 audio.\n# Cover art is extracted from AAX files if present,\n# otherwise cover.jpg file will be use for artwork.\n\n# File names and target destinations in local (output) and remote targets.\ninput=$1\next=${1##*.}\nmp3=${1/.$ext/.mp3}\noutput=\"/home/raid/audio/Audiobooks\"\nremote=\"lexicon:/home/depot/audio/Audiobooks/\"\n\nif [ \"$2\" == \"wy\" ]; then\n    echo \"INFO: final diretory will include (year)\"\nfi\n\nif [ ! -f \"$1\" ]; then\n    echo \"FATAL: $1 is missing, this will cause encoding to fail!\"\n    exit 1\nfi\n\n# Obtain activation bytes.\n$activation_bytes\nactivation_bytes=$(head -1 ~/.audible_activation_bytes)\n\n# Extract cover art. This will overwrite cover.jpg only if cover art is found.\nart=\"cover.jpg\"\nffmpeg -activation_bytes $activation_bytes -i \"$input\" -an -c:v copy -y \"$art\"\n\n# Extract uncompressed audio.\nffmpeg -activation_bytes $activation_bytes -i \"$input\" -vn -c:a mp3 -y \"$mp3\"\n\n# Extract metadata and chapters.\nchapters=chapters.txt\nmetadata=metadata.txt\nffprobe -activation_bytes $activation_bytes -i \"$input\" -show_chapters &gt;$chapters 2&gt;$metadata\ngenre=28 # Vocal\nalbum=$(grep '^    title' $metadata | sed 's/.* : //')\nartist=$(grep '^    artist' $metadata | sed 's/.* : //')\nyear=$(grep '^    date' $metadata | sed 's/.* : //')\necho \"$album ($year), by $artist\"\n\n# Split and encode chapters (parallel).\ntrack=0\nnumtracks=$(grep -c title chapters.txt)\ntabdata=chapters.tab\ngrep title chapters.txt | cut -f2 -d'=' | while read title; do\n    track=$((track + 1))\n    start=$(grep -B5 \"title=$title$\" chapters.txt | grep start_time | cut -f2 -d=)\n    end=$(grep -B5 \"title=$title$\" chapters.txt | grep end_time | cut -f2 -d=)\n    wav=\"$track. $title.wav\"\n    chapter=${wav/.wav/.mp3}\n    echo -e \"$track\\t$start\\t$end\\t$title\"\ndone &gt;$tabdata\n\n# Process chapters in parallel.\ncut -f1 $tabdata | xargs --max-procs=10 -n 1 aax2mp3-chapter.sh $mp3 $metadata $tabdata\n\n# Move processed chapters into author/book directory.\nauthor=\"$(id3v2 -l 1.*.mp3 | grep TPE1 | cut -f2 -d: | cut -f1 -d, | sed 's/^ //')\"\ntitle=\"$(id3v2 -l 1.*.mp3 | grep TALB | cut -f2 -d: | sed 's/^ //')\"\nbookdir=\"$(echo \"$author/$title\" | sed 's/ /./g' | sed \"s/'//g\" | sed 's/\\.\\././g' | sed 's/&amp;/and/g')\"\nif [ \"$2\" == \"wy\" ]; then\n    echo \"INFO: final diretory will include (year)\"\n    bookdir=\"$bookdir.($year)\"\nfi\necho \"$bookdir\"\nmkdir -p \"$bookdir\"\necho \"$bookdir\" &gt;.bookdirs\nrename 's/^/0/' ?.\\ Chapter*\nrename 's/^/0/' ?.\\ Kapitel*\nls -lh $art *Kapitel*.mp3 *Chapter*.mp3 \"$bookdir\"\nmv -fv $art *Kapitel*.mp3 *Chapter*.mp3 \"$bookdir\"\n\n# Move the book to destinations.\n# Note: DO NOT let destdir end in /\ndestdir=\"$(echo \"$output/$author\" | sed 's/ /./g' | sed \"s/'//g\" | sed 's/\\.\\././g')\"\nrsync -turva \"$bookdir\" \"$remote\"\nmkdir -p \"$destdir\"\nmv -fv \"$bookdir\" \"$destdir\"\n\n# Clean-up\nrm -f chapters.t* *Kapitel*.wav *Chapter*.wav *_ep*.mp3 $input\n</code></pre> <p>The <code>--max-procs</code> flag is set to only 10 so that a few CPU cores are left for other tasks. In a pinch this value can be increased to the number of CPU threads, if the system won't be used by anybody in the meantime, to further reduce the time it takes to encode chapters.</p>"},{"location":"blog/2019/04/09/optimized-aax-to-mp3-conversion/#aax2mp3-chaptersh","title":"<code>aax2mp3-chapter.sh</code>","text":"<p>And to process each chapter, here is  <code>aax2mp3-chapter.sh</code></p> aax2mp3-chapter.sh<pre><code>#!/bin/bash\n#\n# Split and encode a single audiobook chapter, to use with xargs.\n# If available, cover.jpg file will be use for artwork.\n\n# Input parameters.\nmp3=$1\nmetadata=$2\ntabdata=$3\ntrack=$4\nart=cover.jpg\n\n# Recover book metadata from file.\ngenre=28 # Vocal\nalbum=$(grep '^    title' $metadata | sed 's/.* : //')\nartist=$(grep '^    artist' $metadata | sed 's/.* : //')\nyear=$(grep '^    date' $metadata | sed 's/.* : //')\n\nstart=$(grep \"^$track[[:blank:]]\" $tabdata | cut -f2)\nend=$(grep \"^$track[[:blank:]]\" $tabdata | cut -f3)\ntitle=$(grep \"^$track[[:blank:]]\" $tabdata | cut -f4)\nnumtracks=$(cat $tabdata | wc -l)\nwav=\"$track. $title.wav\"\nchapter=${wav/.wav/.mp3}\nffmpeg \\\n    -nostdin \\\n    -i \"$mp3\" \\\n    -vn \\\n    -c:a pcm_s16le \\\n    -f wav \\\n    -ss $start \\\n    -to $end \\\n    -y \"$wav\"\nlame \\\n    -S \\\n    --tt \"$title\" \\\n    --ta \"$artist\" \\\n    --tl \"$album\" \\\n    --ty $year \\\n    --tn $track/$numtracks \\\n    --tg $genre \\\n    --ti $art \\\n    \"$wav\" \"$chapter\"\n</code></pre>"},{"location":"blog/2020/03/31/detailed-system-and-process-monitoring/","title":"Detailed system and process monitoring","text":"<p>Never got the hang of <code>telegraf</code>, it was all too easy to cook my own monitoring...</p>"},{"location":"blog/2020/03/31/detailed-system-and-process-monitoring/#humble-beginnings","title":"Humble Beginnings","text":"<p>In fact, when I started building detailed process monitoring I knew nothing about telegraf, influxdb, grafana or even Raspberry Pi computers.</p> <p>It was back in 2017, when pondering whether to build my next PC around an Intel Core i7-6950X or an AMD Ryzen 5 1600X, that I started looking into measuring CPU usage of a specific process. I wanted to better see and  understand whether more (but slower) CPU cores would be a better investment than faster (but fewer) CPU cores.</p> <p>At the time my PC had a AMD Phenom II X4 965 BE C3 with 4 cores at 3.4GHz, and I had no idea how often those CPU cores were all used to their full extent. To learn more about the possibilities (and limitations) of fully multi-threading CPU-bound applications, I started running <code>top</code> commands in a loop and dumping lines in <code>.csv</code> files to then plot charts in Google Sheets. This was very crude, but it did show the difference between rendering a video in Blender (not multi-threaded) compared to using the pulverize tool to fully multi-thread the same task:</p> <p></p> <p></p> <p>This early ad-hoc effort resulted in a few scripts to measure per-proccess CPU usage, overall CPU with thermals, and even GPU usage.</p>"},{"location":"blog/2020/03/31/detailed-system-and-process-monitoring/#mt-top","title":"<code>mt-top</code>","text":"<p>This script measures only CPU usage for a single process:</p> mt-top<pre><code>#!/bin/bash\n#\n# CPU usage stats across all threads / instances for a process.\n#\n# Given a process name (as it appears when running the \"top\" command),\n# use the \"top\" command to capture aggregate CPU usage for all threads\n# of that process every second.\n#\n# If that process is not yet running, wait until it does. Keep tracking\n# CPU usage until that process ends, or Ctrl-C is pressed.\n#\n# Usage: mt-top blender\n\nif test \"$#\" != 1\nthen\n  echo \"Usage: $0 &lt;process name&gt;\"\n  exit 1\nfi\n\npname=$1\npstarted=0\n\necho -e \"time\\tcpu\"\nwhile true\ndo\n  time=$(date +\"%H:%M:%S\")\n  cpu=$(top -b -n 1 | egrep -i \"$pname\" | sort -n -k1 -t\\  | awk '{print $9}' | tr '\\n' '+' | sed 's/+$/\\n/' | bc -ql)\n  if [ -n \"$cpu\" ]\n  then\n    echo -e \"$time\\t$cpu\"\n    if [[ $pstarted == 0 ]]\n    then\n      pstarted=1\n    fi\n  else\n    if [[ $pstarted == 1 ]]\n    then\n      # Process has finished.\n      exit 0\n    fi\n  fi\n  sleep 1\ndone\n</code></pre>"},{"location":"blog/2020/03/31/detailed-system-and-process-monitoring/#mt-top-temp","title":"<code>mt-top-temp</code>","text":"<p>This measures the overall CPU usage along with thermals:</p> mt-top-temp<pre><code>#!/bin/bash\n#\n# CPU usage % and temperature, sampled every second.\n#\n# Usage: mt-top-temp ... Ctrl+C\n\necho -e \"time\\tcpu\\ttemp\"\nwhile true\ndo\n  time=$(date +\"%H:%M:%S\")\n  cpu=$(top -b -n 1 |awk '{print $9}' | egrep '[0-9]\\.[0-9]|^[0-9][0-9]$|^[0-9][0-9][0-9]$|^[0-9][0-9][0-9][0-9]$' | tr '\\n' '+' | sed 's/+$/\\n/' | bc -ql)\n  temp=$(sensors -A | grep temp1 | awk '{print $2}')\n  echo -e \"$time\\t$cpu\\t$temp\"\n  sleep 1\ndone\n</code></pre>"},{"location":"blog/2020/03/31/detailed-system-and-process-monitoring/#mt-top-gpu","title":"<code>mt-top-gpu</code>","text":"<p>This script measures (overall)  GPU usage:</p> mt-top-gpu<pre><code>#!/bin/bash\n#\n# GPU usage % and temperature, sampled every second.\n#\n# Usage: mt-top-gpu ... Ctrl+C\n\necho -e \"time\\tcpu\"\nwhile true\ndo\n  time=$(date +\"%H:%M:%S\")\n  gpu=$(nvidia-smi -i 0 --query-gpu=temperature.gpu,utilization.gpu  --format=csv,noheader)\n  echo -e \"$time\\t$gpu\"\n  sleep 1\ndone\n</code></pre>"},{"location":"blog/2020/03/31/detailed-system-and-process-monitoring/#enter-influxdb-grafana","title":"Enter InfluxDB &amp; Grafana","text":"<p>A few days ago someone shared a screenshot of their  good-looking weather monitoring and mentioned two things I had never seen before: influxdb and grafana. They also mentioned they were running their monitoring on a Raspberry Pi computer.</p> <p>Now I know what those are, I have a Raspberry Pi 3 model B with CUPS to share the printer over WiFi; much cheaper than even the cheapest WiFi-enabled printers.</p>"},{"location":"blog/2020/03/31/detailed-system-and-process-monitoring/#influxdb","title":"InfluxDB","text":"<p>Installing InfluxDB couldn't be easier, if you don't mind running a fairly old version:</p> <pre><code># apt install influxdb influxdb-client -y\n# dpkg -l influxdb | grep influxdb\nii  influxdb       1.6.4-1+deb10u1 armhf        Scalable datastore for metrics, events, and real-time analytics\n</code></pre> <p>For a more recent version, one can install InfluxDB OSS 1.7 or InfluxDB 2.7.</p> <p>Once installed, one or more databases need to be crated to start collecting data. Get started with InfluxDB OSS to create a database (e.g. <code>monitoring</code>) and set a retention policy:</p> <pre><code># influx\nConnected to http://localhost:8086 version 1.6.7~rc0\nInfluxDB shell version: 1.6.7~rc0\n&gt; CREATE DATABASE monitoring\n&gt; CREATE RETENTION POLICY \"30_days\" ON \"monitoring\" DURATION 30d REPLICATION 1\n&gt; ALTER RETENTION POLICY \"30_days\" on \"monitoring\" DURATION 30d REPLICATION 1 DEFAULT\n</code></pre> <p>As soon as the database is created, data can be inserted. There is no need to define columns, instead just Write data with the InfluxDB API to feed simple data such as CPU load and temperature:</p> <pre><code>curl -i -XPOST \\\n  --data-binary \"cpu,host=$host value=$cpu\" \\\n  'http://localhost:8086/write?db=monitoring'\ncurl -i -XPOST \\\n  --data-binary \"temperature,host=$host value=$temp\" \\\n  'http://localhost:8086/write?db=monitoring'\n</code></pre> <p>The body of the POST or InfluxDB line protocol contains the time series data that you want to store. Data includes:</p> <ul> <li>Measurement (required): the thing to measure, e.g.    <code>cpu</code> in this case to measure global CPU load.</li> <li>Tags: Strictly speaking, tags are optional but most    series include tags to differentiate data sources and    to make querying both easy and efficient. Both tag keys    and tag values are strings.</li> <li>Fields (required): Field keys are required and are    always strings, and,     by default,    field values are floats.</li> <li>Timestamp: Supplied at the end of the line in Unix    time in nanoseconds since January 1, 1970 UTC - is    optional. If you do not specify a timestamp, InfluxDB   uses the server\u2019s local nanosecond timestamp in Unix   epoch. Time in InfluxDB is in UTC format by default.</li> </ul>"},{"location":"blog/2020/03/31/detailed-system-and-process-monitoring/#minimal-update-to-post-to-influxdb","title":"Minimal update to post to InfluxDB","text":"<p>The scripts above can now feed data to it in addition to producing TSV files, e.g. <code>mt-top-temp</code> can be updated as follows:</p> mt-top-temp<pre><code>#!/bin/bash\n#\n# CPU usage % and temperature, sampled every second.\n#\n# Usage: mt-top-temp ... Ctrl+C\n\necho -e \"time\\tcpu\\ttemp\"\nwhile true\ndo\n  time=$(date +\"%H:%M:%S\")\n  cpu=$(top -b -n 1 |awk '{print $9}' | egrep '[0-9]\\.[0-9]|^[0-9][0-9]$|^[0-9][0-9][0-9]$|^[0-9][0-9][0-9][0-9]$' | tr '\\n' '+' | sed 's/+$/\\n/' | bc -ql)\n  temp=$(sensors -A | grep temp1 | awk '{print $2}' | egrep -o '[0-9]+\\.[0-9]')\n  host=$(hostname)\n  curl -i -XPOST \\\n    --data-binary \"cpu,host=$host value=$cpu\" \\\n    'http://localhost:8086/write?db=monitoring' \\\n    2&gt;&amp;1 &gt; /dev/null\n  curl -i -XPOST \\\n    --data-binary \"temperature,host=$host value=$temp\" \\\n    'http://localhost:8086/write?db=monitoring'\n    2&gt;&amp;1 &gt; /dev/null\n  echo -e \"$time\\t$cpu\\t$temp\"\n  sleep 1\ndone\n</code></pre>"},{"location":"blog/2020/03/31/detailed-system-and-process-monitoring/#grafana","title":"Grafana","text":"<p>The next step is visualizing these time series in fancy charts, and that's where Grafana comes in.</p> <p>Install Grafana OSS, start the server with <code>systemd</code> and reset the Admin password:</p> <pre><code># echo \"deb https://packages.grafana.com/oss/deb stable main\" \\\n  | tee /etc/apt/sources.list.d/grafana.list\n# curl https://packages.grafana.com/gpg.key | sudo apt-key add -\n# apt update\n# apt install grafana\n# systemctl daemon-reload\n# systemctl start grafana-server\n# grafana-cli admin reset-admin-password \\\n  PLEASE_CHOOSE_A_SENSIBLE_PASSWORD\nINFO[03-20|15:02:11] Connecting to DB                         logger=sqlstore dbtype=sqlite3\nINFO[03-20|15:02:11] Starting DB migrations                   logger=migrator\nAdmin password changed successfully \u2714\n</code></pre> <p>At this point Grafana is available on http://localhost:3000/ for the Admin user. </p> <p>Add your InfluxDB data source to Grafana, create a new Dashboard and Add &gt; Visualization for each measurement (<code>cpu</code>, <code>temp</code>, <code>gpu</code>, etc.).</p> <p>Tweak <code>/etc/grafana/grafana.ini</code> as follows to enable anonymous authentication and allow anonymous users to view dashboards in the default org:</p> /etc/grafana/grafana.ini<pre><code>#################################### Anonymous Auth ######################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\norg_role = Viewer\n\n# systemctl restart grafana-server.service\n</code></pre>"},{"location":"blog/2020/03/31/detailed-system-and-process-monitoring/#continuous-monitoring","title":"Continuous Monitoring","text":"<p>With InfluxDB and Grafana in place, collection and reporting of metrics can be done continuously, rather than having to run the scripts each time. </p> <p>For a minimal start, create a script that reports total CPU usage every second, e.g as <code>/usr/local/bin/conmon</code></p> /usr/local/bin/conmon<pre><code>#!/bin/bash\n#\n# Export system monitoring metrics to influxdb.\n\n# InfluxDB target.\nDBNAME=monitoring\nTARGET='http://localhost:8086'\n\n# Data file for batch POST.\nDDIR=\"/dev/shm/$$\"\nDATA=\"${DDIR}/DATA.txt\"\nmkdir -p \"${DDIR}\"\n\nhost=$(hostname)\n\ntimestamp_ns() {\n  date +'%s%N'\n}\n\nstore_line() {\n  # Write a line of data to the temporary in-memory file.\n  # Exit immediately if this fails.\n  echo $1 &gt;&gt;\"${DATA}\" || exit 1\n}\n\nreport_top() {\n  # Stats from top: CPU (overall and per process) and RAM (per process).\n  ts=$(timestamp_ns)\n  cpu_load=$(top -b -n 1 |awk '{print $9}' | egrep '[0-9]\\.[0-9]|^[0-9][0-9]$|^[0-9][0-9][0-9]$|^[0-9][0-9][0-9][0-9]$' | tr '\\n' '+' | sed 's/+$/\\n/' | bc -ql)\n  store_line \"top,host=${host} value=${cpu_load} ${ts}\"\n}\n\npost_lines_to_influxdb() {\n  # POST data to InfluxDB in batch, when target is available.\n  # Depends on: nc.\n  sleep ${DELAY_POST}\n  host_and_port=$(echo \"${TARGET}\" | sed 's/.*\\///' | tr : ' ')\n  if nc 2&gt;&amp;1 -zv ${host_and_port} | grep -q succeeded; then\n    # All other tasks write data to the file in append mode (&gt;&gt;).\n    # This task reads everything at once and immediately deletes the file.\n    # This makes all the other tasks write to the same file, created anew.\n    mv -f \"${DATA}\" \"${DATA}.POST\"\n    cut -f1 -d, \"${DATA}.POST\" | sort | uniq -c\n    curl &gt;/dev/null 2&gt;/dev/null -i -XPOST \"${TARGET}/write?db=${DBNAME}\" --data-binary @\"${DATA}.POST\"\n  fi\n}\n\n# Run all the above tasks in a loop.\n# Each task is responsible of its own checks.\nwhile true; do\n  report_top\n  post_lines_to_influxdb\ndone\n</code></pre> <p>Create a new service to run this upon reboot, e.g. as <code>/etc/systemd/system/conmon.service</code></p> /etc/systemd/system/conmon.service<pre><code>[Unit]\nDescription=Continuous Monitoring\nAfter=influxd.service\nWants=influxd.service\n\n[Service]\nExecStart=/usr/local/bin/conmon\nRestart=on-failure\nStandardOutput=null\nUser=root\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Then load and start the new service</p> <pre><code># systemctl daemon-reload\n# systemctl enable conmon.service\nCreated symlink /etc/systemd/system/multi-user.target.wants/conmon.service \u2192 /etc/systemd/system/conmon.service.\n# systemctl start conmon.service\n</code></pre> <p>From here on, new functions can be added to the <code>conmon</code> script to collect additional metrics, which will be then posted by the script periodically. Many other metrics can be added later, here are some ideas:</p> <ul> <li>Total RAM usage</li> <li>CPU usage per process</li> <li>RAM usage per process</li> <li>Network I/O per interface</li> <li>Disk I/O per disk</li> <li>Disk I/O per process</li> <li>Disk used/free (per partition)</li> <li>GPU load, VRAM, temperature, fan speed, power draw</li> </ul> <p>These and more may be added later, keep an eye on the full scripts in the Continuous Monitoring project page.</p>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/","title":"Low-effort homelab server with Ubuntu Server on Intel NUC","text":"<p>Need. More. Server. Need. More. POWER!!!</p> <p>But only a little bit, maybe just enough to run a Minecraft server, which refuses to start on my Raspberry Pi 4 because it has only a meagre 2 GB of RAM.</p> <p>I had known about Intel NUC tiny PCs for a while, and how handy they can be to have a dedicated physical PC for experimentation. There was a very real possibility that I would have to set one up as a light gaming PC in the near future, so I thought cutting my teeth on a simpler server setup would be a good way to get acquainted with this hardware platform and its Linux support.</p>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#hardware","title":"Hardware","text":"<p>Ubuntu's list of Recommended and Certified Hardware is quite short and includes only very old models, and searching the web for this model in relation to Linux or Ubuntu tends to yield mostly useless results (e.g. shop listings, or forum threads related to other models).</p> <p>Not being at all sure this would be a good choice of platform, I kept this build low-cost by choosing the low-end on the CPU:</p> <ul> <li>Intel\u00ae NUC 11 Performance kit - NUC11PAHi3 ($285)</li> <li>VENGEANCE\u00ae Series 32GB RAM (2 x 16GB) DDR4 SODIMM ($120)</li> <li>SSD 970 EVO Plus NVMe\u00ae M.2 2 TB SSD ($180)</li> </ul>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#update-1-2022-10-12","title":"Update #1 (2022-10-12)","text":"<p>Encouraged by a sudden price fall (by 15% down to $300), and spurred by the recent failure of 6TB HDD RAID, I added a Crucial MX500 4TB 3D NAND SATA SSD to serve as an backup to some of my precious files in that cursed RAID.</p> <p>Turns out, Crucial MX500 SSD are problematic.</p>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#update-2-2023-09-23","title":"Update #2 (2023-09-23)","text":"<p>Often I wonder whether 32 GB of RAM was too much, and it probably was. I hardly ever see any process use more than 5 GB, the whole system hardly ever has more thn 10 GB used. On the other hand, maybe it is better to have enough RAM for those rare occasions when it's needed. Just once in the last 30 days, did one process (InfluxDB) got up to 17 GB of used RAM, most likely because I made an unreasonable request such as retrieving too many days' worth of data.</p> <p></p>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#ubuntu-desktop-2204","title":"Ubuntu Desktop 22.04","text":"<p>Although the intended use for this mini PC is to serve as a server, this being my first experience with this hardware platform I tried first installing Ubuntu Desktop 22.04 to see what may be different on this hardware.</p> <p>The most important difference between Intel NUC is that, starting precisely at this generation (11), NUC 11 Kits &amp; Mini PCs no longer support Legacy BIOS Support. It seems even in the previous generation, this was already the case for Performance Kits (NUC10ixFN). This means Secure Boot is required, which posed a bit of a challenge for me since I had never had to set this up.</p> <p>With that in mind, the installation itself was simple as it tends to be the case with Ubuntu:</p> <ol> <li>Boot from USB, with the option to Try or Install Ubuntu</li> <li>Select language (English) and then Install Ubuntu</li> <li>Select keyboard layout and update and other software to    Install third-party software for graphics and Wifi hardware and additional media formats, on top of an    otherwise Minimal installation.</li> <li>Configure Secure Boot (simply provide a password).</li> <li>Create a new partition table on <code>/dev/nvme0n1</code> with    4 primary partitions:</li> <li>512 MB EFI System Partition mounted on <code>/boot/efi</code></li> <li>50,000 MB btrfs mounted on <code>/</code></li> <li>50,000 MB btrfs without mount point (for future use)</li> <li>1,899,886 MB (remainder) btrfs mounted on <code>/home</code></li> <li>Select Install Now to confirm and create the new    partition table and install everything. Confirm location    (time zone), first non-root user name (<code>ponder</code>) and    computer name (<code>lexicon</code>).</li> <li>Once it's done, select Restart    (remove install media and hit <code>Enter</code>).</li> </ol>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#first-boot-mok-management","title":"First boot &amp; MOK management","text":"<p>The first time the system boots, after installing for the first time, the MOK management blue menu appears. This is the point where the user installs Ubuntu on a new system and the correct option to choose is Enroll MOK. I was midly confused, hadn't read the documentation carefully before hand, and didn't know which option to chose. After a few seconds of inactivity, the menu time out and the system boots.</p> <p>Once the system has started, </p> <pre><code># dmesg | egrep -i 'secure|mok|boot'\n[    0.000000] Command line: BOOT_IMAGE=/@/boot/vmlinuz-5.15.0-40-generic root=UUID=a61cf166-64e3-48ce-be02-07981ef82e33 ro rootflags=subvol=@ quiet splash vt.handoff=7\n[    0.000000] efi: ACPI=0x40cbe000 ACPI 2.0=0x40cbe014 TPMFinalLog=0x40cc8000 SMBIOS=0x41568000 SMBIOS 3.0=0x41567000 MEMATTR=0x34ac0298 ESRT=0x351e3d18 MOKvar=0x3111c000 RNG=0x4151df18 TPMEventLog=0x310f5018 \n[    0.000000] secureboot: Secure boot enabled\n[    0.000000] Kernel is locked down from EFI Secure Boot mode; see man kernel_lockdown.7\n[    0.018245] secureboot: Secure boot enabled\n[    0.069027] smpboot: Allowing 4 CPUs, 0 hotplug CPUs\n[    0.069056] Booting paravirtualized kernel on bare hardware\n[    0.069204] Kernel command line: BOOT_IMAGE=/@/boot/vmlinuz-5.15.0-40-generic root=UUID=a61cf166-64e3-48ce-be02-07981ef82e33 ro rootflags=subvol=@ quiet splash vt.handoff=7\n[    0.069250] Unknown kernel command line parameters \"splash BOOT_IMAGE=/@/boot/vmlinuz-5.15.0-40-generic\", will be passed to user space.\n[    0.150970] smpboot: Estimated ratio of average max frequency by base frequency (times 1024): 1399\n[    0.150970] smpboot: CPU0: 11th Gen Intel(R) Core(TM) i3-1115G4 @ 3.00GHz (family: 0x6, model: 0x8c, stepping: 0x1)\n[    0.150970] x86: Booting SMP configuration:\n[    0.151228] smpboot: Max logical packages: 1\n[    0.151228] smpboot: Total of 4 processors activated (23961.60 BogoMIPS)\n[    0.659056] ACPI: \\_SB_.PC00.LPCB.H_EC: Boot DSDT EC used to handle transactions\n[    1.003909] ACPI: \\_SB_.PC00.LPCB.H_EC: Boot DSDT EC initialization complete\n[    1.003960] pci 0000:00:02.0: vgaarb: setting as boot VGA device\n[    1.292708] smpboot: Estimated ratio of average max frequency by base frequency (times 1024): 1399\n[    1.305119] efifb: showing boot graphics\n[    1.339073] Loaded X.509 cert 'Canonical Ltd. Secure Boot Signing: 61482aa2830d0ab2ad5af10b7250da9033ddcef0'\n[    1.341583] integrity: Loading X.509 certificate: UEFI:MokListRT (MOKvar table)\n[    1.403263]     BOOT_IMAGE=/@/boot/vmlinuz-5.15.0-40-generic\n[    4.704413] systemd[1]: Condition check resulted in First Boot Complete being skipped.\n[    5.271931] Bluetooth: hci0: Bootloader revision 0.4 build 0 week 30 2018\n[    5.272963] Bluetooth: hci0: Secure boot is enabled\n[    7.059956] Bluetooth: hci0: Waiting for device to boot\n[    7.074922] Bluetooth: hci0: Device booted in 14646 usecs\n</code></pre> <p>Secure Boot is enabled and there is only one key enrolled:</p> <pre><code># mokutil --sb-state\nSecureBoot enabled\n\n# mokutil --list-enrolled\n[key 1]\nSHA1 Fingerprint: 76:a0:92:06:58:00:bf:37:69:01:c3:72:cd:55:a9:0e:1f:de:d2:e0\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            b9:41:24:a0:18:2c:92:67\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: C=GB, ST=Isle of Man, L=Douglas, O=Canonical Ltd., CN=Canonical Ltd. Master Certificate Authority\n        Validity\n            Not Before: Apr 12 11:12:51 2012 GMT\n            Not After : Apr 11 11:12:51 2042 GMT\n        Subject: C=GB, ST=Isle of Man, L=Douglas, O=Canonical Ltd., CN=Canonical Ltd. Master Certificate Authority\n        Subject Public Key Info:\n            Public Key Algorithm: rsaEncryption\n                Public-Key: (2048 bit)\n                Modulus:\n                    00:bf:5b:3a:16:74:ee:21:5d:ae:61:ed:9d:56:ac:\n                    bd:de:de:72:f3:dd:7e:2d:4c:62:0f:ac:c0:6d:48:\n                    08:11:cf:8d:8b:fb:61:1f:27:cc:11:6e:d9:55:3d:\n                    39:54:eb:40:3b:b1:bb:e2:85:34:79:ca:f7:7b:bf:\n                    ba:7a:c8:10:2d:19:7d:ad:59:cf:a6:d4:e9:4e:0f:\n                    da:ae:52:ea:4c:9e:90:ce:c6:99:0d:4e:67:65:78:\n                    5d:f9:d1:d5:38:4a:4a:7a:8f:93:9c:7f:1a:a3:85:\n                    db:ce:fa:8b:f7:c2:a2:21:2d:9b:54:41:35:10:57:\n                    13:8d:6c:bc:29:06:50:4a:7e:ea:99:a9:68:a7:3b:\n                    c7:07:1b:32:9e:a0:19:87:0e:79:bb:68:99:2d:7e:\n                    93:52:e5:f6:eb:c9:9b:f9:2b:ed:b8:68:49:bc:d9:\n                    95:50:40:5b:c5:b2:71:aa:eb:5c:57:de:71:f9:40:\n                    0a:dd:5b:ac:1e:84:2d:50:1a:52:d6:e1:f3:6b:6e:\n                    90:64:4f:5b:b4:eb:20:e4:61:10:da:5a:f0:ea:e4:\n                    42:d7:01:c4:fe:21:1f:d9:b9:c0:54:95:42:81:52:\n                    72:1f:49:64:7a:c8:6c:24:f1:08:70:0b:4d:a5:a0:\n                    32:d1:a0:1c:57:a8:4d:e3:af:a5:8e:05:05:3e:10:\n                    43:a1\n                Exponent: 65537 (0x10001)\n        X509v3 extensions:\n            X509v3 Subject Key Identifier: \n                AD:91:99:0B:C2:2A:B1:F5:17:04:8C:23:B6:65:5A:26:8E:34:5A:63\n            X509v3 Authority Key Identifier: \n                AD:91:99:0B:C2:2A:B1:F5:17:04:8C:23:B6:65:5A:26:8E:34:5A:63\n            X509v3 Basic Constraints: critical\n                CA:TRUE\n            X509v3 Key Usage: \n                Digital Signature, Certificate Sign, CRL Sign\n            X509v3 CRL Distribution Points: \n                Full Name:\n                  URI:http://www.canonical.com/secure-boot-master-ca.crl\n    Signature Algorithm: sha256WithRSAEncryption\n    Signature Value:\n        3f:7d:f6:76:a5:b3:83:b4:2b:7a:d0:6d:52:1a:03:83:c4:12:\n        a7:50:9c:47:92:cc:c0:94:77:82:d2:ae:57:b3:99:04:f5:32:\n        3a:c6:55:1d:07:db:12:a9:56:fa:d8:d4:76:20:eb:e4:c3:51:\n        db:9a:5c:9c:92:3f:18:73:da:94:6a:a1:99:38:8c:a4:88:6d:\n        c1:fc:39:71:d0:74:76:16:03:3e:56:23:35:d5:55:47:5b:1a:\n        1d:41:c2:d3:12:4c:dc:ff:ae:0a:92:9c:62:0a:17:01:9c:73:\n        e0:5e:b1:fd:bc:d6:b5:19:11:7a:7e:cd:3e:03:7e:66:db:5b:\n        a8:c9:39:48:51:ff:53:e1:9c:31:53:91:1b:3b:10:75:03:17:\n        ba:e6:81:02:80:94:70:4c:46:b7:94:b0:3d:15:cd:1f:8e:02:\n        e0:68:02:8f:fb:f9:47:1d:7d:a2:01:c6:07:51:c4:9a:cc:ed:\n        dd:cf:a3:5d:ed:92:bb:be:d1:fd:e6:ec:1f:33:51:73:04:be:\n        3c:72:b0:7d:08:f8:01:ff:98:7d:cb:9c:e0:69:39:77:25:47:\n        71:88:b1:8d:27:a5:2e:a8:f7:3f:5f:80:69:97:3e:a9:f4:99:\n        14:db:ce:03:0e:0b:66:c4:1c:6d:bd:b8:27:77:c1:42:94:bd:\n        fc:6a:0a:bc\n</code></pre> <p>As is tytpically the case, the system required installing a few updates. After doing this, rebooting was required again. After rebooting, MOK management was not offered. No drivers seem to be missing, so everything seems to be good.</p> <p>For a moment I thought I'd need to set up a new MOK password (<code>update-secureboot-policy --enroll-key</code>) but didn't need to. I still feel like I don't really understand, and should read more about, what exactly is MOK in Linux for?</p>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#ubuntu-server-2204","title":"Ubuntu Server 22.04","text":"<p>With the Secure Boot setup out of the way, it was soon time to install Ubuntu Server 22.04 as the final, permanent system to run this server.</p> <p>The installation process is a bit different:</p> <ol> <li>Boot from USB, with the option to Try or Install Ubuntu</li> <li>Wait for the <code>cloud-init</code> scripts to do finish (a few minutes).</li> <li>Select language (English) and then Install Ubuntu</li> <li>Update to the new installer.</li> <li>Select keyboard layout.</li> <li>Select Type of install: Ubuntu Server (not minimized)</li> <li>Configure network interfaces:</li> <li>Setup <code>enp89s0 eth</code> with 192.168.0.121</li> <li>Leave <code>wlo1 wlan</code> not connected</li> <li>For Storage configuration select Custom storage layout</li> <li>Use partition 1 (512 MB) as EFI System Partition mounted on <code>/boot/efi</code></li> <li>Use partition 2 (50,000 MB) btrfs without mount point (alrady used for Ubuntu Studio)</li> <li>Use partition 3 (50,000 MB) btrfs mounted on <code>/</code></li> <li>Use partition 4 (remainder) btrfs mounted on <code>/home</code>      (not formatting)</li> <li>Select Install Now to confirm the partition selection.</li> <li>Confirm location (time zone), first non-root user name    (<code>ponder</code>) and computer name (<code>lexicon</code>).</li> <li>Under SSH Setup select</li> <li>Install OpenSSH server: Yes</li> <li>Import SSH identity: No</li> <li>Select Featured server snaps: none</li> <li>Once it's done, select Restart    (remove install media and hit <code>Enter</code>).</li> </ol> <p>At this point both Ubuntu desktop and server are installed. The newly installed grub only boots the server installation, which is fine because there is no need for the desktop anymore.</p>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#tweak-bash-prompt","title":"Tweak Bash prompt","text":"<p>Tweak Bash prompt for <code>root</code> to make user name red, host name blue and path green; with this in <code>.bashrc</code>:</p> .bashrc<pre><code>if [ \"$color_prompt\" = yes ]; then\n    PS1='\\[\\e]0;\\u@\\h: \\w\\a\\]${debian_chroot:+($debian_chroot)}\\[\\033[01;31m\\]\\u@\\[\\033[01;34m\\]\\h\\[\\033[00m\\] \\[\\033[01;32m\\]\\w \\$\\[\\033[00m\\] '\nelse\n    PS1='${debian_chroot:+($debian_chroot)}\\u@\\h \\w \\$ '\nfi\n</code></pre> <p>Other users' Bash prompt is left as default, which renders all green. The idea is that <code>root</code>'s prompt is visually different, to remind me that with great power comes great responsibility.</p>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#add-local-hosts-to-etchosts","title":"Add local hosts to <code>/etc/hosts</code>","text":"<p>This being the latest system added to the network, its <code>/etc/hosts</code> will contain all the other systems' LAN IPs:</p> <pre><code># cat /etc/hosts\n127.0.0.1 localhost\n127.0.1.1 lexicon\n\n# The following lines are desirable for IPv6 capable hosts\n::1     ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n\n# Google (forced) Safe Search\n# https://support.google.com/websearch/answer/186669\n216.239.38.120 www.google.com #forcesafesearch\n216.239.38.120 www.google.ch  #forcesafesearch\n\n# K8s tests.\n192.168.6.1     k8s.example.com\n192.168.6.1     webapp.k8s.example.com\n\n# Ethernet\n10.0.0.2        rapture-lan\n10.0.0.3        computer-lan\n10.0.0.4        pi-f1-lan\n10.0.0.5        smart-computer-lan\n10.0.0.6        lexicon-lan\n192.168.0.2     rapture\n192.168.0.3     computer\n192.168.0.148   pi-f1\n192.168.0.8     smart-computer-lan\n\n# Wi-Fi\n192.168.0.12    pi3a\n192.168.0.41    pi-f1-w\n192.168.0.101   pi-z1\n192.168.0.95    smart-computer\n192.168.0.95    smart-computer-wlan\n192.168.0.143   lexicon-wlan\n192.168.0.216   pi-z2\n\n# UniFi APs\nunify-ap-lr     192.168.0.143\nunify-ap-lite   192.168.0.69\n</code></pre>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#enable-ntp","title":"Enable NTP","text":"<p>I like to have all machines on the same clock, and for this there's nothing like good old NTP:</p> <pre><code># apt install ntp ntpdate -y\n</code></pre>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#setup-ssh","title":"Setup SSH","text":"<p>Since this server would be reachable by SSH from outside the LAN, it is imperative that no password ever will work on it.</p> <p>Instead, only a few trusted certificates will be able to log in, i.e. those added to <code>.ssh/authorized_keys</code> for each user.</p>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#add-authorized-keys-to-ssh-authentication","title":"Add authorized keys to SSH authentication","text":"<p>In order to disable password authentication, first add a few trusted keys to <code>/root/.ssh/authorized_keys</code> and (optionally) a few [more / other] to <code>/home/ponder/.ssh/authorized_keys</code></p>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#disable-ssh-password-authentication","title":"Disable SSH password authentication","text":"<pre><code># vi /etc/ssh/sshd_config\nPasswordAuthentication no\nPermitEmptyPasswords no\n# systemctl restart ssh\n# systemctl restart sshd\n</code></pre> <p>Note</p> <p>The reason for doing this is obvious after looking at <code>/var/log/auth.log</code> once the SSH port has been exposed externally for a while. Before doing this, there were 100,240 failed attempts to ssh in as <code>root</code> from 2,765 IPs.</p>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#setup-fail2ban","title":"Setup Fail2Ban","text":"<p>On top of disabling password authentication, the SSH server will be less busy is those pesky bad actors are blocked from reaching it's port. To do this, install fail2ban.</p> <p>How to install fail2ban on Ubuntu Server 22.04: Jammy Jellyfish explains this in more details; install with <code>apt</code> and setup:</p> <pre><code># apt-get install fail2ban -y\n# systemctl enable --now fail2ban\n</code></pre> <p>Within seconds a few IPs are banned already:</p> <pre><code># iptables -L\n...\nChain f2b-sshd (1 references)\ntarget     prot opt source               destination\nREJECT     all  --  45.118.145.178       anywhere             reject-with icmp-port-unreachable\nREJECT     all  --  207.200.202.35.bc.googleusercontent.com  anywhere             reject-with icmp-port-unreachable\nREJECT     all  --  68.183.235.43        anywhere             reject-with icmp-port-unreachable\nREJECT     all  --  138.68.72.245        anywhere             reject-with icmp-port-unreachable\nREJECT     all  --  207.154.220.120      anywhere             reject-with icmp-port-unreachable\nRETURN     all  --  anywhere             anywhere\n</code></pre> <p>I like to spice it up to make a little more trigger-happy:</p> /etc/fail2ban/jail.conf<pre><code># \"bantime\" is the number of seconds that a host is banned.\nbantime  = 3d\n# A host is banned if it has generated \"maxretry\" during the last \"findtime\"\n# seconds.\nfindtime  = 1d\n# \"maxretry\" is the number of failures before a host get banned.\nmaxretry = 3\n# \"bantime.increment\" allows to use database for searching of previously banned ip's to increase a \n# default ban time using special formula, default it is banTime * 1, 2, 4, 8, 16, 32...\nbantime.increment = true\n</code></pre> <p>Apply the changes by restarting the service:</p> <pre><code># systemctl restart fail2ban\n</code></pre> <p>Fail2ban can be setup for other services, see Gitea's guide to Fail2ban setup to block users after failed login attempts</p> <p>Other resources for future consideration (GitHub repositories):</p> <ul> <li>awesome-selfhosted/awesome-selfhosted</li> <li>kahun/awesome-sysadmin</li> <li>pluja/awesome-privacy</li> </ul>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#multiple-static-ips-on-lan","title":"Multiple Static IPs on LAN","text":"<p>There are 2 physical Local Area Networks in this environment, a wire network connected directly to the router, and a wireless network with a UniFi AC Long-Range access point that bridges both networks.</p> <p>This has the side effect that using IP addresses in the DHCP range (leased by the router) may lead to data flowing through the wireless network despite the hosts being physically connected through the wired network.</p> <p>To avoid this, LAN interfaces on all systems (that have them) are assigned a second IP address on a different range, so the traffic between them can't jump over the wireless network.</p> <p>During installation, network setup defaulted to DHCP on Ethernet and no wifi setup:</p> /etc/netplan/00-installer-config.yaml<pre><code># This is the network config written by 'subiquity'\nnetwork:\nethernets:\n    enp89s0:\n    dhcp4: true\nversion: 2\n</code></pre> /etc/netplan/00-installer-config-wifi.yaml<pre><code># This is the network config written by 'subiquity'\nnetwork:\nversion: 2\nwifis: {}\n</code></pre> <p>This lead to the system getting a leased IP in the 192.168.0.0/24 range and, most importantly, the relevant DNS servers:</p> <pre><code># ip a | grep enp89s0\n2: enp89s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    inet 192.168.0.121/24 metric 100 brd 192.168.0.255 scope global dynamic enp89s0\n\n# resolvectl status\nGlobal\n       Protocols: -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\nresolv.conf mode: stub\n\nLink 2 (enp89s0)\n    Current Scopes: DNS\n         Protocols: +DefaultRoute +LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\nCurrent DNS Server: 62.2.24.158\n       DNS Servers: 62.2.24.158 62.2.17.61\n        DNS Domain: v.cablecom.net\n\nLink 3 (wlo1)\nCurrent Scopes: none\n     Protocols: -DefaultRoute +LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\n</code></pre> <p>To setup both addresses as static, need to grab the DNS servers and create a new netplan configuration in <code>/etc/netplan/00-installer-config.yaml</code></p> /etc/netplan/00-installer-config.yaml<pre><code># Dual static IP on LAN, nothing else.\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp89s0:\n      dhcp4: no\n      dhcp6: no\n      # Ser IP address &amp; subnet mask\n      addresses: [ 10.0.0.6/24, 192.168.0.6/24 ]\n      # Set default gateway\n      routes:\n       - to: default\n         via: 192.168.0.1\n      # Set DNS name servers\n      nameservers:\n        addresses: [62.2.24.158, 62.2.17.61]\n</code></pre> <pre><code># netplan apply\n# ip a | grep enp89s0\n2: enp89s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    inet 10.0.0.6/24 brd 10.0.0.255 scope global enp89s0\n    inet 192.168.0.6/24 brd 192.168.0.255 scope global enp89s0\n</code></pre>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#weekly-btrfs-scrub","title":"Weekly btrfs scrub","text":"<p>To keep BTRFS file systems healthy, it is recommended to run a weekly scrub to check everything for consistency. For this, I run the script from crontab every Saturday morning, early enough that it will be done by the time anyone wakes up.</p> <pre><code># crontab -l | grep btrfs\n# m h  dom mon dow   command\n50 5 * * 6 /usr/local/bin/btrfs-scrub-all\n</code></pre> <p>The whole process takes less than 10 minutes with a 2TB NVMe SSD:</p> <p></p>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#conclusion","title":"Conclusion","text":"<p>I quite like this hardware!</p> <p>Secure Boot was a little confusing and intimidating at first, now it's just a little confusing but at least not so scary.</p> <p>With an Intel NUC Mini PC you get literally and Intel architecture (<code>x84_64</code>) and familiar hardware without much in the way of platform-specific quirks.</p> <p>It does come at a cost though, this system costed nearly $600 which is more than triple the cost of a modest Raspberry Pi 4 setup. On the other hand, Raspberry Pi computers are good up to a point but, past that point, an Intel NUC offers a really good power to size ratio, packing quite some compute power in a still quite small package.</p>"},{"location":"blog/2022/07/03/low-effort-homelab-server-with-ubuntu-server-on-intel-nuc/#future-updates","title":"Future Updates","text":"<p>Watch this space for links to future posts!</p> <ul> <li>2022-10-12 Crucial MX500 SSD found problematic</li> <li>2023-09-16 Migrating a Plex Media Server to Kubernetes</li> </ul>"},{"location":"blog/2022/09/27/undead-yes--unraid-no/","title":"Undead Yes \u2500 UnRAID No","text":"<p>My only NAS is my PC. At least, what people would usually do with, or build a NAS for, I just do it with my PC.</p> <p>Most of my disk storage space is a BTRFS RAID 1 using two 6TB WD BLACK 3.5\u2033 HDD. This setup offers block-level redundancy which is better than the classic device-level redundancy offered by Linux Software RAID or hardware RAID. To keep BTRFS file systems healthy, it is strongly recommended to run a weekly scrub to check everything for consistency. For this, I run the script from crontab every Saturday night (it usually ends around noon the next day).</p> <p>One Sunday morning, after many successful scrubs, I woke up to both disks failing, each in a different way. But this was not the end of it. And the end of this adventure, disks emerged victorious.</p> <p>Keeping reading to find out how the disks came back from the dead.</p> <p></p>"},{"location":"blog/2022/09/27/undead-yes--unraid-no/#meet-my-disks","title":"Meet My Disks","text":"<p>The disks configured in this RAID1 array are: <code>/dev/sdb</code> and <code>/dev/sdc</code></p> <ul> <li><code>/dev/sdb</code> is a    WD6002FZWX    6TB (128 MB cache) purchased in 2017.</li> <li><code>/dev/sdc</code> is a    WD6003FZBX    6TB (256 MB cache) purchased in 2020.</li> </ul> <p>They are both rated as 227 MB/s but have significantly different performance:</p> <pre><code># btrfs filesystem show\nLabel: 'HomeDepot6TB'  uuid: a4ee872d-b985-445f-94a2-15232e93dcd5\n        Total devices 2 FS bytes used 4.59TiB\n        devid    1 size 5.46TiB used 4.59TiB path /dev/sdb\n        devid    2 size 5.46TiB used 4.59TiB path /dev/sdc\n\n# hdparm -tT /dev/sd[bc]\n\n/dev/sdb:\n Timing cached reads:   58408 MB in  2.00 seconds = 29275.83 MB/sec\n Timing buffered disk reads: 612 MB in  3.00 seconds = 203.77 MB/sec\n\n/dev/sdc:\n Timing cached reads:   55672 MB in  2.00 seconds = 27901.25 MB/sec\n Timing buffered disk reads: 754 MB in  3.01 seconds = 250.89 MB/sec\n</code></pre> <p>This will be noticeable in the I/O charts shown below.</p>"},{"location":"blog/2022/09/27/undead-yes--unraid-no/#what-happened","title":"What Happened","text":"<p>When the scrub starts, both disks are start reading at about 300 MB/s which is the normal for these disks. The problem starts when, after about one hour, <code>sdb</code> becomes inactive:</p> <p></p> <p>Before dropping down to zero, transfer (read) rate on <code>sdb</code> drops sharply on both disks, and that that time RAM usage drops sharply too, down to about half of its previous value:</p> <p></p> <p>At that time, which is about 3900 seconds after scrub starts on that disk, btrfs errors start showing up in dmesg warning of tasks being blocked waiting for I/O:</p> <pre><code>[21158.812097] BTRFS info (device sdb): balance: start -musage=0 -susage=0\n[21158.819155] BTRFS info (device sdb): balance: ended with status: 0\n[21159.105609] BTRFS info (device sdb): balance: start -musage=20 -susage=20\n[21159.107671] BTRFS info (device sdb): relocating block group 16994275426304 flags system|raid1\n[21159.661753] BTRFS info (device sdb): found 53 extents, stage: move data extents\n[21160.134413] BTRFS info (device sdb): balance: ended with status: 0\n[21160.357813] BTRFS info (device sdb): balance: start -dusage=0\n[21160.363849] BTRFS info (device sdb): balance: ended with status: 0\n[21160.608367] BTRFS info (device sdb): balance: start -dusage=20\n[21160.616713] BTRFS info (device sdb): balance: ended with status: 0\n[21160.730133] BTRFS info (device sdb): scrub: started on devid 1\n[21160.732650] BTRFS info (device sdb): scrub: started on devid 2\n\n\u2026 3,910 seconds later \u2026\n\n[25068.665995] INFO: task btrfs-transacti:1291 blocked for more than 122 seconds.\n[25068.666007]       Not tainted 5.15.0-47-lowlatency #53-Ubuntu\n[25068.666009] \"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\n[25068.666011] task:btrfs-transacti state:D stack:    0 pid: 1291 ppid:     2 flags:0x00004000\n[25068.666015] Call Trace:\n[25068.666018]  &lt;TASK&gt;\n[25068.666023]  __schedule+0x240/0x5c0\n[25068.666032]  schedule+0x67/0xe0\n[25068.666035]  btrfs_scrub_pause+0x9a/0x100 [btrfs]\n[25068.666095]  ? wait_woken+0x70/0x70\n[25068.666101]  btrfs_commit_transaction+0x277/0xb50 [btrfs]\n[25068.666135]  ? start_transaction+0xd1/0x5f0 [btrfs]\n[25068.666169]  ? __bpf_trace_timer_class+0x10/0x10\n[25068.666173]  transaction_kthread+0x137/0x1b0 [btrfs]\n[25068.666209]  ? btrfs_cleanup_transaction.isra.0+0x550/0x550 [btrfs]\n[25068.666243]  kthread+0x13b/0x160\n[25068.666246]  ? set_kthread_struct+0x50/0x50\n[25068.666249]  ret_from_fork+0x22/0x30\n[25068.666254]  &lt;/TASK&gt;\n[25068.666450] INFO: task btrfs:3834821 blocked for more than 122 seconds.\n[25068.666453]       Not tainted 5.15.0-47-lowlatency #53-Ubuntu\n[25068.666455] \"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\n[25068.666456] task:btrfs           state:D stack:    0 pid:3834821 ppid:970896 flags:0x00004002\n[25068.666517] Call Trace:\n[25068.666518]  &lt;TASK&gt;\n[25068.666519]  __schedule+0x240/0x5c0\n[25068.666522]  ? autoremove_wake_function+0x12/0x40\n[25068.666525]  schedule+0x67/0xe0\n[25068.666526]  __scrub_blocked_if_needed+0x77/0xd0 [btrfs]\n[25068.666570]  ? wait_woken+0x70/0x70\n[25068.666573]  scrub_pause_off+0x26/0x60 [btrfs]\n[25068.666614]  scrub_enumerate_chunks+0x376/0x7a0 [btrfs]\n[25068.666658]  ? wait_woken+0x70/0x70\n[25068.666660]  btrfs_scrub_dev+0x1d1/0x510 [btrfs]\n[25068.666714]  ? __check_object_size.part.0+0x134/0x150\n[25068.666719]  ? _copy_from_user+0x2e/0x70\n[25068.666724]  btrfs_ioctl+0x651/0x1250 [btrfs]\n[25068.666764]  ? get_task_io_context+0x54/0x90\n[25068.666768]  ? set_task_ioprio+0xa1/0xb0\n[25068.666771]  ? __fget_light+0xa7/0x130\n[25068.666775]  __x64_sys_ioctl+0x95/0xd0\n[25068.666778]  do_syscall_64+0x5c/0xc0\n[25068.666783]  ? exit_to_user_mode_prepare+0x37/0xb0\n[25068.666787]  ? syscall_exit_to_user_mode+0x27/0x50\n[25068.666789]  ? do_syscall_64+0x69/0xc0\n[25068.666792]  ? irqentry_exit_to_user_mode+0x9/0x20\n[25068.666794]  ? irqentry_exit+0x3b/0x50\n[25068.666795]  ? exc_page_fault+0x89/0x190\n[25068.666797]  entry_SYSCALL_64_after_hwframe+0x61/0xcb\n[25068.666802] RIP: 0033:0x7f57d6944aff\n[25068.666804] RSP: 002b:00007f57d6825c40 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\n[25068.666807] RAX: ffffffffffffffda RBX: 000055b4db0cb4f0 RCX: 00007f57d6944aff\n[25068.666809] RDX: 000055b4db0cb4f0 RSI: 00000000c400941b RDI: 0000000000000003\n[25068.666810] RBP: 0000000000000000 R08: 00007fff102d3e9f R09: 0000000000000000\n[25068.666811] R10: 0000000000000000 R11: 0000000000000246 R12: 00007f57d6826640\n[25068.666812] R13: 000000000000006f R14: 00007f57d68be850 R15: 00007fff102d3ee0\n[25068.666815]  &lt;/TASK&gt;\n\n\u2026 4,032 seconds later \u2026\n\n[25191.545405] INFO: task btrfs-transacti:1291 blocked for more than 245 seconds.\n\n\u2026 4,155 seconds later \u2026\n\n[25314.425912] INFO: task btrfs-transacti:1291 blocked for more than 368 seconds.\n\n\u2026 4,278 seconds later \u2026\n\n[25437.306444] INFO: task btrfs-transacti:1291 blocked for more than 491 seconds.\n\n\u2026 4,401 seconds later \u2026\n\n[25560.188030] INFO: task btrfs-transacti:1291 blocked for more than 614 seconds.\n</code></pre> <p>These timings indicate that, from the time scrub starts:</p> <ul> <li>3,788 s. (01:03:08 after 01:20:50 \u2192 02:23:58) disk gets stuck</li> <li>4,279 s. (01:11:19 after 01:20:50 \u2192 02:32:09) disk was still stuck</li> </ul>"},{"location":"blog/2022/09/27/undead-yes--unraid-no/#what-happened-to-sdb","title":"What happened to <code>sdb</code>","text":"<p>Comparing these timings to read transfer rate on sdb:</p> <ul> <li>Scrub starts at 01:20:50 (320-350 MB/s)</li> <li>Slows down around 01:54:40 to 270-300 MB/s</li> <li>Drops down to nearly zero at 02:04:00</li> <li>Resumes reading at only 25 MB/s at 02:05:45</li> <li>Stops reading at 02:21:30</li> <li>There is a small burst of read activity at 03:20:00 peaking at 25 MB/s</li> <li>Then another burst of activity from 04:30 to 04:45 peaking at 62 MB/s</li> <li>Last, attempts of read activity every 10 minutes, tiny spikes of 500 B/s</li> </ul> <p> </p> <p>At this point attempting to read <code>sdb</code> just doesn\u2019t work at all; it times out without even a message on <code>dmesg</code> (it was from <code>btrfs</code>), and can\u2019t be interrupted:</p> <pre><code>root@rapture:~# hdparm -tT /dev/sdb\n\n/dev/sdb:\n^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C\n</code></pre> <p>This attempt to read sdb produced just one spike shy of 10 kB/s, and then nothing more than the spikes of 512 B/s every 10 minutes, as above.</p>"},{"location":"blog/2022/09/27/undead-yes--unraid-no/#what-happened-to-sdc","title":"What happened to <code>sdc</code>","text":"<p>Meanwhile, read activity on sdc started off pretty good at 350-400 MB/s but then, after about an hour, it gradually slowed down from 380 MB/s to 250 MB/s over the next 5.5 hours, and then is started to intermittently drop abruptly to only 75 MB/s:</p> <p></p> <p>The rate of read from btrfs itself is significantly lower:</p> <p></p> <p>After about 12 hours, the read rate on sdc drops even lower, to 45 MB/s:</p> <p></p> <p>Thousands of operations are flagged by <code>handle_bad_sector</code> as attempting to read beyond the end of the device:</p> <pre><code>[63670.333955] attempt to access beyond end of device\n               sdc: rw=0, want=11721045248, limit=11721045168\n[63670.334052] attempt to access beyond end of device\n               sdc: rw=0, want=11721045504, limit=11721045168\n\n[63670.334465] attempt to access beyond end of device\n               sdc: rw=0, want=11721047552, limit=11721045168\n[63675.334648] handle_bad_sector: 154343 callbacks suppressed\n[63675.334652] attempt to access beyond end of device\n               sdc: rw=0, want=11760559616, limit=11721045168\n\n[64903.722343] attempt to access beyond end of device\n               sdc: rw=0, want=11957755648, limit=11721045168\n</code></pre> <p>This starts to happen nearly 12 hours (11:48:31) after the start of the scrub and then keeps going with a few more thousand entries every couple of seconds.</p> <p>At this point there seems to be no hope left for this process to finish. If there is a chance that the beyond end of device errors on <code>sdc</code> are being caused by the timeouts on <code>sdb</code>, maybe removing <code>sdb</code> from the RAID 1 configuration could make <code>sdc</code> usable, at least for a while.</p> <p>First, shut down and boot back into the system, so that it is no longer stuck trying to read a bad disk.</p> <p>Then modified the <code>btrfs-scrub</code> script to skip <code>sdb</code> and run it on the SSD and NVME drives. That was fast \ud83d\ude01</p> <p>Turns out, after rebooting the RAID 1 seems to work fine. Both <code>sdb</code> and <code>sdc</code> can be read. write operations seem to be happening successfully on both disks as the mirrors they are. It appears the problem in <code>sdb</code> only triggers after a certain amount of reading, so the idea now is to read everything from <code>sdc</code> while (if) it can still be read.</p> <p>There is a method to convert the RAID 1 back to a single drive, and even though <code>sdb</code> times out during the <code>balance</code> operation, there is a way to avoid reading from that disk. From here, I was thinking I could just use <code>-r 2</code> to read only from <code>sdc</code> by converting it to a single drive, but someone (and I\u2019m sorry to say, I lost track of the source) figured out it takes more than that:</p> <p>First, disable automounting the partition in <code>/etc/fstab</code> and reboot.</p> <p>Then, spin the drive down with and check with <code>dmesg</code> that it does stop:</p> <pre><code># echo 1 | sudo tee /sys/block/sdb/device/delete\n\n# dmesg | egrep 'sdb|ata'\n[  190.547826] sd 1:0:0:0: [sdb] Synchronizing SCSI cache\n[  190.548096] sd 1:0:0:0: [sdb] Stopping disk\n[  191.560912] ata2.00: disabled\n</code></pre> <p>Now, with the bad disk truly out of play, mount the RAID with in degraded state:</p> <pre><code># mount /home/raid -o degraded\n# df -h | head -1; df -h | grep home\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme0n1p5  1.7T  879G  834G  52% /home\n/dev/sde        3.7T  2.7T  988G  74% /home/ssd\n/dev/sdc        5.5T  4.6T  511G  91% /home/raid\n</code></pre> <p>Note the messages in <code>dmesg</code> when doing this:</p> <pre><code>[   70.409785] BTRFS info (device sdc): flagging fs with big metadata feature\n[   70.409792] BTRFS info (device sdc): allowing degraded mounts\n[   70.409795] BTRFS info (device sdc): disk space caching is enabled\n[   70.409797] BTRFS info (device sdc): has skinny extents\n[   70.422129] BTRFS warning (device sdc): devid 1 uuid 3941c5c1-08b5-4cb8-98bd-455ffa38ad2b is missing\n[   70.668878] BTRFS info (device sdc): bdev /dev/sdb errs: wr 0, rd 0, flush 0, corrupt 4, gen 0\n[   70.668884] BTRFS info (device sdc): bdev /dev/sdc errs: wr 0, rd 0, flush 0, corrupt 2, gen 0\n</code></pre> <p>Next, begin the rebalancing operation to convert <code>/home/raid</code> from RAID 1 to single disk. This would take about 14 hours:</p> <pre><code># btrfs balance start -f -mconvert=single -dconvert=single /home/raid\n</code></pre> <p>Note the messages in <code>dmesg</code> when doing this:</p> <pre><code>[  206.773574] BTRFS info (device sdc): balance: force reducing metadata redundancy\n[  207.233822] BTRFS info (device sdc): balance: start -f -dconvert=single -mconvert=single -sconvert=single\n[  207.237653] BTRFS info (device sdc): relocating block group 17087791628288 flags data\n[  207.610162] BTRFS info (device sdc): found 5 extents, stage: move data extents\n[  207.707770] BTRFS info (device sdc): found 5 extents, stage: update data pointers\n[  207.812576] BTRFS info (device sdc): relocating block group 17087758073856 flags system\n[  207.895458] BTRFS info (device sdc): found 3 extents, stage: move data extents\n[  207.982061] BTRFS info (device sdc): relocating block group 17086684332032 flags metadata\n[  208.075232] BTRFS info (device sdc): relocating block group 17086650777600 flags system|raid1\n[  208.171430] BTRFS info (device sdc): found 51 extents, stage: move data extents\n[  208.245601] BTRFS info (device sdc): relocating block group 17085577035776 flags data|raid1\n</code></pre> <p>Progress can be checked with</p> <pre><code># btrfs balance status -v /home/raid\nBalance on '/home/raid' is running\n14 out of about 4840 chunks balanced (15 considered), 100% left\nDumping filters: flags 0xf, state 0x1, force is on\n  DATA (flags 0x100): converting, target=281474976710656, soft is off\n  METADATA (flags 0x100): converting, target=281474976710656, soft is off\n  SYSTEM (flags 0x100): converting, target=281474976710656, soft is off\n</code></pre> <p>Sadly, this failed with I/O errors after 9h 20min.:</p> <pre><code>ERROR: error during balancing '/home/raid': Read-only file system\nThere may be more info in syslog - try dmesg | tail\n</code></pre> <p>Indeed there was plenty of details in <code>dmesg</code>:</p> <pre><code>[34044.600737] BTRFS info (device sdc): found 227 extents, stage: move data extents\n[34045.507151] BTRFS info (device sdc): found 227 extents, stage: update data pointers\n[34046.546190] BTRFS info (device sdc): relocating block group 10686444863488 flags data|raid1\n[34057.848567] BTRFS info (device sdc): found 318 extents, stage: move data extents\n[34059.062519] BTRFS info (device sdc): found 318 extents, stage: update data pointers\n[34060.419531] BTRFS info (device sdc): relocating block group 10685371121664 flags metadata|raid1\n[34067.303863] ------------[ cut here ]------------\n[34067.303865] WARNING: CPU: 10 PID: 55370 at fs/btrfs/extent-tree.c:862 lookup_inline_extent_backref+0x638/0x700 [btrfs]\n[34067.303898] Modules linked in: nvme_fabrics rfcomm bnep intel_rapl_msr snd_hda_codec_realtek snd_hda_codec_generic snd_hda_codec_hdmi intel_rapl_common ledtrig_audio joydev edac_mce_amd snd_hda_intel snd_intel_dspcfg snd_intel_sdw_acpi snd_hda_codec kvm snd_hda_core snd_hwdep btusb btrtl snd_pcm rapl btbcm input_leds snd_seq_midi snd_seq_midi_event btintel snd_rawmidi ath9k bluetooth ath9k_common eeepc_wmi wmi_bmof ecdh_generic mxm_wmi ecc snd_seq ath9k_hw snd_seq_device ath k10temp snd_timer mac80211 snd ccp soundcore cfg80211 libarc4 mac_hid nvidia_uvm(POE) sch_fq_codel cuse ipmi_devintf ipmi_msghandler msr parport_pc ppdev lp parport ramoops reed_solomon pstore_blk pstore_zone mtd efi_pstore ip_tables x_tables autofs4 btrfs blake2b_generic xor zstd_compress raid6_pq libcrc32c dm_mirror dm_region_hash dm_log nvidia_drm(POE) nvidia_modeset(POE) nvidia(POE) r8153_ecm cdc_ether usbnet drm_kms_helper r8152 mii syscopyarea mfd_aaeon sysfillrect hid_generic sysimgblt asus_wmi fb_sys_fops\n[34067.303942]  sparse_keymap cec video usbhid hid crct10dif_pclmul crc32_pclmul ghash_clmulni_intel aesni_intel crypto_simd rc_core platform_profile igb cryptd drm i2c_piix4 nvme ahci dca gpio_amdpt xhci_pci i2c_algo_bit nvme_core libahci xhci_pci_renesas wmi gpio_generic\n[34067.303958] CPU: 10 PID: 55370 Comm: btrfs Tainted: P           OE     5.15.0-47-lowlatency #53-Ubuntu\n[34067.303960] Hardware name: System manufacturer System Product Name/PRIME X370-PRO, BIOS 5220 09/12/2019\n[34067.303961] RIP: 0010:lookup_inline_extent_backref+0x638/0x700 [btrfs]\n[34067.303999] Code: e8 5d 6e 03 00 e9 6c fe ff ff 48 83 c3 01 48 83 fb 08 0f 85 ca fd ff ff e9 67 fe ff ff 4d 89 e6 31 db 4d 89 ec e9 15 fc ff ff &lt;0f&gt; 0b b8 fb ff ff ff e9 1c fb ff ff 80 7d c7 bf 0f 87 44 fe ff ff\n[34067.304000] RSP: 0018:ffffaaf2d40bf6b8 EFLAGS: 00010202\n[34067.304002] RAX: 0000000000000001 RBX: 0000000000000000 RCX: 0002ca395bedce00\n[34067.304003] RDX: 0000000000000001 RSI: 0000000000000002 RDI: ffff99f79a123548\n[34067.304004] RBP: ffffaaf2d40bf750 R08: 00000000000000b5 R09: ffff99f7ce4bbe00\n[34067.304005] R10: 0000000000000001 R11: 0000000000000001 R12: ffff99f7ce4bbe00\n[34067.304006] R13: ffff99f7ce4bbe00 R14: ffff99f8c7941a10 R15: ffff99f78ce2c800\n[34067.304008] FS:  00007f017cee58c0(0000) GS:ffff99fe7ec80000(0000) knlGS:0000000000000000\n[34067.304009] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n[34067.304010] CR2: 0000558bc3908340 CR3: 0000000246ad0000 CR4: 00000000003506e0\n[34067.304012] Call Trace:\n[34067.304013]  &lt;TASK&gt;\n[34067.304016]  insert_inline_extent_backref+0x5c/0xf0 [btrfs]\n[34067.304041]  ? kmem_cache_alloc+0x1b3/0x330\n[34067.304046]  __btrfs_inc_extent_ref.isra.0+0x77/0x240 [btrfs]\n[34067.304071]  run_delayed_data_ref+0x15f/0x180 [btrfs]\n[34067.304105]  btrfs_run_delayed_refs_for_head+0x185/0x500 [btrfs]\n[34067.304139]  __btrfs_run_delayed_refs+0x8c/0x1d0 [btrfs]\n[34067.304174]  btrfs_run_delayed_refs+0x73/0x200 [btrfs]\n[34067.304205]  ? btrfs_update_root+0x1a2/0x2d0 [btrfs]\n[34067.304229]  btrfs_commit_transaction+0x63/0xb50 [btrfs]\n[34067.304255]  ? btrfs_update_reloc_root+0x126/0x230 [btrfs]\n[34067.304289]  prepare_to_merge+0x29b/0x320 [btrfs]\n[34067.304323]  relocate_block_group+0x2c7/0x570 [btrfs]\n[34067.304356]  btrfs_relocate_block_group+0x1e1/0x390 [btrfs]\n[34067.304389]  btrfs_relocate_chunk+0x2c/0x100 [btrfs]\n[34067.304420]  __btrfs_balance+0x2fc/0x4d0 [btrfs]\n[34067.304452]  btrfs_balance+0x4cb/0x7d0 [btrfs]\n[34067.304482]  ? kmem_cache_alloc_trace+0x1a6/0x320\n[34067.304485]  btrfs_ioctl_balance+0x325/0x3e0 [btrfs]\n[34067.304516]  btrfs_ioctl+0x340/0x1250 [btrfs]\n[34067.304547]  ? rseq_ip_fixup+0x72/0x1a0\n[34067.304550]  ? __fput+0x123/0x260\n[34067.304553]  __x64_sys_ioctl+0x95/0xd0\n[34067.304556]  do_syscall_64+0x5c/0xc0\n[34067.304560]  ? __x64_sys_close+0x11/0x50\n[34067.304562]  ? do_syscall_64+0x69/0xc0\n[34067.304564]  ? __fput+0x123/0x260\n[34067.304566]  ? __rseq_handle_notify_resume+0x2d/0xd0\n[34067.304568]  ? exit_to_user_mode_loop+0x10d/0x160\n[34067.304571]  ? exit_to_user_mode_prepare+0x37/0xb0\n[34067.304573]  ? syscall_exit_to_user_mode+0x27/0x50\n[34067.304575]  ? __x64_sys_close+0x11/0x50\n[34067.304576]  ? do_syscall_64+0x69/0xc0\n[34067.304578]  entry_SYSCALL_64_after_hwframe+0x61/0xcb\n[34067.304581] RIP: 0033:0x7f017d002aff\n[34067.304583] Code: 00 48 89 44 24 18 31 c0 48 8d 44 24 60 c7 04 24 10 00 00 00 48 89 44 24 08 48 8d 44 24 20 48 89 44 24 10 b8 10 00 00 00 0f 05 &lt;41&gt; 89 c0 3d 00 f0 ff ff 77 1f 48 8b 44 24 18 64 48 2b 04 25 28 00\n[34067.304584] RSP: 002b:00007ffeb2da5f10 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\n[34067.304586] RAX: ffffffffffffffda RBX: 00007ffeb2da6008 RCX: 00007f017d002aff\n[34067.304587] RDX: 00007ffeb2da6008 RSI: 00000000c4009420 RDI: 0000000000000003\n[34067.304588] RBP: 0000000000000003 R08: 000000000000006e R09: 000000007fffffff\n[34067.304590] R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000000000\n[34067.304591] R13: 0000000000000000 R14: 00007ffeb2da828b R15: 0000000000000001\n[34067.304594]  &lt;/TASK&gt;\n[34067.304594] ---[ end trace 41f3048f844218d9 ]---\n[34067.304597] BTRFS: error (device sdc) in btrfs_run_delayed_refs:2150: errno=-5 IO failure\n[34067.304600] BTRFS info (device sdc): forced readonly\n[34067.304742] BTRFS info (device sdc): balance: ended with status: -30\n</code></pre> <p>The file system seems to be at least somewhat readable, for now...</p> <pre><code># btrfs filesystem show /home/raid\nLabel: 'HomeDepot6TB'  uuid: 300760fb-f533-4513-9710-50f283f3dbf4\n    Total devices 2 FS bytes used 4.59TiB\n    devid    2 size 5.46TiB used 4.73TiB path /dev/sdc\n    *** Some devices missing\n\n# df -h | head -1; df -h | grep raid\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sdc         11T  6.6T  893G  89% /home/raid\n\n# time du -sh /home/raid/*\n366G    /home/raid/audio\n569G    /home/raid/backups\n2.1T    /home/raid/depot\n148K    /home/raid/dmesg\n1.6T    /home/raid/video\n</code></pre>"},{"location":"blog/2022/09/27/undead-yes--unraid-no/#what-happened-to-the-data","title":"What happened to the Data","text":"<p>Remember, kids: a RAID is not a backup!**</p> <p>Depending on the array configuration, a RAID can give you performance (RAID 0), uptime (RAID 1), or a mix of both (RAID 5, 10, etc.).</p> <p>An array with redundancy (RAID 1, 5, 10, etc.) gives you uptime; a single disk failing doesn\u2019t immediately make the entire array unavailable. As seen above, the file system is still readable despite taking one drive out. However, it can make data unrecoverable, if a second disk also becomes unreadable or contains corrupt data that can no longer be corrected.</p> <p>You may find it ironic that my RAID 1 which is not a backup contains a backups folder. You may be amused to hear that this RAID is the first backup for my most precious files (family photos and other personal files), as well as the main storage for the heaviest files (audiobooks, podcasts, home videos and really old backups I never find time to clean up).</p> <p>But this is only one piece in a broader 3-2-1 backup setup, where everything important is stored in at least 3 copies, on at least 2 types of disks and at least 1 remote location. My setup is far from perfect but, given the constrains I have to work with, works well enough and has allowed to me recover from disk failures multiple times over the last 10 years.</p> <p>Despite being the biggest file system around the house, everything that was stored in this RAID was also stored in at least one other disk, so there was no need to attempt data recovery. Much time had been sunk already into trying, which was useful to show the disks were dead.</p>"},{"location":"blog/2022/09/27/undead-yes--unraid-no/#regaining-control","title":"Regaining Control","text":"<p>The above results showed at least one disk was probably dead, possibly both. However, there was yet more to test in order to confirm the disks were truly dead and one of those tests would be required before sending the disks out for RMA: wipe out all the data.</p> <p>Following Arch Linux Securely wipe disk, I settled for a single pass of shred with urandom followed by zeroes:</p> <pre><code># time shred --verbose --random-source=/dev/urandom -n1 --zero /dev/sdc\n\u2026\nshred: /dev/sdc: pass 1/2 (random)...32GiB/5.5TiB 0%\n\u2026\nshred: /dev/sdc: pass 1/2 (random)...4.9TiB/5.5TiB 89%\nshred: /dev/sdc: pass 1/2 (random)...5.0TiB/5.5TiB 91%\n\u2026\nshred: /dev/sdc: pass 1/2 (random)...5.4TiB/5.5TiB 98%\nshred: /dev/sdc: pass 1/2 (random)...5.5TiB/5.5TiB 100%\nshred: /dev/sdc: pass 2/2 (000000)...\nshred: /dev/sdc: pass 2/2 (000000)...1.2GiB/5.5TiB 0%\n\u2026\nshred: /dev/sdc: pass 2/2 (000000)...5.5TiB/5.5TiB 100%\n</code></pre> <p>The progress indication would come in handy later. If this operation also failed after 9-12 hours (that\u2019d be between 18:30 and 21:30 on 9/21) it would be a stronger indication that the disk was truly dead. Starting just 6.5 hours later (around 4pm) the chart of Disk I/O started to show (a) general slowdown from sustained 215-220 MB/s to (b) slightly less sustained 195-210 MB/s in about 45 min. If this trend was to continue linearly, in the following 3 hours Disk I/O would slow further down to somewhere between 95 and 170 MB/s.</p>"},{"location":"blog/2022/09/27/undead-yes--unraid-no/#3-hours-later","title":"3 Hours Later\u2026","text":"<p>Pretty much exactly 3 hours later, the writing of random patterns finished at 140-150 MB/s and, when the writing of zeros started, transfer rate jumped up to 310-330 MB/s:</p> <p></p> <p>35 minutes later the downward trend is still there, just displaced by the upward jump. Eventually the operation finished, slowing down to 140-150 MB/s by the end of it.</p> <p></p> <p>The next step would have been wiping ~1% of the disk in each pass, starting from the end, in batches of 118394372 bytes:</p> <pre><code># fdisk -l /dev/sdc\nDisk /dev/sdc: 5.46 TiB, 6001175126016 bytes, 11721045168 sectors\nDisk model: WDC WD6003FZBX-0\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\n</code></pre> <pre><code>for i in $(seq 99); do\n  start=$((11721045168-118394372*i));\n  echo \"Starting on sector $start...\"; \n  time dd if=/dev/urandom of=/dev/sdc bs=512 \\\n    seek=$start count=118394372 status=progress;\ndone\n</code></pre> <p>As it turned out, this was not necessary. Surprised by the success in the first operation, I decided to do the same on <code>sdb</code> first. It took 20 hours, but it worked!</p> <p></p> <p>Both disks slowed down in the same pattern, but never timed out. This was very surprising given the I/O errors and timeouts they had caused before.</p> <p>Reading from both disks was also successful, reading the entire disk in one go:</p> <pre><code># time dd if=/dev/sdb of=/dev/null bs=512 status=progress\n134832546304 bytes (135 GB, 126 GiB) copied, 603 s, 224 MB/s\n6001174704640 bytes (6.0 TB, 5.5 TiB) copied, 34417 s, 174 MB/s\n11721045168+0 records in\n11721045168+0 records out\n6001175126016 bytes (6.0 TB, 5.5 TiB) copied, 34423.2 s, 174 MB/s\n\nreal    573m43.223s\nuser    32m34.391s\nsys     199m35.531s\n\n# time dd if=/dev/sdc of=/dev/null bs=512 status=progress\n134832546304 bytes (135 GB, 126 GiB) copied, 603 s, 224 MB/s\n6001103282688 bytes (6.0 TB, 5.5 TiB) copied, 29290 s, 205 MB/s\n11721045168+0 records in\n11721045168+0 records out\n6001175126016 bytes (6.0 TB, 5.5 TiB) copied, 29293.3 s, 205 MB/s\n\nreal    488m13.316s\nuser    28m54.533s\nsys     176m39.930s\n</code></pre> <p>Once again, the data transfer rate slowed down over time at a pretty linear rate:</p> <p></p> <p>As I learned from a colleague, the progressive slow down is normal, caused by outer cylinders being \u201cfaster\u201d.</p> <p>The good news was: after 9 hours for <code>sdb</code> and 10.5 hours for <code>sdc</code>, there were no I/O errors or timeouts. Not a single one, after having written through both entire disks twice, and then read it back.</p> <p>At this point I started considering, should I just rebuild the original setup and restore the data?</p>"},{"location":"blog/2022/09/27/undead-yes--unraid-no/#get-smart","title":"Get S.M.A.R.T.","text":"<p>To further check the hard drives\u2019 hardware, I decided to run S.M.A.R.T. long test on both drives:</p> <pre><code># smartctl -t long /dev/sdb\nsmartctl 7.2 2020-12-30 r5155 [x86_64-linux-5.15.0-48-lowlatency] (local build)\nCopyright (C) 2002-20, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF OFFLINE IMMEDIATE AND SELF-TEST SECTION ===\nSending command: \"Execute SMART Extended self-test routine immediately in off-line mode\".\nDrive command \"Execute SMART Extended self-test routine immediately in off-line mode\" successful.\nTesting has begun.\nPlease wait 713 minutes for test to complete.\nTest will complete after Sat Sep 24 09:13:06 2022 CEST\nUse smartctl -X to abort test.\n\n# smartctl -t long /dev/sdc\nsmartctl 7.2 2020-12-30 r5155 [x86_64-linux-5.15.0-48-lowlatency] (local build)\nCopyright (C) 2002-20, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF OFFLINE IMMEDIATE AND SELF-TEST SECTION ===\nSending command: \"Execute SMART Extended self-test routine immediately in off-line mode\".\nDrive command \"Execute SMART Extended self-test routine immediately in off-line mode\" successful.\nTesting has begun.\nPlease wait 645 minutes for test to complete.\nTest will complete after Sat Sep 24 08:05:10 2022 CEST\nUse smartctl -X to abort test.\n</code></pre> <p>This also took several hours to complete, eventually to show No <code>Errors Logged</code> and <code>Completed without error</code> on both disks.</p> <pre><code># smartctl -a /dev/sdb\nsmartctl 7.2 2020-12-30 r5155 [x86_64-linux-5.15.0-48-lowlatency] (local build)\nCopyright (C) 2002-20, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nDevice Model:     WDC WD6002FZWX-00GBGB0\nSerial Number:    K1H9M1TD\nLU WWN Device Id: 5 000cca 255d27662\nFirmware Version: 81.H0A81\nUser Capacity:    6,001,175,126,016 bytes [6.00 TB]\nSector Sizes:     512 bytes logical, 4096 bytes physical\nRotation Rate:    7200 rpm\nForm Factor:      3.5 inches\nDevice is:        Not in smartctl database [for details use: -P showall]\nATA Version is:   ACS-2, ATA8-ACS T13/1699-D revision 4\nSATA Version is:  SATA 3.1, 6.0 Gb/s (current: 6.0 Gb/s)\nLocal Time is:    Sat Sep 24 09:57:52 2022 CEST\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nGeneral SMART Values:\nOffline data collection status:  (0x82) Offline data collection activity\n                    was completed without error.\n                    Auto Offline Data Collection: Enabled.\nSelf-test execution status:      (   0) The previous self-test routine completed\n                    without error or no self-test has ever \n                    been run.\nTotal time to complete Offline \ndata collection:        (  113) seconds.\nOffline data collection\ncapabilities:            (0x5b) SMART execute Offline immediate.\n                    Auto Offline data collection on/off support.\n                    Suspend Offline collection upon new\n                    command.\n                    Offline surface scan supported.\n                    Self-test supported.\n                    No Conveyance Self-test supported.\n                    Selective Self-test supported.\nSMART capabilities:            (0x0003) Saves SMART data before entering\n                    power-saving mode.\n                    Supports SMART auto save timer.\nError logging capability:        (0x01) Error logging supported.\n                    General Purpose Logging supported.\nShort self-test routine \nrecommended polling time:    (   2) minutes.\nExtended self-test routine\nrecommended polling time:    ( 713) minutes.\nSCT capabilities:          (0x0035) SCT Status supported.\n                    SCT Feature Control supported.\n                    SCT Data Table supported.\n\nSMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE\n  1 Raw_Read_Error_Rate     0x000b   100   100   016    Pre-fail  Always       -       0\n  2 Throughput_Performance  0x0005   137   137   054    Pre-fail  Offline      -       104\n  3 Spin_Up_Time            0x0007   131   131   024    Pre-fail  Always       -       497 (Average 503)\n  4 Start_Stop_Count        0x0012   100   100   000    Old_age   Always       -       1908\n  5 Reallocated_Sector_Ct   0x0033   100   100   005    Pre-fail  Always       -       0\n  7 Seek_Error_Rate         0x000b   100   100   067    Pre-fail  Always       -       0\n  8 Seek_Time_Performance   0x0005   128   128   020    Pre-fail  Offline      -       18\n  9 Power_On_Hours          0x0012   098   098   000    Old_age   Always       -       17768\n 10 Spin_Retry_Count        0x0013   100   100   060    Pre-fail  Always       -       0\n 12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       1903\n192 Power-Off_Retract_Count 0x0032   098   098   000    Old_age   Always       -       2427\n193 Load_Cycle_Count        0x0012   098   098   000    Old_age   Always       -       2427\n194 Temperature_Celsius     0x0002   142   142   000    Old_age   Always       -       42 (Min/Max 18/49)\n196 Reallocated_Event_Count 0x0032   100   100   000    Old_age   Always       -       0\n197 Current_Pending_Sector  0x0022   100   100   000    Old_age   Always       -       0\n198 Offline_Uncorrectable   0x0008   100   100   000    Old_age   Offline      -       0\n199 UDMA_CRC_Error_Count    0x000a   200   200   000    Old_age   Always       -       0\n\nSMART Error Log Version: 1\nNo Errors Logged\n\nSMART Self-test log structure revision number 1\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Extended offline    Completed without error       00%     17767         -\n# 2  Extended offline    Completed without error       00%     17354         -\n# 3  Extended offline    Completed without error       00%      8902         -\n# 4  Extended offline    Completed without error       00%      7409         -\n# 5  Extended offline    Interrupted (host reset)      20%      7397         -\n\nSMART Selective self-test log data structure revision number 1\n SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n    1        0        0  Not_testing\n    2        0        0  Not_testing\n    3        0        0  Not_testing\n    4        0        0  Not_testing\n    5        0        0  Not_testing\nSelective self-test flags (0x0):\n  After scanning selected spans, do NOT read-scan remainder of disk.\nIf Selective self-test is pending on power-up, resume after 0 minute delay.\n</code></pre> <pre><code># smartctl -a /dev/sdc\nsmartctl 7.2 2020-12-30 r5155 [x86_64-linux-5.15.0-48-lowlatency] (local build)\nCopyright (C) 2002-20, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nDevice Model:     WDC WD6003FZBX-00K5WB0\nSerial Number:    V8HD3R0R\nLU WWN Device Id: 5 000cca 098d399e2\nFirmware Version: 01.01A01\nUser Capacity:    6,001,175,126,016 bytes [6.00 TB]\nSector Sizes:     512 bytes logical, 4096 bytes physical\nRotation Rate:    7200 rpm\nForm Factor:      3.5 inches\nDevice is:        Not in smartctl database [for details use: -P showall]\nATA Version is:   ACS-2, ATA8-ACS T13/1699-D revision 4\nSATA Version is:  SATA 3.2, 6.0 Gb/s (current: 6.0 Gb/s)\nLocal Time is:    Sat Sep 24 09:58:12 2022 CEST\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nGeneral SMART Values:\nOffline data collection status:  (0x82) Offline data collection activity\n                    was completed without error.\n                    Auto Offline Data Collection: Enabled.\nSelf-test execution status:      (   0) The previous self-test routine completed\n                    without error or no self-test has ever \n                    been run.\nTotal time to complete Offline \ndata collection:        (   87) seconds.\nOffline data collection\ncapabilities:            (0x5b) SMART execute Offline immediate.\n                    Auto Offline data collection on/off support.\n                    Suspend Offline collection upon new\n                    command.\n                    Offline surface scan supported.\n                    Self-test supported.\n                    No Conveyance Self-test supported.\n                    Selective Self-test supported.\nSMART capabilities:            (0x0003) Saves SMART data before entering\n                    power-saving mode.\n                    Supports SMART auto save timer.\nError logging capability:        (0x01) Error logging supported.\n                    General Purpose Logging supported.\nShort self-test routine \nrecommended polling time:    (   2) minutes.\nExtended self-test routine\nrecommended polling time:    ( 645) minutes.\nSCT capabilities:          (0x0035) SCT Status supported.\n                    SCT Feature Control supported.\n                    SCT Data Table supported.\n\nSMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE\n  1 Raw_Read_Error_Rate     0x000b   100   100   016    Pre-fail  Always       -       0\n  2 Throughput_Performance  0x0004   132   132   054    Old_age   Offline      -       96\n  3 Spin_Up_Time            0x0007   165   165   024    Pre-fail  Always       -       401 (Average 330)\n  4 Start_Stop_Count        0x0012   100   100   000    Old_age   Always       -       670\n  5 Reallocated_Sector_Ct   0x0033   100   100   005    Pre-fail  Always       -       0\n  7 Seek_Error_Rate         0x000a   100   100   067    Old_age   Always       -       0\n  8 Seek_Time_Performance   0x0004   128   128   020    Old_age   Offline      -       18\n  9 Power_On_Hours          0x0012   099   099   000    Old_age   Always       -       9962\n 10 Spin_Retry_Count        0x0012   100   100   060    Old_age   Always       -       0\n 12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       664\n192 Power-Off_Retract_Count 0x0032   100   100   000    Old_age   Always       -       1030\n193 Load_Cycle_Count        0x0012   100   100   000    Old_age   Always       -       1030\n194 Temperature_Celsius     0x0002   144   144   000    Old_age   Always       -       38 (Min/Max 20/46)\n196 Reallocated_Event_Count 0x0032   100   100   000    Old_age   Always       -       0\n197 Current_Pending_Sector  0x0022   100   100   000    Old_age   Always       -       0\n198 Offline_Uncorrectable   0x0008   100   100   000    Old_age   Offline      -       0\n199 UDMA_CRC_Error_Count    0x000a   200   200   000    Old_age   Always       -       0\n\nSMART Error Log Version: 1\nNo Errors Logged\n\nSMART Self-test log structure revision number 1\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Extended offline    Completed without error       00%      9959         -\n# 2  Extended offline    Aborted by host               90%      9569         -\n# 3  Extended offline    Completed without error       00%      9569         -\n\nSMART Selective self-test log data structure revision number 1\n SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n    1        0        0  Not_testing\n    2        0        0  Not_testing\n    3        0        0  Not_testing\n    4        0        0  Not_testing\n    5        0        0  Not_testing\nSelective self-test flags (0x0):\n  After scanning selected spans, do NOT read-scan remainder of disk.\nIf Selective self-test is pending on power-up, resume after 0 minute delay.\n</code></pre>"},{"location":"blog/2022/09/27/undead-yes--unraid-no/#got-bad-blocks","title":"Got Bad Blocks?","text":"<p>Even if S.M.A.R.T. extended tests passed there may still be bad blocks that escaped detection, so the next step is to check for bad blocks on both disks. Need to use <code>-b 4096</code> to avoid 32-bit limitation:</p> <pre><code># time badblocks -b 4096 -o sdb_bb.txt /dev/sdb\nreal    573m30.844s\nuser    0m32.405s\nsys     11m49.075s\n\n# time badblocks -b 4096 -o sdb_bb.txt /dev/sdc\nreal    488m9.397s\nuser    0m29.813s\nsys     10m56.105s\n</code></pre> <p>Left both commands running in parallel, both running pretty fast, for the next ~10 hours:</p> <p></p> <p>The output is a list of bad blocks in each drive, which btrfs can\u2019t use. However, both lists came out empty, i.e. no bad blocks were detected.</p>"},{"location":"blog/2022/09/27/undead-yes--unraid-no/#back-to-normal","title":"Back To Normal","text":"<p>Both disks being now empty and seemingly undead, it\u2019s time to recreate the RAID1 array and copy data back into it:</p> <pre><code># mkfs.btrfs -d raid1 /dev/sdb /dev/sdc\nbtrfs-progs v5.16.2\nSee http://btrfs.wiki.kernel.org for more information.\n\nNOTE: several default settings have changed in version 5.15, please make sure\n      this does not affect your deployments:\n      - DUP for metadata (-m dup)\n      - enabled no-holes (-O no-holes)\n      - enabled free-space-tree (-R free-space-tree)\n\nLabel:              (null)\nUUID:               a4ee872d-b985-445f-94a2-15232e93dcd5\nNode size:          16384\nSector size:        4096\nFilesystem size:    10.92TiB\nBlock group profiles:\n  Data:             RAID1             1.00GiB\n  Metadata:         RAID1             1.00GiB\n  System:           RAID1             8.00MiB\nSSD detected:       no\nZoned device:       no\nIncompat features:  extref, skinny-metadata, no-holes\nRuntime features:   free-space-tree\nChecksum:           crc32c\nNumber of devices:  2\nDevices:\n   ID        SIZE  PATH\n    1     5.46TiB  /dev/sdb\n    2     5.46TiB  /dev/sdc\n\n# dmesg | tail\nBTRFS: device fsid a4ee872d-b985-445f-94a2-15232e93dcd5 devid 1 transid 6 /dev/sdb scanned by mkfs.btrfs (3135708)\nBTRFS: device fsid a4ee872d-b985-445f-94a2-15232e93dcd5 devid 2 transid 6 /dev/sdc scanned by mkfs.btrfs (3135708)\nBTRFS info (device sdb): flagging fs with big metadata feature\nBTRFS info (device sdb): using free space tree\nBTRFS info (device sdb): has skinny extents\nBTRFS info (device sdb): checking UUID tree\n\n# vi /etc/fstab\nUUID=a4ee872d-b985-445f-94a2-15232e93dcd5 /home/raid  btrfs defaults 0 0\n\n# btrfs filesystem label /home/raid HomeDepot6TB\n# btrfs filesystem show /home/raid\nLabel: 'HomeDepot6TB'  uuid: a4ee872d-b985-445f-94a2-15232e93dcd5\n        Total devices 2 FS bytes used 192.00KiB\n        devid    1 size 5.46TiB used 2.01GiB path /dev/sdb\n        devid    2 size 5.46TiB used 2.01GiB path /dev/sdc\n</code></pre> <p>From here, all that was left was <code>rsync</code>\u2018ing files across disks and computers.</p>"},{"location":"blog/2022/09/27/undead-yes--unraid-no/#what-next","title":"What Next?","text":"<p>In the last 10 years I\u2019ve had 3 or more spinning-plate hard drives fail, at least one of them abruptly enough to lose data. In the same 10 years, I\u2019ve worked with more SSD (and NVME) disks and only one failed, so at this point I\u2019m really eager to do get rid of all spinning-plate disks and move to 100% SSD.</p>"},{"location":"blog/2022/09/29/accented-characters-missing-on-all-browsers/","title":"Accented characters missing on all browsers","text":"<p>At some point all browsers in my PC started to refuse showing accented characters (e.g. \u00e8) when the correct keys are pressed (first ` then e). This started happening before reinstalling the OS (Ubuntu Studio 22.04). Reinstalling the system, which came about for other reasons, did not help.</p> <p>This problem was specific to web browsers only, even though it affected all of them equally. Typing accented characters continued to work fine everywhere else, e.g. in Konsole and Libreoffice. For a while the workaround was to type those characters in Konsole and copy them over.</p> <p>Soon enough, I had enough and started to search for a proper solution.</p> <p>Running each browser from Konsole, or watching <code>~/.xsession-errors</code>, Chrome and Firefox produced different warning messages each time I tried to type an accented character:</p> <pre><code>$ tail -f ~/.xsession-errors\n(google-chrome-stable:14941): Gdk-WARNING **: 15:19:37.785: gdk_window_set_user_time called on non-toplevel\n** (firefox:2091768): WARNING **: 15:19:57.550: Error converting text from IM to UTF-8: Invalid byte sequence in conversion input\n</code></pre> <p>Searching for the warning from Chrome message, there are very few matching posts. This one suggested to unsetting the <code>GTK_IM_MODULE</code> variable, which I had set to <code>xim</code>:</p> <pre><code>$ echo $GTK_IM_MODULE\nxim\n$ unset GTK_IM_MODULE\n</code></pre> <p>This didn\u2019t seem to help, at least not without restarting Chrome. A permanent solution could be to either fix <code>xim</code> or prevent the <code>GTK_IM_MODULE</code> variable from pointing to <code>xim</code> at all.</p> <p>The variable seems to be set in <code>/usr/share/im-config/data/80_kinput2.rc</code> which is part of the <code>im-config</code> package.</p> <p>To get a better idea of what <code>xim</code> was doing, I run <code>im-config -a -s</code> (without changing anything). This suggested I had manually selected <code>xim</code> via <code>~/.xinputrc</code> and indeed that as the case, although this looks like the config was generated rather than manually created by me:</p> <pre><code>$ cat .xinputrc \n# im-config(8) generated on Sun, 24 Apr 2016 07:53:00 +0200\nrun_im xim\n# im-config signature: 29c43083ff25c566c8c3ff6659294e6c  -\n\n$ ls -l .xinputrc\n-rw-rw-r-- 1 coder coder 130 Apr 24  2016 .xinputrc\n</code></pre> <p>Considering this config dates back to a long long time ago, I removed it and restarted the desktop to see if that would help... and it did!</p> <p>While search for related posts I also found a tangentially related hint to run <code>dpkg-reconfigure locales</code> and add <code>es_ES.UTF-8</code> (and/or others as desired) as a supported locale/s for the system (and regenerate them).</p> <p>And then I remembered the System Notification Helper popup message telling me that Language support is incomplete, additional packages are required. Since I was feeling generous and this only takes about 120 MB of disk space, I just installed the missing packages:</p> <pre><code># apt install $(check-language-support)\nThe following NEW packages will be installed:\n  hunspell-en-au hunspell-en-ca hunspell-en-gb hunspell-en-us\n  hunspell-en-za hyphen-en-ca hyphen-en-gb hyphen-en-us\n  libreoffice-help-common libreoffice-help-en-gb\n  libreoffice-help-en-us libreoffice-l10n-en-gb\n  libreoffice-l10n-en-za mythes-en-au mythes-en-us\n  thunderbird-locale-en-gb thunderbird-locale-en-us\n</code></pre>"},{"location":"blog/2022/10/12/crucial-mx500-ssd-found-problematic/","title":"Crucial MX500 SSD found problematic","text":"<p>Crucial MX500 SSD disks are problematic,  in strange ways they should not be.</p>"},{"location":"blog/2022/10/12/crucial-mx500-ssd-found-problematic/#prologue","title":"Prologue","text":"<p>Encouraged by a sudden price fall (by 15% down to $300), and spurred by the recent failure of 6TB HDD RAID, I added a Crucial MX500 4TB 3D NAND SATA SSD to serve as an backup to some of my precious files in that cursed RAID.</p> <p>This should have been all too easy and trouble-free, but alas these disks have something (in their firmware?) that makes these disks crash when writing to them too fast.</p> <p>Before diving into the details, it should be noted that this was an entirely new problem, never encountered with Intel or Samsung SATA SSDs, at least those used in this environment since 2010:</p> <ul> <li>Intel X25-V G2 40GB SATA-II (2010)</li> <li>Samsung SSD 840 Basic 120GB SATA (2013)</li> <li>Samsung 850 EVO Basic 250GB SATA (2016)</li> <li>Samsung 850 EVO Basic 1000GB SATA (2017)</li> <li>Samsung 860 EVO Basic 2000GB SATA (2019)</li> <li>Samsung SSD 860 EVO M.2 1TB SATA (2019)</li> <li>Samsung 870 EVO 4000GB SATA (2020)</li> <li>Samsung 860 EVO 500GB m.2 SATA (2021)</li> <li>Samsung 970 EVO Plus 1000 GB M.2 NVMe PCIe 3.0 (2021)</li> <li>3 Samsung 970 EVO Plus 2000 GB M.2 NVMe PCIe 3.0 (2022)</li> </ul> <p>Of all these, the only one ever to fail was the Samsung 860 EVO Basic 2000GB from 2019, failing in 2021. When it failed, it started with the <code>WRITE FPDMA QUEUED</code> errors that appeared this time around too. Other than that, Samsung SSD disks have proven reliable and trouble-free for the last 12 years which is more than can be said for nearly any other product.</p>"},{"location":"blog/2022/10/12/crucial-mx500-ssd-found-problematic/#what-happened","title":"What Happened","text":"<p>Since the previous files (my Audible library and select Podcasts) take up nearly 500 MB, the natural move would be to copy the files directly.</p> <p>First, formatted the (entire) drive as BTRFS and mounted it at <code>/home/ssd</code> (there is only room for one SSD).</p> <p>Then the problem: when trying to do this, possibly because the files were originally in the NVMe SSD that is much faster, the Crucial SSD failed after copying less than 8 GB:</p> <pre><code>[  614.810936] BTRFS info (device sda): flagging fs with big metadata feature\n[  614.810947] BTRFS info (device sda): using free space tree\n[  614.810951] BTRFS info (device sda): has skinny extents\n[  614.814588] BTRFS info (device sda): enabling ssd optimizations\n[  614.814892] BTRFS info (device sda): checking UUID tree\n[  750.641105] ata1.00: exception Emask 0x0 SAct 0x80e00003 SErr 0x0 action 0x6 frozen\n[  750.641124] ata1.00: failed command: WRITE FPDMA QUEUED\n[  750.641128] ata1.00: cmd 61/00:00:d0:3f:6c/0a:00:00:00:00/40 tag 0 ncq dma 1310720 ou\n                        res 40/00:00:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)\n[  750.641144] ata1.00: status: { DRDY }\n[  750.641148] ata1.00: failed command: WRITE FPDMA QUEUED\n[  750.641152] ata1.00: cmd 61/00:08:d0:49:6c/0a:00:00:00:00/40 tag 1 ncq dma 1310720 ou\n                        res 40/00:00:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)\n[  750.641164] ata1.00: status: { DRDY }\n[  750.641168] ata1.00: failed command: WRITE FPDMA QUEUED\n[  750.641171] ata1.00: cmd 61/70:a8:60:21:6c/00:00:00:00:00/40 tag 21 ncq dma 57344 out\n                        res 40/00:00:00:4f:c2/00:00:00:00:00/00 Emask 0x4 (timeout)\n[  750.641182] ata1.00: status: { DRDY }\n[  750.641186] ata1.00: failed command: WRITE FPDMA QUEUED\n[  750.641189] ata1.00: cmd 61/00:b0:d0:21:6c/0a:00:00:00:00/40 tag 22 ncq dma 1310720 ou\n                        res 40/00:ff:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)\n[  750.641202] ata1.00: status: { DRDY }\n[  750.641206] ata1.00: failed command: WRITE FPDMA QUEUED\n[  750.641209] ata1.00: cmd 61/00:b8:d0:2b:6c/0a:00:00:00:00/40 tag 23 ncq dma 1310720 ou\n                        res 40/00:01:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)\n[  750.641220] ata1.00: status: { DRDY }\n[  750.641224] ata1.00: failed command: WRITE FPDMA QUEUED\n[  750.641227] ata1.00: cmd 61/00:f8:d0:35:6c/0a:00:00:00:00/40 tag 31 ncq dma 1310720 ou\n                        res 40/00:00:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)\n[  750.641238] ata1.00: status: { DRDY }\n[  750.641244] ata1: hard resetting link\n[  756.009469] ata1: link is slow to respond, please be patient (ready=0)\n[  760.677688] ata1: COMRESET failed (errno=-16)\n[  760.677734] ata1: hard resetting link\n[  766.017924] ata1: link is slow to respond, please be patient (ready=0)\n[  770.690140] ata1: COMRESET failed (errno=-16)\n[  770.690185] ata1: hard resetting link\n[  776.018382] ata1: link is slow to respond, please be patient (ready=0)\n[  805.739547] ata1: COMRESET failed (errno=-16)\n[  805.739590] ata1: limiting SATA link speed to 3.0 Gbps\n[  805.739594] ata1: hard resetting link\n[  810.743856] ata1: COMRESET failed (errno=-16)\n[  810.743904] ata1: reset failed, giving up\n[  810.743910] ata1.00: disabled\n[  810.745057] ata1: EH complete\n[  810.745101] sd 0:0:0:0: [sda] tag#2 FAILED Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK cmd_age=91s\n[  810.745109] sd 0:0:0:0: [sda] tag#2 CDB: Write(16) 8a 00 00 00 00 00 00 6c 35 d0 00 00 0a 00 00 00\n[  810.745111] blk_update_request: I/O error, dev sda, sector 7091664 op 0x1:(WRITE) flags 0x104000 phys_seg 20 prio class 0\n[  810.745135] sd 0:0:0:0: [sda] tag#3 FAILED Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK cmd_age=91s\n[  810.745138] sd 0:0:0:0: [sda] tag#3 CDB: Write(16) 8a 00 00 00 00 00 00 6c 2b d0 00 00 0a 00 00 00\n[  810.745140] blk_update_request: I/O error, dev sda, sector 7089104 op 0x1:(WRITE) flags 0x104000 phys_seg 20 prio class 0\n[  810.745151] sd 0:0:0:0: [sda] tag#4 FAILED Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK cmd_age=91s\n[  810.745154] sd 0:0:0:0: [sda] tag#4 CDB: Write(16) 8a 00 00 00 00 00 00 6c 21 d0 00 00 0a 00 00 00\n[  810.745156] blk_update_request: I/O error, dev sda, sector 7086544 op 0x1:(WRITE) flags 0x104000 phys_seg 20 prio class 0\n[  810.745165] sd 0:0:0:0: [sda] tag#5 FAILED Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK cmd_age=91s\n[  810.745168] sd 0:0:0:0: [sda] tag#5 CDB: Write(16) 8a 00 00 00 00 00 00 6c 21 60 00 00 00 70 00 00\n[  810.745170] blk_update_request: I/O error, dev sda, sector 7086432 op 0x1:(WRITE) flags 0x100000 phys_seg 1 prio class 0\n[  810.745180] BTRFS error (device sda): bdev /dev/sda errs: wr 1, rd 0, flush 0, corrupt 0, gen 0\n[  810.747865] sd 0:0:0:0: [sda] tag#6 FAILED Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK cmd_age=91s\n[  810.747872] sd 0:0:0:0: [sda] tag#6 CDB: Write(16) 8a 00 00 00 00 00 00 6c 49 d0 00 00 0a 00 00 00\n[  810.747875] blk_update_request: I/O error, dev sda, sector 7096784 op 0x1:(WRITE) flags 0x104000 phys_seg 20 prio class 0\n[  810.747894] sd 0:0:0:0: [sda] tag#7 FAILED Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK cmd_age=91s\n[  810.747899] sd 0:0:0:0: [sda] tag#7 CDB: Write(16) 8a 00 00 00 00 00 00 6c 3f d0 00 00 0a 00 00 00\n[  810.747902] blk_update_request: I/O error, dev sda, sector 7094224 op 0x1:(WRITE) flags 0x104000 phys_seg 20 prio class 0\n[  810.748109] sd 0:0:0:0: [sda] tag#2 FAILED Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK cmd_age=0s\n[  810.748120] sd 0:0:0:0: [sda] tag#2 CDB: Write(16) 8a 00 00 00 00 00 00 6c 53 d0 00 00 0a 00 00 00\n[  810.748125] blk_update_request: I/O error, dev sda, sector 7099344 op 0x1:(WRITE) flags 0x104000 phys_seg 20 prio class 0\n[  810.748156] sd 0:0:0:0: [sda] tag#3 FAILED Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK cmd_age=0s\n[  810.748161] sd 0:0:0:0: [sda] tag#3 CDB: Write(16) 8a 00 00 00 00 00 00 6c 5d d0 00 00 0a 00 00 00\n[  810.748164] blk_update_request: I/O error, dev sda, sector 7101904 op 0x1:(WRITE) flags 0x104000 phys_seg 20 prio class 0\n[  810.748181] sd 0:0:0:0: [sda] tag#4 FAILED Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK cmd_age=0s\n[  810.748186] sd 0:0:0:0: [sda] tag#4 CDB: Write(16) 8a 00 00 00 00 00 00 6c 67 d0 00 00 0a 00 00 00\n[  810.748189] blk_update_request: I/O error, dev sda, sector 7104464 op 0x1:(WRITE) flags 0x104000 phys_seg 20 prio class 0\n[  810.748205] sd 0:0:0:0: [sda] tag#5 FAILED Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK cmd_age=0s\n[  810.748210] sd 0:0:0:0: [sda] tag#5 CDB: Write(16) 8a 00 00 00 00 00 00 6c 71 d0 00 00 0a 00 00 00\n[  810.748213] blk_update_request: I/O error, dev sda, sector 7107024 op 0x1:(WRITE) flags 0x104000 phys_seg 20 prio class 0\n[  810.748327] BTRFS error (device sda): bdev /dev/sda errs: wr 2, rd 0, flush 0, corrupt 0, gen 0\n[  810.748350] BTRFS error (device sda): bdev /dev/sda errs: wr 3, rd 0, flush 0, corrupt 0, gen 0\n[  810.750196] BTRFS error (device sda): bdev /dev/sda errs: wr 4, rd 0, flush 0, corrupt 0, gen 0\n[  810.750211] BTRFS error (device sda): bdev /dev/sda errs: wr 5, rd 0, flush 0, corrupt 0, gen 0\n[  810.750218] BTRFS error (device sda): bdev /dev/sda errs: wr 6, rd 0, flush 0, corrupt 0, gen 0\n[  810.750224] BTRFS error (device sda): bdev /dev/sda errs: wr 7, rd 0, flush 0, corrupt 0, gen 0\n[  810.750231] BTRFS error (device sda): bdev /dev/sda errs: wr 8, rd 0, flush 0, corrupt 0, gen 0\n[  810.750237] BTRFS error (device sda): bdev /dev/sda errs: wr 9, rd 0, flush 0, corrupt 0, gen 0\n[  810.750244] BTRFS error (device sda): bdev /dev/sda errs: wr 10, rd 0, flush 0, corrupt 0, gen 0\n[  810.752702] BTRFS: error (device sda) in btrfs_commit_transaction:2438: errno=-5 IO failure (Error while writing out transaction)\n[  810.752722] BTRFS info (device sda): forced readonly\n[  810.752728] BTRFS warning (device sda): Skipping commit of aborted transaction.\n[  810.752732] BTRFS: error (device sda) in cleanup_transaction:2011: errno=-5 IO failure\n[ 1229.016176] scsi_io_completion_action: 5045 callbacks suppressed\n[ 1229.016188] sd 0:0:0:0: [sda] tag#23 FAILED Result: hostbyte=DID_BAD_TARGET driverbyte=DRIVER_OK cmd_age=0s\n[ 1229.016197] sd 0:0:0:0: [sda] tag#23 CDB: ATA command pass through(16) 85 06 20 00 00 00 00 00 00 00 00 00 00 00 e5 00\n</code></pre> <p>The disk did not actually write any data; although <code>df</code> reported 1.4G used at this point, it reported only a few MB after reboot:</p> <pre><code># /home/depot # df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda        3.7T  1.4G  3.7T   1% /home/ssd\n</code></pre> <p>After this failure, the disk was not just read only, it was entirely disabled and unavailable:</p> <pre><code># du -sh /home/ssd/*\ndu: cannot access '/home/ssd/*': Input/output error\n</code></pre> <p>After reboot:</p> <pre><code># /home/depot # df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda        3.7T  3.6M  3.7T   1% /home/ssd\n</code></pre>"},{"location":"blog/2022/10/12/crucial-mx500-ssd-found-problematic/#get-smart","title":"Get S.M.A.R.T.","text":"<p>Based on the experience with the recently failed Samsung 860 EVO Basic 2000GB, this looked like it could be a defective disk that would need an RMA. In search of more definitive signs of hardware failure, I run S.M.A.R.T. long test: <code>smartctl -t long /dev/sda</code></p> <p>After some time the results were as follows:</p> <pre><code># smartctl -a /dev/sda\nsmartctl 7.2 2020-12-30 r5155 [x86_64-linux-5.15.0-48-generic] (local build)\nCopyright (C) 2002-20, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nDevice Model:     CT4000MX500SSD1\nSerial Number:    2221E632FD50\nLU WWN Device Id: 5 00a075 1e632fd50\nFirmware Version: M3CR044\nUser Capacity:    4,000,787,030,016 bytes [4.00 TB]\nSector Sizes:     512 bytes logical, 4096 bytes physical\nRotation Rate:    Solid State Device\nForm Factor:      2.5 inches\nTRIM Command:     Available\nDevice is:        Not in smartctl database [for details use: -P showall]\nATA Version is:   ACS-3 T13/2161-D revision 5\nSATA Version is:  SATA 3.3, 6.0 Gb/s (current: 6.0 Gb/s)\nLocal Time is:    Wed Oct 12 21:31:31 2022 CEST\nSMART support is: Available - device has SMART capability.\nSMART support is: Enabled\n\n=== START OF READ SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nGeneral SMART Values:\nOffline data collection status:  (0x03) Offline data collection activity\n                                        is in progress.\n                                        Auto Offline Data Collection: Disabled.\nSelf-test execution status:      (   0) The previous self-test routine completed\n                                        without error or no self-test has ever \n                                        been run.\nTotal time to complete Offline \ndata collection:                ( 1402) seconds.\nOffline data collection\ncapabilities:                    (0x7b) SMART execute Offline immediate.\n                                        Auto Offline data collection on/off support.\n                                        Suspend Offline collection upon new\n                                        command.\n                                        Offline surface scan supported.\n                                        Self-test supported.\n                                        Conveyance Self-test supported.\n                                        Selective Self-test supported.\nSMART capabilities:            (0x0003) Saves SMART data before entering\n                                        power-saving mode.\n                                        Supports SMART auto save timer.\nError logging capability:        (0x01) Error logging supported.\n                                        General Purpose Logging supported.\nShort self-test routine \nrecommended polling time:        (   2) minutes.\nExtended self-test routine\nrecommended polling time:        (  30) minutes.\nConveyance self-test routine\nrecommended polling time:        (   2) minutes.\nSCT capabilities:              (0x0031) SCT Status supported.\n                                        SCT Feature Control supported.\n                                        SCT Data Table supported.\n\nSMART Attributes Data Structure revision number: 16\nVendor Specific SMART Attributes with Thresholds:\nID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE\n  1 Raw_Read_Error_Rate     0x002f   100   100   000    Pre-fail  Always       -       0\n  5 Reallocated_Sector_Ct   0x0032   100   100   010    Old_age   Always       -       0\n  9 Power_On_Hours          0x0032   100   100   000    Old_age   Always       -       0\n 12 Power_Cycle_Count       0x0032   100   100   000    Old_age   Always       -       6\n171 Unknown_Attribute       0x0032   100   100   000    Old_age   Always       -       0\n172 Unknown_Attribute       0x0032   100   100   000    Old_age   Always       -       0\n173 Unknown_Attribute       0x0032   100   100   000    Old_age   Always       -       0\n174 Unknown_Attribute       0x0032   100   100   000    Old_age   Always       -       1\n180 Unused_Rsvd_Blk_Cnt_Tot 0x0033   000   000   000    Pre-fail  Always       -       217\n183 Runtime_Bad_Block       0x0032   100   100   000    Old_age   Always       -       0\n184 End-to-End_Error        0x0032   100   100   000    Old_age   Always       -       0\n187 Reported_Uncorrect      0x0032   100   100   000    Old_age   Always       -       0\n194 Temperature_Celsius     0x0022   070   068   000    Old_age   Always       -       30 (Min/Max 25/32)\n196 Reallocated_Event_Count 0x0032   100   100   000    Old_age   Always       -       0\n197 Current_Pending_Sector  0x0032   100   100   000    Old_age   Always       -       0\n198 Offline_Uncorrectable   0x0030   100   100   000    Old_age   Offline      -       0\n199 UDMA_CRC_Error_Count    0x0032   100   100   000    Old_age   Always       -       0\n202 Unknown_SSD_Attribute   0x0030   100   100   001    Old_age   Offline      -       0\n206 Unknown_SSD_Attribute   0x000e   100   100   000    Old_age   Always       -       0\n210 Unknown_Attribute       0x0032   100   100   000    Old_age   Always       -       0\n246 Unknown_Attribute       0x0032   100   100   000    Old_age   Always       -       2843016\n247 Unknown_Attribute       0x0032   100   100   000    Old_age   Always       -       22592\n248 Unknown_Attribute       0x0032   100   100   000    Old_age   Always       -       11555\n\nSMART Error Log Version: 1\nWarning: ATA error count 0 inconsistent with error log pointer 1\n\nATA Error Count: 0\n        CR = Command Register [HEX]\n        FR = Features Register [HEX]\n        SC = Sector Count Register [HEX]\n        SN = Sector Number Register [HEX]\n        CL = Cylinder Low Register [HEX]\n        CH = Cylinder High Register [HEX]\n        DH = Device/Head Register [HEX]\n        DC = Device Command Register [HEX]\n        ER = Error register [HEX]\n        ST = Status register [HEX]\nPowered_Up_Time is measured from power on, and printed as\nDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\nSS=sec, and sss=millisec. It \"wraps\" after 49.710 days.\n\nError 0 occurred at disk power-on lifetime: 0 hours (0 days + 0 hours)\n  When the command that caused the error occurred, the device was in an unknown state.\n\n  After command completion occurred, registers were:\n  ER ST SC SN CL CH DH\n  -- -- -- -- -- -- --\n  00 ec 00 00 00 00 00\n\n  Commands leading to the command that caused the error were:\n  CR FR SC SN CL CH DH DC   Powered_Up_Time  Command/Feature_Name\n  -- -- -- -- -- -- -- --  ----------------  --------------------\n  ec 00 00 00 00 00 00 00      00:00:00.000  IDENTIFY DEVICE\n  ec 00 00 00 00 00 00 00      00:00:00.000  IDENTIFY DEVICE\n  ec 00 00 00 00 00 00 00      00:00:00.000  IDENTIFY DEVICE\n  ec 00 00 00 00 00 00 00      00:00:00.000  IDENTIFY DEVICE\n  c8 00 00 00 00 00 00 00      00:00:00.000  READ DMA\n\nSMART Self-test log structure revision number 1\nNum  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error\n# 1  Extended offline    Completed without error       00%         0         -\n\nSMART Selective self-test log data structure revision number 1\n SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS\n    1        0        0  Not_testing\n    2        0        0  Not_testing\n    3        0        0  Not_testing\n    4        0        0  Not_testing\n    5        0        0  Not_testing\nSelective self-test flags (0x10):\n  After scanning selected spans, do NOT read-scan remainder of disk.\nIf Selective self-test is pending on power-up, resume after 0 minute delay.\n</code></pre> <p>The long test completed without error, but there are other signs that this disk may be problematic.</p>"},{"location":"blog/2022/10/12/crucial-mx500-ssd-found-problematic/#whats-wrong-1","title":"What's Wrong #1","text":"<p>First, the disk seems to think it can go as fast as 6.0 TB/s which is definitely not true; that would be 12 times fater than the 560/530 (read/write) MB/s it is rated as, and confirmed by <code>hdparm</code>:</p> <pre><code># hdparm -t -T /dev/sda\n\n/dev/sda:\n Timing cached reads:   34646 MB in  2.00 seconds = 17350.03 MB/sec\n Timing buffered disk reads: 1596 MB in  3.00 seconds = 531.82 MB/sec\n</code></pre> <p>Note this line from <code>smartctl</code>, in the <code>INFORMATION SECTION</code> at the top:</p> <pre><code>SATA Version is:  SATA 3.3, 6.0 Gb/s (current: 6.0 Gb/s)\n</code></pre> <p>It seems the kernel is expecting up to 6.0 Gbps</p> <pre><code>[    1.625648] ahci 0000:00:17.0: AHCI 0001.0301 32 slots 2 ports 6 Gbps 0x3 impl SATA mode\n[    1.646123] ata1: SATA max UDMA/133 abar m2048@0x6a602000 port 0x6a602100 irq 131\n[    1.646129] ata2: SATA max UDMA/133 abar m2048@0x6a602000 port 0x6a602180 irq 131\n[    1.963187] ata2: SATA link down (SStatus 4 SControl 300)\n[    1.963249] ata1: SATA link up 6.0 Gbps (SStatus 133 SControl 300)\n</code></pre> <p>A transfer rate of 6.0 Gbps would be 750 MB/s, which is about 50% more than this disk is rated for, but that shouldn't be a problem. It certainly is not a problem with the Samsung 870 EVO 4000GB SATA disk, which does show the same up to 6.0 Gbps line in <code>dmesg</code>:</p> <pre><code>[    1.178462] ata6: SATA max UDMA/133 abar m2048@0xfc600000 port 0xfc600180 irq 66\n[    1.651823] ata6: SATA link up 6.0 Gbps (SStatus 133 SControl 300)\n[    1.654633] ata6.00: supports DRM functions and may not be fully accessible\n[    1.655166] ata6.00: ATA-11: Samsung SSD 870 EVO 4TB, SVT01B6Q, max UDMA/133\n</code></pre> <p>Somehow, the Crucial SSD is unable to self-regulate when given data to write at any rate higher than what it's rated for, while (all) Samsung disks have no such problem.</p>"},{"location":"blog/2022/10/12/crucial-mx500-ssd-found-problematic/#whats-wrong-2","title":"What's Wrong #2","text":"<p>Second, back to <code>smartctl</code> output, this entire block is new and unique to the Crucial SSD:</p> <pre><code>Warning: ATA error count 0 inconsistent with error log pointer 1\n\nATA Error Count: 0\n        CR = Command Register [HEX]\n        FR = Features Register [HEX]\n        SC = Sector Count Register [HEX]\n        SN = Sector Number Register [HEX]\n        CL = Cylinder Low Register [HEX]\n        CH = Cylinder High Register [HEX]\n        DH = Device/Head Register [HEX]\n        DC = Device Command Register [HEX]\n        ER = Error register [HEX]\n        ST = Status register [HEX]\nPowered_Up_Time is measured from power on, and printed as\nDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\nSS=sec, and sss=millisec. It \"wraps\" after 49.710 days.\n\nError 0 occurred at disk power-on lifetime: 0 hours (0 days + 0 hours)\n  When the command that caused the error occurred, the device was in an unknown state.\n\n  After command completion occurred, registers were:\n  ER ST SC SN CL CH DH\n  -- -- -- -- -- -- --\n  00 ec 00 00 00 00 00\n\n  Commands leading to the command that caused the error were:\n  CR FR SC SN CL CH DH DC   Powered_Up_Time  Command/Feature_Name\n  -- -- -- -- -- -- -- --  ----------------  --------------------\n  ec 00 00 00 00 00 00 00      00:00:00.000  IDENTIFY DEVICE\n  ec 00 00 00 00 00 00 00      00:00:00.000  IDENTIFY DEVICE\n  ec 00 00 00 00 00 00 00      00:00:00.000  IDENTIFY DEVICE\n  ec 00 00 00 00 00 00 00      00:00:00.000  IDENTIFY DEVICE\n  c8 00 00 00 00 00 00 00      00:00:00.000  READ DMA\n</code></pre> <p>This a long way of saying, the READ DMA command lead to an error state, reflected as value <code>0xEC</code> in the <code>Status register</code>.</p>"},{"location":"blog/2022/10/12/crucial-mx500-ssd-found-problematic/#workaround","title":"Workaround","text":"<p>Manually limiting the transfer rate when writing files seems to be the only way to get large write operations to succeed. Luckly, this is rather easy when using <code>rsync</code>:</p> <pre><code>$ rsync -turva \\\n  --bwlimit=500000 \\\n  /home/depot/audio/Audiobooks \\\n  /home/ssd/audio/\n</code></pre> <p>Attempting the same with a higher <code>--bwlimit</code> value (700 MB/s) reproduces the error almost immediately:</p> <pre><code>[10745.786389] BTRFS error (device sda): bdev /dev/sda errs: wr 4, rd 0, flush 0, corrupt 0, gen 0\n[10745.786405] BTRFS error (device sda): bdev /dev/sda errs: wr 5, rd 0, flush 0, corrupt 0, gen 0\n[10745.786418] BTRFS error (device sda): bdev /dev/sda errs: wr 6, rd 0, flush 0, corrupt 0, gen 0\n[10745.786429] BTRFS error (device sda): bdev /dev/sda errs: wr 7, rd 0, flush 0, corrupt 0, gen 0\n[10745.786532] BTRFS error (device sda): bdev /dev/sda errs: wr 8, rd 0, flush 0, corrupt 0, gen 0\n[10745.788424] BTRFS error (device sda): bdev /dev/sda errs: wr 9, rd 0, flush 0, corrupt 0, gen 0\n[10745.788687] BTRFS error (device sda): bdev /dev/sda errs: wr 10, rd 0, flush 0, corrupt 0, gen 0\n[10745.811622] BTRFS: error (device sda) in btrfs_commit_transaction:2438: errno=-5 IO failure (Error while writing out transaction)\n[10745.811633] BTRFS info (device sda): forced readonly\n[10745.811635] BTRFS warning (device sda): Skipping commit of aborted transaction.\n[10745.811637] BTRFS: error (device sda) in cleanup_transaction:2011: errno=-5 IO failure\n</code></pre>"},{"location":"blog/2022/10/12/crucial-mx500-ssd-found-problematic/#root-cause-confirmed","title":"Root Cause Confirmed","text":"<p>Shortly after dealing with this problem in Lexicon, the very same problem reproduced with another Crucial MX500 SSD on a different system (Rapture) with a ASUS PRIME X370-PRO motherboard, which is the one where the Samsung 870 EVO 4000GB SATA disk exhibits no such problem.</p> <p>Crucial MX500 SSD are no longer welcome or recommended, cheap as they may be.</p>"},{"location":"blog/2022/11/05/the-weirdest-corrupted-video-on-an-nvidia-card/","title":"The weirdest corrupted video on an NVidia card","text":"<p>This is the kind of thing that makes you think, this really only happens to me.</p> <p>Back in June, when the availability and price of graphics card finally approached relatively normal values, I got myself an new ASUS GeForce TUF Gaming RTX 3070 Ti OC Edition (to replace the old ASUS GeForce GTX 1070 STRIX from 2017). It still was still nearly $800 but it was clearly never going to come down to $570 the old one costed back in August 2017.</p> <p>Then, in September, the new card died. Somewhat surreptitiously...</p>"},{"location":"blog/2022/11/05/the-weirdest-corrupted-video-on-an-nvidia-card/#what-happened","title":"What Happened","text":"<p>It started small, like oak trees.</p> <p>At first there were just faint glitchy thin lines blinking across the screen. Searching for posts discussing this kind of video artifacts, came up empty.</p> <p>When the problem started I was running Ubuntu Studio 20.04 with KDE Plasma. After a couple of weeks of trying a few tweaks (e.g. disable compositor) and seeing how nothing really helped, I decided to install Ubuntu Studio 22.04 and at first it looked perfect, but then the problem manifested again as soon as the first reboot with the NVidia driver.</p> <p>The artifact would manifest mostly when the screen was locked, sometimes when it wasn\u2019t, and affect only the top 10-15% of the screen, plus a fixed-size square area down-and-right of the mouse cursor. It got worse day after day, and soon the entire screen was affected, the corruption looked like large areas of the image would turn one color (blue, magenta, green, etc.) and the artifacts followed the content of the screen, so that the corrupted areas would align with the clouds, sky and water in landscape photos.</p> <p>From there, the problem quickly evolved to the point where graphics are corrupted as soon as the login manager started up, upon login the very simple splash screen was corrupted, and then the whole desktop environment was so badly corrupted it was barely possible to even see where the mouse cursor was and the text in Konsole was unreadable. At that point, going back the text-only terminal with <code>Ctrl+Alt+F2</code> presented such badly corrupted output it was unreadable.</p>"},{"location":"blog/2022/11/05/the-weirdest-corrupted-video-on-an-nvidia-card/#regaining-control","title":"Regaining Control","text":"<p>The first suspect is usually the proprietary NVidia drivers, so the first workaround, to be able to use the PC, was to switch to the <code>nouveau</code> driver:</p> <pre><code># dpkg -P nvidia-driver-515\n# apt autoremove\n# apt update\n# apt install xserver-xorg-video-nouveau\n</code></pre> <p>The last 2 commands didn\u2019t actually do anything. After this, rebooting led to a perfectly usable system without any graphical glitches.</p> <p>Ubuntu 22.04 had version 470 available for easy installation, and this version  supports the RTX 3070 cards, so that was the next workaround. The trick to switch back to an older version is to hold / freeze them to that version:</p> <pre><code># apt install nvidia-driver-470\n# apt-mark hold nvidia-driver-470 libnvidia-cfg1-470 libnvidia-common-470 \\\n  libnvidia-compute-470 libnvidia-compute-470:i386 libnvidia-decode-470 \\\n  libnvidia-decode-470:i386 libnvidia-egl-wayland1 libnvidia-encode-470 \\\n  libnvidia-encode-470:i386 libnvidia-extra-470 libnvidia-fbc1-470 \\\n  libnvidia-fbc1-470:i386 libnvidia-gl-470 libnvidia-gl-470:i386 libnvidia-ifr1-470 \\\n  libnvidia-ifr1-470:i386 libxnvctrl0 nvidia-compute-utils-470 nvidia-dkms-470 \\\n  nvidia-kernel-common-470 nvidia-kernel-source-470 nvidia-prime nvidia-settings \\\n  nvidia-utils-470 screen-resolution-extra xserver-xorg-video-nvidia-470\n</code></pre> <p>After this, rebooting led to straight back into the problem; corrupted video as soon as SDDM came up, and still corrupted after going to TTY.</p> <p>Uninstall the NVidia drivers and rebooting solved the problem again. This time needed to uninstall all packages explicitly:</p> <pre><code># apt remove $(dpkg -l |grep 'nvidia.*470' | awk '{print $2}')\n</code></pre> <p>That was enough to go back to the nouveau driver. A few more packages had to be removed later:</p> <pre><code># apt remove screen-resolution-extra nvidia-settings nvidia-prime \\\n  libnvidia-egl-wayland1:amd64 libxnvctrl0:amd64\n</code></pre> <p>It became clear that switching back and forth between the Nvidia and nouveau drivers was going to be necessary more than a few times, so I create a couple of helper scripts I could run despite not being able to read the screen:</p> <pre><code>#!/bin/sh\necho \"blacklist nouveau\" \\\n  &gt;  /etc/modprobe.d/blocklist-nvidia-nouveau.conf\necho \"options nouveau modeset=0\" \\\n  &gt;&gt; /etc/modprobe.d/blocklist-nvidia-nouveau.conf\nupdate-initramfs -u\napt install nvidia-driver-515\n</code></pre> <pre><code>#!/bin/sh\ndpkg -P nvidia-driver-515\napt remove -y $(dpkg -l | grep 'nvidia.*515' | awk '{print $2}')\napt autoremove -y\nrm -f /etc/modprobe.d/blocklist-nvidia-nouveau.conf\n</code></pre> <p>The lines to disable the <code>nouveau</code> driver are a precaution to keep the output from <code>nvidia-bug-report.sh</code> (part of <code>nvidia-utils-515</code>) from mentions the <code>nouveau</code> driver when it is not in use.</p>"},{"location":"blog/2022/11/05/the-weirdest-corrupted-video-on-an-nvidia-card/#root-cause-analysis","title":"Root Cause Analysis","text":"<p>With this easy-switch toolkit in hand, it was time to go on a Wild Hunt for the root cause of the problem by changing one thing at a time...</p> <ul> <li>Going back to Ubuntu 20.04 (still available in its old partition)    and try with NVidia drivers 470, 510 and 515.</li> <li>Use the same screen on a laptop.</li> <li>Connect the screen directly to the PC.</li> <li>Try every DisplayPort cables at hand.</li> <li>Confirm all DisplayPort cables are rated DP 1.2 (rated 4k@60).</li> <li>Replaced DisplayPort 1.2 with HDMI 2.0 cable (rated 4k@60).</li> <li>Tried Pop!_OS live USB because it    ships with the latest NVidia driver.</li> </ul> <p>Normally the PC and laptop share the screen and keyboard via StarTech SV231DPDDUA2 DisplayPort KVM Switch rated 4K 60Hz. I took this KVM switch out of the equation early on, so most of the tests were on a direct PC-to-screen DP 1.2 connection.</p> <p>Being quite confident that the problem was in the graphics card itself, but no idea how to go from here, I posted the problem in the NVidia forum: Extremely corrupted graphics only with NVidia driver 515 on RTX 3070 Ti.</p> <p>Shortly after that, a friend suggested trying with Pop!_OS live USB because it ships with the latest NVidia driver. Having already removed everything else out of the way, including the OS, this last test also reproduced the problem:</p> <p></p> <p>Notice the glitch lines around the top-right corner and through the Select button, plus the fixed-size square area down-and-right from the mouse cursor. This area followed the cursor, as you can see in this video.</p> <p> </p> <p>In the NVidia forum, a top contributor suggested to check for a general hardware fault using gpu-burn or cuda-gpumemtest.</p> <p>None of these tools are directly available in Ubuntu 22.04 so first I had to install them.</p>"},{"location":"blog/2022/11/05/the-weirdest-corrupted-video-on-an-nvidia-card/#gpu-burn","title":"<code>gpu-burn</code>","text":"<p>First I needed to reinstall the NVidia driver and CUDA libraries. To avoid corrupting the graphics or interfering with the test, I chose to disable SDDM before rebooting:</p> <pre><code># systemctl disable sddm\n# /root/nvidia-on.sh\n# reboot\n</code></pre> <p>Installed gpu-burn from wilicc/gpu-burn and followed the instructions from wili.cc/blog/gpu-burn.html; using <code>COMPUTE=8.6</code> for RTX 3070 Ti (source):</p> <p>Apparently, one does not simply <code>make</code> this. Many headers are missing or not found:</p> <pre><code>g++ -O3 -Wno-unused-result -I/usr/local/cuda/include -c gpu_burn-drv.cpp\ngpu_burn-drv.cpp:51:10: fatal error: cuda.h: No such file or directory\n   51 | #include &lt;cuda.h&gt;\n      |          ^~~~~~~~\ncompilation terminated.\nmake: *** [Makefile:32: gpu_burn-drv.o] Error 1\n</code></pre> <p>Installing the <code>nvidia-cuda-dev</code> or <code>nvidia-cuda-toolkit</code> packages was not an option because both wanted to remove the latest NVidia driver (<code>nvidia-driver-515</code>) and replace it with older drivers. Instead, found the missing headers and added <code>-I</code> flags to <code>gcc</code>:</p> <pre><code># g++ -O3 -Wno-unused-result \\\n  -I/usr/local/cuda/include \\\n  -I/usr/src/linux-headers-5.15.0-47-lowlatency/include/linux \\\n  -I/usr/src/linux-headers-5.15.0-47-lowlatency/include \\\n  -I/usr/src/linux-headers-5.15.0-47-lowlatency/arch/x86/include/generated \\\n  -I/usr/src/linux-lowlatency-headers-5.15.0-47/arch/x86/include \\\n  -c gpu_burn-drv.cpp\n</code></pre> <p>Still not being enough, the build failed with yet another error:</p> <pre><code>gpu_burn-drv.cpp:52:10: fatal error: cublas_v2.h: No such file or directory\n   52 | #include \"cublas_v2.h\"\n      |          ^~~~~~~~~~~~~\ncompilation terminated.\n</code></pre> <p>This one was trickier. There is was trace of <code>cublas_v2.h</code> anywhere, installing the <code>libcublas11</code> and <code>libcublaslt11</code> packages did not help.</p> <p>The only clue I found was in github.com/NVIDIA/apex/issues/957 to install a package directly, but it is not installable in Ubuntu 22.04 so I had to follow the instructions for Ubuntu 22.04 at developer.nvidia.com/cuda-downloads:</p> <pre><code># wget -O /etc/apt/preferences.d/cuda-repository-pin-600 \\\n  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n# wget https://developer.download.nvidia.com/compute/cuda/11.7.1/local_installers/cuda-repo-ubuntu2204-11-7-local_11.7.1-515.65.01-1_amd64.deb\n# dpkg -i cuda-repo-ubuntu2204-11-7-local_11.7.1-515.65.01-1_amd64.deb\n# cp /var/cuda-repo-ubuntu2204-11-7-local/cuda-*-keyring.gpg /usr/share/keyrings/\n# apt-get update\n# apt-get -y install cuda\n</code></pre> <p>Note</p> <p>To undo this change later, remove <code>/etc/apt/preferences.d/cuda-repository-pin-600</code> and run <code>apt update</code>.</p> <p>Finally, one can simply <code>make</code> and run <code>gpu_burn</code>:</p> <pre><code># make COMPUTE=8.6\n# ./gpu_burn 120\nBurning for 120 seconds.\nGPU 0: NVIDIA GeForce RTX 3070 Ti (UUID: GPU-0c349bdd-3426-f2d3-4b26-c0b4bf0aad2e)\nInitialized device 0 with 7974 MB of memory (7451 MB available, using 6706 MB of it), using FLOATS\nResults are 16777216 bytes each, thus performing 417 iterations\n10.8%  proc'd: 8757 (13280 Gflop/s)   errors: 0   temps: 54 C \n    Summary at:   Mon Sep 19 05:38:21 PM CEST 2022\n\n21.7%  proc'd: 18765 (13235 Gflop/s)   errors: 0   temps: 59 C \n    Summary at:   Mon Sep 19 05:38:34 PM CEST 2022\n\n32.5%  proc'd: 28773 (13186 Gflop/s)   errors: 0   temps: 61 C \n    Summary at:   Mon Sep 19 05:38:47 PM CEST 2022\n\n43.3%  proc'd: 38781 (13178 Gflop/s)   errors: 0   temps: 64 C \n    Summary at:   Mon Sep 19 05:39:00 PM CEST 2022\n\n53.3%  proc'd: 47955 (13135 Gflop/s)   errors: 0   temps: 64 C \n    Summary at:   Mon Sep 19 05:39:12 PM CEST 2022\n\n64.2%  proc'd: 57963 (13125 Gflop/s)   errors: 0   temps: 66 C \n    Summary at:   Mon Sep 19 05:39:25 PM CEST 2022\n\n75.0%  proc'd: 67971 (13144 Gflop/s)   errors: 0   temps: 67 C \n    Summary at:   Mon Sep 19 05:39:38 PM CEST 2022\n\n85.8%  proc'd: 77562 (13033 Gflop/s)   errors: 0   temps: 67 C \n    Summary at:   Mon Sep 19 05:39:51 PM CEST 2022\n\n96.7%  proc'd: 87570 (13038 Gflop/s)   errors: 0   temps: 68 C \n    Summary at:   Mon Sep 19 05:40:04 PM CEST 2022\n\n100.0%  proc'd: 91323 (13090 Gflop/s)   errors: 0   temps: 68 C \nKilling processes.. Freed memory for dev 0\nUninitted cublas\ndone\n\nTested 1 GPUs:\n    GPU 0: OK\n</code></pre> <p>Then run it again using doubles (see usage):</p> <pre><code># ./gpu_burn -d 120\nBurning for 120 seconds.\nGPU 0: NVIDIA GeForce RTX 3070 Ti (UUID: GPU-0c349bdd-3426-f2d3-4b26-c0b4bf0aad2e)\nInitialized device 0 with 7974 MB of memory (7451 MB available, using 6706 MB of it), using DOUBLES\nResults are 33554432 bytes each, thus performing 207 iterations\n10.8%  proc'd: 207 (289 Gflop/s)   errors: 0   temps: 48 C \n    Summary at:   Mon Sep 19 05:41:43 PM CEST 2022\n\n25.0%  proc'd: 414 (312 Gflop/s)   errors: 0   temps: 50 C \n    Summary at:   Mon Sep 19 05:42:00 PM CEST 2022\n\n37.5%  proc'd: 621 (312 Gflop/s)   errors: 0   temps: 51 C \n    Summary at:   Mon Sep 19 05:42:15 PM CEST 2022\n\n48.3%  proc'd: 1035 (316 Gflop/s)   errors: 0   temps: 52 C \n    Summary at:   Mon Sep 19 05:42:28 PM CEST 2022\n\n62.5%  proc'd: 1242 (316 Gflop/s)   errors: 0   temps: 53 C \n    Summary at:   Mon Sep 19 05:42:45 PM CEST 2022\n\n75.0%  proc'd: 1449 (316 Gflop/s)   errors: 0   temps: 53 C \n    Summary at:   Mon Sep 19 05:43:00 PM CEST 2022\n\n85.8%  proc'd: 1863 (316 Gflop/s)   errors: 0   temps: 54 C \n    Summary at:   Mon Sep 19 05:43:13 PM CEST 2022\n\n100.0%  proc'd: 2070 (316 Gflop/s)   errors: 0   temps: 54 C \n    Summary at:   Mon Sep 19 05:43:30 PM CEST 2022\n\n100.0%  proc'd: 2070 (316 Gflop/s)   errors: 0   temps: 54 C \nKilling processes.. Freed memory for dev 0\nUninitted cublas\ndone\n\nTested 1 GPUs:\n    GPU 0: OK\n</code></pre> <p>Both operations brought the GPU to 100% utilization and about 90% of VRAM usage, but the first run went harder on power and thermals:</p> <p></p> <p>Later I re-run <code>gpu-burn</code> for a longer time. The example in GitHub is to run for 1 hour, the recommendation I got in the NVidia forum was just 10 minutes, so to meet them in the middle I tried with 20 minutes:</p> <pre><code># ./gpu_burn -d 1200\nBurning for 1200 seconds.\nGPU 0: NVIDIA GeForce RTX 3070 Ti (UUID: GPU-0c349bdd-3426-f2d3-4b26-c0b4bf0aad2e)\nInitialized device 0 with 7974 MB of memory (7451 MB available, using 6706 MB of it), using DOUBLES\nResults are 33554432 bytes each, thus performing 207 iterations\n10.4%  proc'd: 2070 (316 Gflop/s)   errors: 0   temps: 53 C \n    Summary at:   Mon Sep 19 10:32:55 PM CEST 2022\n\n20.8%  proc'd: 4554 (316 Gflop/s)   errors: 0   temps: 56 C \n    Summary at:   Mon Sep 19 10:34:59 PM CEST 2022\n\n30.8%  proc'd: 6624 (314 Gflop/s)   errors: 0   temps: 57 C \n    Summary at:   Mon Sep 19 10:37:00 PM CEST 2022\n\n41.2%  proc'd: 8901 (314 Gflop/s)   errors: 0   temps: 56 C \n    Summary at:   Mon Sep 19 10:39:05 PM CEST 2022\n\n51.7%  proc'd: 11178 (314 Gflop/s)   errors: 0   temps: 56 C \n    Summary at:   Mon Sep 19 10:41:10 PM CEST 2022\n\n61.7%  proc'd: 13455 (314 Gflop/s)   errors: 0   temps: 56 C \n    Summary at:   Mon Sep 19 10:43:10 PM CEST 2022\n\n71.8%  proc'd: 15525 (314 Gflop/s)   errors: 0   temps: 56 C \n    Summary at:   Mon Sep 19 10:45:11 PM CEST 2022\n\n82.2%  proc'd: 17802 (314 Gflop/s)   errors: 0   temps: 56 C \n    Summary at:   Mon Sep 19 10:47:16 PM CEST 2022\n\n92.2%  proc'd: 20079 (314 Gflop/s)   errors: 0   temps: 56 C \n    Summary at:   Mon Sep 19 10:49:16 PM CEST 2022\n\n100.0%  proc'd: 21735 (314 Gflop/s)   errors: 0   temps: 56 C \nKilling processes.. Freed memory for dev 0\nUninitted cublas\ndone\n\nTested 1 GPUs:\n    GPU 0: OK\n</code></pre> <p></p>"},{"location":"blog/2022/11/05/the-weirdest-corrupted-video-on-an-nvidia-card/#cuda-gpumemtest","title":"<code>cuda-gpumemtest</code>","text":"<p>Installed <code>cuda_memtest</code> from ComputationalRadiationPhysics/cuda_memtest because the <code>cuda-gpumemtest</code> in sourceforge.net/cudagpumemtest was last updated in 2012.</p> <pre><code># apt-get -y install cmake\n# git clone \\\n  https://github.com/ComputationalRadiationPhysics/cuda_memtest.git\n# cd cuda_memtest/\n# mkdir build\n# cd build\n# cmake \\\n  -DCMAKE_CUDA_ARCHITECTURES=86 \\\n  -DCMAKE_CUDA_COMPILER:PATH=/usr/local/cuda-11.7/bin/nvcc\n# make\n</code></pre> <p>This actually worked on the first try \ud83d\ude01</p> <pre><code># ./cuda_memtest --stress\n[09/19/2022 17:58:29][rapture][0]:Running cuda memtest, version 1.2.3\n[09/19/2022 17:58:29][rapture][0]:NVRM version: NVIDIA UNIX x86_64 Kernel Module  515.65.01  Wed Jul 20 14:00:58 UTC 2022\n[09/19/2022 17:58:29][rapture][0]:num_gpus=1\n[09/19/2022 17:58:29][rapture][0]:Device name=NVIDIA GeForce RTX 3070 Ti, global memory size=8361607168, serial=unknown (NVML runtime error)\n[09/19/2022 17:58:30][rapture][0]:Attached to device 0 successfully.\n[09/19/2022 17:58:30][rapture][0]:WARNING: driver reported at least 8192524288 bytes are free but largest possible allocation is 8191475712 bytes.\n[09/19/2022 17:58:30][rapture][0]:Allocated 7812 MB\n[09/19/2022 17:58:30][rapture][0]:Test10 [Memory stress test]\n[09/19/2022 17:58:30][rapture][0]:Test10 with pattern=0x10a8a7b723cac6bc\n[09/19/2022 17:58:45][rapture][0]:Test10 finished in 14.9 seconds\n[09/19/2022 17:58:45][rapture][0]:Test10 [Memory stress test]\n[09/19/2022 17:58:45][rapture][0]:Test10 with pattern=0x3a96c31549d494a5\n[09/19/2022 17:58:59][rapture][0]:Test10 finished in 14.9 seconds\n</code></pre> <p>This test took much longer; after 4.5 hours it was still running and hadn\u2019t found any errors.</p>"},{"location":"blog/2022/11/05/the-weirdest-corrupted-video-on-an-nvidia-card/#worse-than-ever","title":"Worse Than Ever","text":"<p>While the above results seemed to indicate the GPU was \u201cOK\u201d, sure enough after reenabling SDDM (<code>systemctl enable sddm</code>) and rebooting, the issue happened again. And this time, worse than ever.</p> <p>Not only the corrupted graphics happened again, it was much worse. While previously it wouldn\u2019t happen until Xorg started, now the graphics are corrupted as soon as Grub shows up, and the login screen that used to be barely corrupted is now extremely corrupted. These are just 2 consecutive frames from the video of SDDM (below):</p> <p></p> <p></p> <p> </p>"},{"location":"blog/2022/11/05/the-weirdest-corrupted-video-on-an-nvidia-card/#root-cause-confirmation","title":"Root Cause Confirmation","text":"<p>With the new card now entirely unusable, there was nothing left to do with it than to remove it and repackage it for RMA.</p> <p>Installing the old card back and re-enabling the NVidia drivers was absolutely free of any troubles, offering further proof than nothing else was wrong.</p> <p>Another issue that I had previously dismissed as unrelated suddenly seemed very much related. In the weeks leading up to this issue, I had noticed mildly broken graphics in Skyrim: sometimes textures would go missing and a character\u2019s armor or face would be all shiny smooth blue. This issue also stopped when switching back to the old card.</p> <p>It was all too easy to assume this was caused by bugs in the game (which is famous for), since I had also experienced the Black Blobs bug in Mass Effect caused by poor support of hardware much newer than the game.</p>"},{"location":"blog/2022/11/05/the-weirdest-corrupted-video-on-an-nvidia-card/#back-to-normal","title":"Back To Normal","text":"<p>Fortunately, this happened within the warranty period of the new card (just a few months), so eventually I received a replacement card from the shop. The new card has been working quite well for a few days so far.</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/","title":"Ubuntu Studio 22.04 on Computer, for a young artist","text":"<p>The young artist in the house, who will soon have an upgraded PC for the new year, will be then running Ubuntu Studio 22.04, made for creative people.</p> <p>The current system is running Ubuntu Studio 20.04 on a very old system, based on an AMD Phenom II X4 and an Asus M4A89GTD Pro/USB3 from 2010. In preparation for the upcoming upgrade, the process starts by installing Ubuntu Studio 22.04 on a old SSD on my own PC (Rapture), which can later be transplanted to the new PC.</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#prepare-120-gb-ssd-in-rapture","title":"Prepare 120 GB SSD in Rapture","text":"<p>If only out of convenience, because this disk is already in Rapture and no longer in use, we\u2019ll install a new Ubuntu Studio 22.04.1 LTS on an old Samsung SSD 840 Basic 120GB SATA which has been working flawlessly since August 2013.</p> <p>The current partition table has 3 partitions, all too small for today\u2019s Ubuntu (for me):</p> <pre><code># fdisk -l /dev/sde\nDisk /dev/sde: 111.79 GiB, 120034123776 bytes, 234441648 sectors\nDisk model: Samsung SSD 840 \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0x6d4c7104\n\nDevice     Boot     Start       End  Sectors  Size Id Type\n/dev/sde1  *         2048  83888127 83886080   40G 83 Linux\n/dev/sde2        83888128 167774207 83886080   40G 83 Linux\n/dev/sde3       167774208 234441647 66667440 31.8G 83 Linux\n</code></pre> <p>Previous systems in this disk are Ubuntu Studio 20.04 on <code>/dev/sde1</code> (<code>/</code>) and <code>/dev/sde3</code> (<code>/usr</code>) because it wouldn\u2019t fit in <code>/dev/sde1</code> alone; and Ubuntu 18.04 in <code>/dev/sde2</code>.</p> <pre><code># df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sde3        32G   18G   13G  59% /media/ponder/\u2026 (Ubuntu 20.04 /)\n/dev/sde2        40G   27G   13G  68% /media/ponder/\u2026 (Ubuntu 18.04 /)\n/dev/sde1        40G   29G  8.6G  78% /media/ponder/\u2026 (Ubuntu 20.04 /usr)\n</code></pre> <p>First, back these up to the old RAID1 in Rapture, using a small script <code>backup-root-fs</code> to keep track of all the <code>--exclude</code> flags:</p> <pre><code>#!/bin/bash\nsrc=$1\ndst=$2\ntime rsync \\\n  -avxHAX \\\n  --progress \\\n  --numeric-ids \\\n  --exclude=\"$src/dev\" \\\n  --exclude=\"$src/proc\" \\\n  --exclude=\"$src/sys\" \\\n  --exclude='/dev' \\\n  --exclude='/proc' \\\n  --exclude='/sys' \\\n$src $dst\n</code></pre> <p>Used this script to backup all the old Ubuntu Studio's partitions:</p> <pre><code># /root/backup-root-fs \\\n  /media/ponder/518be5a3-5c8c-4674-a4b1-bebf455dc61f/@/ \\\n  /home/raid/backups/ubuntu-18.04/\n\n# /root/backup-root-fs \\\n  /media/ponder/498381d9-71e8-4941-a682-e37302da5392/ \\\n  /home/raid/backups/ubuntu-20.04/\n\n# rm -f /home/raid/backups/ubuntu-20.04/usr\n# mkdir /home/raid/backups/ubuntu-20.04/usr\n\n# /root/backup-root-fs \\\n  /media/ponder/f8f7aee8-e172-4eda-bf9e-962dbfb51331 \\\n  /home/raid/backups/ubuntu-20.04/usr/\n</code></pre> <p>Both Ubuntu 20.04 and 22.04 take about 45 GB in their roots:</p> <pre><code># du -sh /home/raid/backups/ubuntu-*\n27G     /home/raid/backups/ubuntu-18.04\n46G     /home/raid/backups/ubuntu-20.04\n\n# df -h\nFilesystem      Size  Used Avail Use% Mounted on\ntmpfs           3.2G  2.4M  3.2G   1% /run\n/dev/nvme0n1p2   50G   44G  3.2G  94% /\n</code></pre> <p>Rather than spend hours trying to delete unused clutter, because there is no longer enough unused clutter that it would make a dent in disk usage, we need to plan for bigger root filesystems in the new partition schema.</p> <p>Assuming a root filesystem of at least 45 GB, make that 50 GB, set that as the 70% (or less) of desired partition size  (use the 70/30 rule to optimize your system to  keep SSDs speedy), which means at least 71.42 GB, make that 75 GB or more.</p> <p>In the new 2000 GB m.2 SSD in Rapture, that\u2019d mean converting the 3 50 GB partitions into 2 75 GB partitions, which is necessarily a destructive change for all partitions. Luckily the current system root is on the first of the 3, so we can start by replacing the other 2 with a 75 GB partition, and possibly enlarging the root filesystem later with parted on a USB stick.</p> <p>In the old 120 GB SSD for Computer, we could get away with 2 60 GB partitions, which would be slightly smaller, but there is a newer 250 GB SSD in Computer we can use later for a future system\u2019s root filesystem, so we might as well take the whole disk with now.</p> <p>Create a 250 MB EFI partition, just in case this disk is moved to an EFI-capable system.</p> <pre><code># umount /media/ponder/518be5a3-5c8c-4674-a4b1-bebf455dc61f\n# umount /media/ponder/f8f7aee8-e172-4eda-bf9e-962dbfb51331\n# umount /media/ponder/498381d9-71e8-4941-a682-e37302da5392\n# fdisk /dev/sde\n\u2026\nDisk /dev/sde: 111.79 GiB, 120034123776 bytes, 234441648 sectors\nDisk model: Samsung SSD 840 \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: dos\nDisk identifier: 0x6d4c7104\n\nDevice     Boot  Start       End   Sectors   Size Id Type\n/dev/sde1         2048    514047    512000   250M 83 Linux\n/dev/sde2  *    514048 234441647 233927600 111.5G 83 Linux\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#install-ubuntu-studio-2204-in-rapture","title":"Install Ubuntu Studio 22.04 (in Rapture)","text":"<p>Prepared the USB stick with <code>usb-creator-kde</code> and booted into it, then used the \u201cInstall Ubuntu\u201d launcher on the desktop.</p> <ol> <li>Boot from USB, with the option to Try or Install Ubuntu</li> <li>Select language (English) and then Install Ubuntu</li> <li>Select time zone and keyboard layout.</li> <li>Select Type of install: Ubuntu Server (not minimized)</li> <li>For Storage configuration select Custom storage layout</li> <li>Select storage device: Samsung SSD 840 Series 120 GB      and then Erase disk.</li> <li>Boot loader location: MBR on /dev/sdd (120 GB SSD) \u2500      not on another disk!</li> <li>Create partition 1 (250 MB) as EFI System Partition mounted on <code>/boot/efi</code></li> <li>Create partition 2 (111.55 GiB) ext4 mounted on <code>/</code></li> <li>Select Install Now to confirm the partition selection.</li> <li>Confirm first non-root user name (<code>ponder</code>) and computer    name (<code>computer</code>).</li> <li>Once it's done, select Restart    (remove install media and hit <code>Enter</code>).</li> </ol> <p>Note</p> <p>While this SSD is in Rapture, booting into this new system requires selecting the 120 GB disk in the BIOS boot menu (F8 / F10).</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#nvidia-drivers","title":"NVidia drivers","text":"<p>With the new kernel running, bring the system up to date with all the updates and install NVidia drivers:</p> <pre><code># apt update\n# apt dist-upgrade\n# apt install gkrellm nvidia-settings nvidia-driver-520 xserver-xorg-video-nvidia-520 -y\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#install-software","title":"Install Software","text":""},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#apt-packages","title":"APT packages","text":"<p>The first thing I like to do is installing everything that is easily to install with APT:</p> <pre><code># apt install gdebi-core wget gkrellm vim curl gkrellm-hdplop gkrellm-x86info \\\n  gkrellm-xkb gkrellm-cpufreq geeqie playonlinux exfat-fuse clementine id3v2 htop \\\n  vnstat neofetch tigervnc-viewer xcalib scummvm wine gamemode python-is-python3 \\\n  exiv2 python3-selenium sox speedtest-cli python3-pip netcat rename scrot \\\n  jstest-gtk etherwake lm-sensors sysstat ttf-mscorefonts-installer \\\n  ttf-mscorefonts-installer:i386 tigervnc-tools winetricks icc-profiles tor \\\n  unrar iotop-c xdotool redshift-gtk inxi vainfo vdpauinfo ffmpeg mpv screen -y   \n# apt -y autoremove\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#google-chrome","title":"Google Chrome","text":"<p>Installing Google Chrome is as simple as downloading the Debian package and installing it:</p> <pre><code># dpkg -i google-chrome-stable_current_amd64.deb \n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#steam","title":"Steam","text":"<p>Installing the Steam client requires first installing several <code>i386</code> (32-bit) libraries:</p> <pre><code># apt install libgl1-mesa-glx:i386 libc6:amd64 libc6:i386 libegl1:amd64 libegl1:i386 \\\n  libgbm1:amd64 libgbm1:i386 libgl1-mesa-dri:amd64 libgl1-mesa-dri:i386 libgl1:amd64 \\\n  libgl1:i386 steam-libs-amd64:amd64 steam-libs-i386:i386 -y\n</code></pre> <p>With those installed, one can download <code>steam_latest.deb</code> from store.steampowered.com/about and install it with <code>gdebi</code>:</p> <pre><code># gdebi steam_latest.deb\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#minecraft","title":"Minecraft","text":"<p>The <code>Minecraft_staging.deb</code> launcher no longer works, because it only allows trying to log in with Mojang accounts, and those have been deprecated.</p> <p>This was very unexpected, given how the Alternative Download Options for Minecraft: Java Edition still points to the old <code>Minecraft.deb</code> and a recent article on how to Install Minecraft on Ubuntu 22.04 still presents this option as viable.</p> <p>The only method that proved successful to install Minecraft: Java Edition was to copy <code>/usr/bin/minecraft-launcher</code> from the previous system, from a backup in lexicon:</p> <pre><code>lexicon:~/computer-backup$ md5sum \\\n  usr/bin/minecraft-launcher \\\n  usr/share/applications/minecraft-launcher.desktop \\\n  usr/share/icons/hicolor/symbolic/apps/minecraft-launcher.svg\n5d0a29a858de070384fcbe84540fcdc9  usr/bin/minecraft-launcher\naf82416995d92945ad5e6a24ac23f503  usr/share/applications/minecraft-launcher.desktop\n1afb79bae5991dfdd73d15a3e2bbd731  usr/share/icons/hicolor/symbolic/apps/minecraft-launcher.svg\n</code></pre> <p>With this, plus personal files, Minecraft should be playable. To test this (and everything else) before moving the 120 GB SSD to Computer, copy most of <code>/home/artist</code> (except Steam games) to Rapture and mount its <code>/home</code> partition:</p> <pre><code># Rapture /home is on /dev/nvme0n1p5\nUUID=18238846-d411-4dcb-af87-a2d19a17fef3 /home btrfs   defaults 0 2\n\n# Computer /home is on /dev/sdc2\n#UUID=3accdc44-232e-4924-a99f-019427900938 /home btrfs   defaults 0 2\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#continuous-monitoring","title":"Continuous Monitoring","text":"<p>To have detailed system and process monitoring, install the <code>conmon-mt</code> (multi-thread) script from Continuous Monitoring.</p> <p>Once it's running, go to Grafana and save a copy of the Rapture dashboard as New Computer, then update all the queries to filter for <code>hostname = computer</code>.</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#system-configuration","title":"System Configuration","text":"<p>Once software is installed, further system configuration is desired to make some of it work better for certain purposes.</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#logitech-trackman-marble","title":"Logitech Trackman Marble","text":"<p>To enable 2-axis scrolling with the Logitech Trackman Marble create or edit <code>/usr/share/X11/xorg.conf.d/10-libinput.conf</code> like this:</p> /usr/share/X11/xorg.conf.d/10-libinput.conf<pre><code>Section \"InputClass\"\n    Identifier      \"Marble Mouse\"\n    MatchProduct    \"Logitech USB Trackball\"\n    MatchIsPointer  \"on\"\n    MatchDevicePath \"/dev/input/event*\"\n    Driver          \"libinput\"\n    Option          \"SendCoreEvents\" \"true\"\n\n    #  Physical buttons come from the mouse as:\n    #     Big:   1 3\n    #     Small: 8 9\n    #\n    # This makes right small button (9) into the middle, and puts\n    #  scrolling on the left small button (8).\n    #\n    Option \"Buttons\"            \"9\"\n    Option \"ButtonMapping\"      \"1 8 3 4 5 6 7 9 2\"\n    Option \"ScrollMethod\"       \"button\"\n    Option \"ScrollButton\"       \"8\"\n    Option \"EmulateWheel\"       \"true\"\n    Option \"EmulateWheelButton\" \"8\"\n    Option \"YAxisMapping\"       \"4 5\"\n    Option \"XAxisMapping\"       \"6 7\"\nEndSection\n</code></pre> <p>On a tangentially related note, it is also good to forbid joystick mouse emulation when using certain joystick-like gaming controllers. To do this,  create or edit <code>/usr/share/X11/xorg.conf.d/50-joystick.conf</code></p> /usr/share/X11/xorg.conf.d/50-joystick.conf<pre><code>Section \"InputClass\"\n    Identifier \"joystick catchall\"\n    MatchIsJoystick \"on\"\n    MatchDevicePath \"/dev/input/event*\"\n    Driver \"joystick\"\n    Option \"StartKeysEnabled\" \"False\"       #Disable mouse\n    Option \"StartMouseEnabled\" \"False\"      #support\nEndSection\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#ssh-server","title":"SSH Server","text":"<p>Ubuntu Studio being a desktop distro, doesn't enable the SSH server by default, but this is very useful to adjust the system remotely at any time:</p> <pre><code># apt install ssh -y\n# sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config\n# systemctl enable --now ssh\n</code></pre> <p>Note</p> <p>Remember to copy over files under <code>/root</code> from previous system/s, in case it contains useful scripts (and/or SSH keys worth keeping under <code>.ssh</code>).</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#multiple-ips-on-lan","title":"Multiple IPs on LAN","text":"<p>Connecting via Ethernet we get a DHCP lease in the same network used by the WiFi  (<code>192.168.0.0/24</code>). To add a static IP address isolated from WiFi, set it up like this:</p> <pre><code># ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: enp8s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 60:45:cb:a0:16:7b brd ff:ff:ff:ff:ff:ff\n    inet 192.168.0.31/24 brd 192.168.0.255 scope global dynamic noprefixroute enp8s0\n       valid_lft 80045sec preferred_lft 80045sec\n    inet6 fe80::495e:55c0:ed37:9a21/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n3: enx70b5e8f62f4d: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc fq_codel state DOWN group default qlen 1000\n    link/ether 70:b5:e8:f6:2f:4d brd ff:ff:ff:ff:ff:ff\n4: wlp6s0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000\n    link/ether a0:f3:c1:1d:71:b0 brd ff:ff:ff:ff:ff:ff\n</code></pre> <p>We only need to get the DNS add them with the 2nd IP:</p> <pre><code># resolvectl status\nGlobal\n       Protocols: -LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\nresolv.conf mode: stub\n\nLink 2 (enp8s0)\n    Current Scopes: DNS\n         Protocols: +DefaultRoute +LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\nCurrent DNS Server: 62.2.24.158\n       DNS Servers: 62.2.24.158 62.2.17.61\n        DNS Domain: v.cablecom.net\n\nLink 3 (enx70b5e8f62f4d)\nCurrent Scopes: none\n     Protocols: -DefaultRoute +LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\n\nLink 4 (wlp6s0)\nCurrent Scopes: none\n     Protocols: -DefaultRoute +LLMNR -mDNS -DNSOverTLS DNSSEC=no/unsupported\n</code></pre> <p>Note</p> <p>Additional DNS Servers found by the previous system: </p><pre><code>62.2.17.60\n62.2.17.61\n62.2.24.158\n62.2.24.162\n</code></pre><p></p> <p>With these, we can go back to static IPs and simply add both addresses together:</p> /etc/netplan/01-network-manager-all.yaml<pre><code># Dual static IP on LAN, nothing else.\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp8s0:\n      dhcp4: no\n      dhcp6: no\n      # Ser IP address &amp; subnet mask\n      addresses: [ 10.0.0.3/24, 192.168.0.3/24 ]\n      # Set default gateway\n      routes:\n       - to: default\n         via: 192.168.0.1\n      nameservers:\n        # Set DNS name servers\n        addresses: [62.2.24.158, 62.2.17.61]\n</code></pre> <p>Apply the changes with netplan apply \u2500 there may be a warning, but it works:</p> <pre><code># netplan apply\n# ip a\n\u2026\n2: enp8s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 60:45:cb:a0:16:7b brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.2/24 brd 10.0.0.255 scope global enp8s0\n       valid_lft forever preferred_lft forever\n    inet 192.168.0.9/24 brd 192.168.0.255 scope global enp8s0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::495e:55c0:ed37:9a21/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n\u2026\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#wake-on-lan","title":"Wake-on-LAN","text":"<p>First, confirm Wake up on PCI event is enabled in the BIOS, then enable it in the NIC:</p> <pre><code># apt install ethtool -y\n# ethtool -s enp8s0 wol g\n# ethtool enp8s0 | grep -i wake\n        Supports Wake-on: pumbg\n        Wake-on: g\n</code></pre> <p>Note</p> <p>Issuing this command is required after each boot. The system's networking is configured via netplan, so we need to add the <code>ethtool -s &lt;NIC&gt; wol g</code> command to a script under <code>/etc/networkd-dispatcher/configure.d/</code>, e.g. <code>50-enable-wol</code></p> /etc/networkd-dispatcher/configure.d/50-enable-wol<pre><code>#!/bin/sh\nif [ \"$IFACE\" = \"enp8s0\" ]\nthen\n  /usr/sbin/ethtool -s $IFACE wol g\nfi\n</code></pre> <p>Now the wake-on-LAN remains enabled in the NIC even after shutdown. This is a bit futile because the PC is plugged to a power extension that gets turned off overnight and even during the day, and when power is cut off from the PSU the WOL setting is lost.</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#make-sddm-look-good","title":"Make SDDM Look Good","text":"<p>Ubuntu Studio 22.04 uses  Simple Desktop Display Manager (SDDM) (sddm/sddm in GitHub) which is quite good looking out of the box, but I like to customize this for each computer.</p> <p>For most computers my favorite SDDM theme is Breeze-Noir-Dark, which I like to install system-wide:</p> <pre><code># unzip Breeze-Noir-Dark.zip\n# mv Breeze-Noir-Dark /usr/share/sddm/themes/breeze-noir-dark\n</code></pre> <p>Warning</p> <p>Action icons won\u2019t render due to the different directory name, need to change the directory name in the <code>iconSource</code> fields in <code>Main.qml</code> to make them all lowercase so icons show up.</p> <p>Other than installing this theme, all I really change in it is the background image, e.g. assuming it's downloaded as <code>/tmp/background.jpg</code></p> <pre><code># mv /tmp/background.jpg /usr/share/sddm/themes/breeze-noir-dark\n# mv /tmp/ponyo.jpg /usr/share/sddm/themes/breeze-noir-dark\n# cd /usr/share/sddm/themes/breeze-noir-dark\n# vi theme.conf\nbackground=/usr/share/sddm/themes/breeze-noir-dark/background.jpg\n# vi theme.conf.user\nbackground=background.jpg\n</code></pre> <p>Note</p> <p>It appears that this only works when the background image is specified in <code>theme.conf.user</code> and is stored in the theme\u2019s directory.</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#make-sddm-listen-to-tcp","title":"Make SDDM Listen to TCP","text":"<p>By default, SDDM launches X.org with <code>-nolisten tcp</code> for security reasons. To override this, set the flag under the <code>[X11]</code> section in <code>/etc/sddm.conf.d/kde_settings.conf</code></p> /etc/sddm.conf.d/kde_settings.conf<pre><code>[X11]\nServerArguments=-listen tcp\n</code></pre> <p>Then add a short script to authorize connections from <code>localhost</code> to the user (<code>artist</code>) session, e.g. as <code>~/bin/xhost-localhost</code></p> <pre><code>#!/bin/bash\nxhost +localhost\n</code></pre> <p>This allows an SSH session for the user (<code>artist</code>) to send messages to the screen with <code>zenity</code>:</p> <pre><code>DISPLAY=localhost:0 /usr/bin/zenity --warning \\\n  --text='Computer Will Shut Down in 20 Minutes'\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#user-specific-settings","title":"User-specific Settings","text":"<p>Once the system is generally ready, a Personal Computer naturally requires additional settings and software specific to the user/s who will use it the most.</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#redshift","title":"RedShift","text":"<p>Redshift adjusts the color temperature of your screen according to your surroundings. This may help your eyes hurt less if you are working in front of the screen at night.</p> <p>More importantly, blue light can affect your sleep, because exposure to blue light before bedtime can disrupt sleep patterns as it affects when our bodies create melatonin.</p> <p>This is why <code>redshift-gtk</code> was already installed as part of the first APT packages. What is left to do for a full customization is to adjust the color temperature values and manual location in <code>~/.config/redshift.conf</code></p> ~/.config/redshift.conf<pre><code>[redshift]\ntemp-day=5500\ntemp-night=4500\ngamma=0.6\nadjustment-method=randr\nlocation-provider=manual\n\n[manual]\nlat=47\nlon=0\n</code></pre> <p>RedShift can be temporarily disabled and/or reset to the default values (based on the config above) by activating the Redshift Control Widget. Usually this is done by clicking on it, but it can be done through a keyboard shortcut as well which is very useful while playing video games.</p> <p>Warning</p> <p>Adding the Redshift Control Widget to a KDE Plasma panel runs a risk of scrolling the mouse wheel on it and making the screen so dark it becomes unusuable. In case this happens, it is highly recommended to add a keyboard shortcut as follows.</p> <p>My personal choice of shortcut to Activate Redshift Control Widget is <code>Ctrl+Alt+Shift+R</code>.</p> <p></p> <p>Note</p> <p>It seems no longer necessary to manually add Redshift to the user's desktop session. Previously, it would be necessary to launch Autostart and Add Application\u2026 to add Redshift.</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#wacom-tablet","title":"Wacom tablet","text":"<p>This young artist enjoys digital painting on Krita and even 3D sculpting on Blender, using a Wacom graphic tablet with a few buttons mapped to the Undo/Redo and Zoom In/out functions.</p> <p>Note</p> <p>This setup is specific to the Wacom Intuos Art Pen &amp; Touch M South.</p> <p>To make Krita work best with this, we use a Python script to listen for the Wacom tablet, so that when it is plugged in the button mapping is loaded and then Krita is launched.</p> <p>This script depends on pyudev and idle for Python 3:</p> <pre><code># apt install python3-pyudev idle-python3.10 -y\n</code></pre> <p>The script <code>wait-for-wacom-and-launch-krita.py</code> launches Krita right after setting the tablet up, and supports both 16:9 and non-16:9 screens (e.g. 3440x1440):</p> <code>wait-for-wacom-and-launch-krita.py</code> <pre><code>#!/usr/bin/python3\n#\n# Wait for a (specific) Wacom Intous table and map its 4 buttons to: undo, redo, zoom in/out.\n# Exit (successfully or not) after mapping the buttons.\n\nimport gi\nimport pyudev\nimport socket\nimport subprocess\nimport sys\n\nEXPECTED_ID_SERIAL = 'Wacom_Co._Ltd._Intuos_PTM'\nXSETWACOM_CMD = '/usr/bin/xsetwacom'\nXSETWACOM_PAD_DEV = 'Wacom Intuos PT M 2 Pad pad'\nXSETWACOM_STL_DEV = 'Wacom Intuos PT M 2 Pen stylus'\nXSETWACOM_MAP = list(range(10))\nXSETWACOM_MAP[3] = 'key ctrl z'\nXSETWACOM_MAP[1] = 'key ctrl shift z'\nXSETWACOM_MAP[8] = 'key +'\nXSETWACOM_MAP[9] = 'key -'\n\nHOST_CONFIG = {\n  'computer': {\n    'krita-cmd': '/home/artist/Desktop/.bin/krita',\n  },\n  'rapture': {\n    'krita-cmd': '/home/ponder/bin/krita-es',\n    # Adjust drawing area in 3440x1440 screen with\n    # xsetwacom --set \"Wacom Intuos PT M 2 Pen stylus\" Area \n    'stylus-area': '0 0 21600 9042',\n  },\n}\n\ndef device_event(device):\n    id_serial = device.get('ID_SERIAL')\n    if device.action == 'bind' and id_serial == EXPECTED_ID_SERIAL:\n      total_status = 0\n      hostname = socket.gethostname()\n      config = HOST_CONFIG[hostname]\n      # Map PAD buttons.\n      for i in (1, 3, 8, 9):\n        xsetwacom_command = [\n            XSETWACOM_CMD, 'set', XSETWACOM_PAD_DEV, \n            'Button', str(i), XSETWACOM_MAP[i]]\n        print(xsetwacom_command)\n        status = subprocess.call(xsetwacom_command)\n        total_status += status\n      # If necessary, assign stylus area in non-16:9 screen.\n      if 'stylus-area' in config:\n        xsetwacom_command = [\n            XSETWACOM_CMD, 'set', XSETWACOM_STL_DEV,\n            'Area', config['stylus-area']]\n        print(xsetwacom_command)\n        status = subprocess.call(xsetwacom_command)\n        total_status += status\n      if total_status &gt; 0:\n        status = subprocess.call([\n            '/usr/bin/zenity',\n            '--error',\n            '--text=\"Could not configure %s\"' % id_serial])\n        sys.exit(1)\n      else:\n        status = subprocess.call([\n            '/usr/bin/zenity', \n            '--info',\n            '--text=\"Configured %s\"' % id_serial])\n        krita_cmd = config['krita-cmd']\n        status = subprocess.call([krita_cmd])\n        sys.exit(0)\n\n\nif __name__ == '__main__':\n  from gi.repository import GLib\n\n  context = pyudev.Context()\n  monitor = pyudev.Monitor.from_netlink(context)\n  monitor.filter_by(subsystem='usb')\n  observer = pyudev.MonitorObserver(monitor, callback=device_event)\n  observer.start()\n\n  status = subprocess.call([\n      '/usr/bin/zenity', \n      '--warning',\n      '--text=\"Plug the tablet in to continue...\"'])\n  loop = GLib.MainLoop()\n  loop.run()\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#scratch-30-in-chrome","title":"Scratch 3.0 in Chrome","text":"<p>This artist is also into visual programming with Scratch.</p> <p>Scratch 2.0 was infesible to run, it was based on defunct technology (Flash) and itself deprecated in favor of web-based Scratch 3.0 \u2500 much better!</p> <p>However, Scratch 3.0 has a tendency to crash Google Chrome. This can be worked around by  disabling Renderer Code Integrity. To make this easily accessible, this can be added in a custom launcher (e.g. <code>~/Desktop/Scratch.desktop</code>):</p> ~/Desktop/Scratch.desktop<pre><code>[Desktop Entry]\nName=Scratch 3.0\nComment=Scratch 3.0\nExec=/opt/google/chrome/chrome-kiosk --disable-features=RendererCodeIntegrity \"https://scratch.mit.edu/projects/editor\"\nIcon=/home/artist/Desktop/.icons/scratch.png\nType=Application\nPath=\nTerminal=false\nStartupNotify=false\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#roccat-tools","title":"Roccat Tools","text":"<p>This artist has a very peculiar mechanical keyboard: Roccat Ryos MK Pro</p> <p>This keyboard has 5 keys for macros, let\u2019s see how to make use of them\u2026</p> <pre><code>$ sudo apt-get install gcc cmake libdbus-glib-1-dev libgtk2.0-dev libgudev-1.0-dev libx11-dev libgaminggear-dev \\\n  lua5.3-dev liblua5.3-dev luajit libluajit-5.1-dev libtexluajit-dev -y\n$ git clone https://github.com/roccat-linux/roccat-tools.git\n$ cd roccat-tools/\n$ mkdir build\n$ cd build\n$ cmake \\\n  -DCMAKE_INSTALL_PREFIX=\"/usr\" \\\n  -DWITH_LUA=5.3 \\\n  -isystem /usr/include/harfbuzz \\\n  -DCMAKE_C_FLAGS=\"$(pkg-config --cflags harfbuzz)\" ..\n$ make\n$ sudo make install\n$ sudo scripts/post_install\n$ sudo usermod -a -G roccat artist\n$ sudo usermod -a -G roccat ponder\n</code></pre> <p>At this point we have to log out and back in, or reboot.</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#parental-controls","title":"Parental Controls","text":""},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#squid-proxy","title":"Squid Proxy","text":"<p>This system being setup for a very young artist, responsible parenting demands that certain parts of the Wild Internet be, if not entirely unreachable, at least time-gated.</p> <p>Install Squid proxy and set it up for localhost only:</p> <pre><code># apt install squid -y\n</code></pre> /etc/squid/squid.conf<pre><code>acl localnet src 10.0.0.0/8 # RFC1918 possible internal network\nacl SSL_ports port 443\nacl Safe_ports port 80      # http\nacl Safe_ports port 21      # ftp\nacl Safe_ports port 443     # https\nacl Safe_ports port 70      # gopher\nacl Safe_ports port 210     # wais\nacl Safe_ports port 1025-65535  # unregistered ports\nacl Safe_ports port 280     # http-mgmt\nacl Safe_ports port 488     # gss-http\nacl Safe_ports port 591     # filemaker\nacl Safe_ports port 777     # multiling http\nacl CONNECT method CONNECT\nhttp_access deny !Safe_ports\nhttp_access deny CONNECT !SSL_ports\nhttp_access allow localhost manager\nhttp_access deny manager\nhttp_access allow localhost\nhttp_access deny all\nhttp_port 3128\ncoredump_dir /var/spool/squid\nrefresh_pattern ^ftp:       1440    20% 10080\nrefresh_pattern ^gopher:    1440    0%  1440\nrefresh_pattern -i (/cgi-bin/|\\?) 0 0%  0\nrefresh_pattern (Release|Packages(.gz)*)$      0       20%     2880\nrefresh_pattern .       0   20% 4320\n</code></pre> <pre><code># systemctl restart squid.service\n# systemctl status squid.service\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#block-youtube","title":"Block YouTube","text":"<p>This section shows how to block YouTube because that is the one site that contains most domains. The same setup can be used to block simpler sites, and it is useful to have this setup replicated for each site as needed.</p> <p>To block the site, create a file with the rules in How to block YouTube Videos, updated here to reflect the latest YouTube embed URL pattern. To allow the site, create a file with just a comment.</p> <p>For each site to block/allow, create a dedicated directory:</p> <pre><code># mkdir -p /etc/squid/conf.d/youtube/\n</code></pre> <p>In this directory, create a file (e.g. <code>no-youtube.conf</code>) with the necessary rules:</p> <pre><code># Blocking YouTube Videos\n# https://wiki.squid-cache.org/ConfigExamples/Streams/YouTube\n## The videos come from several domains\nacl youtube_domains dstdomain .youtube.com .googlevideo.com .ytimg.com\n## G* services authentication domain\nacl google_accounts dstdomain accounts.youtube.com\nhttp_access deny youtube_domains !google_accounts\n## Block YT clips\nacl yt_clips url_regex .youtube\\.com\\/embed\\/\nhttp_access deny yt_clips\n</code></pre> <p>Create also a file (e.g. <code>yes-youtube.conf</code>) with just a comment, this will simply not block anything:</p> <pre><code># No Blocking YouTube Videos\n</code></pre> <p>In the main Squid config file, include what will be a symlink to either of the above:</p> <pre><code># cd /etc/squid/conf.d/youtube/\n# ln -s no-youtube.conf youtube.conf\n</code></pre> <p>Add an <code>include</code> rules at the end of <code>/etc/squid/squid.conf</code></p> /etc/squid/squid.conf<pre><code># INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS\ninclude /etc/squid/conf.d/*.conf\n# Blocking (or not) YouTube Videos\ninclude /etc/squid/conf.d/youtube/youtube.conf\n</code></pre> <p>To quickly alternate between the two states, create two scripts:</p> /root/bin/youtube-disable<pre><code>#!/bin/bash\nln -sf /etc/squid/no-youtube.conf /etc/squid/youtube.conf\n/usr/sbin/squid -k reconfigure\n</code></pre> /root/bin/youtube-enable<pre><code>#!/bin/bash\nln -sf /etc/squid/yes-youtube.conf /etc/squid/youtube.conf\n/usr/sbin/squid -k reconfigure\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#squid-analyzer","title":"Squid Analyzer","text":"<p>Squid Analyzer is useful to get a sense of what parts of the Internet are begin requested, whether allowed or blocked.</p> <ol> <li> <p>Download from squidanalyzer.darold.net</p> </li> <li> <p>Unpack and apply changes in commit 616d742</p> <pre><code># tar xvfz squidanalyzer-6.6.tar.gz\n# cd squidanalyzer-6.6/\n# echo 9 &gt; debian/compat\n# vi debian/rules +7\noverride_dh_auto_configure:\n    perl Makefile.PL \\\n        INSTALLDIRS=vendor \\\n        LOGFILE=/var/log/squid3/access.log \\\n</code></pre> </li> <li> <p>Build Debian and install package</p> <pre><code># apt install -y build-essential apache2-dev\n# dpkg-buildpackage -us -uc\n# dpkg -i ../squidanalyzer_6.6-1_all.deb\n</code></pre> </li> <li> <p>Setup <code>crontab</code> as per      squidanalyzer.darold.net</p> <pre><code># crontab -e\n00 20 * * * /usr/bin/squid-analyzer /var/log/squid/access.log\n</code></pre> </li> <li> <p>Authorize other PCs to visit, by adding this to <code>squidanalyzer.conf</code></p> /etc/apache2/conf-available/squidanalyzer.conf<pre><code>Alias /squidreport /var/lib/squidanalyzer\n&lt;Directory /var/lib/squidanalyzer&gt;\n    Options -Indexes +FollowSymLinks +MultiViews\n    AllowOverride None\n    Order deny,allow\n    Allow from 10.0.0.1/8\n    Require all granted\n&lt;/Directory&gt;\n</code></pre> </li> </ol>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#kioskify-browsers","title":"Kioskify browsers","text":"<p>To keep the youngest using the above Proxy and also not going astray by opening new tabs on arbitrary URLs, replace all web browser binaries with scripts that launch them with specific flags to</p> <ul> <li>enable Kiosk mode,</li> <li>always start at a chosen URL, and</li> <li>force traffic through the local Proxy.</li> </ul>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#google-chrome_1","title":"Google Chrome","text":"<p>This method to kioskify a browser consists of</p> <ol> <li>renaming the original browser binary,</li> <li>replace it with a launcher script, and</li> <li>ensure the process applies only to the original binary.</li> </ol> <p>To this effect, run <code>/opt/google/chrome/kioskify-google-chrome</code> every minute:</p> /opt/google/chrome/kioskify-google-chrome<pre><code>#!/bin/bash\n\n# Works only on a specific installation path.\ncd /opt/google/chrome || exit 1\n\n# Works only if the 'chrome' file is an ELF executable.\nfile chrome | grep -q ELF || exit 0\n\n# Rename chrome as chrome-normal\nif [ ! -f backup-chrome ]\nthen\n  cp -fv chrome backup-chrome\nfi\nmv -fv chrome chrome-normal\n\n# Create chrome-kiosk \n(cat &lt;&lt; __EOF__\n#!/bin/bash\nexec /opt/google/chrome/chrome-normal \\\\\n  --kiosk \\\\\n  --proxy-server=\"127.0.0.1:3128\" \\\\\n  \\$*\n__EOF__\n)&gt;chrome-kiosk \nchmod +x chrome-kiosk \n\ngrep -v kiosk chrome-kiosk &gt; chrome-no-kiosk \nchmod +x chrome-no-kiosk \n\n# Create chrome (sink to fixed URL)\n(cat &lt;&lt; __EOF__\n#!/bin/bash\nexec /opt/google/chrome/chrome-kiosk https://www.anton.app/de\n__EOF__\n)&gt;chrome\n\nchmod +x chrome\n</code></pre> <p>This can be run every minute, to immediately kioskify each new browser version as soon as it's installed:</p> <pre><code># chmod +x kioskify-google-chrome\n# crontab -e\n* * * * * /opt/google/chrome/kioskify-google-chrome\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#crontab-playtime","title":"Crontab Playtime","text":"<pre><code>$ crontab -l\n# m h  dom mon dow   command\n* 12 * * 6,7 /home/artist/Desktop/.bin/restore .edu .fun\n* 18 * * 1,2,3,4,5 /home/artist/Desktop/.bin/restore .edu .fun\n00 20 * * * /home/artist/Desktop/.bin/restore\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#crontab-bedtime","title":"Crontab Bedtime","text":"<p>Everybody has a natural tendency to stay in front of their computer, or other entertaining devices, for longer than is good for them. This can be very detrimental when it impacts sleep pattners, to avoid this this computer will shut down on a regular schedule:</p> <pre><code># crontab -l | grep -i 'shut.*down'\n\n# Shut down at 20:30 (Sun-Thu)\n00 20 * * 0-4 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in thirty minutes\"\n10 20 * * 0-4 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in twenty minutes\"\n15 20 * * 0-4 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in fifteen minutes\"\n20 20 * * 0-4 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in ten minutes\"\n25 20 * * 0-4 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in five minutes\"\n26 20 * * 0-4 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in four minutes\"\n27 20 * * 0-4 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in three minutes\"\n28 20 * * 0-4 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in two minutes\"\n29 20 * * 0-4 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn final warning _comma computer will shut down in sixty seconds\"\n30 20 * * 0-4 /sbin/shutdown -h now\n\n# Shut down at 21:30 (Fri-Sat)\n30 20 * * 5-6 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in thirty minutes\"\n40 20 * * 5-6 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in twenty minutes\"\n45 20 * * 5-6 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in fifteen minutes\"\n50 20 * * 5-6 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in ten minutes\"\n55 20 * * 5-6 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in five minutes\"\n56 20 * * 5-6 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in four minutes\"\n57 20 * * 5-6 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in three minutes\"\n58 20 * * 5-6 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn warning _comma computer will shut down in two minutes\"\n59 20 * * 5-6 ssh pi@pi-f1 \"bin/half-life-vox-say buzwarn final warning _comma computer will shut down in sixty seconds\"\n00 21 * * 5-6 /sbin/shutdown -h now\n</code></pre> <p>Note</p> <p>The audible warnings are played on the Raspberry Pi 4 so that they will play even if no user is logged in.</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#time-tracking","title":"Time Tracking","text":"<p>Keep track of where your times goes is an on-going struggle for everybody using computers, not just young people. Even at work, it can be quite useful to know which tasks tend to be the bigger time sinks. For such purposes it is useful to run <code>xdotool</code> in a loop polling for the title of the active window, which tends to correlate pretty well with the task at hand, e.g.</p> <ul> <li>name of the video game being played</li> <li>title of the video being watched (YouTube, Netflix, etc.)</li> <li>name of the file being edited (Blender, GIMP, Krita, etc.)</li> </ul> <p>For this purpose, add <code>poll-focused-window-every-second</code> and <code>report-daily-time-per-focused-window</code> as login scripts to capture a screenshot of the full screen and then make a timelapse video of each desktop session. This will cause a small CPU spike when logging in, but is not too noticeable</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#poll-focused-window-every-second","title":"<code>poll-focused-window-every-second</code>","text":"<pre><code>#!/bin/bash\n#\n# Poll every second for the name of the active window, store it in a log file.\n\nLDIR=\"${HOME}/xdotool-logs\"\nTSTM=\"$(date +'%Y-%m-%d-%H-%M-%S')\"\nLOGF=\"${LDIR}/${TSTM}.txt\"\n\nmkdir -p \"${LDIR}\"\nwhile true; do\n  # Skip if screen is locked (KDE Plasma)\n  if qdbus org.freedesktop.ScreenSaver /ScreenSaver org.freedesktop.ScreenSaver.GetActive 2&gt;/dev/null |\n    grep -q true; then\n    continue\n  fi\n  wid=$(xdotool getwindowfocus -f)\n  if [[ -z \"${wid}\" ]]; then\n    continue\n  fi\n  wname=$(xdotool getwindowname \"${wid}\")\n  echo \"${wid} ${wname}\" &gt;&gt;\"${LOGF}\"\n  sleep 1\ndone\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#report-daily-time-per-focused-window","title":"<code>report-daily-time-per-focused-window</code>","text":"<pre><code>#!/bin/bash\n#\n# Analyze time spent per window for all days up to (and including) yesterday.\n# Produce output in TSV format; with .csv file names for easier import in office suites.\n\nLDIR=\"${HOME}/xdotool-logs\"\nmkdir -p \"${LDIR}/tsv\"\n\nrewrite_window_name() {\n  echo \"$1\" |\n    sed 's/ - Google Chrome//' |\n    sed 's/ - Google Docs//'\n}\n\nfirst_window_name() {\n  wid=$1\n  logf=$2\n  grep \"^${wid} \" \"${logf}\" |\n    sed \"s/^${wid} //\" |\n    grep -Ev '^Untitled - Google Chrome$|^New Tab - Google Chrome$|^New Incognito Tab$' |\n    head -1\n}\n\nmost_time_spent_window_name() {\n  wid=$1\n  logf=$2\n  grep \"^${wid} *\" \"${logf}\" |\n    sed \"s/^${wid} //\" |\n    grep -Ev '^Untitled - Google Chrome$|^New Tab - Google Chrome$|^New Incognito Tab$' |\n    sort | uniq -c | sort -nr | head -1 |\n    while read -r line; do\n      secs=$(echo \"${line}\" | grep -Eo '[0-9]+' | head -1)\n      title=$(echo \"${line}\" | sed \"s/${secs}//\")\n      echo \"${title}\"\n    done\n}\n\n# Find all dates (up to yesterday) missing TSV report.\ntoday=$(date +\"%Y-%m-%d\")\nfind \"${LDIR}\" -name \"*.txt\" |\n  grep -o '/2...-..-..' |\n  sed 's/\\///' |\n  grep -v \"${today}\" |\n  sort -u |\n  while read -r date; do\n    tsvout=\"${LDIR}/tsv/${date}.csv\"\n    # Avoid re-processing existing reports.\n    if [[ -f \"${tsvout}\" ]]; then\n      echo \"Report for ${date} already in ${tsvout}\"\n      continue\n    fi\n    # Produce output in TSV format, leaving the 'task' column blank.\n    echo -e \"Date\\tDuration\\tTask\\tOpening\\tMost time spent in\" &gt;\"${tsvout}\"\n    find \"${LDIR}\" -name \"${date}-*\" | while read -r logf; do\n      cut -f1 -d' ' \"${logf}\" | sort -u | grep -v '^ $' | grep -v '^$' | while read -r wid; do\n        secs=$(grep -c \"^${wid} \" \"${logf}\")\n        # Skip windows with less than 10 min.\n        if [[ \"${secs}\" -lt 600 ]]; then\n          continue\n        fi\n        # Find the first (not useless) title and the one with the most time.\n        first=$(rewrite_window_name \"$(first_window_name \"${wid}\" \"${logf}\")\")\n        mostt=$(rewrite_window_name \"$(most_time_spent_window_name \"${wid}\" \"${logf}\")\")\n        if [[ \"${first}\" == \"${mostt}\" ]]; then\n          mostt=\"\"\n        fi\n        hours=0\n        mins=0\n        if [[ ${secs} -ge 3600 ]]; then\n          hours=$((secs / 3600))\n          secs=$((secs - hours * 3600))\n        fi\n        if [[ ${secs} -ge 60 ]]; then\n          mins=$((secs / 60))\n          secs=$((secs - mins * 60))\n        fi\n        echo -e \"${date}\\t${hours}:${mins}:${secs}\\t\\t${first}\\t${mostt}\"\n      done\n    done | sort -nr -k 2 &gt;&gt;\"${tsvout}\"\n\n    echo \"Report for ${date} ready in ${tsvout}\"\n  done\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#desktop-timelapses","title":"Desktop Timelapses","text":"<p>Having a timelapse of what has been on screen can be useful for both the young artist and their responsible parents, in fact most likely useful to showcase the artist's process through a lenghty work of digital art than to the parents to gain detailed insights into any other activities.</p> <p>For either purpose, add <code>take-a-screenshot-every-second</code> and <code>make-all-timelapses-from-screenshots</code> as login scripts to capture a screenshot of the full screen and then make a timelapse video of each desktop session. This will cause a small CPU spike when logging in, but is not too noticeable since it only takes 2 or 3 CPU cores for a few minutes.</p> <p>Note</p> <p><code>make-all-timelapses-from-screenshots</code> depends on <code>make-one-timelapse-from-screenshots</code> and runs 4 instances of it in parallel.</p>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#take-a-screenshot-every-second","title":"<code>take-a-screenshot-every-second</code>","text":"<pre><code>#!/bin/bash\n\n# Do not run again if already running.\np=$(pidof -x \"$0\")\nn=$(echo \"${p}\" | wc -w)\nif [[ \"${n}\" -gt 1 ]]; then\n  echo \"Already running! (${p})\"\n  exit 0\nfi\n\n# Where to store screenshots. Create one directory per start time.\nSTART=$(date +\"%Y_%m_%d_%H_%M_%S\")\nBASEDIR=\"${HOME}/Videos/Desktop_Timelapse\"\nOUTDIR=\"${BASEDIR}/${START}\"\n\n# Write first to the shared memory filesystem (faster).\nTMPDIR=\"/dev/shm/${USER}/screenshot-every-second/${START}\"\nmkdir -p \"${TMPDIR}\" \"${OUTDIR}\"\nwhile true; do\n  # Take a screenshot, but only if the screen is not locked.\n  if dbus-send --session --dest=org.freedesktop.ScreenSaver --type=method_call --print-reply /org/freedesktop/ScreenSaver org.freedesktop.ScreenSaver.GetActive | grep -q 'boolean false'; then\n    scrot \"${TMPDIR}/%Y-%m-%d-%H-%M-%S.jpg\"\n  fi\n  # Delay for (nearly) a second. This is so that system load is not a problem.\n  sleep 0.9\n  # Save files to final destination every minute.\n  if [[ $(date +'%S' | sed 's/^0//') -eq 59 ]]; then\n    mv -f \"${TMPDIR}/*.jpg\" \"${OUTDIR}\"\n  fi\ndone\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#make-all-timelapses-from-screenshots","title":"<code>make-all-timelapses-from-screenshots</code>","text":"<pre><code>#!/bin/bash\nBASEDIR=${HOME}/Videos/Desktop_Timelapse\nCWD=$(dirname \"$0\")\n\n# Wait a bit in case a new directory is being created, because that is the one\n# that should be ignored below.\nsleep 2\n\n# Create a timelapse from the screenshot in each directory in BASEDIR.\n# Ignore the last one (presumed incomplete).\nlast=$(find \"${BASEDIR}\" -maxdepth 1 -type d -name \"20*\" | sort | tail -1)\nfind \"${BASEDIR}\" -maxdepth 1 -type d -name \"20*\" |\n  grep -v \"${last}\" |\n  xargs -P 4 -n 1 \"${CWD}/make-one-timelapse-from-screenshots\"\n\n# Clean up discarded timelapses (zero-size videos).\nfind \"${BASEDIR}\" -maxdepth 1 -size 0 | while read -r f; do\n  b=$(basename \"${f}\")\n  d=\"${b/.avi/}\"\n  td=\"${BASEDIR}/trash/${d}\"\n  test -d \"${td}\" &amp;&amp; rm -rf \"${td}\"\n  test -f \"${f}\" &amp;&amp; rm -f \"${f}\"\ndone\n</code></pre>"},{"location":"blog/2022/11/12/ubuntu-studio-2204-on-computer-for-a-young-artist/#make-one-timelapse-from-screenshots","title":"<code>make-one-timelapse-from-screenshots</code>","text":"<pre><code>#!/bin/bash\n\n# Create a timelapse from the screenshot in one directory.\n# At a framerate of 30 FPS; if screenshots are taken every second, 2 minutes of timelapse represents 1 hour.\n# Screenshtos must be all of the same pixel size, otherwise ffmpeg will fail.\nday=\"$1\"\ntrash=\"$(dirname \"${day}\")/trash\"\nmkdir -p \"${trash}\"\nffmpeg -vsync 0 -framerate 30 \\\n  -pattern_type glob -i \"${day}/*.jpg\" \\\n  -b:v 5M -c:v libx264 -preset ultrafast \\\n  -y \"${day}\".avi &amp;&amp; mv -f \"${day}\" \"${trash}\"\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/","title":"Single-node Kubernetes cluster on Ubuntu Server (lexicon)","text":"<p>After playing around with a few Docker containers and Docker compose, I decided it was time to dive into Kubernetes. But I only have one server: lexicon.</p> <p>For the most part I followed Computing for Geeks' article Install Kubernetes Cluster on Ubuntu 22.04 using kubeadm, while taking some bits from How to install Kubernetes on Ubuntu 22.04 Jammy Jellyfish Linux (from LinuxConfig) and How to Install Kubernetes Cluster on Ubuntu 22.04 (from LinuxTechi).</p>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#docker","title":"Docker","text":"<p>Since this was my first experience with containers, it all started by installing docker and playing around with it.</p> <p>Installing Docker is probably not required to the later installation of Kubernetes, but it did influence my journey.</p> <pre><code># apt update\n# apt install -y ca-certificates curl gnupg lsb-release\n# curl -fsSL https://download.docker.com/linux/ubuntu/gpg \\\n  | gpg --dearmor -o /usr/share/keyrings/docker.gpg\n# arch=$(dpkg --print-architecture)\n# signed_by=signed-by=/usr/share/keyrings/docker.gpg\n# url=https://download.docker.com/linux/ubuntu\n# echo \"deb [arch=$arch $signed_by] $url $(lsb_release -cs) stable\" \\\n  &gt; /etc/apt/sources.list.d/docker.list &gt; /dev/null\n# apt update\n# apt install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre> <p>To check the basic Docker components work:</p> <pre><code># docker run hello-world\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n2db29710123e: Pull complete \nDigest: sha256:aa0cc8055b82dc2509bed2e19b275c8f463506616377219d9642221ab53cf9fe\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#storage-requirements","title":"Storage Requirements","text":"<p>Docker images are store under <code>/var/lib/docker</code> by default, this may not be a good location if the root partition is not very big. In my case, most of the disk space is given to the <code>/home</code> partition, so that's where I moved Docker images.</p> <p>First stop the Docker runtime service and move the files:</p> <pre><code># systemctl stop docker.service\n# systemctl stop docker.socket\n# systemctl stop containerd.service\n# mkdir /home/lib\n# time cp -a /var/lib/docker/ /home/lib\n# find /home/lib/docker \\\n  -name config.v2.json \\\n  -exec sed -i 's:/var/lib/:/home/lib/:g' {} \\;\n# containerd config default | sed 's:var/lib:home/lib:' \\\n&gt; /etc/containerd/config.toml\n</code></pre> <p>Once files are moved, edit <code>/etc/docker/daemon.json</code> to point the Docker runtime service to the new location:</p> <pre><code>{\n  \"data-root\": \"/home/lib/docker\",\n  ...\n}\n</code></pre> <p>Finally, start the service again and check the change has taken effect:</p> <pre><code># systemctl start docker.socket\n# systemctl start docker.service\n# systemctl start containerd.service\n# docker info | grep 'Docker Root Dir'\n Docker Root Dir: /home/lib/docker\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#discard-unused-images","title":"Discard Unused Images","text":"<p>Over time, storage usage of <code>/home/lib/containerd</code> will grow with stale images. These can be cleaned up manually with <code>crictl rmi --prune</code> pointing it to the correct endpoint:</p> <pre><code># du -sh /home/lib/containerd/\n109G    /home/lib/containerd/\n# crictl \\\n  -r unix:///run/containerd/containerd.sock \\\n  rmi --prune\nDeleted: registry.k8s.io/pause:3.9\n# du -sh /home/lib/containerd/\n8.1G    /home/lib/containerd/\n</code></pre> <p>The same can be achieved on an on-going basis by adjusting the <code>ImageGCHighThresholdPercent</code> setting to trigger Kubernetes' built-in garbage collection:</p> <p>Setting this threshold involves updating the <code>KubeletConfiguration</code>:</p> <pre><code>$ kubectl edit cm -n kube-system kubelet-config\n</code></pre> <p>Find the <code>imageMinimumGCAge</code> variable and add the threshold for garbage collection under it; very carefully using the exact same blank characters for indentation (otherwise the YAML is invalid):</p> <pre><code>    imageMinimumGCAge: 0s\n    imageGCLowThresholdPercent: 65\n    imageGCHighThresholdPercent: 70\n</code></pre> <p>Run <code>kubeadm upgrade</code> as below to download the latest <code>kubelet-config</code> ConfigMap contents into the local file <code>/var/lib/kubelet/config.yaml</code> and restart the <code>kubelet</code> service:</p> <pre><code># kubeadm upgrade node phase kubelet-config\n[upgrade] Reading configuration from the cluster...\n[upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[upgrade] The configuration for this node was successfully updated!\n[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.\n\n# systemctl restart kubelet\n</code></pre> <p>Note</p> <p>This may not be effective when the images are not in the root partition.</p> <p>It also possible to set <code>discard_unpacked_layers: true</code> on the CRI plugin configuration (<code>/etc/containerd/config.toml</code>) so that it discards the compressed layer data after unpacking images. This data is only really needed if you are going to unpack the image again.</p>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#important-tweak-for-btrfs","title":"Important Tweak for BTRFS","text":"<p>Docker gradually exhausts disk space on BTRFS #27653. This was a known issue since October 2016, and was closed in August 2023 with no plan to fix:</p> <p>There are no improvements planned in this area (BTRFS graph driver). containerd BTRFS is where future improvements might go. The overlay2 graph driver or overlayfs snapshotter is recommended for all users.</p> <p>Docker still defaults to using its <code>btrfs</code> driver, so this should be changed early on to avoid problems.</p> <p>Warning: doing this later on will likely result in losing all Docker images. Ask Me How I Know.</p> <p>First stop the Docker runtime service and move the files:</p> <pre><code># systemctl stop docker.service\n# systemctl stop docker.socket\n# systemctl stop containerd.service\n</code></pre> <p>Once files are moved, edit <code>/etc/docker/daemon.json</code> to override storage driver as recommended:</p> <pre><code>{\n  ...\n  \"storage-driver\": \"overlay2\"\n}\n</code></pre> <p>Finally, start the service again and check the change has taken effect:</p> <pre><code># systemctl start docker.socket\n# systemctl start docker.service\n# systemctl start containerd.service\n# docker info | grep 'Storage Driver'\n Storage Driver: overlay2\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#ask-me-how-i-know","title":"Ask Me How I Know","text":"<p>I learned about the above issues the hard way, and it resulted in losing all Docker images.</p> <p>One day I found the root filesystem was nearly 100% full.</p> <p>Pruning didn\u2019t help much until I stopped all leftover clusters from previous exercises, and even then it only helped a bit:</p> <pre><code># docker image prune -a -f\n...\nTotal reclaimed space: 2.244GB\n</code></pre> <p>This reclaimed enough to bring <code>/var/lib/docker/btrfs/subvolumes</code> down from 49G to 8.7G; and the root filesystem went down to 78%. At this point there were only 2 images running: code-server and gitea. Stopped them, moved their volume\u2019s <code>_data</code> directories to another partition (under <code>/home/docker</code>), updated their <code>docker-compose</code> files and it was all good. Then stopped them again and removed all docker volumes, except the only one that was in use.</p> <p>With that, the root filesystem went down to 69%. Then used this trick to remove all the subvolumes not in use; and got the root filesystem down to 66%. But all that was only a temporary relief; the partition would quickly fill up again without overriding the storage driver to use <code>overlay2</code>.</p>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#kubernetes","title":"Kubernetes","text":"<p>The core components of Kubernetes can be installed via APT:</p> <pre><code># apt update\n# apt full-upgrade\n# curl -fsSLo \\\n  /etc/apt/keyrings/kubernetes-archive-keyring.gpg \\\n  https://packages.cloud.google.com/apt/doc/apt-key.gpg\n# echo \"deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" \\\n  &gt; /etc/apt/sources.list.d/kubernetes.list\n# apt-get update\n# apt-get install -y wget curl vim git kubelet kubeadm kubectl\n# kubectl version --output=yaml\nclientVersion:\n  buildDate: \"2023-03-15T13:40:17Z\"\n  compiler: gc\n  gitCommit: 9e644106593f3f4aa98f8a84b23db5fa378900bd\n  gitTreeState: clean\n  gitVersion: v1.26.3\n  goVersion: go1.19.7\n  major: \"1\"\n  minor: \"26\"\n  platform: linux/amd64\nkustomizeVersion: v4.5.7\n</code></pre> <p>The following command should be able to connect to the Kubernetes <code>etcd</code> service now; you might need to run this as root, or not:</p> <pre><code>$ kubeadm version\nkubeadm version: &amp;version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:56:50Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#networking-setup","title":"Networking Setup","text":"<p>The following steps were not necessary. Enabling IP forwarding and NAT is not necessary in Ubuntu 22.04 server, it is all already enabled by default apparently.</p> <pre><code># modprobe overlay\n# modprobe br_netfilter\n# tee /etc/sysctl.d/kubernetes.conf&lt;&lt;EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n# sysctl --system\n</code></pre> <p>The result, which can be checked upfront to confirm that the above steps are not necessary, can be checked with:</p> <pre><code># lsmod | egrep 'overlay|bridge'\nbridge                307200  1 br_netfilter\nstp                    16384  1 bridge\nllc                    16384  2 bridge,stp\noverlay               151552  0\n# sysctl -a | egrep 'net.ipv4.ip_forward |net.bridge.bridge-nf-call-ip'\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#migration-to-containerd","title":"Migration to <code>containerd</code>","text":"<p>Normally, the next step would be to install a container runtime, with the options being *  Docker, the original one. *  CRI-O, a lightweight option. *  containerd, an industry-standard.</p> <p>Docker and containerd were already installed as part of the above first steps with Docker, Dockershim was deprecated in Kubernetes 1.24 and the above step installed Kubernetes 1.26.</p> <p>If we were already running Kubernetes (<code>kubelet</code>) with Docker CE, at this point we\u2019d have to point <code>kubelet</code> to containerd by updating the flags in <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> or  <code>/var/lib/kubelet/kubeadm-flags.env</code> to</p> <pre><code>--container-runtime=remote \n--container-runtimeendpoint=unix:///run/containerd/containerd.sock\"\n</code></pre> <p>But since we\u2019re starting from scratch, we\u2019ll do that from <code>kubeadm</code> later:</p> <pre><code># systemctl enable kubelet\n# systemctl status kubelet\n\u25cf kubelet.service - kubelet: The Kubernetes Node Agent\n     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)\n    Drop-In: /etc/systemd/system/kubelet.service.d\n             \u2514\u250010-kubeadm.conf\n     Active: activating (auto-restart) (Result: exit-code) since Wed 2023-03-22 22:28:17 CET; 8s ago\n\n# kubeadm config images pull \\\n  --cri-socket unix:/run/containerd/containerd.sock\n[config/images] Pulled registry.k8s.io/kube-apiserver:v1.26.3\n[config/images] Pulled registry.k8s.io/kube-controller-manager:v1.26.3\n[config/images] Pulled registry.k8s.io/kube-scheduler:v1.26.3\n[config/images] Pulled registry.k8s.io/kube-proxy:v1.26.3\n[config/images] Pulled registry.k8s.io/pause:3.9\n[config/images] Pulled registry.k8s.io/etcd:3.5.6-0\n[config/images] Pulled registry.k8s.io/coredns/coredns:v1.9.3\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#bootstrap","title":"Bootstrap","text":"<p>For a single-node cluster, bootstrap it in the simplest way:</p> <pre><code># kubeadm init \\\n  --pod-network-cidr=10.244.0.0/16 \\\n  --cri-socket unix:/run/containerd/containerd.sock\n[init] Using Kubernetes version: v1.26.3\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local lexicon] and IPs [10.96.0.1 10.0.0.6]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [lexicon localhost] and IPs [10.0.0.6 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [lexicon localhost] and IPs [10.0.0.6 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\n[apiclient] All control plane components are healthy after 6.505763 seconds\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Skipping phase. Please see --upload-certs\n[mark-control-plane] Marking the node lexicon as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n[mark-control-plane] Marking the node lexicon as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]\n[bootstrap-token] Using token: 8gotcn.nco65l9atfr0l77c\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 10.0.0.6:6443 --token 8gotcn.nco65l9atfr0l77c \\\n        --discovery-token-ca-cert-hash \\\n  sha256:40002a46e8b15a41883d4d35fa68cef6ff2203fe79095da867b56a090abafa1a\n</code></pre> <p>Confirm the flag is sent in <code>/var/lib/kubelet/kubeadm-flags.env</code> to use the desired container runtime:</p> <pre><code># grep container-runtime /var/lib/kubelet/kubeadm-flags.env\nKUBELET_KUBEADM_ARGS=\"--container-runtime-endpoint=unix:/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.9\"\n</code></pre> <p>Check the customer status; as <code>root</code> one can simply point <code>KUBECONFIG</code> to the cluster's <code>admin.conf</code>:</p> <pre><code># export KUBECONFIG=/etc/kubernetes/admin.conf\n# kubectl cluster-info\nKubernetes control plane is running at https://10.0.0.6:6443\nCoreDNS is running at https://10.0.0.6:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre> <p>To run <code>kubectl</code> as non-root, make a copy of that file under your own <code>~/.kube</code> directory:</p> <pre><code>$ mkdir $HOME/.kube\n$ sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config\n$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n$ ls -l $HOME/.kube/config\n-rw------- 1 coder coder 5659 Feb 19   2023 /home/coder/.kube/config\n\n$ kubectl cluster-info\nKubernetes control plane is running at https://10.0.0.6:6443\nCoreDNS is running at https://10.0.0.6:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#enable-systemd-cgroups","title":"Enable systemd cgroups","text":"<p>Containerd defauls to disabling systemd cgroups, which causes the cluster to go into a crash-loop, making <code>kubectl</code> fail:</p> <pre><code>E0322 22:45:47.847433 1825724 memcache.go:265] couldn't get current server API group list:\nGet \"https://10.0.0.6:6443/api?timeout=32s\": dial tcp 10.0.0.6:6443: connect: connection refused\n</code></pre> <p>In this scenario <code>etcd</code> will not be listening on port 6443 (check with <code>netstat -na</code>) and <code>journalctl</code> will show lots of errors, starting with:</p> <pre><code># journalctl -xe | egrep -i 'container|docker|kube|clust'\nMar 22 22:46:50 lexicon kubelet[1658461]: E0322 22:46:50.716943 1658461 pod_workers.go:965]\n\"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff:\n\\\"back-off 5m0s restarting failed container=kube-apiserver\npod=kube-apiserver-lexicon_kube-system(b003695d86b7fa8a74df14eff26934a5)\\\"\"\npod=\"kube-system/kube-apiserver-lexicon\" podUID=b003695d86b7fa8a74df14eff26934a5\n</code></pre> <p>This <code>CrashLoopBackOff</code> error is a sign of  containerd issue #6009. To fix the problem, the workaround is to set <code>SystemdCgroup = true</code> in <code>/etc/containerd/config.toml</code> and restart the cluster with</p> <pre><code># systemctl restart containerd\n# systemctl restart kubelet\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#network-plugin","title":"Network Plugin","text":"<p>A Kubernetes cluster needs a simple layer 3 network for nodes to communicate (even for a single-node cluster):</p> <pre><code>$ wget https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml\n\n$ kubectl apply -f kube-flannel.yml\nnamespace/kube-flannel created\nclusterrole.rbac.authorization.k8s.io/flannel created\nclusterrolebinding.rbac.authorization.k8s.io/flannel created\nserviceaccount/flannel created\nconfigmap/kube-flannel-cfg created\ndaemonset.apps/kube-flannel-ds created\n\n$ kubectl get pods -n kube-flannel\nNAME                    READY   STATUS    RESTARTS   AGE\nkube-flannel-ds-nrrg6   1/1     Running   0          22s\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#add-worker-nodes","title":"Add Worker Nodes","text":"<p>First, confirm the master node is ready:</p> <pre><code>$ kubectl get nodes -o wide\nNAME      STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nlexicon   Ready    control-plane   9d    v1.26.3   10.0.0.6      &lt;none&gt;        Ubuntu 22.04.2 LTS   5.15.0-69-generic   containerd://1.6.19\n</code></pre> <p>At this point, for a single-node cluster, the command provided by <code>kubeadm init</code> is used to add the same system as a worker:</p> <pre><code># kubeadm join 10.0.0.6:6443 --token 8gotcn.nco65l9atfr0l77c \\\n        --discovery-token-ca-cert-hash \\\n  sha256:40002a46e8b15a41883d4d35fa68cef6ff2203fe79095da867b56a090abafa1a\n[preflight] Running pre-flight checks\nerror execution phase preflight: [preflight] Some fatal errors occurred:\n        [ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists\n        [ERROR Port-10250]: Port 10250 is in use\n        [ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists\n[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`\nTo see the stack trace of this error execute with --v=5 or higher\n</code></pre> <p>Note</p> <p><code>kubeadm join</code> must be run as <code>root</code>.</p> <p>At first the <code>kubeadm join</code> above fails because a Kubernetes cluster is not supposed to have the same system work as both master and worker. This manifests as the node being tainted:</p> <pre><code>$ kubectl describe node lexicon\nName:               lexicon\nRoles:              control-plane\n...\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\n</code></pre> <p>For a single-node cluster the only option appears to be Control plane node isolation as recommended in  github.com/calebhailey/homelab/issues/3. After this, there is no taint:</p> <pre><code>$ kubectl taint nodes --all \\\n  node-role.kubernetes.io/control-plane-node/lexicon untainted\n\n$ kubectl describe node lexicon\nName:               lexicon\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=lexicon\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"32:ae:4c:6b:5d:6f\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.0.0.6\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 22 Mar 2023 22:37:43 +0100\nTaints:             &lt;none&gt;\n</code></pre> <p>At this point the Kubernetes cluster is up and running and can be tested deploying a test application:</p> <pre><code>$ kubectl apply -f https://k8s.io/examples/pods/commands.yaml\n\n$ kubectl events pods\nLAST SEEN   TYPE     REASON      OBJECT             MESSAGE\n44m         Normal   NodeReady   Node/lexicon       Node lexicon status is now: NodeReady\n27s         Normal   Scheduled   Pod/command-demo   Successfully assigned default/command-demo to lexicon\n27s         Normal   Pulling     Pod/command-demo   Pulling image \"debian\"\n21s         Normal   Pulled      Pod/command-demo   Successfully pulled image \"debian\" in 5.452640392s (5.452649743s including waiting)\n21s         Normal   Created     Pod/command-demo   Created container command-demo-container\n21s         Normal   Started     Pod/command-demo   Started container command-demo-container\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#metallb-load-balancer","title":"MetalLB Load Balancer","text":"<p>The MetalLB Load Balancer is going to be necessary for the Dashboard and future applications, to expose individual services via open ports on the server (<code>NodePort</code>) or virtual IP addresses.</p> <pre><code>$ wget \\\nhttps://raw.githubusercontent.com/metallb/metallb/v$MetalLB_RTAG/config/manifests/metallb-native.yaml\n\n$ kubectl apply -f metallb-native.yaml\nnamespace/metallb-system created\ncustomresourcedefinition.apiextensions.k8s.io/addresspools.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/communities.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io created\nserviceaccount/controller created\nserviceaccount/speaker created\nrole.rbac.authorization.k8s.io/controller created\nrole.rbac.authorization.k8s.io/pod-lister created\nclusterrole.rbac.authorization.k8s.io/metallb-system:controller created\nclusterrole.rbac.authorization.k8s.io/metallb-system:speaker created\nrolebinding.rbac.authorization.k8s.io/controller created\nrolebinding.rbac.authorization.k8s.io/pod-lister created\nclusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created\nclusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created\nsecret/webhook-server-cert created\nservice/webhook-service created\ndeployment.apps/controller created\ndaemonset.apps/speaker created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/metallb-webhook-configuration created\n</code></pre> <p>After a few seconds the deployment should have the controller and speaker running:</p> <pre><code>$ kubectl get all -n metallb-system \nNAME                              READY   STATUS    RESTARTS   AGE\npod/controller-68bf958bf9-kzcfg   1/1     Running   0          32s\npod/speaker-78bvh                 1/1     Running   0          32s\n\nNAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\nservice/webhook-service   ClusterIP   10.96.89.141   &lt;none&gt;        443/TCP   32s\n\nNAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\ndaemonset.apps/speaker   1         1         1       1            1           kubernetes.io/os=linux   32s\n\nNAME                         READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/controller   1/1     1            1           32s\n\nNAME                                    DESIRED   CURRENT   READY   AGE\nreplicaset.apps/controller-68bf958bf9   1         1         1       32s\n\n$ kubectl get pods -n metallb-system \nNAME                          READY   STATUS    RESTARTS   AGE\ncontroller-68bf958bf9-kzcfg   1/1     Running   0          92s\nspeaker-78bvh                 1/1     Running   0          92s\n</code></pre> <p>At this point the components are idle waiting for a working configuration; edit <code>ipaddress_pools.yaml</code> to set a range of IP addresses:</p> ipaddress_pools.yaml<pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: production\n  namespace: metallb-system\nspec:\n  addresses:\n  - 192.168.0.122-192.168.0.140\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: l2-advert\n  namespace: metallb-system\n</code></pre> <p>The range 192.168.0.122-192.168.0.140 is based on the local DHCP server being configured to lease 228 addresses starting with 192.168.0.2. The current active leases are reserved so they don\u2019t change, and the range 122-140 are just not leased so far. The reason to use IPs from the leased range is that the router only allows adding port forwarding rules for those. This range is intentionally on the same network range and subnet as the DHCP server so that no routing is required to reach MetalLB IP addresses.</p> <p>Note</p> <p>Layer 2 mode does not require the IPs to be bound to the network interfaces of your worker nodes. It works by responding to ARP requests on your local network directly, to give the machine\u2019s MAC address to clients.</p> <pre><code>$ kubectl apply -f ipaddress_pools.yaml \nipaddresspool.metallb.io/production created\nl2advertisement.metallb.io/l2-advert created\n\n$ kubectl get ipaddresspool.metallb.io -n metallb-system\nNAME         AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES\nproduction   true          false             [\"192.168.0.122-192.168.0.140\"]\n\n$ kubectl get l2advertisement.metallb.io -n metallb-system\nNAME        IPADDRESSPOOLS   IPADDRESSPOOL SELECTORS   INTERFACES\nl2-advert                                              \n\n$ kubectl describe ipaddresspool.metallb.io production -n metallb-system\nName:         production\nNamespace:    metallb-system\nAPI Version:  metallb.io/v1beta1\nKind:         IPAddressPool\nSpec:\n  Addresses:\n    192.168.0.122-192.168.0.140\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#test-metallb","title":"Test MetalLB","text":"<p>To test the load balancer with a demo web, create a  deployment with <code>web-app-demo.yaml</code> as follows:</p> web-app-demo.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: web\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\n  namespace: web\nspec:\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: httpd\n        image: httpd:alpine\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-server-service\n  namespace: web\nspec:\n  selector:\n    app: web\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n  type: LoadBalancer\n</code></pre> <pre><code>$ kubectl apply -f web-app-demo.yaml\n$ kubectl get all -n web\nNAME                              READY   STATUS    RESTARTS   AGE\npod/web-server-5879949fb7-4c6r9   1/1     Running   0          26s\n\nNAME                         TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)        AGE\nservice/web-server-service   LoadBalancer   10.111.32.131   192.168.0.122   80:32468/TCP   3s\n\nNAME                         READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/web-server   1/1     1            1           26s\n\nNAME                                    DESIRED   CURRENT   READY   AGE\nreplicaset.apps/web-server-5879949fb7   1         1         1       26s\n\n$ curl http://192.168.0.122/\n&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\n</code></pre> <p>This only works from other hosts in the local network when using an IP range in the same subnet, otherwise requests will time out.</p> <p>Also, this is not enough to make this IP\u2019s port 80 reachable from other networks. Adding a port forwarding rule in the router to redirect port 12080 externally to port 80 on 192.168.0.122  works in that requests are forwarded to the right port, but then requests are rejected.</p>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#ingress-controller","title":"Ingress Controller","text":"<p>In addition to the load balancer, I also wanted to deploy Nginx Ingress Controller to redirect HTTPS requests to different services.</p> <pre><code>$ controller_tag=$(curl -s https://api.github.com/repos/kubernetes/ingress-nginx/releases/latest | grep tag_name | cut -d '\"' -f 4)\n$ wget -O nginx-ingress-controller-deploy.yaml \\\n  https://raw.githubusercontent.com/kubernetes/ingress-nginx/${controller_tag}/deploy/static/provider/baremetal/deploy.yaml\n$ sed -i 's/type: NodePort/type: LoadBalancer/g' nginx-ingress-controller-deploy.yaml\n</code></pre> <p>Note</p> <p>Since we already have MetalLB configured with a range of IP addresses, change line 365 in <code>nginx-ingress-controller-deploy.yaml</code> to <code>type: LoadBalancer</code> so that it gets an External IP.</p> <pre><code>$ kubectl apply -f nginx-ingress-controller-deploy.yaml\nnamespace/ingress-nginx created\nserviceaccount/ingress-nginx created\nserviceaccount/ingress-nginx-admission created\nrole.rbac.authorization.k8s.io/ingress-nginx created\nrole.rbac.authorization.k8s.io/ingress-nginx-admission created\nclusterrole.rbac.authorization.k8s.io/ingress-nginx created\nclusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created\nrolebinding.rbac.authorization.k8s.io/ingress-nginx created\nrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\nconfigmap/ingress-nginx-controller created\nservice/ingress-nginx-controller created\nservice/ingress-nginx-controller-admission created\ndeployment.apps/ingress-nginx-controller created\njob.batch/ingress-nginx-admission-create created\njob.batch/ingress-nginx-admission-patch created\ningressclass.networking.k8s.io/nginx created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created\n\n$ kubectl get all -n ingress-nginx\nNAME                                            READY   STATUS      RESTARTS   AGE\npod/ingress-nginx-admission-create-wjzv4        0/1     Completed   0          107s\npod/ingress-nginx-admission-patch-lcj2j         0/1     Completed   1          107s\npod/ingress-nginx-controller-6b58ffdc97-2k2hk   1/1     Running     0          107s\n\nNAME                                         TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                      AGE\nservice/ingress-nginx-controller             LoadBalancer   10.100.229.186   192.168.0.122   80:31137/TCP,443:31838/TCP   6s\nservice/ingress-nginx-controller-admission   ClusterIP      10.99.206.192    &lt;none&gt;          443/TCP                      6s\n\nNAME                                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/ingress-nginx-controller   1/1     1            1           107s\n\nNAME                                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/ingress-nginx-controller-6b58ffdc97   1         1         1       107s\n\nNAME                                       COMPLETIONS   DURATION   AGE\njob.batch/ingress-nginx-admission-create   1/1           7s         107s\njob.batch/ingress-nginx-admission-patch    1/1           8s         107s\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#dashboard","title":"Dashboard","text":"<p>Installing the Kubernetes Dashboard</p> <p>Download the manifest and change spec.type to LoadBalancer so it gets an External IP from MetalLB:</p> <pre><code>$ VER=$(curl -s https://api.github.com/repos/kubernetes/dashboard/releases/latest|grep tag_name|cut -d '\"' -f 4)\n$ echo $VER\nv2.7.0\n$ wget -O kubernetes-dashboard.yaml \\\n  https://raw.githubusercontent.com/kubernetes/dashboard/$VER/aio/deploy/recommended.yaml \n</code></pre> <p>To make the dashboard easily available in the local network, edit <code>kubernetes-dashboard.yaml</code> (around line 40) to set the service to <code>LoadBalancer</code>:</p> kubernetes-dashboard.yaml<pre><code>  namespace: kubernetes-dashboard\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 443\n</code></pre> <p>Then deploy the Dashboard:</p> <pre><code>$ kubectl apply -f kubernetes-dashboard.yaml\nnamespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n\n$ kubectl get all -n kubernetes-dashboard\nNAME                                            READY   STATUS    RESTARTS   AGE\npod/dashboard-metrics-scraper-7bc864c59-6k5tm   1/1     Running   0          33s\npod/kubernetes-dashboard-6c7ccbcf87-qd5cg       1/1     Running   0          33s\n\nNAME                                TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)         AGE\nservice/dashboard-metrics-scraper   ClusterIP      10.98.87.20     &lt;none&gt;          8000/TCP        33s\nservice/kubernetes-dashboard        LoadBalancer   10.99.222.155   192.168.0.123   443:31490/TCP   33s\n\nNAME                                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/dashboard-metrics-scraper   1/1     1            1           33s\ndeployment.apps/kubernetes-dashboard        1/1     1            1           33s\n\nNAME                                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/dashboard-metrics-scraper-7bc864c59   1         1         1       33s\nreplicaset.apps/kubernetes-dashboard-6c7ccbcf87       1         1         1       33s\n</code></pre> <p>At this the dashboard is available at https://192.168.0.123/</p>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#localpath-pv-provisioner","title":"LocalPath PV provisioner","text":"<p>By default, a Kubernetes cluster is not set up to provide storage to pods. The recommended way is to use dynamic volume provisioning, which is not currently enabled: <code>kube-apiserver</code> is running with only <code>--enable-admission-plugins=NodeRestriction</code> which means we need to enable the <code>DefaultStorageClass</code> admission controller on the API server, by updating this flag in line 20 of <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code></p> <p>Next, we need to create persistent volumes and mark a storage class as default. Gitlab recommends setting <code>reclaimPolicy</code> to <code>Retain</code>.</p> <p>The name of a <code>StorageClass</code> object is significant, and is how users can request a particular class. Administrators set the name and other parameters of a class when first creating <code>StorageClass</code> objects, and the objects cannot be updated once they are created.</p> <p>To use Local volumes, will support <code>WaitForFirstConsumer</code> with pre-created <code>PersistentVolume</code> binding, but not dynamic provisioning.</p> <p>For an easy way, we use the Local Path Provisioner from rancher.io with <code>/home/k8s/local-path-storage</code></p> <p>To add a default storage class, I followed Computing for Geeks' article Dynamic hostPath PV Creation in Kubernetes using Local Path Provisioner.</p> <p>First, the cluster needs to be enabled for dynamic volume provisioning, which means making sure <code>kube-apiserver</code> is running with <code>--enable-admission-plugins=DefaultStorageClass</code> to enable the <code>DefaultStorageClass</code> admission controller, by updating this flag in line 20 of  <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> and restarting the <code>kubelet</code> service:</p> kube-apiserver.yaml<pre><code>spec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --advertise-address=10.0.0.6\n    - --allow-privileged=true\n    - --authorization-mode=Node,RBAC\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --enable-admission-plugins=NodeRestriction,DefaultStorageClass\n</code></pre> <pre><code># systemctl restart kubelet.service\n</code></pre> <p>Create a directory in the a file system where there is plenty of space:</p> <pre><code># mkdir /home/k8s/local-path-storage\n# chmod 1777 /home/k8s/local-path-storage\n</code></pre> <p>Create a deployment with Rancher\u2019s PV provisioner, with the annotation to make it the default storage class and create a deployment, e.g. <code>local-path-storage-as-default-class.yaml</code></p> Kubernetes deployment: <code>local-path-storage-as-default-class.yaml</code> local-path-storage-as-default-class.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: local-path-storage\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: local-path-provisioner-service-account\n  namespace: local-path-storage\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: local-path-provisioner-role\nrules:\n  - apiGroups: [ \"\" ]\n    resources: [ \"nodes\", \"persistentvolumeclaims\", \"configmaps\" ]\n    verbs: [ \"get\", \"list\", \"watch\" ]\n  - apiGroups: [ \"\" ]\n    resources: [ \"endpoints\", \"persistentvolumes\", \"pods\" ]\n    verbs: [ \"*\" ]\n  - apiGroups: [ \"\" ]\n    resources: [ \"events\" ]\n    verbs: [ \"create\", \"patch\" ]\n  - apiGroups: [ \"storage.k8s.io\" ]\n    resources: [ \"storageclasses\" ]\n    verbs: [ \"get\", \"list\", \"watch\" ]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: local-path-provisioner-bind\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: local-path-provisioner-role\nsubjects:\n  - kind: ServiceAccount\n    name: local-path-provisioner-service-account\n    namespace: local-path-storage\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: local-path-provisioner\n  namespace: local-path-storage\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: local-path-provisioner\n  template:\n    metadata:\n      labels:\n        app: local-path-provisioner\n    spec:\n      serviceAccountName: local-path-provisioner-service-account\n      containers:\n        - name: local-path-provisioner\n          image: rancher/local-path-provisioner:master-head\n          imagePullPolicy: IfNotPresent\n          command:\n            - local-path-provisioner\n            - --debug\n            - start\n            - --config\n            - /etc/config/config.json\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config/\n          env:\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n      volumes:\n        - name: config-volume\n          configMap:\n            name: local-path-config\n\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: local-path\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\nprovisioner: rancher.io/local-path\nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\nreclaimPolicy: Retain\n\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: local-path-config\n  namespace: local-path-storage\ndata:\n  config.json: |-\n    {\n            \"nodePathMap\":[\n            {\n                    \"node\":\"DEFAULT_PATH_FOR_NON_LISTED_NODES\",\n                    \"paths\":[\"/home/k8s/local-path-storage\"]\n            }\n            ]\n    }\n  setup: |-\n    #!/bin/sh\n    set -eu\n    mkdir -m 0777 -p \"$VOL_DIR\"\n  teardown: |-\n    #!/bin/sh\n    set -eu\n    rm -rf \"$VOL_DIR\"\n  helperPod.yaml: |-\n    apiVersion: v1\n    kind: Pod\n    metadata:\n      name: helper-pod\n    spec:\n      containers:\n      - name: helper-pod\n        image: busybox\n        imagePullPolicy: IfNotPresent\n</code></pre> <pre><code>$ kubectl apply -f \\\n  local-path-storage-as-default-class.yaml\nnamespace/local-path-storage created\nserviceaccount/local-path-provisioner-service-account created\nclusterrole.rbac.authorization.k8s.io/local-path-provisioner-role created\nclusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind created\ndeployment.apps/local-path-provisioner created\nstorageclass.storage.k8s.io/local-path created\nconfigmap/local-path-config created\n\n$ kubectl get storageclass \nNAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nlocal-path (default)   rancher.io/local-path   Retain          WaitForFirstConsumer   true                   11s\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#https-with-lets-encrypt","title":"HTTPS with Let's Encrypt","text":"<p>Enabling HTTPS and configuring SSL certificates with Let's Encrypt requires running Certbot to confirm/certify control of the web host. Since nothing is listening on port 80, this should be pretty easy. The certificate should be for a specific subdomain (e.g. <code>ssl.uu.am</code>) which will be mapped to the IP, then other subdomains redirect to specific ports.</p>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#host-os-setup","title":"Host OS setup","text":"<p>Install certbot and request a certificate for <code>ssl.uu.am</code></p> <pre><code># apt install -y certbot\n# certbot certonly --standalone -d ssl.uu.am\nSaving debug log to /var/log/letsencrypt/letsencrypt.log\nEnter email address (used for urgent renewal and security notices)\n (Enter 'c' to cancel): root@uu.am\n...\n\n# ls -l /etc/letsencrypt/live/ssl.uu.am/\ntotal 20\nlrwxrwxrwx 1 root root  38 Feb 13 22:00 cert.pem -&gt; ../../archive/ssl.uu.am/cert1.pem\nlrwxrwxrwx 1 root root  39 Feb 13 22:00 chain.pem -&gt; ../../archive/ssl.uu.am/chain1.pem\nlrwxrwxrwx 1 root root  43 Feb 13 22:00 fullchain.pem -&gt; ../../archive/ssl.uu.am/fullchain1.pem\nlrwxrwxrwx 1 root root  41 Feb 13 22:00 privkey.pem -&gt; ../../archive/ssl.uu.am/privkey1.pem\n-rw-r--r-- 1 root root 692 Feb 13 22:00 README\n\n# cat /etc/letsencrypt/live/ssl.uu.am/README\nThis directory contains your keys and certificates.\n\n`privkey.pem`  : the private key for your certificate.\n`fullchain.pem`: the certificate file used in most server software.\n`chain.pem`    : used for OCSP stapling in Nginx &gt;=1.3.7.\n`cert.pem`     : will break many server configurations, and should not be used\n                 without reading further documentation (see link below).\n\nWARNING: DO NOT MOVE OR RENAME THESE FILES!\n         Certbot expects these files to remain in this location in order\n         to function properly!\n\nWe recommend not moving these files. For more information, see the Certbot\nUser Guide at https://certbot.eff.org/docs/using.html#where-are-my-certificates.\n</code></pre> <p>Actual files are in <code>/etc/letsencrypt/archive/ssl.uu.am/</code> but have numbered names (e.g. cert1.pem) while the links in <code>/etc/letsencrypt/live/ssl.uu.am/</code> will have constant file names (e.g. <code>cert.pem</code>).</p>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#renewal-automated","title":"Renewal (automated)","text":"<p>The <code>certbot</code> package installs a service and a timer to renew certificates every 30 days, trying 2x daily:</p> <pre><code># systemctl status certbot.timer \n\u25cf certbot.timer - Run certbot twice daily\n     Loaded: loaded (/lib/systemd/system/certbot.timer; enabled; vendor preset: enabled)\n     Active: active (running) since Sun 2023-04-16 13:42:11 CEST; 2s ago\n    Trigger: n/a\n   Triggers: \u25cf certbot.service\n\nApr 16 13:42:11 lexicon systemd[1]: Stopped Run certbot twice daily.\nApr 16 13:42:11 lexicon systemd[1]: Stopping Run certbot twice daily...\nApr 16 13:42:11 lexicon systemd[1]: Started Run certbot twice daily.\n\n# systemctl status certbot.service \n\u25cf certbot.service - Certbot\n     Loaded: loaded (/lib/systemd/system/certbot.service; static)\n     Active: activating (start) since Sun 2023-04-16 13:42:11 CEST; 10s ago\nTriggeredBy: \u25cf certbot.timer\n       Docs: file:///usr/share/doc/python-certbot-doc/html/index.html\n             https://certbot.eff.org/docs\n   Main PID: 2015732 (certbot)\n      Tasks: 1 (limit: 37940)\n     Memory: 32.2M\n        CPU: 256ms\n     CGroup: /system.slice/certbot.service\n             \u2514\u25002015732 /usr/bin/python3 /usr/bin/certbot -q renew\n\nApr 16 13:42:11 lexicon systemd[1]: Starting Certbot...\n</code></pre> <p>For this to work long-term, the external port 80 must be forwarded to the hosts\u2019 IP. If this forward is removed at some point (e.g. forwarded to another IP), the renewal will fail:</p> <pre><code># systemctl status certbot.service \n\u00d7 certbot.service - Certbot\n     Loaded: loaded (/lib/systemd/system/certbot.service; static)\n     Active: failed (Result: exit-code) since Sun 2023-04-16 04:53:14 CEST; 8h ago\nTriggeredBy: \u25cf certbot.timer\n       Docs: file:///usr/share/doc/python-certbot-doc/html/index.html\n             https://certbot.eff.org/docs\n    Process: 279698 ExecStart=/usr/bin/certbot -q renew (code=exited, status=1/FAILURE)\n   Main PID: 279698 (code=exited, status=1/FAILURE)\n        CPU: 632ms\n\nApr 16 04:50:22 lexicon systemd[1]: Starting Certbot...\nApr 16 04:53:14 lexicon certbot[279698]: Failed to renew certificate ssl.uu.am with error: Some challenges have&gt;\nApr 16 04:53:14 lexicon certbot[279698]: All renewals failed. The following certificates could not be renewed:\nApr 16 04:53:14 lexicon certbot[279698]:   /etc/letsencrypt/live/ssl.uu.am/fullchain.pem (failure)\nApr 16 04:53:14 lexicon certbot[279698]: 1 renew failure(s), 0 parse failure(s)\nApr 16 04:53:14 lexicon systemd[1]: certbot.service: Main process exited, code=exited, status=1/FAILURE\nApr 16 04:53:14 lexicon systemd[1]: certbot.service: Failed with result 'exit-code'.\nApr 16 04:53:14 lexicon systemd[1]: Failed to start Certbot.\n</code></pre> <p>To fix this, restore the port forwarding and restart the timer (not the service):</p> <pre><code># systemctl restart certbot.timer \n# systemctl status certbot.timer \n\u25cf certbot.timer - Run certbot twice daily\n     Loaded: loaded (/lib/systemd/system/certbot.timer; enabled; vendor preset: enabled)\n     Active: active (running) since Sun 2023-04-16 13:42:11 CEST; 2s ago\n    Trigger: n/a\n   Triggers: \u25cf certbot.service\n</code></pre> <p>After a while (random delay under 10 minutes) there should be a lot of activity logged in <code>/var/log/letsencrypt/letsencrypt.log</code> ending with</p> <pre><code>2023-04-16 13:49:44,607:DEBUG:certbot._internal.renewal:no renewal failures\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#kubernetes-setup","title":"Kubernetes setup","text":"<p>Having a successful setup to Enable HTTPS with Let's Encrypt, the question is how to set up Nginx reverse proxy.</p> <p>We have an Ingress Controller already configured with a MetalLB IP 192.168.0.122 listening on ports 80 and 443. We can forward external port 443 to this IP so we have the same page/s on (not yet secure) https://192.168.0.121/ and https://ssl.uu.am/ (just Nginx 404 now).</p> <p>Need to make 2 big changes to the Nginx controller:</p> <ol> <li>Install Let\u2019s Encrypt certificate as a secret</li> <li>Add Ingress for the first pod</li> </ol>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#install-lets-encrypt-certificate-as-a-secret","title":"Install Let\u2019s Encrypt certificate as a secret","text":"<p>Need to add a secret in the <code>ingress-nginx</code> namespace with the Let\u2019s Encrypt certificate. Otherwise, if nothing else is found, Nginx will use the default \"Kubernetes Ingress Controller Fake Certificate\".</p> <p>How to add the certificate as a secret?</p> <p>How to Install Kubernetes Cert-Manager and Configure Let\u2019s Encrypt is based on Securing NGINX-ingress which we could follow starting from Step 5 - Deploy Cert Manager by installing directly in Kubernetes (with or without Helm). Install the latest version of cert-manager using Helm:</p> <pre><code>$ helm repo add jetstack https://charts.jetstack.io\n$ helm repo update\n$ helm install cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.11.0 \\\n  --set installCRDs=true\nNAME: cert-manager\nLAST DEPLOYED: Sun Apr 16 15:25:25 2023\nNAMESPACE: cert-manager\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\ncert-manager v1.11.0 has been deployed successfully!\n\nIn order to begin issuing certificates, you will need to set up a ClusterIssuer\nor Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).\n\nMore information on the different types of issuers and how to configure them\ncan be found in our documentation:\n\nhttps://cert-manager.io/docs/configuration/\n\nFor information on how to configure cert-manager to automatically provision\nCertificates for Ingress resources, take a look at the `ingress-shim`\ndocumentation:\n\nhttps://cert-manager.io/docs/usage/ingress/\n\n$ kubectl get all -n cert-manager\nNAME                                           READY   STATUS    RESTARTS   AGE\npod/cert-manager-64f9f45d6f-qx4hs              1/1     Running   0          4m29s\npod/cert-manager-cainjector-56bbdd5c47-ltjgx   1/1     Running   0          4m29s\npod/cert-manager-webhook-d4f4545d7-tf4l6       1/1     Running   0          4m29s\n\nNAME                           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/cert-manager           ClusterIP   10.110.150.206   &lt;none&gt;        9402/TCP   4m29s\nservice/cert-manager-webhook   ClusterIP   10.107.245.49    &lt;none&gt;        443/TCP    4m29s\n\nNAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/cert-manager              1/1     1            1           4m29s\ndeployment.apps/cert-manager-cainjector   1/1     1            1           4m29s\ndeployment.apps/cert-manager-webhook      1/1     1            1           4m29s\n\nNAME                                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/cert-manager-64f9f45d6f              1         1         1       4m29s\nreplicaset.apps/cert-manager-cainjector-56bbdd5c47   1         1         1       4m29s\nreplicaset.apps/cert-manager-webhook-d4f4545d7       1         1         1       4m29s\n</code></pre> <p>Everything seems to be running fine, but just in case verify the installation by creating a test cert.</p> <p>Assuming that works, install the Kubectl plugin; get the latest release from github.com/cert-manager/cert-manager to match the Helm chart installed.</p> <pre><code>$ curl -L -o kubectl-cert-manager.tar.gz \\\n  https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/kubectl-cert_manager-linux-amd64.tar.gz\n$ tar xfz kubectl-cert-manager.tar.gz \n$ sudo install -m 755 kubectl-cert_manager /usr/local/bin/\n$ kubectl cert-manager check api\nThe cert-manager API is ready\n</code></pre> <p>Create a <code>ClusterIssuer</code> which applies across all Ingress resources in the cluster, by deploying the following <code>cert-manager-issuer.yml</code></p> cert-manager-issuer.yml<pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: root@uu.ma\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n      - http01:\n          ingress:\n            class: nginx\n</code></pre> <pre><code>$ kubectl create -f cert-manager-issuer.yml\nclusterissuer.cert-manager.io/letsencrypt-prod created\n$ kubectl describe clusterissuer.cert-manager.io/letsencrypt-prod\nName:         letsencrypt-prod\nNamespace:    \nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         ClusterIssuer\nMetadata:\n  Creation Timestamp:  2023-04-16T14:11:19Z\n  Generation:          1\n  Resource Version:    3171554\n  UID:                 2176e44f-421a-498c-935b-34e71a21cae1\nSpec:\n  Acme:\n    Email:            root@uu.ma\n    Preferred Chain:  \n    Private Key Secret Ref:\n      Name:  letsencrypt-prod\n    Server:  https://acme-v02.api.letsencrypt.org/directory\n    Solvers:\n      http01:\n        Ingress:\n          Class:  nginx\nStatus:\n  Acme:\n    Last Registered Email:  root@uu.ma\n    Uri:                    https://acme-v02.api.letsencrypt.org/acme/acct/1063900577\n  Conditions:\n    Last Transition Time:  2023-04-16T14:11:20Z\n    Message:               The ACME account was registered with the ACME server\n    Observed Generation:   1\n    Reason:                ACMEAccountRegistered\n    Status:                True\n    Type:                  Ready\n</code></pre>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#add-ingress-for-the-first-pod","title":"Add Ingress for the first pod","text":"<p>Now that we have <code>cert-manager</code> running in the cluster, lets see how to update Ingress resources to request a production certificate. Do this by changing the value of the <code>cert-manager.io/cluster-issuer</code> annotation to <code>letsencrypt-prod</code> (the <code>ClusterIssuer</code> above).</p> <p>The example that follows shows the rather bumpy journe of adding HTTPS to a pod running Gitea.</p> <p>One way to do this would be using kubectl to apply the change in Gitea's own Ingress (<code>vi gitea-ingress.yml</code>):</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: gitea-ingress\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: \"git.ssl.uu.am\"\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: gitea-http\n                port:\n                  number: 3000\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - \"git.ssl.uu.am\"\n</code></pre> <pre><code>$ kubectl -n gitea apply -f gitea-ingress.yml\n</code></pre> <p>Better yet, it should be possible to incorporate this into values passed to the Gitea Helm chart, applying the changes to <code>gitea-values.yaml</code> as follows:</p> <pre><code>ingress:\n  enabled: true\n  className: nginx\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    kubernetes.io/tls-acme: \"true\"\n  hosts:\n    - host: git.ssl.uu.am\n      paths:\n        - path: /\n          pathType: Prefix\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - \"git.ssl.uu.am\"\n</code></pre> <pre><code>$ helm install gitea gitea-charts/gitea \\\n  --create-namespace \\\n  --namespace=gitea \\\n  --values=gitea-values.yaml \\\n  --dry-run --debug\n# Source: gitea/templates/gitea/ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: gitea\n  labels:\n    helm.sh/chart: gitea-8.1.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: gitea\n    app.kubernetes.io/version: \"1.19.1\"\n    version: \"1.19.1\"\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    kubernetes.io/ingress.class: nginx\n    kubernetes.io/tls-acme: \"true\"\nspec:\n  rules:\n    - host: \"git.ssl.uu.am\"\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: gitea-http\n                port:\n                  number: 3000\n\n$ helm install gitea gitea-charts/gitea \\\n  --create-namespace \\\n  --namespace=gitea \\\n  --values=gitea-values.yaml \ncoalesce.go:175: warning: skipped value for memcached.initContainers: Not a table.\nNAME: gitea\nLAST DEPLOYED: Sun Apr 16 16:32:07 2023\nNAMESPACE: gitea\nSTATUS: deployed\nREVISION: 1\nNOTES:\n1. Get the application URL by running these commands:\n  http://git.ssl.uu.am/\n\n$ kubectl get ingress -A\nNAMESPACE   NAME    CLASS    HOSTS                ADDRESS    PORTS   AGE\ngitea       gitea   &lt;none&gt;   git.ssl.uu.am   10.0.0.6   80      20m\n\n$ kubectl get all -n gitea\nNAME                                   READY   STATUS    RESTARTS   AGE\npod/gitea-0                            1/1     Running   0          10m\npod/gitea-memcached-6559f55668-s8nl6   1/1     Running   0          10m\npod/gitea-postgresql-0                 1/1     Running   0          10m\n\nNAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nservice/gitea-http            NodePort    10.100.158.203   &lt;none&gt;        3000:30080/TCP   10m\nservice/gitea-memcached       ClusterIP   10.111.252.194   &lt;none&gt;        11211/TCP        10m\nservice/gitea-postgresql      ClusterIP   10.96.77.223     &lt;none&gt;        5432/TCP         10m\nservice/gitea-postgresql-hl   ClusterIP   None             &lt;none&gt;        5432/TCP         10m\nservice/gitea-ssh             NodePort    10.105.190.218   &lt;none&gt;        22:30022/TCP     10m\n\nNAME                              READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/gitea-memcached   1/1     1            1           10m\n\nNAME                                         DESIRED   CURRENT   READY   AGE\nreplicaset.apps/gitea-memcached-6559f55668   1         1         1       10m\n\nNAME                                READY   AGE\nstatefulset.apps/gitea              1/1     10m\nstatefulset.apps/gitea-postgresql   1/1     10m\n</code></pre> <p>This doesn\u2019t work: Ingress gitea doesn\u2019t have a class and points to port 80 (wrong, should be 3000) Nginx on (external and node\u2019s) port 443 hasn\u2019t changed (404 Not Found, fake cert).</p> <p>Maybe the way to use this is to have Nginx (somewhere else) point to this service\u2019s port 80?</p> <p>While we have Gitea already running, we can try using kubectl to apply this change to <code>gitea-ingress.yml</code>:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: gitea-ingress\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: \"git.ssl.uu.am\"\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: gitea-http\n                port:\n                  number: 3000\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - \"git.ssl.uu.am\"\n</code></pre> <p>But this still doesn't work:</p> <pre><code>$ kubectl -n gitea apply -f gitea-ingress.yml\nError from server (BadRequest): error when creating \"gitea-ingress.yml\": admission webhook \"validate.nginx.ingress.kubernetes.io\" denied the request: host \"git.ssl.uu.am\" and path \"/\" is already defined in ingress gitea/gitea\n</code></pre> <p>So we have to not try to add this directly to the Helm chart, to avoid this conflict. Once that\u2019s removed, and Gitea reinstall (w/ Helm), we\u2019re back at having an Ingress entity jus like the one above:</p> <pre><code>$ kubectl get ingress -A\nNAMESPACE   NAME            CLASS   HOSTS                ADDRESS    PORTS   AGE\ngitea       gitea-ingress   nginx   git.ssl.uu.am   10.0.0.6   80      56m\n</code></pre> <p>Note</p> <p>Creating <code>gitea-ingress</code> in the <code>ingress-nginx</code> namespace won\u2019t work because then Nginx can\u2019t find the gitea-http service, resulting in HTTP 503 error at https://git.ssl.uu.am</p> <pre><code>W0416 15:55:11.088261       7 controller.go:1044] Error obtaining Endpoints for Service \"ingress-nginx/gitea-http\": no object matching key \"ingress-nginx/gitea-http\" in local store\n</code></pre> <p>At this point it should be possible to hit the external IP on port 443, which is forwarded to the MetalLB IP, and get the Gitea home page at https://git.ssl.uu.am</p> <p>That partially works: we get to the Gitea service, but still use the fake cert.</p> <pre><code>$ kubectl -n ingress-nginx get pods | grep ingress-nginx-controller\ningress-nginx-controller-6b58ffdc97-rt5lm   1/1     Running     1 (4d12h ago)   14d\n\n$ kubectl -n ingress-nginx logs ingress-nginx-controller-6b58ffdc97-rt5lm | tail -10\n2023/04/16 16:08:53 [crit] 4655#4655: *6295724 SSL_do_handshake() failed (SSL: error:0A00006C:SSL routines::bad key share) while SSL handshaking, client: 10.244.0.1, server: 0.0.0.0:443\n10.244.0.1 - - [16/Apr/2023:16:49:42 +0000] \"GET / HTTP/2.0\" 200 13744 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\" 464 0.004 [gitea-gitea-http-3000] [] 10.244.0.252:3000 13757 0.003 200 1df33130aa3f23144c62d7eb91650dcc\n</code></pre> <p>Not sure whether the SSL error there is caused by, or causing, the use of the fake cert.</p> <pre><code>$ kubectl -n gitea describe ingress gitea-ingress\nName:             gitea-ingress\nLabels:           &lt;none&gt;\nNamespace:        gitea\nAddress:          10.0.0.6\nIngress Class:    nginx\nDefault backend:  &lt;default&gt;\nRules:\n  Host                Path  Backends\n  ----                ----  --------\n  git.ssl.uu.am  \n                      /   gitea-http:3000 (10.244.0.252:3000)\nAnnotations:          cert-manager.io/cluster-issuer: letsencrypt-prod\nEvents:               &lt;none&gt;\n</code></pre> <p>docs.bitnami.com/kubernetes/.../secure-ingress-resources looks like perhaps the cert-manager.io/cluster-issuer: <code>letsencrypt-prod</code> annotation needs to go in the ingress.class instead of each ingress resource</p> <p>At this point, we the annotation to <code>nginx-ingress-controller-deploy.yaml</code> under line 601, and check the diff and apply:</p> <pre><code>  kind: IngressClass\n  metadata:\n    annotations:\n      cert-manager.io/cluster-issuer: letsencrypt-prod\n</code></pre> <pre><code>$ kubectl diff -f nginx-ingress-controller-deploy.yaml \n...\n kind: IngressClass\n metadata:\n   annotations:\n+    cert-manager.io/cluster-issuer: letsencrypt-prod\n...\n\n$ kubectl apply -f nginx-ingress-controller-deploy.yaml\n$ kubectl -n ingress-nginx describe ingressclass.networking.k8s.io/nginx\nName:         nginx\nLabels:       app.kubernetes.io/component=controller\n              app.kubernetes.io/instance=ingress-nginx\n              app.kubernetes.io/name=ingress-nginx\n              app.kubernetes.io/part-of=ingress-nginx\n              app.kubernetes.io/version=1.7.0\nAnnotations:  cert-manager.io/cluster-issuer: letsencrypt-prod\nController:   k8s.io/ingress-nginx\nEvents:       &lt;none&gt;\n</code></pre> <p>That doesn\u2019t seem to make any difference, so we keep looking and find out we\u2019re missing the tls section in <code>gitea-ingress.yml</code> which turned to be important:</p> <pre><code>  tls:\n    - secretName: tls-secret\n      hosts:\n        - \"git.ssl.uu.am\"\n</code></pre> <pre><code>$ kubectl -n gitea apply -f gitea/gitea-ingress.yml \ningress.networking.k8s.io/gitea-ingress configured\n\n$ kubectl -n gitea describe ingress gitea-ingress\nName:             gitea-ingress\nLabels:           &lt;none&gt;\nNamespace:        gitea\nAddress:          10.0.0.6\nIngress Class:    nginx\nDefault backend:  &lt;default&gt;\nTLS:\n  tls-secret terminates git.ssl.uu.am\nRules:\n  Host                Path  Backends\n  ----                ----  --------\n  git.ssl.uu.am\n                      /   gitea-http:3000 (10.244.0.252:3000)\nAnnotations:          cert-manager.io/cluster-issuer: letsencrypt-prod\nEvents:\n  Type    Reason             Age                 From                       Message\n  ----    ------             ----                ----                       -------\n  Normal  Sync               3s (x3 over 3h17m)  nginx-ingress-controller   Scheduled for sync\n  Normal  CreateCertificate  3s                  cert-manager-ingress-shim  Successfully created Certificate \"tls-secret\"\n</code></pre> <p>We still get the fake cert on https://git.ssl.uu.am (and it\u2019s not the browser caching it).</p> <p>The ingress controller is complaining that it can\u2019t find those tls-secret</p> <pre><code>$ kubectl -n ingress-nginx logs ingress-nginx-controller-6b58ffdc97-rt5lm | egrep -i 'ssl.*cert|ssl.*hand' | grep -v gitlab\nI0412 04:07:33.044575       7 main.go:104] \"SSL fake certificate created\" file=\"/etc/ingress-controller/ssl/default-fake-certificate.pem\"\nI0412 04:07:33.075730       7 ssl.go:533] \"loading tls certificate\" path=\"/usr/local/certificates/cert\" key=\"/usr/local/certificates/key\"\n2023/04/16 12:41:20 [crit] 3974#3974: *6093931 SSL_do_handshake() failed (SSL: error:0A00006C:SSL routines::bad key share) while SSL handshaking, client: 10.244.0.1, server: 0.0.0.0:443\n2023/04/16 15:12:03 [crit] 4112#4112: *6240379 SSL_do_handshake() failed (SSL: error:0A00006C:SSL routines::bad key share) while SSL handshaking, client: 10.244.0.1, server: 0.0.0.0:443\n2023/04/16 16:08:53 [crit] 4655#4655: *6295724 SSL_do_handshake() failed (SSL: error:0A00006C:SSL routines::bad key share) while SSL handshaking, client: 10.244.0.1, server: 0.0.0.0:443\nW0416 19:13:29.051149       7 controller.go:1372] Error getting SSL certificate \"gitea/tls-secret\": local SSL certificate gitea/tls-secret was not found. Using default certificate\nW0416 19:13:29.088408       7 backend_ssl.go:47] Error obtaining X.509 certificate: no object matching key \"gitea/tls-secret\" in local store\n...\nW0416 19:20:05.541182       7 controller.go:1372] Error getting SSL certificate \"kubernetes-dashboard/tls-secret\": local SSL certificate kubernetes-dashboard/tls-secret was not found. Using default certificate\nW0416 19:20:05.573367       7 backend_ssl.go:47] Error obtaining X.509 certificate: no object matching key \"kubernetes-dashboard/tls-secret\" in local store\n</code></pre> <p>The secrets do exist, do they not contain valid certificates?</p> <pre><code>$ kubectl -n gitea describe cert\nName:         tls-secret\nNamespace:    gitea\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         Certificate\nMetadata:\n  Creation Timestamp:  2023-04-16T19:13:29Z\n  Generation:          1\n  Owner References:\n    API Version:           networking.k8s.io/v1\n    Block Owner Deletion:  true\n    Controller:            true\n    Kind:                  Ingress\n    Name:                  gitea-ingress\n    UID:                   6e4dfaeb-23d8-4a24-906e-c457431f14e5\n  Resource Version:        3200901\n  UID:                     cb10b560-974f-4533-9eff-c5ff1a8d3933\nSpec:\n  Dns Names:\n    git.ssl.uu.am\n  Issuer Ref:\n    Group:      cert-manager.io\n    Kind:       ClusterIssuer\n    Name:       letsencrypt-prod\n  Secret Name:  tls-secret\n  Usages:\n    digital signature\n    key encipherment\nStatus:\n  Conditions:\n    Last Transition Time:        2023-04-16T19:13:29Z\n    Message:                     Issuing certificate as Secret does not exist\n    Observed Generation:         1\n    Reason:                      DoesNotExist\n    Status:                      True\n    Type:                        Issuing\n    Last Transition Time:        2023-04-16T19:13:29Z\n    Message:                     Issuing certificate as Secret does not exist\n    Observed Generation:         1\n    Reason:                      DoesNotExist\n    Status:                      False\n    Type:                        Ready\n  Next Private Key Secret Name:  tls-secret-84npl\nEvents:\n  Type    Reason     Age   From                                       Message\n  ----    ------     ----  ----                                       -------\n  Normal  Issuing    12m   cert-manager-certificates-trigger          Issuing certificate as Secret does not exist\n  Normal  Generated  12m   cert-manager-certificates-key-manager      Stored new private key in temporary Secret resource \"tls-secret-84npl\"\n  Normal  Requested  12m   cert-manager-certificates-request-manager  Created new CertificateRequest resource \"tls-secret-hm94m\"\n\n$ kubectl -n kubernetes-dashboard describe cert\nName:         tls-secret\nNamespace:    kubernetes-dashboard\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         Certificate\nMetadata:\n  Creation Timestamp:  2023-04-16T19:20:05Z\n  Generation:          1\n  Owner References:\n    API Version:           networking.k8s.io/v1\n    Block Owner Deletion:  true\n    Controller:            true\n    Kind:                  Ingress\n    Name:                  kubernetes-dashboard-ingress\n    UID:                   dd2d357c-6558-41fd-bb0c-5b7eb4dc36d5\n  Resource Version:        3201584\n  UID:                     09f28e55-bbfd-4b0b-a171-80502fa8c88b\nSpec:\n  Dns Names:\n    k8s.ssl.uu.am\n  Issuer Ref:\n    Group:      cert-manager.io\n    Kind:       ClusterIssuer\n    Name:       letsencrypt-prod\n  Secret Name:  tls-secret\n  Usages:\n    digital signature\n    key encipherment\nStatus:\n  Conditions:\n    Last Transition Time:        2023-04-16T19:20:05Z\n    Message:                     Issuing certificate as Secret does not exist\n    Observed Generation:         1\n    Reason:                      DoesNotExist\n    Status:                      True\n    Type:                        Issuing\n    Last Transition Time:        2023-04-16T19:20:05Z\n    Message:                     Issuing certificate as Secret does not exist\n    Observed Generation:         1\n    Reason:                      DoesNotExist\n    Status:                      False\n    Type:                        Ready\n  Next Private Key Secret Name:  tls-secret-dbggr\nEvents:\n  Type    Reason     Age   From                                       Message\n  ----    ------     ----  ----                                       -------\n  Normal  Issuing    6m    cert-manager-certificates-trigger          Issuing certificate as Secret does not exist\n  Normal  Generated  6m    cert-manager-certificates-key-manager      Stored new private key in temporary Secret resource \"tls-secret-dbggr\"\n  Normal  Requested  6m    cert-manager-certificates-request-manager  Created new CertificateRequest resource \"tls-secret-8w9rt\"\n</code></pre> <p>Looking at Issuer not found #3066, it seems we have in-flight orders for certificates, but are still waiting to receive them:</p> <pre><code>$ kubectl -n gitea describe CertificateRequest tls-secret-hm94m\nName:         tls-secret-hm94m\nNamespace:    gitea\nLabels:       &lt;none&gt;\nAnnotations:  cert-manager.io/certificate-name: tls-secret\n              cert-manager.io/certificate-revision: 1\n              cert-manager.io/private-key-secret-name: tls-secret-84npl\nAPI Version:  cert-manager.io/v1\nKind:         CertificateRequest\nMetadata:\n  Creation Timestamp:  2023-04-16T19:13:29Z\n  Generate Name:       tls-secret-\n  Generation:          1\n  Owner References:\n    API Version:           cert-manager.io/v1\n    Block Owner Deletion:  true\n    Controller:            true\n    Kind:                  Certificate\n    Name:                  tls-secret\n    UID:                   cb10b560-974f-4533-9eff-c5ff1a8d3933\n  Resource Version:        3200917\n  UID:                     bb31c94f-fd83-4f62-9edc-5de1cc883b0a\nSpec:\n  Extra:\n    authentication.kubernetes.io/pod-name:\n      cert-manager-64f9f45d6f-qx4hs\n    authentication.kubernetes.io/pod-uid:\n      75a75d95-2318-4fc2-a3fa-0be649a24a31\n  Groups:\n    system:serviceaccounts\n    system:serviceaccounts:cert-manager\n    system:authenticated\n  Issuer Ref:\n    Group:  cert-manager.io\n    Kind:   ClusterIssuer\n    Name:   letsencrypt-prod\n  Request:  \u2026S0K\n  UID:      088d0ec3-f284-4387-893c-e4a0eaa223e9\n  Usages:\n    digital signature\n    key encipherment\n  Username:  system:serviceaccount:cert-manager:cert-manager\nStatus:\n  Conditions:\n    Last Transition Time:  2023-04-16T19:13:29Z\n    Message:               Certificate request has been approved by cert-manager.io\n    Reason:                cert-manager.io\n    Status:                True\n    Type:                  Approved\n    Last Transition Time:  2023-04-16T19:13:29Z\n    Message:               Waiting on certificate issuance from order gitea/tls-secret-hm94m-3966586170: \"pending\"\n    Reason:                Pending\n    Status:                False\n    Type:                  Ready\nEvents:\n  Type    Reason              Age   From                                                Message\n  ----    ------              ----  ----                                                -------\n  Normal  WaitingForApproval  19m   cert-manager-certificaterequests-issuer-vault       Not signing CertificateRequest until it is Approved\n  Normal  WaitingForApproval  19m   cert-manager-certificaterequests-issuer-selfsigned  Not signing CertificateRequest until it is Approved\n  Normal  WaitingForApproval  19m   cert-manager-certificaterequests-issuer-venafi      Not signing CertificateRequest until it is Approved\n  Normal  WaitingForApproval  19m   cert-manager-certificaterequests-issuer-acme        Not signing CertificateRequest until it is Approved\n  Normal  WaitingForApproval  19m   cert-manager-certificaterequests-issuer-ca          Not signing CertificateRequest until it is Approved\n  Normal  cert-manager.io     19m   cert-manager-certificaterequests-approver           Certificate request has been approved by cert-manager.io\n  Normal  OrderCreated        19m   cert-manager-certificaterequests-issuer-acme        Created Order resource gitea/tls-secret-hm94m-3966586170\n\n$ kubectl -n kubernetes-dashboard describe CertificateRequest tls-secret-8w9rt\nName:         tls-secret-8w9rt\nNamespace:    kubernetes-dashboard\nLabels:       &lt;none&gt;\nAnnotations:  cert-manager.io/certificate-name: tls-secret\n              cert-manager.io/certificate-revision: 1\n              cert-manager.io/private-key-secret-name: tls-secret-dbggr\nAPI Version:  cert-manager.io/v1\nKind:         CertificateRequest\nMetadata:\n  Creation Timestamp:  2023-04-16T19:20:05Z\n  Generate Name:       tls-secret-\n  Generation:          1\n  Owner References:\n    API Version:           cert-manager.io/v1\n    Block Owner Deletion:  true\n    Controller:            true\n    Kind:                  Certificate\n    Name:                  tls-secret\n    UID:                   09f28e55-bbfd-4b0b-a171-80502fa8c88b\n  Resource Version:        3201600\n  UID:                     37fe511f-f278-4145-84f4-22d54a08f23b\nSpec:\n  Extra:\n    authentication.kubernetes.io/pod-name:\n      cert-manager-64f9f45d6f-qx4hs\n    authentication.kubernetes.io/pod-uid:\n      75a75d95-2318-4fc2-a3fa-0be649a24a31\n  Groups:\n    system:serviceaccounts\n    system:serviceaccounts:cert-manager\n    system:authenticated\n  Issuer Ref:\n    Group:  cert-manager.io\n    Kind:   ClusterIssuer\n    Name:   letsencrypt-prod\n  Request:  \u2026S0K\n  UID:      088d0ec3-f284-4387-893c-e4a0eaa223e9\n  Usages:\n    digital signature\n    key encipherment\n  Username:  system:serviceaccount:cert-manager:cert-manager\nStatus:\n  Conditions:\n    Last Transition Time:  2023-04-16T19:20:05Z\n    Message:               Certificate request has been approved by cert-manager.io\n    Reason:                cert-manager.io\n    Status:                True\n    Type:                  Approved\n    Last Transition Time:  2023-04-16T19:20:05Z\n    Message:               Waiting on certificate issuance from order kubernetes-dashboard/tls-secret-8w9rt-548771682: \"pending\"\n    Reason:                Pending\n    Status:                False\n    Type:                  Ready\nEvents:\n  Type    Reason              Age   From                                                Message\n  ----    ------              ----  ----                                                -------\n  Normal  WaitingForApproval  13m   cert-manager-certificaterequests-issuer-ca          Not signing CertificateRequest until it is Approved\n  Normal  WaitingForApproval  13m   cert-manager-certificaterequests-issuer-acme        Not signing CertificateRequest until it is Approved\n  Normal  WaitingForApproval  13m   cert-manager-certificaterequests-issuer-selfsigned  Not signing CertificateRequest until it is Approved\n  Normal  WaitingForApproval  13m   cert-manager-certificaterequests-issuer-vault       Not signing CertificateRequest until it is Approved\n  Normal  WaitingForApproval  13m   cert-manager-certificaterequests-issuer-venafi      Not signing CertificateRequest until it is Approved\n  Normal  cert-manager.io     13m   cert-manager-certificaterequests-approver           Certificate request has been approved by cert-manager.io\n  Normal  OrderCreated        13m   cert-manager-certificaterequests-issuer-acme        Created Order resource kubernetes-dashboard/tls-secret-8w9rt-548771682\n  Normal  OrderPending        13m   cert-manager-certificaterequests-issuer-acme        Waiting on certificate issuance from order kubernetes-dashboard/tls-secret-8w9rt-548771682: \"\"\n\n$ kubectl describe clusterissuer letsencrypt-prod\nName:         letsencrypt-prod\nNamespace:    \nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         ClusterIssuer\nMetadata:\n  Creation Timestamp:  2023-04-16T14:11:19Z\n  Generation:          1\n  Resource Version:    3171554\n  UID:                 2176e44f-421a-498c-935b-34e71a21cae1\nSpec:\n  Acme:\n    Email:            root@uu.am\n    Preferred Chain:  \n    Private Key Secret Ref:\n      Name:  letsencrypt-prod\n    Server:  https://acme-v02.api.letsencrypt.org/directory\n    Solvers:\n      http01:\n        Ingress:\n          Class:  nginx\nStatus:\n  Acme:\n    Last Registered Email:  root@uu.am\n    Uri:                    https://acme-v02.api.letsencrypt.org/acme/acct/1063900577\n  Conditions:\n    Last Transition Time:  2023-04-16T14:11:20Z\n    Message:               The ACME account was registered with the ACME server\n    Observed Generation:   1\n    Reason:                ACMEAccountRegistered\n    Status:                True\n    Type:                  Ready\nEvents:                    &lt;none&gt;\n</code></pre> <p>The orders are pending, possibly because the self-check / challenge is failing due to incomplete network setup, i.e. not enough port forwardings:</p> <pre><code>$ kubectl -n gitea get order tls-secret-hm94m-3966586170\nNAME                          STATE     AGE\ntls-secret-hm94m-3966586170   pending   31m\n$ kubectl -n kubernetes-dashboard get order tls-secret-8w9rt-548771682\nNAME                         STATE     AGE\ntls-secret-8w9rt-548771682   pending   24m\n</code></pre> <p>Troubleshooting Orders shows how to find the orders, their challenges and why they are failing:</p> <pre><code>$ kubectl -n gitea describe order tls-secret-hm94m-3966586170 | tail -4\nEvents:\n  Type    Reason   Age   From                 Message\n  ----    ------   ----  ----                 -------\n  Normal  Created  34m   cert-manager-orders  Created Challenge resource \"tls-secret-hm94m-3966586170-1941395475\" for domain \"git.ssl.uu.am\"\n\n$ kubectl -n gitea describe challenge tls-secret-hm94m-3966586170-1941395475 | grep Reason:\n  Reason:      Waiting for HTTP-01 challenge propagation: failed to perform self check GET request 'http://git.ssl.uu.am/.well-known/acme-challenge/cw9i3YCPZQmQXrofkEt4inKYr5x3fEc9wJ6R2ydpeAg': Get \"http://git.ssl.uu.am/.well-known/acme-challenge/cw9i3YCPZQmQXrofkEt4inKYr5x3fEc9wJ6R2ydpeAg\": dial tcp 217.162.57.64:80: connect: connection refused\n\n$ kubectl -n kubernetes-dashboard describe order tls-secret-8w9rt-548771682 | tail -4\nEvents:\n  Type    Reason   Age   From                 Message\n  ----    ------   ----  ----                 -------\n  Normal  Created  28m   cert-manager-orders  Created Challenge resource \"tls-secret-8w9rt-548771682-3606431526\" for domain \"k8s.ssl.uu.am\"\n\n$ kubectl -n kubernetes-dashboard describe challenge tls-secret-8w9rt-548771682-3606431526 |grep Reason:\n  Reason:      Waiting for HTTP-01 challenge propagation: failed to perform self check GET request 'http://k8s.ssl.uu.am/.well-known/acme-challenge/Eu-aIJIbu4gRAqgRW2ZcPpGyVk9ZmIBFy23zDQNr14s': Get \"http://k8s.ssl.uu.am/.well-known/acme-challenge/Eu-aIJIbu4gRAqgRW2ZcPpGyVk9ZmIBFy23zDQNr14s\": dial tcp 217.162.57.64:80: connect: connection refused\n</code></pre> <p>The question is where to redirect that port 80 to; currently it points to the node\u2019s port 80. Before trying to sort out that port 80 forwarding, HTTP01 troubleshooting looks like we want to add 2 more annotations to the Ingress resources in <code>gitea-ingress.yml</code> and <code>kubernetes-dashboard-ingress.yaml</code>:</p> <pre><code>cert-manager.io/issue-temporary-certificate: \"true\"\nacme.cert-manager.io/http01-edit-in-place: \"true\"\n</code></pre> <p>Add these and re-apply:</p> <pre><code>$ kubectl -n kubernetes-dashboard apply -f  dashboard/kubernetes-dashboard-ingress.yaml\ningress.networking.k8s.io/kubernetes-dashboard-ingress configured\n\n$ kubectl -n gitea apply -f gitea-ingress.yml \ningress.networking.k8s.io/gitea-ingress configured\n</code></pre> <p>That doesn\u2019t help, at least not immediately, so let\u2019s sort out that port forwarding:  the trick is to find each acme resolver and forward external port 80 to its <code>NodePort</code>:</p> <pre><code>$ kubectl -n gitea get svc | grep acme\ncm-acme-http-solver-wmmvk   NodePort    10.103.37.86     &lt;none&gt;        8089:31544/TCP   51m\n</code></pre> <p>More generally, for other cert managers:</p> <pre><code>$ kubectl get svc -A | grep acme\ncode-server              cm-acme-http-solver-8n9ks                               NodePort       10.101.32.112    &lt;none&gt;          8089:32220/TCP                                                                                  3d19h\nkubernetes-dashboard     cm-acme-http-solver-b8l2x                               NodePort       10.100.2.141     &lt;none&gt;          8089:32328/TCP\n</code></pre> <p>After that port was made accessible, the challenge disappears and the order is completed successfully:</p> <pre><code>$ kubectl -n gitea describe order tls-secret-hm94m-3966586170 | tail -4\n  Type    Reason    Age   From                 Message\n  ----    ------    ----  ----                 -------\n  Normal  Created   54m   cert-manager-orders  Created Challenge resource \"tls-secret-hm94m-3966586170-1941395475\" for domain \"git.ssl.uu.am\"\n  Normal  Complete  82s   cert-manager-orders  Order completed successfully\n\n$ kubectl -n gitea describe CertificateRequest tls-secret-hm94m\nName:         tls-secret-hm94m\n\nStatus:\n  Certificate:  LS0\u2026==\n  Conditions:\n    Last Transition Time:  2023-04-16T19:13:29Z\n    Message:               Certificate request has been approved by cert-manager.io\n    Reason:                cert-manager.io\n    Status:                True\n    Type:                  Approved\n    Last Transition Time:  2023-04-16T20:06:31Z\n    Message:               Certificate fetched from issuer successfully\n    Reason:                Issued\n    Status:                True\n    Type:                  Ready\nEvents:\n...\n  Normal  CertificateIssued   2m35s  cert-manager-certificaterequests-issuer-acme        Certificate fetched from issuer successfully\n\n$ kubectl -n gitea describe cert\nName:         tls-secret\nNamespace:    gitea\n...\nStatus:\n  Conditions:\n    Last Transition Time:  2023-04-16T20:06:31Z\n    Message:               Certificate is up to date and has not expired\n    Observed Generation:   1\n    Reason:                Ready\n    Status:                True\n    Type:                  Ready\n...\nEvents:\n...\n  Normal  Issuing    5m    cert-manager-certificates-issuing          The certificate has been successfully issued\n</code></pre> <p>Now we have certificates, and a new problem!</p> <pre><code>$ kubectl -n ingress-nginx logs ingress-nginx-controller-6b58ffdc97-rt5lm | grep -C1 error:0A00006C:SSL\n10.244.0.1 - - [16/Apr/2023:20:08:27 +0000] \"GET / HTTP/1.1\" 200 13779 \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\" 742 0.003 [gitea-gitea-http-3000] [] 10.244.0.252:3000 13757 0.003 200 2ec12a59924906d8edd0900ff547afeb\n2023/04/16 20:08:32 [crit] 5747#5747: *6530552 SSL_do_handshake() failed (SSL: error:0A00006C:SSL routines::bad key share) while SSL handshaking, client: 10.244.0.1, server: 0.0.0.0:443\n10.244.0.1 - - [16/Apr/2023:20:10:55 +0000] \"GET / HTTP/2.0\" 200 36815 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\" 599 0.020 [gitea-gitea-http-3000] [] 10.244.0.252:3000 36828 0.020 200 566dde9f9d34e65ae66f7b1ff4efe1cd\n</code></pre> <p>Maybe the only problem was impatience; left alone for a while and it works perfectly now \ud83d\ude42</p>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#add-ingress-for-kubernetes-dashboard","title":"Add Ingress for Kubernetes Dashboard","text":"<p>Once the above works, we can do the same for the Kubernetes Dashboard:</p> <pre><code>$ kubectl -n kubernetes-dashboard get svc\nNAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE\ndashboard-metrics-scraper   ClusterIP   10.96.35.216    &lt;none&gt;        8000/TCP        13d\nkubernetes-dashboard        NodePort    10.99.147.241   &lt;none&gt;        443:32000/TCP   13d\n</code></pre> <p>Update <code>kubernetes-dashboard-ingress.yaml</code> like this:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard-ingress\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"false\"\n    nginx.ingress.kubernetes.io/whitelist-source-range: \"10.244.0.0/16\"\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: \"k8s.ssl.uu.am\"\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard\n                port:\n                  number: 8443\n</code></pre> <pre><code>$ kubectl -n kubernetes-dashboard \\\n  apply -f kubernetes-dashboard-ingress.yaml \ningress.networking.k8s.io/kubernetes-dashboard-ingress created\n\n$ kubectl -n kubernetes-dashboard get ingress\nNAME                           CLASS   HOSTS                ADDRESS    PORTS   AGE\nkubernetes-dashboard-ingress   nginx   k8s.ssl.uu.am   10.0.0.6   80      3m50s\n\n$ kubectl -n kubernetes-dashboard describe ingress/kubernetes-dashboard-ingress\nName:             kubernetes-dashboard-ingress\nLabels:           &lt;none&gt;\nNamespace:        kubernetes-dashboard\nAddress:          10.0.0.6\nIngress Class:    nginx\nDefault backend:  &lt;default&gt;\nTLS:\n  tls-secret terminates k8s.ssl.uu.am\nRules:\n  Host                Path  Backends\n  ----                ----  --------\n  k8s.ssl.uu.am  \n                      /   kubernetes-dashboard:8443 (10.244.0.82:8443)\nAnnotations:          cert-manager.io/cluster-issuer: letsencrypt-prod\n                      nginx.ingress.kubernetes.io/auth-tls-verify-client: false\n                      nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n                      nginx.ingress.kubernetes.io/whitelist-source-range: 10.244.0.0/16\nEvents:\n  Type    Reason             Age                 From                       Message\n  ----    ------             ----                ----                       -------\n  Normal  Sync               16s (x6 over 123m)  nginx-ingress-controller   Scheduled for sync\n  Normal  CreateCertificate  16s                 cert-manager-ingress-shim  Successfully created Certificate \"tls-secret\"\n\n$ kubectl get all -A -o wide | egrep 10.244.0.82\nkubernetes-dashboard   pod/kubernetes-dashboard-6c7ccbcf87-vwxnt       1/1     Running     1 (4d13h ago)    13d     10.244.0.82    lexicon   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>This does not yet work 100%: - ssl.uu.am:32000 works,   port 32000 is forwarded to the correct <code>NodePort</code> - k8s.uu.am redirects there,   with an HTTP redirect rule - k8s.ssl.uu.am/   doesn\u2019t work (HTTP ERROR 400) - <code>k8s.ssl.uu.am</code> resolves to external IP, where port    443 is forwarded to the    MetalLB IP 192.168.0.122    for <code>ingress-nginx-controller</code>.</p> <p>Looking at the logs of the Nginx controller, it looks like a problem with/in the Cluster internal network:</p> <pre><code>$ kubectl -n ingress-nginx logs ingress-nginx-controller-6b58ffdc97-rt5lm | tail -2\n2023/04/16 17:26:40 [error] 4790#4790: *6371572 recv() failed (104: Connection reset by peer) while reading upstream, client: 10.244.0.1, server: k8s.ssl.uu.am, request: \"GET / HTTP/2.0\", upstream: \"http://10.244.0.82:8443/\", host: \"k8s.ssl.uu.am\", referrer: \"https://www.google.com/\"\n10.244.0.1 - - [16/Apr/2023:17:26:40 +0000] \"GET / HTTP/2.0\" 400 48 \"https://www.google.com/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\" 465 0.002 [kubernetes-dashboard-kubernetes-dashboard-443] [] 10.244.0.82:8443 48 0.002 400 c2e45b0dee94255bb284054654b987a4\n</code></pre> <p>Tried mapping to port 8443 but still got the same error. The problem was that Nginx was trying to send HTTP requests to an HTTPS endpoint and the solution was to add more annotations in <code>kubernetes-dashboard-ingress.yaml</code> and re-apply:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard-ingress\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"false\"\n    nginx.ingress.kubernetes.io/whitelist-source-range: \"10.244.0.0/16\"\n</code></pre> <p>These additional annotations might be necessary later, but it\u2019s not clear when or why:</p> <pre><code>    nginx.ingress.kubernetes.io/configuration-snippet: |-\n      proxy_ssl_server_name on;\n      proxy_ssl_name $host;\n</code></pre> <p>In the end, what was missing was the <code>tls</code> section and after troubleshooting challenges, we got it to work.</p> <p>Finally, we can have http://k8s.uu.am redirect to  k8s.ssl.uu.am which not only works but is also entirely safe! \ud83d\ude42</p>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#monthly-renewal-of-certificates-manual","title":"Monthly renewal of certificates (manual)","text":"<p>A montly renewal of certificates is scheduled for Kubernetes deployments, but then again it will create challenges that can't be fulfilled until port 80 on the public IP is forwarded to the right node port, one at a time.</p> <p>This can be automated by having port 80 on the public IP permanently forwarded to the host port 80, which is never listening, and then redirecting that port to the node port for each <code>acme</code> resolver, one at a time.</p> <p>To find all <code>acme</code> resolvers:</p> <pre><code>$ kubectl get svc -A | grep acme\ncode-server              cm-acme-http-solver-8n9ks                               NodePort       10.101.32.112    &lt;none&gt;          8089:32220/TCP                                                                                  3d19h\nkubernetes-dashboard     cm-acme-http-solver-b8l2x                               NodePort       10.100.2.141     &lt;none&gt;          8089:32328/TCP\n</code></pre> <p>In this situation, we'd want to forward port 32220 to port 80 until the <code>acme</code> resolver listening on that port is no longer running. Then forward port 32328 to port 80 until the <code>acme</code> resolver listening on that port is no longer running. And so on.</p>"},{"location":"blog/2023/03/25/single-node-kubernetes-cluster-on-ubuntu-server-lexicon/#monthly-renewal-of-certificates-automated","title":"Monthly renewal of certificates (automated)","text":"<p>Changing port forwarding on the router is very slow, so instead we want to change service ports using patch on those <code>cm-acme-http-solver</code> services to change their <code>nodePort</code> to one the router is always redirecting port 80 to.</p> <p>Once the service is patched, the external connection reaches the solver pod and the renewal is completed very shortly, which then deletes the pod and service.</p> <p>Running this in a crontab daily should do the job:</p> <pre><code>#!/bin/bash\n#\n# Patch the nodePort of running cert-manager renewal challenge, to listen\n# on port 32080 which is the one the router is forwarding port 80 to.\n\n# Check if there is a LetsEncrypt challenge resolver (acme) running.\nexport KUBECONFIG=/etc/kubernetes/admin.conf\nnodeport=$(kubectl get svc -A | grep acme | awk '{print $6}' | cut -f2 -d: | cut -f1 -d/ | head -1)\nnamespace=$(kubectl get svc -A | grep acme | awk '{print $1}' | head -1)\nservice=$(kubectl get svc -A | grep acme | awk '{print $2}' | head -1)\n\n# Patch the service to listen on port 32080 (set up in router).\nif [ -n \"${namespace}\" ] &amp;&amp; [ -n \"${service}\" ]; then\n    kubectl -n \"${namespace}\" patch service \"${service}\" -p '{\"spec\":{\"ports\": [{\"port\": 8089, \"nodePort\":32080}]}}'\nfi\n</code></pre> <pre><code># crontab -e\n# Hourly patch montly cert renewal solvers.\n30 * * * * /root/bin/cert-renewal-port-fwd.sh\n</code></pre>"},{"location":"blog/2023/05/29/running-visual-studio-code-server-on-kubernetes/","title":"Running Visual Studio Code Server on Kubernetes","text":"<p>I would like to be able to code from anywhere, or at least have a development environment that doesn't depend on my PC. Visual Studio Code Server seems like one of the best and/or most popular options out there, so I decided it to try.</p> <p>Since I've already got my Kubernetes cluster running, the best option seems to be the lscr.io/linuxserver/code-server docker image. I took the deployment from  Deploying VSCode on a Kubernetes Cluster and added an Nginx ingress.</p> <p>First, create a dedicated directory in the partition with plenty of space available:</p> <pre><code># mkdir /home/k8s/code-server/\n# chown -R coder.coder /home/k8s/code-server/\n</code></pre> <p>A persistent volume is necessary because otherwise everything (settings and uncommitted changes) is lost when the container is restarted, including when the node is restarted. Tried claiming a persistent volume from the default storage class local-path but never managed to get the pod to see the claim, until the claim was changed to the manual storage class to get a <code>hostPath</code> volume, which worked immediately (only, after granting <code>coder</code> all rights on the <code>hostPath</code> directory).</p> <p>Then create <code>code-server.yaml</code> with the deployment and <code>apply</code> it:</p> Kubernetes deployment: <code>code-server.yaml</code> code-server.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: code-server\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: code-server\nnamespace: code-server\nspec:\nports:\n- port: 80\n  targetPort: 8080\nselector:\n  app: code-server\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: code-server-pv\n  labels:\n    type: local\n  namespace: code-server\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/home/k8s/code-server\"\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: code-server-pv-claim\n  namespace: code-server\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: code-server\n  name: code-server\n  namespace: code-server\nspec:\n  selector:\n    matchLabels:\n      app: code-server\n  template:\n    metadata:\n      labels:\n        app: code-server\n    spec:\n      volumes:\n        - name: code-server-storage\n          persistentVolumeClaim:\n            claimName: code-server-pv-claim\n      containers:\n      - image: codercom/code-server\n        imagePullPolicy: IfNotPresent\n        name: code-server\n        ports:\n        - containerPort: 8080\n        env:\n        - name: PASSWORD\n          value: \"*************************\"\n        volumeMounts:\n          - mountPath: \"/home/coder\"\n            name: code-server-storage\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: code-server-ingress\n  namespace: code-server\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: \"code.ssl.uu.am\"\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: code-server\n                port:\n                  number: 80\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - \"code.ssl.uu.am\"\n</code></pre> <pre><code>$ kubectl apply -f code-server.yaml\n</code></pre> <p>Add an A record for <code>code.ssl.uu.am</code> pointing to the external IP and go to https://code.ssl.uu.am to get the certificate creation started, forward port 80 to the <code>NodePort</code> of the <code>cm-acme-http-solver</code>:</p> <pre><code>$ kubectl -n code-server get svc\nNAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\ncm-acme-http-solver-dcwjx   NodePort    10.109.153.157   &lt;none&gt;        8089:31691/TCP   8m53s\n</code></pre>"},{"location":"blog/2023/05/31/simple-qmk-firmware-for-the-romac-macro-pad/","title":"Simple QMK firmware for the RoMac macro pad","text":"<p>QMK firmware for the RoMac macro pad was not simple enough for me, so I had to make my own.</p> <p>The RoMac Macro Pad is a wonderful, very useful 12-key macro pad that can be a custom numpad or, better yet, an additional numpad for the left hand, just not for numbers. This macro pad, which can be purchased from customkbd.com (Australia) or mechboards.co.uk (United Kingdom), the latter also having Relegendable Keycaps that are great to custom-label each key.</p> <p></p> <p>Warning</p> <p>This is not a keyboard you can just buy.</p> <p>To build this keyboard you need to buy the kit, with a controller (e.g. the Pro Micro), plus 20 switches and 20 keycaps, plus a USB cable (micro-B or C depending on the controller). And then you have to actually build the keyboard, following the (very detailed) RoMac rev2.1 Build Guide, which involves a lot of soldering. There is another build guide for the Fauxmac (RoMac+) variant (with OLED screen and RGB backlight).</p> <p>And then you have to choose, or write, a keymap for it.</p> <p>Although not included in the upstream QMK firmware, my preferred use for this keyboard is adding back the F13-F24 keys some old keyboards used to have. This is very useful to trigger actions or scripts by pressiong a single key without taking the right hand away from the mouse.</p> <p>So here is how to create the simplest keymap: F13-F24 keys.</p> <p>First, clone the QMK firmware repository:</p> <pre><code>$ python3 -m pip install -U qmk\n$ git clone --recurse-submodules \\\n  https://github.com/qmk/qmk_firmware.git\n$ cd qmk_firmware\n$ qmk setup -y\n</code></pre> <p>Then create a new file (and folder) <code>keyboards/kingly_keys/romac/keymaps/simple/keymap.c</code> with this code:</p> keymap.c<pre><code>/* Copyright 2023 Ponder Stibbons\n *\n * This program is free software: you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation, either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.\n */\n\n#include QMK_KEYBOARD_H\n\n#define _BASE 0\n#define _FN1 1\n\nconst uint16_t PROGMEM keymaps[][MATRIX_ROWS][MATRIX_COLS] = {\n\n        [_BASE] = LAYOUT(\n                KC_F13, KC_F14, KC_F15, \\\n                KC_F16, KC_F17, KC_F18, \\\n                KC_F19, KC_F20, KC_F21, \\\n                KC_F22, KC_F23, KC_F24 \\\n        )\n};\n</code></pre> <p>Finally, compile and flash the keymap into the keyboard.</p> <p>Note</p> <p><code>qmk flash</code> won't work unless the keyboard is plugged in and you may need to press the reset button (the tiny black button on the PCB) at the right time.</p> <pre><code>$ qmk compile -kb kingly_keys/romac -km simple\n$ qmk flash -kb kingly_keys/romac -km simple\n\u03a8 Compiling keymap with make --jobs=1 kingly_keys/romac:simple:flash\n\n\nMaking kingly_keys/romac with keymap simple and target flash\n\navr-gcc (GCC) 5.4.0\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nSize before:\n   text    data     bss     dec     hex filename\n      0   16234       0   16234    3f6a kingly_keys_romac_simple.hex\n\nCopying kingly_keys_romac_simple.hex to qmk_firmware folder                                         [OK]\nChecking file size of kingly_keys_romac_simple.hex                                                  [OK]\n * The firmware size is fine - 16234/28672 (56%, 12438 bytes free)\nFlashing for bootloader: caterina\nWaiting for USB serial port - reset your controller now (Ctrl+C to cancel)......\nDevice /dev/ttyACM0 has appeared; assuming it is the controller.\nWaiting for /dev/ttyACM0 to become writable.\n\nConnecting to programmer: .\nFound programmer: Id = \"CATERIN\"; type = S\n    Software Version = 1.0; No Hardware Version given.\nProgrammer supports auto addr increment.\nProgrammer supports buffered memory access with buffersize=128 bytes.\n\nProgrammer supports the following devices:\n    Device code: 0x44\n\navrdude: AVR device initialized and ready to accept instructions\n\nReading | ################################################## | 100% 0.00s\n\navrdude: Device signature = 0x1e9587 (probably m32u4)\navrdude: NOTE: \"flash\" memory has been specified, an erase cycle will be performed\n         To disable this feature, specify the -D option.\navrdude: erasing chip\navrdude: reading input file \".build/kingly_keys_romac_simple.hex\"\navrdude: input file .build/kingly_keys_romac_simple.hex auto detected as Intel Hex\navrdude: writing flash (16234 bytes):\n\nWriting | ################################################## | 100% 1.19s\n\navrdude: 16234 bytes of flash written\navrdude: verifying flash memory against .build/kingly_keys_romac_simple.hex:\navrdude: load data flash data from input file .build/kingly_keys_romac_simple.hex:\navrdude: input file .build/kingly_keys_romac_simple.hex auto detected as Intel Hex\navrdude: input file .build/kingly_keys_romac_simple.hex contains 16234 bytes\navrdude: reading on-chip flash data:\n\nReading | ################################################## | 100% 0.13s\n\navrdude: verifying ...\navrdude: 16234 bytes of flash verified\n\navrdude: safemode: Fuses OK (E:CB, H:D8, L:FF)\n\navrdude done.  Thank you.\n</code></pre> <p>Now the keyboard is ready to emit the keycodes for the function keys from F13 to F24, but those keycodes may be already assigned by your OS to something else.</p> <p>In Linux systems, Xorg assigns a few of these keycodes to vendor-specific keys found in laptops (e.g. mute mic). To override these, add the following lines to your <code>~/.Xmodmap</code> file:</p> <pre><code>keycode 191 = F13 F13 F13\nkeycode 192 = F14 F14 F14\nkeycode 193 = F15 F15 F15\nkeycode 194 = F16 F16 F16\nkeycode 195 = F17 F17 F17\nkeycode 196 = F18 F18 F18\nkeycode 197 = F19 F19 F19\nkeycode 198 = F20 F20 F20\nkeycode 199 = F21 F21 F21\nkeycode 200 = F22 F22 F22\nkeycode 201 = F23 F23 F23\nkeycode 202 = F24 F24 F24\n</code></pre> <p>What you do with these is now up to you, and depends on your OS and desktop environments. For instance, in KDE Plasma you can assign these to standard Keyboard Shortcuts or create more flexible (powerful) Custom Shortcuts.</p>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/","title":"Running Minecraft Java Server for Bedrock clients on Kubernetes","text":"<p>Minecraft Java Edition requires that servers match the version of the clients and updating the server each time is a bit of a chore, so it is more convenient to run it on the Kubernetes cluster.</p>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#basic-setup","title":"Basic Setup","text":"<p>Always run Minecraft servers under their own dedicated, non-privileged user. Create also a dedicated directory in the partition with plenty of space available:</p> <pre><code># useradd minecraft\n# mkdir /home/k8s/minecraft-server\n# chown -R minecraft.minecraft /home/k8s/minecraft-server\n# ls -ldn /home/k8s/minecraft-server\ndrwxr-xr-x 1 1003 1003 0 May 29 14:43 /home/k8s/minecraft-server\n</code></pre> <p>A persistent volume is necessary because otherwise everything (server settings and the whole world) is lost when the container is restarted, including when the node is restarted.</p> <p>Note that UID &amp; GID 1003 will be needed later to run the server as this user.</p>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>Create and apply the deployment in <code>minecraft-server.yaml</code> using the itzg/minecraft-server docker image (GitHub: itzg/docker-minecraft-server):</p> Kubernetes deployment: <code>minecraft-server.yaml</code> minecraft-server.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: minecraft-server\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minecraft-server\n  namespace: minecraft-server\nspec:\n  type: NodePort\n  ports:\n    - name: java-tcp\n      port: 25565\n      nodePort: 32565\n      targetPort: 25565\n      protocol: TCP\n    - name: bedrock-udp\n      port: 19132\n      nodePort: 32132\n      targetPort: 19132\n      protocol: UDP\n  selector:\n    app: minecraft-server\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: minecraft-server-pv\n  labels:\n    type: local\n  namespace: minecraft-server\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 100Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/k8s/minecraft-server\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: minecraft-server-pv-claim\n  namespace: minecraft-server\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 30Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: minecraft-server\n  name: minecraft-server\n  namespace: minecraft-server\nspec:\n  selector:\n    matchLabels:\n      app: minecraft-server\n  template:\n    metadata:\n      labels:\n        app: minecraft-server\n    spec:\n      volumes:\n        - name: minecraft-server-storage\n          persistentVolumeClaim:\n            claimName: minecraft-server-pv-claim\n      containers:\n        - image: itzg/minecraft-server\n          imagePullPolicy: Always\n          name: minecraft-server\n          ports:\n            - containerPort: 25565\n          env:\n            - name: EULA\n              value: \"TRUE\"\n            - name: TYPE\n              value: SPIGOT\n            - name: MEMORY\n              value: 4G\n          volumeMounts:\n            - mountPath: /data\n              name: minecraft-server-storage\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 1003\n            runAsGroup: 1003\n</code></pre> <p>Note</p> <p>To make the server accessible to clients, the above <code>NodePort</code> is required for each the Java and Bedrock protols separately:</p> <ul> <li>The Java server listens on TCP port 25565, the    cluster exposes the server to the local network as    <code>192.168.0.6:32565</code></li> <li>The Bedrock server listens on UDP port 19132, the    cluster exposes the server to the local network as    <code>192.168.0.6:32132</code></li> </ul> <p>Both ports can then be exposed externally by adding port forwarding rules in the local router.</p> <p>Once the deployment is applied, confirm everything is running and check the logs:</p> <pre><code>$ kubectl apply -f minecraft-server.yaml\nnamespace/minecraft-server unchanged\nservice/minecraft-server unchanged\npersistentvolume/minecraft-server-pv unchanged\npersistentvolumeclaim/minecraft-server-pv-claim unchanged\ndeployment.apps/minecraft-server configured\n\n$ kubectl -n minecraft-server get all\nNAME                                    READY   STATUS    RESTARTS      AGE\npod/minecraft-server-88f84b5fc-ptb5p   1/1     Running   0          2m38s\n\n\nNAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE\nservice/minecraft-server   ClusterIP   10.110.215.139   &lt;none&gt;        25565/TCP   2m38s\nservice/minecraft-server   NodePort   10.110.215.139   &lt;none&gt;        25565:32565/TCP,19132:32132/UDP   2m38s\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/minecraft-server   1/1     1            1           2m38s\n\nNAME                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/minecraft-server-88f84b5fc    1         1         1       2m38s\n\n$ kubectl -n minecraft-server logs pod/minecraft-server-88f84b5fc-ptb5p\n[init] Running as uid=1003 gid=1003 with /data as 'drwxrwxr-x 1 1003 1003 336 May 29 12:57 /data'\n[init] Resolved version given LATEST into 1.19.4 and major version 1.19\n[init] Resolving type given VANILLA\n[init] Setting initial memory to 1G and max to 1G\n[init] Starting the Minecraft server...\nStarting net.minecraft.server.Main\n[12:59:32] [ServerMain/INFO]: Environment: authHost='https://authserver.mojang.com', accountsHost='https://api.mojang.com', sessionHost='https://sessionserver.mojang.com', servicesHost='https://api.minecraftservices.com', name='PROD'\n[12:59:33] [ServerMain/INFO]: Loaded 7 recipes\n[12:59:33] [ServerMain/INFO]: Loaded 1179 advancements\n[12:59:34] [Server thread/INFO]: Starting minecraft server version 1.19.4\n[12:59:34] [Server thread/INFO]: Loading properties\n[12:59:34] [Server thread/INFO]: Default game type: SURVIVAL\n[12:59:34] [Server thread/INFO]: Generating keypair\n[12:59:34] [Server thread/INFO]: Starting Minecraft server on *:25565\n[12:59:34] [Server thread/INFO]: Using epoll channel type\n[12:59:34] [Server thread/INFO]: Preparing level \"world\"\n[12:59:39] [Server thread/INFO]: Preparing start region for dimension minecraft:overworld\n[12:59:39] [Server thread/INFO]: Preparing spawn area: 0%\n[12:59:39] [Worker-Main-2/INFO]: Preparing spawn area: 0%\n[12:59:40] [Worker-Main-2/INFO]: Preparing spawn area: 91%\n[12:59:40] [Worker-Main-1/INFO]: Preparing spawn area: 91%\n[12:59:40] [Server thread/INFO]: Time elapsed: 1816 ms\n[12:59:40] [Server thread/INFO]: Done (6.264s)! For help, type \"help\"\n[12:59:40] [Server thread/INFO]: Starting remote control listener\n[12:59:40] [Server thread/INFO]: Thread RCON Listener started\n[12:59:40] [Server thread/INFO]: RCON running on 0.0.0.0:25575\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#geyser-plugin-for-bedrock-clients","title":"Geyser plugin for Bedrock clients","text":"<p>GeyserMC is a program that allows Bedrock clients to join Java servers. Installing this requires a Paper or Spigot server (source: geysermc.org/download#spigot). This is why the above deployment specifies the server to be of type <code>SPIGOT</code>.</p> <p>Install from latest binary release:</p> <pre><code># su minecraft -c \"wget -O /home/k8s/minecraft-server/plugins/Geyser-Spigot.jar https://download.geysermc.org/v2/projects/geyser/versions/latest/builds/latest/downloads/spigot\"\n# ls -hal /home/k8s/minecraft-server/plugins/\ntotal 14M\ndrwxrwxr-x 1 minecraft minecraft  60 May 29 16:08 .\ndrwxrwxr-x 1 minecraft minecraft 572 May 29 15:48 ..\n-rw-rw-r-- 1 minecraft minecraft 14M May 27 15:19 Geyser-Spigot.jar\ndrwxrwxr-x 1 minecraft minecraft  20 May 29 15:47 PluginMetrics\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#server-commands","title":"Server Commands","text":"<p>To enter commands into the running server, see Interacting with the server and in particular the <code>rcon-cli</code> command that can be run in the container.</p> <p>No need to get the full name of the current pod, which changes when deployment restarts:</p> <pre><code>$ kubectl -n minecraft-server \\\n  exec deploy/minecraft-server \\\n   -- rcon-cli difficulty peaceful\n</code></pre> <p>Note that using <code>deploy/minecraft-server</code> allows running the command in the pod of this deployment without having to get the full name of the pod with <code>kubectl -n minecraft-server get pods</code></p> <p>With this, one can <code>reload</code> the configurations (e.g. <code>server.properties</code>) or even <code>restart</code> the whole Java server without restarting the pod or deployment.</p> <p>Warning</p> <p>Do not try attaching to the TTY of the container.</p> <pre><code>$ kubectl -n minecraft-server get pods\nNAME                                READY   STATUS    RESTARTS       AGE\nminecraft-server-b89954df9-t9dxg   1/1     Running   2 (112s ago)   2m21s\n$ kubectl -n minecraft-server attach -it minecraft-server-b89954df9-t9dxg\nIf you don't see a command prompt, try pressing enter.\n/help\n[15:27:18] [Server thread/INFO]: Unknown command. Type \"/help\" for help.\n</code></pre> <p>This only seems to show the logs, but commands are not accepted, not even <code>/help</code>. Even worse: the only way to detach from the pod is with <code>Ctrl+C</code> and that kills the server without saving the worlds! So it seems the <code>stdin</code> and <code>tty</code> options are not a good idea. </p>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#server-config","title":"Server Config","text":"<p>To adjust server-wide or default settings, edit the <code>server.properties</code> file and reload it, e.g.</p> <pre><code># su minecraft -c \"vi /home/k8s/minecraft-server/server.properties\"\ngamemode=creative\nmotd=Be good\npvp=false\ndifficulty=easy\nmax-players=5\n</code></pre> <p>Then send the reload command to reload the config without restarting the server.</p>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#access-control","title":"Access Control","text":"<p>To restrict access to a few (trusted) users, add them with their UUID to the <code>whitelist.json</code> file:</p> vi/home/k8s/minecraft-server/whitelist.json<pre><code>[\n  {\n    \"uuid\": \"____1e97-____-____-____-f7187fd7____\",\n    \"name\": \"L________a\"\n  },\n  {\n    \"uuid\": \"____41f4-____-____-____-de0061cf____\",\n    \"name\": \"M________t\"\n  }\n]\n</code></pre> <p>To find users\u2019 UUID, check the server\u2019s logs:</p> <pre><code># grep -i uuid /home/k8s/minecraft-server/logs/latest.log \n[14:35:10] [User Authenticator #2/INFO]: UUID of player L________a is ____1e97-____-____-____-f7187fd7____\n[14:44:06] [User Authenticator #3/INFO]: UUID of player M________t is ____41f4-____-____-____-de0061cf____\n</code></pre> <p>Need to restart the deployment to make those changes effective, but first must activate the whitelist by setting the following values in <code>/home/k8s/minecraft-server/server.properties</code></p> /home/k8s/minecraft-server/server.properties<pre><code>white-list=true\nenforce-whitelist=true\n</code></pre> <p>Then send the reload command to reload the config without restarting the server.</p> <p>If restarting the whole server becomes necessary:</p> <pre><code>$ kubectl rollout restart \\\n  deployment/minecraft-server -n minecraft-server\n</code></pre> <p>An easier way would be to use the EasyWhitelist tool in SpigotMC, but it looks like it's broken.</p>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#hourly-backups","title":"Hourly Backups","text":"<p>Minecraft servers rely too much on players behaving, which of course is a strategy that has proven problematic many times over.</p> <p>To recover from any disasters, create hourly backups as the <code>root</code> user in the node. The following scripts creates one full backup per hour, so that even within a single day multiple backups are available to restore:</p> <pre><code>#!/bin/bash\nminecraft_server_cmd () {\n  su coder -c \"kubectl -n minecraft-server exec deploy/minecraft-server -- rcon-cli $*\"\n}\nminecraft_server_cmd \"say Starting full backup.\"\nminecraft_server_cmd \"save-off\"\nminecraft_server_cmd \"save-all\"\nsleep 5\nrsync -ahrtp --delete \\\n  /home/k8s/minecraft-server \\\n  /home/k8s/minecraft-server-backups/$(date +\"%H\")\nminecraft_server_cmd \"save-on\"\nminecraft_server_cmd \"say Backup complete.\"\n</code></pre> <p>Put this script in a dedicated directory for the backups and run it every hour with <code>crontab</code>:</p> <pre><code># mkdir /home/k8s/minecraft-server-backups\n# vi /home/k8s/minecraft-server-backups/backup.sh\n# crontab -e\n00  * * * * /home/k8s/minecraft-server-backups/backup.sh\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#daily-server-restart","title":"Daily Server Restart","text":"<p>To keep the server up to date, it is easiest to just restart the whole deployment every day. Besides, there is no need to keep the server running overnight because this is a private server, not used from multiple timezones.</p> <p>Find the <code>minecraft-start-k8s</code> and <code>minecraft-stop-k8s</code> script in the Appendix below.</p> <p>Note</p> <p>These commands must be run as the user who has the credentials to run <code>kubectl</code>:</p> <pre><code>$ crontab -e\n10  6 * * *   /home/coder/bin/minecraft-start-k8s\n30 22 * * *   /home/coder/bin/minecraft-stop-k8s\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#world-reset","title":"World Reset","text":"<p>If at some point we want to start a new world, it is as simple as renaming <code>/home/k8s/minecraft-server/world</code> to any name when the server is not running. The next time the server starts, a new  <code>/home/k8s/minecraft-server/world</code> folder will  be created with a whole new world.</p> <pre><code># /home/coder/bin/minecraft-stop-k8s\n# mv /home/k8s/minecraft-server/world \\\n  /home/k8s/minecraft-server/old_world\n# /home/coder/bin/minecraft-start-k8s\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#appendix-more-server-commands","title":"Appendix: more server commands","text":""},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#minecraft-server-fortune","title":"<code>minecraft-server-fortune</code>","text":"<p>The <code>minecraft-server-fortune</code> script quotes funny lines on everybody's console.</p> <p>{% raw %} </p>minecraft-server-fortune<pre><code>#!/bin/bash\nminecraft_server_cmd () {\n  kubectl -n minecraft-server exec deploy/minecraft-server -- rcon-cli $*\n}\n\ndeclare -a fortunes\nfortunes[0]=\"Be good, or be gone.\"\nfortunes[1]=\"Be nice, or pay the price.\"\nfortunes[2]=\"Think of the others, don't be a bother.\"\nfortunes[3]=\"Utilize Bearded Dragon.\"\nfortunes[4]=\"Agree to disagree.\"\nfortunes[5]=\"Steams my broccoli.\"\nlen=${#fortunes[@]}-1\ni=$(shuf -i 0-$((len-1)) -n 1)\nfortune=${fortunes[$i]}\nminecraft_server_cmd \"say $fortune\"\n</code></pre><p></p>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#minecraft-server-kick","title":"<code>minecraft-server-kick</code>","text":"<p>The <code>minecraft-server-kick</code> script will <code>kick</code> the user that is passed as argument:</p> minecraft-server-kick<pre><code>#!/bin/bash\nminecraft_server_cmd () {\n  kubectl -n minecraft-server exec deploy/minecraft-server -- rcon-cli $*\n}\n\nminecraft_server_cmd \"kick $1\"\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#minecraft-server-make-creative","title":"<code>minecraft-server-make-creative</code>","text":"<p>The <code>minecraft-server-make-creative</code> script changes the server into creative mode.</p> minecraft-server-make-creative<pre><code>#!/bin/bash\nminecraft_server_cmd () {\n  kubectl -n minecraft-server exec deploy/minecraft-server -- rcon-cli $*\n}\n\nminecraft_server_cmd \"gamemode creative M________t\"\nminecraft_server_cmd \"gamemode creative L________a\"\nminecraft_server_cmd \"difficulty peaceful\"\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#minecraft-server-make-survival","title":"<code>minecraft-server-make-survival</code>","text":"<p>The <code>minecraft-server-make-survival</code> script changes the server into survival mode.</p> minecraft-server-make-survival<pre><code>#!/bin/bash\nminecraft_server_cmd () {\n  kubectl -n minecraft-server exec deploy/minecraft-server -- rcon-cli $*\n}\n\nminecraft_server_cmd \"gamemode survival M________t\"\nminecraft_server_cmd \"gamemode survival L________a\"\nminecraft_server_cmd \"difficulty normal\"\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#minecraft-server-say-shutdown","title":"<code>minecraft-server-say-shutdown</code>","text":"<p>The <code>minecraft-server-say-shutdown</code> script shuts the server down (necessary before making a backup).</p> minecraft-server-say-shutdown<pre><code>#!/bin/bash\nminecraft_server_cmd () {\n  kubectl -n minecraft-server exec deploy/minecraft-server -- rcon-cli $*\n}\n\nminecraft_server_cmd \"say WARNING: server WILL SHUT DOWN in $time\"\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#minecraft-server-tp-l-a-m-t","title":"<code>minecraft-server-tp-l--------a-m--------t</code>","text":"<p>The <code>minecraft-server-tp-l--------a-m--------t</code> script teleports (<code>tp</code>) a certain user wher the other one is.</p> minecraft-server-tp-l--------a-m--------t<pre><code>#!/bin/bash\nminecraft_server_cmd () {\n  kubectl -n minecraft-server exec deploy/minecraft-server -- rcon-cli $*\n}\n\nminecraft_server_cmd \"tp L________a M________t\"\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#minecraft-server-tp-l-a-m-t_1","title":"<code>minecraft-server-tp-l--------a-m--------t</code>","text":"<p>The <code>minecraft-server-tp-S-----------e-m--------t</code> script  teleports (<code>tp</code>) a certain user wher the other one is.</p> minecraft-server-tp-l--------a-m--------t<pre><code>#!/bin/bash\nminecraft_server_cmd () {\n  kubectl -n minecraft-server exec deploy/minecraft-server -- rcon-cli $*\n}\n\nminecraft_server_cmd \"tp S______________1 M________t\"\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#minecraft-start-k8s","title":"<code>minecraft-start-k8s</code>","text":"<p>The <code>minecraft-start-k8s</code> script starts the server by applying the deployment.</p> minecraft-start-k8s<pre><code>#!/bin/bash\n\ncd /home/coder/head/lexicon-deployments\ngit pull\nkubectl apply -f minecraft-server.yaml\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#minecraft-stop-k8s","title":"<code>minecraft-stop-k8s</code>","text":"<p>The <code>minecraft-stop-k8s</code> script stops the server by removing the deployment.</p> minecraft-stop-k8s<pre><code>#!/bin/bash\n\nkubectl delete -n minecraft-server deployment minecraft-server\n</code></pre>"},{"location":"blog/2023/08/10/running-minecraft-java-server-for-bedrock-clients-on-kubernetes/#minecraft-logs","title":"<code>minecraft-logs</code>","text":"<p>The <code>minecraft-logs</code> script shows server logs as they are produces (with <code>-f</code>).</p> minecraft-logs<pre><code>#!/bin/bash\n\npod=$(kubectl get pods -n minecraft-server | grep '1/1' | awk '{print $1}')\nkubectl -n minecraft-server logs -f \"${pod}\"\n</code></pre>"},{"location":"blog/2023/08/27/xhci-host-controller-not-responding-assume-dead/","title":"xHCI host controller not responding, assume dead","text":"<p>Today, a number of USB devices were unavailable for no apparent reason.</p> <p>Having a terminal always visible running <code>dmesg -w</code> the following showed up:</p> <pre><code>xhci_hcd 0000:0b:00.3: Abort failed to stop command ring: -110\nxhci_hcd 0000:0b:00.3: xHCI host controller not responding, assume dead\nxhci_hcd 0000:0b:00.3: HC died; cleaning up\n</code></pre> <p>Hmm, assume dead... that doesn\u2019t sounds good.</p> <p>Searching for posts about these error messages, the best match that showed up was  techoverflow.net/2021/09/16/how-i-fixed-xhci-host-controller-not-responding-assume-dead which summarizes the discussion and findings from this thread in the ArchLinux forums: xHCI host controller not responding, assume dead.</p> <p>The solution provided there worked in this case too, without even waiting for 5 seconds between the unbind and bind commands:</p> <pre><code>echo -n 0000:0b:00.3 &gt; /sys/bus/pci/drivers/xhci_hcd/unbind\necho -n 0000:0b:00.3 &gt; /sys/bus/pci/drivers/xhci_hcd/bind\n</code></pre> <p>After restarting this controller a few times, it was never again problematic.</p> <p>Created script for future use and reference:</p> restart-usb.sh<pre><code>#!/bin/bash\n#\n# Workaround for\n#\n# xhci_hcd 0000:0b:00.3: Abort failed to stop command ring: -110\n# xhci_hcd 0000:0b:00.3: xHCI host controller not responding, assume dead\n# xhci_hcd 0000:0b:00.3: HC died; cleaning up\n#\n# Run with 0000:0b:00.3 (id).\n#\n# Sources:\n# https://techoverflow.net/2021/09/16/how-i-fixed-xhci-host-controller-not-responding-assume-dead/\n\nid=$1\necho -n \"$id\" &gt; /sys/bus/pci/drivers/xhci_hcd/unbind\nsleep 1\necho -n \"$id\" &gt; /sys/bus/pci/drivers/xhci_hcd/bind\n</code></pre>"},{"location":"blog/2023/08/27/xhci-host-controller-not-responding-assume-dead/#update-2023-12-09","title":"Update (2023-12-09)","text":"<p>The same, or similar, problem happened again when leaving the computer running overnight while most devices powered off. What is most strange is that the problem seems to have started when (be triggered by?) powering most USB devices off:</p> <pre><code>usb 5-2: USB disconnect, device number 3\nusb 5-2.7: USB disconnect, device number 7\nretire_capture_urb: 2 callbacks suppressed\nusb 6-2: USB disconnect, device number 2\nr8152-cfgselector 6-2.6: USB disconnect, device number 3\nusb 5-3.2.3.1: USB disconnect, device number 24\nusb 5-3.2.3.1.6: USB disconnect, device number 25\nusb 5-4.1: USB disconnect, device number 8\nusb 5-4.1.6: USB disconnect, device number 10\nxhci_hcd 0000:0a:00.3: Abort failed to stop command ring: -110\nxhci_hcd 0000:0a:00.3: xHCI host controller not responding, assume dead\nxhci_hcd 0000:0a:00.3: HC died; cleaning up\nxhci_hcd 0000:0a:00.3: Timeout while waiting for configure endpoint command\nxhci_hcd 0000:0a:00.3: Unsuccessful disable slot 23 command, status 25\nusb 5-1: USB disconnect, device number 2\nusb 5-3.3: Not enough bandwidth for altsetting 0\nusb 5-3: USB disconnect, device number 4\nusb 5-3.1: USB disconnect, device number 6\nusb 5-3.2: USB disconnect, device number 9\nusb 5-3.2.1: USB disconnect, device number 13\nusb 5-3.4.4.1: Not enough bandwidth for altsetting 0\nusb 5-3.4.4.1: 1:0: usb_set_interface failed (-19)\n</code></pre>"},{"location":"blog/2023/08/27/xhci-host-controller-not-responding-assume-dead/#update-2025-01-05","title":"Update (2025-01-05)","text":"<p>Again, lots of errors in <code>dmesg</code> when startint the PC after a 2-week break:</p> <pre><code>[    6.794420] usb 5-1.1: new high-speed USB device number 17 using xhci_hcd\n[    6.806582] nvidia-nvlink: Nvlink Core is being initialized, major device number 236\n[    6.807921] nvidia 0000:09:00.0: vgaarb: VGA decodes changed: olddecodes=io+mem,decodes=none:owns=none\n[    6.852458] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  550.120  Fri Sep 13 10:10:01 UTC 2024\n[    6.863691] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver for UNIX platforms  550.120  Fri Sep 13 10:01:25 UTC 2024\n[    6.881884] [drm] [nvidia-drm] [GPU ID 0x00000900] Loading driver\n[    6.905141] input: HDA NVidia HDMI/DP,pcm=7 as /devices/pci0000:00/0000:00:03.1/0000:09:00.1/sound/card0/input23\n[    6.905812] input: HDA NVidia HDMI/DP,pcm=8 as /devices/pci0000:00/0000:00:03.1/0000:09:00.1/sound/card0/input24\n[    6.905950] input: HDA NVidia HDMI/DP,pcm=9 as /devices/pci0000:00/0000:00:03.1/0000:09:00.1/sound/card0/input25\n[    6.908061] input: HD-Audio Generic Rear Mic as /devices/pci0000:00/0000:00:08.1/0000:0b:00.4/sound/card1/input27\n[    6.908214] input: HD-Audio Generic Line as /devices/pci0000:00/0000:00:08.1/0000:0b:00.4/sound/card1/input28\n[    6.908300] input: HD-Audio Generic Line Out Front as /devices/pci0000:00/0000:00:08.1/0000:0b:00.4/sound/card1/input29\n[    6.908403] input: HD-Audio Generic Line Out Surround as /devices/pci0000:00/0000:00:08.1/0000:0b:00.4/sound/card1/input30\n[    6.908502] input: HD-Audio Generic Line Out CLFE as /devices/pci0000:00/0000:00:08.1/0000:0b:00.4/sound/card1/input31\n[    6.908616] input: HD-Audio Generic Front Headphone as /devices/pci0000:00/0000:00:08.1/0000:0b:00.4/sound/card1/input32\n[    7.634589] usb 5-1-port1: Cannot enable. Maybe the USB cable is bad?\n[    7.802425] usb 5-1.1: new high-speed USB device number 18 using xhci_hcd\n[    7.876154] BTRFS: device fsid e290b1ae-192c-46b9-9ddb-3680d3cefd73 devid 1 transid 606 /dev/nvme0n1p4 scanned by mount (1754)\n[    7.876234] BTRFS: device fsid 6b809fc0-0b85-4041-ac25-47ec4682f5f5 devid 1 transid 28482 /dev/sdb scanned by mount (1755)\n[    7.876483] BTRFS info (device nvme0n1p4): first mount of filesystem e290b1ae-192c-46b9-9ddb-3680d3cefd73\n[    7.876498] BTRFS info (device nvme0n1p4): using crc32c (crc32c-intel) checksum algorithm\n[    7.876502] BTRFS info (device nvme0n1p4): using free-space-tree\n[    7.880380] BTRFS info (device sda): first mount of filesystem a4ee872d-b985-445f-94a2-15232e93dcd5\n[    7.880386] BTRFS info (device sda): using crc32c (crc32c-intel) checksum algorithm\n[    7.880390] BTRFS info (device sda): using free-space-tree\n[    7.880737] BTRFS: device fsid 5cf65a95-4ae5-41ed-9a14-7d7fbeee1951 devid 1 transid 574916 /dev/sdc scanned by mount (1757)\n[    7.880970] BTRFS info (device sdc): first mount of filesystem 5cf65a95-4ae5-41ed-9a14-7d7fbeee1951\n[    7.880975] BTRFS info (device sdc): using crc32c (crc32c-intel) checksum algorithm\n[    7.880979] BTRFS info (device sdc): disk space caching is enabled\n[    7.884039] BTRFS info (device sdb): first mount of filesystem 6b809fc0-0b85-4041-ac25-47ec4682f5f5\n[    7.884045] BTRFS info (device sdb): using crc32c (crc32c-intel) checksum algorithm\n[    7.884048] BTRFS info (device sdb): using free-space-tree\n[    7.891874] BTRFS info (device sdc): bdev /dev/sdc errs: wr 0, rd 0, flush 0, corrupt 1, gen 0\n[    7.926088] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:09:00.0 on minor 1\n[    8.040524] nvidia_uvm: module uses symbols nvUvmInterfaceDisableAccessCntr from proprietary module nvidia, inheriting taint.\n[    8.062807] nvidia-uvm: Loaded the UVM driver, major device number 234.\n[    8.170951] usb 5-1.1: device descriptor read/64, error -71\n[    8.466340] usb 5-1.1: New USB device found, idVendor=534d, idProduct=2109, bcdDevice=21.00\n[    8.466348] usb 5-1.1: New USB device strings: Mfr=1, Product=0, SerialNumber=0\n[    8.466351] usb 5-1.1: Manufacturer: MACROSILICON\n[    8.506594] hid-generic 0003:534D:2109.0011: hiddev1,hidraw2: USB HID v1.10 Device [MACROSILICON] on usb-0000:0b:00.3-1.1/input4\n[    8.529034] usb 5-1.4.4.2: new full-speed USB device number 19 using xhci_hcd\n[    8.529417] mc: Linux media interface: v0.10\n[    8.543738] videodev: Linux video capture interface: v2.00\n[    8.552875] usb 5-1.1: Found UVC 1.00 device &lt;unnamed&gt; (534d:2109)\n[    8.570883] usbcore: registered new interface driver uvcvideo\n[    8.575010] Bluetooth: Core ver 2.22\n[    8.575027] NET: Registered PF_BLUETOOTH protocol family\n[    8.575028] Bluetooth: HCI device and connection manager initialized\n[    8.575032] Bluetooth: HCI socket layer initialized\n[    8.575034] Bluetooth: L2CAP socket layer initialized\n[    8.575037] Bluetooth: SCO socket layer initialized\n[    8.633644] usbcore: registered new interface driver btusb\n[    8.639658] usb 5-1.4.4.2: New USB device found, idVendor=1038, idProduct=12c2, bcdDevice= 0.74\n[    8.639664] usb 5-1.4.4.2: New USB device strings: Mfr=1, Product=2, SerialNumber=0\n[    8.639667] usb 5-1.4.4.2: Product: SteelSeries Arctis 9\n[    8.639670] usb 5-1.4.4.2: Manufacturer: SteelSeries\n[    8.708836] Bluetooth: hci0: BCM: chip id 63\n[    8.709836] Bluetooth: hci0: BCM: features 0x07\n[    8.725868] Bluetooth: hci0: BCM20702A\n[    8.725873] Bluetooth: hci0: BCM20702A1 (001.002.014) build 0000\n[    8.727715] Bluetooth: hci0: BCM: firmware Patch file not found, tried:\n[    8.728070] Bluetooth: hci0: BCM: 'brcm/BCM20702A1-0b05-17cb.hcd'\n[    8.728316] Bluetooth: hci0: BCM: 'brcm/BCM-0b05-17cb.hcd'\n[    8.825867] hid-generic 0003:1038:12C2.0012: hiddev4,hidraw16: USB HID v1.11 Device [SteelSeries SteelSeries Arctis 9] on usb-0000:0b:00.3-1.4.4.2/input0\n[    8.828010] input: SteelSeries SteelSeries Arctis 9 as /devices/pci0000:00/0000:00:08.1/0000:0b:00.3/usb5/5-1/5-1.4/5-1.4.4/5-1.4.4.2/5-1.4.4.2:1.1/0003:1038:12C2.0013/input/input33\n[    8.879513] hid-generic 0003:1038:12C2.0013: input,hidraw17: USB HID v1.11 Device [SteelSeries SteelSeries Arctis 9] on usb-0000:0b:00.3-1.4.4.2/input1\n[    8.889781] hid-generic 0003:1038:12C2.0014: hiddev5,hidraw18: USB HID v1.11 Device [SteelSeries SteelSeries Arctis 9] on usb-0000:0b:00.3-1.4.4.2/input2\n[    9.080415] usb 5-1.4.4.4: new full-speed USB device number 20 using xhci_hcd\n[    9.139554] usbcore: registered new interface driver snd-usb-audio\n[    9.921744] usb 5-1.4.4.1: new full-speed USB device number 21 using xhci_hcd\n[   10.009656] usb 5-1.4.4.1: New USB device found, idVendor=1038, idProduct=12c4, bcdDevice= 0.03\n[   10.009663] usb 5-1.4.4.1: New USB device strings: Mfr=1, Product=2, SerialNumber=3\n[   10.009665] usb 5-1.4.4.1: Product: SteelSeries Arctis 9\n[   10.009667] usb 5-1.4.4.1: Manufacturer: SteelSeries\n[   10.009669] usb 5-1.4.4.1: SerialNumber: 000000000000\n[   10.330714] hid-generic 0003:1038:12C4.0015: No inputs registered, leaving\n[   10.331716] hid-generic 0003:1038:12C4.0015: hidraw19: USB HID v1.11 Device [SteelSeries SteelSeries Arctis 9] on usb-0000:0b:00.3-1.4.4.1/input5\n[   16.544125] audit: type=1400 audit(1736080991.270:2): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"ch-run\" pid=2463 comm=\"apparmor_parser\"\n[   16.544131] audit: type=1400 audit(1736080991.270:3): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"QtWebEngineProcess\" pid=2456 comm=\"apparmor_parser\"\n[   16.544134] audit: type=1400 audit(1736080991.270:4): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"buildah\" pid=2459 comm=\"apparmor_parser\"\n[   16.544136] audit: type=1400 audit(1736080991.270:5): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"element-desktop\" pid=2468 comm=\"apparmor_parser\"\n[   16.544139] audit: type=1400 audit(1736080991.270:6): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=4D6F6E676F444220436F6D70617373 pid=2455 comm=\"apparmor_parser\"\n[   16.544141] audit: type=1400 audit(1736080991.270:7): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"chrome\" pid=2464 comm=\"apparmor_parser\"\n[   16.544144] audit: type=1400 audit(1736080991.270:8): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"crun\" pid=2466 comm=\"apparmor_parser\"\n[   16.544146] audit: type=1400 audit(1736080991.270:9): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"devhelp\" pid=2467 comm=\"apparmor_parser\"\n[   16.544148] audit: type=1400 audit(1736080991.270:10): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"busybox\" pid=2460 comm=\"apparmor_parser\"\n[   16.544151] audit: type=1400 audit(1736080991.270:11): apparmor=\"STATUS\" operation=\"profile_load\" profile=\"unconfined\" name=\"balena-etcher\" pid=2457 comm=\"apparmor_parser\"\n[   16.596655] cfg80211: Loading compiled-in X.509 certificates for regulatory database\n[   16.596768] Loaded X.509 cert 'sforshee: 00b28ddf47aef9cea7'\n[   16.596857] Loaded X.509 cert 'wens: 61c038651aabdcf94bd0ac7ff06c7248db18c600'\n[   16.800779] Bluetooth: BNEP (Ethernet Emulation) ver 1.3\n[   16.800782] Bluetooth: BNEP filters: protocol multicast\n[   16.800789] Bluetooth: BNEP socket layer initialized\n[   16.802639] Bluetooth: MGMT ver 1.22\n[   16.806138] NET: Registered PF_ALG protocol family\n[   16.956495] loop29: detected capacity change from 0 to 8\n[   16.959123] block nvme0n1: No UUID available providing old NGUID\n[   16.971925] NET: Registered PF_QIPCRTR protocol family\n[   19.452853] igc 0000:05:00.0 enp5s0: NIC Link is Up 1000 Mbps Full Duplex, Flow Control: RX/TX\n[   19.634474] systemd-journald[875]: Time jumped backwards, rotating.\n[   20.856614] usb 5-1.1: reset high-speed USB device number 18 using xhci_hcd\n[   21.706730] usb 5-1-port1: Cannot enable. Maybe the USB cable is bad?\n[   22.562724] usb 5-1-port1: Cannot enable. Maybe the USB cable is bad?\n[   22.730662] usb 5-1.1: reset high-speed USB device number 18 using xhci_hcd\n[   23.146428] usb 5-1.1: device not accepting address 18, error -22\n[   24.002722] usb 5-1-port1: Cannot enable. Maybe the USB cable is bad?\n[   24.003710] usb 5-1.1: USB disconnect, device number 18\n[   24.914594] usb 5-1-port1: Cannot enable. Maybe the USB cable is bad?\n[   25.090417] usb 5-1.1: new high-speed USB device number 23 using xhci_hcd\n[   25.805129] Bluetooth: RFCOMM TTY layer initialized\n[   25.805136] Bluetooth: RFCOMM socket layer initialized\n[   25.805140] Bluetooth: RFCOMM ver 1.11\n[   25.970585] usb 5-1-port1: Cannot enable. Maybe the USB cable is bad?\n[   25.970751] usb 5-1-port1: attempt power cycle\n[   27.122585] usb 5-1-port1: Cannot enable. Maybe the USB cable is bad?\n[   27.290416] usb 5-1.1: new high-speed USB device number 25 using xhci_hcd\n[   27.290457] usb 5-1.1: Device not responding to setup address.\n[   27.498459] usb 5-1.1: Device not responding to setup address.\n[   27.706423] usb 5-1.1: device not accepting address 25, error -71\n[   27.706502] usb 5-1-port1: unable to enumerate USB device\n[   28.666719] usb 5-1-port1: Cannot enable. Maybe the USB cable is bad?\n[   28.834418] usb 5-1.1: new high-speed USB device number 27 using xhci_hcd\n[   29.674720] usb 5-1-port1: Cannot enable. Maybe the USB cable is bad?\n[   29.675017] usb 5-1-port1: attempt power cycle\n[   30.842707] usb 5-1-port1: Cannot enable. Maybe the USB cable is bad?\n[   31.030612] loop29: detected capacity change from 0 to 360192\n[   31.698707] usb 5-1-port1: Cannot enable. Maybe the USB cable is bad?\n[   31.698999] usb 5-1-port1: unable to enumerate USB device\n[   75.560962] usb 5-1.4.3: new full-speed USB device number 30 using xhci_hcd\n[   75.710609] usb 5-1.4.3: New USB device found, idVendor=046d, idProduct=c52b, bcdDevice=24.11\n[   75.710616] usb 5-1.4.3: New USB device strings: Mfr=1, Product=2, SerialNumber=0\n[   75.710618] usb 5-1.4.3: Product: USB Receiver\n[   75.710620] usb 5-1.4.3: Manufacturer: Logitech\n[   75.840778] input: Logitech USB Receiver as /devices/pci0000:00/0000:00:08.1/0000:0b:00.3/usb5/5-1/5-1.4/5-1.4.3/5-1.4.3:1.0/0003:046D:C52B.0016/input/input35\n[   75.937910] hid-generic 0003:046D:C52B.0016: input,hidraw2: USB HID v1.11 Keyboard [Logitech USB Receiver] on usb-0000:0b:00.3-1.4.3/input0\n[   75.941846] input: Logitech USB Receiver Mouse as /devices/pci0000:00/0000:00:08.1/0000:0b:00.3/usb5/5-1/5-1.4/5-1.4.3/5-1.4.3:1.1/0003:046D:C52B.0017/input/input36\n[   75.941972] input: Logitech USB Receiver Consumer Control as /devices/pci0000:00/0000:00:08.1/0000:0b:00.3/usb5/5-1/5-1.4/5-1.4.3/5-1.4.3:1.1/0003:046D:C52B.0017/input/input37\n[   75.994233] input: Logitech USB Receiver System Control as /devices/pci0000:00/0000:00:08.1/0000:0b:00.3/usb5/5-1/5-1.4/5-1.4.3/5-1.4.3:1.1/0003:046D:C52B.0017/input/input38\n[   75.994571] hid-generic 0003:046D:C52B.0017: input,hiddev1,hidraw20: USB HID v1.11 Mouse [Logitech USB Receiver] on usb-0000:0b:00.3-1.4.3/input1\n[   75.998911] hid-generic 0003:046D:C52B.0018: hiddev6,hidraw21: USB HID v1.11 Device [Logitech USB Receiver] on usb-0000:0b:00.3-1.4.3/input2\n[   76.295219] logitech-djreceiver 0003:046D:C52B.0018: hiddev1,hidraw2: USB HID v1.11 Device [Logitech USB Receiver] on usb-0000:0b:00.3-1.4.3/input2\n[   76.405080] input: Logitech Wireless Device PID:406a Keyboard as /devices/pci0000:00/0000:00:08.1/0000:0b:00.3/usb5/5-1/5-1.4/5-1.4.3/5-1.4.3:1.2/0003:046D:C52B.0018/0003:046D:406A.0019/input/input40\n[   76.464687] input: Logitech Wireless Device PID:406a Mouse as /devices/pci0000:00/0000:00:08.1/0000:0b:00.3/usb5/5-1/5-1.4/5-1.4.3/5-1.4.3:1.2/0003:046D:C52B.0018/0003:046D:406A.0019/input/input41\n[   76.464808] hid-generic 0003:046D:406A.0019: input,hidraw20: USB HID v1.11 Keyboard [Logitech Wireless Device PID:406a] on usb-0000:0b:00.3-1.4.3/input2:1\n[   76.646991] input: Logitech MX Anywhere 2S as /devices/pci0000:00/0000:00:08.1/0000:0b:00.3/usb5/5-1/5-1.4/5-1.4.3/5-1.4.3:1.2/0003:046D:C52B.0018/0003:046D:406A.0019/input/input45\n[   76.703921] logitech-hidpp-device 0003:046D:406A.0019: input,hidraw20: USB HID v1.11 Keyboard [Logitech MX Anywhere 2S] on usb-0000:0b:00.3-1.4.3/input2:1\n[   80.767209] logitech-hidpp-device 0003:046D:406A.0019: HID++ 4.5 device connected.\n[   92.328643] show_signal: 123 callbacks suppressed\n</code></pre> <p>Yet eventually all USB devices seem to be available:</p> <pre><code>Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 001 Device 002: ID 1d6b:0104 Linux Foundation Multifunction Composite Gadget\nBus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 003 Device 002: ID 0b05:1939 ASUSTek Computer, Inc. AURA LED Controller\nBus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 005 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 005 Device 002: ID 1a40:0101 Terminus Technology Inc. Hub\nBus 005 Device 003: ID 04a9:1912 Canon, Inc. LiDE 400\nBus 005 Device 005: ID 05e3:0610 Genesys Logic, Inc. Hub\nBus 005 Device 006: ID 041e:324d Creative Technology, Ltd Sound Blaster Play! 3\nBus 005 Device 007: ID 09ea:0130 Generic Virtual HUB\nBus 005 Device 008: ID 05e3:0610 Genesys Logic, Inc. Hub\nBus 005 Device 009: ID 09ea:0131 Generic Virtual HID\nBus 005 Device 010: ID 2109:2817 VIA Labs, Inc. USB2.0 Hub             \nBus 005 Device 011: ID 4b4b:0001 TheRoyalSweatshirt romac\nBus 005 Device 012: ID 3233:1311 Ducky Ducky One 3 RGB\nBus 005 Device 013: ID 046d:c545 Logitech, Inc. USB Receiver\nBus 005 Device 014: ID 0b05:17cb ASUSTek Computer, Inc. Broadcom BCM20702A0 Bluetooth\nBus 005 Device 015: ID 058f:9254 Alcor Micro Corp. Hub\nBus 005 Device 019: ID 1038:12c2 SteelSeries ApS SteelSeries Arctis 9\nBus 005 Device 021: ID 1038:12c4 SteelSeries ApS SteelSeries Arctis 9\nBus 005 Device 030: ID 046d:c52b Logitech, Inc. Unifying Receiver\nBus 006 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\n</code></pre> <p>After turning it off and on again a few hours later, nearly no errors:</p> <pre><code>[   15.659476] usb 5-1.4.4.1: device descriptor read/64, error -32\n[   15.855391] usb 5-1.4.4.1: New USB device found, idVendor=1038, idProduct=12c4, bcdDevice= 0.03\n[   15.855397] usb 5-1.4.4.1: New USB device strings: Mfr=1, Product=2, SerialNumber=3\n[   15.855400] usb 5-1.4.4.1: Product: SteelSeries Arctis 9\n[   15.855402] usb 5-1.4.4.1: Manufacturer: SteelSeries\n[   15.855405] usb 5-1.4.4.1: SerialNumber: 000000000000\n</code></pre> <p>The output from <code>lsusb</code> is the same as above except for the order of devices in <code>Bus 005</code></p>"},{"location":"blog/2023/09/09/the-death-of-wi-fi-and-bluetooth/","title":"The death of Wi-Fi and Bluetooth","text":"<p>The Wi-Fi 6E &amp; Bluetooth 5.2 controllers in my motherboard died today.</p> <p>Until the motherboard can be replaced, the solution is to disable both in the UEFI BIOS. This is the only state in which the PC boots normally. Enabling the Bluetooth controller causes the boot process to spend about a minute trying to initialize the device, enabling the Wi-Fi controller causes the whole system to freeze with at the login screen and/or eventually reboot itself.</p> <p>The motherboard model is  ASUS MB TUF GAMING X570-PRO WIFI II and the wireless controller is, according to this one post on Reddit, is the Mediatek RZ608. The settings to disable both controllers are under Advanced Mode(F7) \u2192 Advanced \u2192 Onboard Devices Configuration.</p> <p>After disabling both controllers, select Exit \u2192 Save &amp; reset</p> <ul> <li><code>Wi-Fi Controller [Enabled]-&gt;[Disabled]</code></li> <li><code>Bluetooth Controller [Enabled]-&gt;[Disabled]</code></li> </ul>"},{"location":"blog/2023/09/09/the-death-of-wi-fi-and-bluetooth/#what-happened","title":"What Happened","text":"<p>Saturday morning, coffee is ready, feeling like playing of Skyrim for a bit...</p> <p>Turn the PC on, suddenly Nature calls...  gotta piss off for a minute.</p> <p>Back to the PC, ready to... wait, where\u2019s the login greeter?</p> <p>The screen is just gray, not even back, only the mouse cursor is visible. I move it around a bit, nothing happens. Hit Enter, the PC reboots. What. The. ...</p> <p>Time to boot into recovery mode then, and pay close attention to what comes up. The boot process stops abruptly after printing messages about all the USB devices found and then, slowly over the next minute, the following shows up:</p> <pre><code>[    6.504390] usb 1-4: device descriptor read/64, error -110\n[   22.376392] usb 1-4: device descriptor read/64, error -110\n[   22.600375] usb 1-4: new high-speed USB device number 3 using xhci_hcd\n[   28.008393] usb 1-4: device descriptor read/64, error -110\n[   43.880388] usb 1-4: device descriptor read/64, error -110\n[   43.990399] usb usb1-port4: attempt power cycle\n[   44.618358] usb 1-4: new high-speed USB device number 4 using xhci_hcd\n[   49.910386] xhci_hcd 0000:05:00.1: Timeout while waiting for setup device command\n[   55.542386] xhci_hcd 0000:05:00.1: Timeout while waiting for setup device command\n[   55.750358] usb 1-4: device not accepting address 4, error -62\n[   55.865359] usb 1-4: new high-speed USB device number 5 using xhci_hcd\n[   61.174387] xhci_hcd 0000:05:00.1: Timeout while waiting for setup device command\n[   66.806387] xhci_hcd 0000:05:00.1: Timeout while waiting for setup device command\n[   67.014360] usb 1-4: device not accepting address 5, error -62\n[   67.015483] usb usb1-port4: unable to enumerate USB device\n</code></pre> These lines scrolled away pretty fast, how to capture them as text? <p>Have a smart phone (or other device with a fast camera) ready to take photos of messages as they show up. Be ready to take many photos so you can later find one that shows what you need.</p> <p></p> <p>Later, you may be able to recover those lines from logs (more on this below).</p> <p>The recover tool shows up but the only option that seems useful at this point is to <code>Drop to root shell</code> so there we go.</p> <p>The system freezes again after just a few seconds, barely enough time to find the script to restart a USB controller, but not time enough to run it.</p> <p>This is ridiculous! How can I possibly even try anything if the whole system freezes within seconds?</p>"},{"location":"blog/2023/09/09/the-death-of-wi-fi-and-bluetooth/#regaining-control","title":"Regaining Control","text":"<p>When hardware is acting up like this, the first thing to do is to strip it down to the barest minimum configuration you can work with.</p> <p>So I unplugged all the USB devices.</p> <p>I have lots of them, including enough hubs I\u2019ve lost count of them.</p> <p>But wait! A keyboard is required and all my keyboards are USB. A mouse is also necessary, or at least very convenient, to navigate the UEFI BIOS utility</p> <p>So the minimum configuration I could work with was with the wireless keyboard (Logitech G G915 TKL) and mouse (Logitech MX Anywhere 2S), each with their own USB receiver, both plugged on the front ports.</p> <p>To further reduce the number of devices involved, I went into the UEFI BIOS utility and disabled the Wi-Fi Controller under Advanced Mode(F7) \u2192 Advanced \u2192 Onboard Devices Configuration.</p> <p>And to reduce also the number of processes involved, the next step was to boot the system in less than recovery mode:</p> <ol> <li>Mash the <code>Shift</code> key to make Grub show the menu (optional).</li> <li>Select the relevant entry (usually the first, default).</li> <li>Press <code>e</code> to edit this entry.</li> <li>Go to the line that starts with <code>linux</code> and     add <code>init=/bin/bash</code> at the end of it.</li> <li>Press <code>Ctrl+x</code> to boot with this modified entry.</li> </ol> <p>This boots the system into just the shell, with only the root file system mounted read-only. If it\u2019s possible to run a barer system, I don\u2019t know how \ud83d\ude05</p> <p>At this point <code>lsusb</code> shows pretty much nothing, as expected:</p> <pre><code># lsusb\nBus 006 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 005 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 003 Device 002: ID 0b05:1939 ASUSTek Computer, Inc. AURA LED Controller\nBus 005 Device 014: ID 046d:c52b Logitech, Inc. Unifying Receiver\nBus 005 Device 012: ID 046d:c545 Logitech, Inc. USB Receiver\nBus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\n</code></pre> <p>The good news is, this barest minimum configuration does not freeze seconds after boot. The bad news is, I don\u2019t really know how to go from here.</p>"},{"location":"blog/2023/09/09/the-death-of-wi-fi-and-bluetooth/#root-cause-analysis","title":"Root Cause Analysis","text":"<p>There is still something wrong with something on USB, which is causing the timeouts to delay the boot process for a whole minute. It would be best to find out what it is, and fix it, before adding complexity back to return to the original (normal) state. The first objective is to find out what is wrong.</p> <p>Time to go and search around for posts about these errors, in particular:</p> <pre><code>[   66.806387] xhci_hcd 0000:05:00.1: Timeout while waiting for setup device command\n[   67.014360] usb 1-4: device not accepting address 5, error -62\n[   67.015483] usb usb1-port4: unable to enumerate USB device\n</code></pre> <p>This looks like USB controller <code>0000:05:00.1</code> is not responding.</p> <p>Among the many posts that came up searching for <code>linux usb \"device not accepting address\" \"error -62\"</code> <code>\"unable to enumerate USB device\"</code> the following offered valuable clues:</p> <ul> <li>What is USB error -62? is explained in     askubuntu.com/a/749864     by pointing to     <code>/usr/include/asm-generic/errno.h</code>     where these error codes are defined. Spoiler: they are time outs.</li> <li>USB device descriptor read/64, error -62 is equally explained in      linuxquestions.org/questions/linux-hardware-18/usb-device-descriptor-read-64-error-62-a-830555     with the additional suggestion that this is typically a hardware     issue.</li> <li>Paul Philippov in     How to fix \u201cdevice not accepting address\u201d error     suggests the hardware issue may be caused by the USB over-current     protection when something draws too much current out of a USB port     and offers a workaround that seems to have helped some people\u2026 not     me today \ud83d\ude1e</li> <li>bbs.archlinux.org/viewtopic.php?id=265272     suggests a different root cause: a faulty PCIe card     (network + bluetooth).</li> </ul> <p>The last of these aligns with the symptom of system freezing shortly after booting when the Wireless controller is not disabled.</p> <p>None of the above actually show how to disable a malfunctioning USB device, so I tried unbind command from the script to restart a USB controller.</p> <pre><code>root@(none):/# /root/restart-usb.sh 0000:05:00.1\n/root/restart-usb.sh: line 15: echo: write error: No such device\n/root/restart-usb.sh: line 17: echo: write error: No such device\nroot@(none):/# echo -n 0000:05:00.1 &gt; /sys/bus/pci/drivers/xhci_hcd/unbind\nbash: echo: write error: No such device\nroot@(none):/# ls -l /sys/bus/pci/drivers/xhci_hcd/*bind\n--w------- 1 root root 4096 Sep  9 19:15 /sys/bus/pci/drivers/xhci_hcd/bind\n--w------- 1 root root 4096 Sep  9 19:15 /sys/bus/pci/drivers/xhci_hcd/unbind\n</code></pre> <p>That doesn\u2019t work! Not even after remounting root file system as read-write.</p> <p>Given than at this point the kernel and bash are the absolute only processes running, I decide to reboot into the less minimum single-user mode:</p> <ol> <li>Mash the <code>Shift</code> key to make Grub show the menu (optional).</li> <li>Select the relevant entry (usually the first, default).</li> <li>Press <code>e</code> to edit this entry.</li> <li>Go to the line that starts with <code>linux</code> and     add <code>single</code> at the end of it.</li> <li>Press <code>Ctrl+x</code> to boot with this modified entry.</li> </ol> <p>This boots the system into single-user mode, which is more than just a shell. This is probably equivalent to booting the (recovery mode) entry directly from Grub and then <code>Drop to root shell</code> which might even be faster.</p> <p>This time the un/bind commands do work, although not to the desired effect:</p> <pre><code>root@rapture:~# ./restart-usb.sh 0000:05:00.1\n[  103.273788] usb 1-4: device descriptor read/64, error -110\n[  119.145773] usb 1-4: device descriptor read/64, error -110\n[  124.777716] usb 1-4: device descriptor read/64, error -110\n[  140.649714] usb 1-4: device descriptor read/64, error -110\n[  152.519706] usb 1-4: device not accepting address 4, error -62\n[  163.783694] usb 1-4: device not accepting address 5, error -62\n[  163.784239] usb usb1-port4: unable to enumerate USB device\nroot@rapture:~# \n</code></pre> <p>After this <code>lsusb</code> shows even less:</p> <pre><code># lsusb\nBus 006 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 005 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\nBus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 003 Device 002: ID 0b05:1939 ASUSTek Computer, Inc. AURA LED Controller\nBus 005 Device 014: ID 046d:c52b Logitech, Inc. Unifying Receiver\nBus 005 Device 012: ID 046d:c545 Logitech, Inc. USB Receiver\nBus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\n</code></pre> <p>Comparing this with the previous output, the last 2 lines are missing:</p> <pre><code>Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub\nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\n</code></pre> <p>At this point it seems clear that restarting the USB controller won\u2019t help, we need to disable it. Ideally before the kernel attempts device enumeration.</p> <p>This could be done by adding (only) the unbind command to <code>/etc/rc.local</code> (and then enable it to run in systemd) but it would still run too late.</p> <p>Searching for a method to disable a specific USB controller at boot time, the closest I found was to disable it upon detection via udev rules:</p> /etc/udev/rules.d/10-disable-dead-usb-controller.rules<pre><code># Disable dead USB controller.\nATTRS{idVendor}==\"1d6b\", ATTRS{idProduct}==\"0002\", PROGRAM=\"/bin/sh -c 'echo -n 0000:05:00.1 &gt; /sys/bus/pci/drivers/xhci_hcd/unbind'\"\nATTRS{idVendor}==\"1d6b\", ATTRS{idProduct}==\"0003\", PROGRAM=\"/bin/sh -c 'echo -n 0000:05:00.1 &gt; /sys/bus/pci/drivers/xhci_hcd/unbind'\"\n</code></pre> <p>With this method the kernel still tries to enumerate devices early in the boot process, having to wait for the operation to timeout after about a minute (looks like 3 attempts timing out after 20 seconds each).</p> <p>Back to the question of what is broken, I made a copy of all the (current and old) <code>dmesg</code> and <code>kern.log</code> files to search for clues in them. The oldest of dmesg logs offered the first clue about what\u2019s behind the <code>usb 1-4 port</code>:</p> <pre><code>[    1.955188] kernel: usb 1-4: New USB device strings: Mfr=5, Product=6, SerialNumber=7\n[    1.955615] kernel: usb 1-4: Product: Wireless_Device\n[    1.956021] kernel: usb 1-4: Manufacturer: MediaTek Inc.\n[    1.956454] kernel: usb 1-4: SerialNumber: 000000000\n</code></pre> <p>This hints at the wireless card integrated in the ASUS MB TUF GAMING X570-PRO WIFI II motherboard. This post on Reddit claims the wireless controller in this motherboard is the Mediatek RZ608 chip, so that\u2019s match.</p> <p>Since it\u2019s common for the Bluetooth controller to be on the same chip as the Wireless controller, and this motherboard has a WIFI 6E &amp; BT 5.2 chip that probably does both, I went and search the logs for messages about bluetooth.</p> <p><code>/var/log/kern.log</code> still contained timeouts from yesterday:</p> <pre><code>Sep  7 20:12:23 [    5.913266] Bluetooth: Core ver 2.22\nSep  7 20:12:23 [    5.913281] NET: Registered PF_BLUETOOTH protocol family\nSep  7 20:12:23 [    5.913282] Bluetooth: HCI device and connection manager initialized\nSep  7 20:12:23 [    5.913284] Bluetooth: HCI socket layer initialized\nSep  7 20:12:23 [    5.913286] Bluetooth: L2CAP socket layer initialized\nSep  7 20:12:23 [    5.913288] Bluetooth: SCO socket layer initialized\nSep  7 20:12:23 [    6.078995] Bluetooth: hci0: Device setup in 136464 usecs\nSep  7 20:12:23 [   13.113408] Bluetooth: BNEP (Ethernet Emulation) ver 1.3\nSep  7 20:12:23 [   13.113410] Bluetooth: BNEP filters: protocol multicast\nSep  7 20:12:23 [   13.113413] Bluetooth: BNEP socket layer initialized\nSep  7 20:12:24 [   13.369207] Bluetooth: hci0: Device setup in 138047 usecs\nSep  7 20:12:27 [   16.272655] Bluetooth: RFCOMM TTY layer initialized\nSep  7 20:12:27 [   16.272662] Bluetooth: RFCOMM socket layer initialized\nSep  7 20:12:27 [   16.272667] Bluetooth: RFCOMM ver 1.11\n---\nSep  8 22:30:44 [    5.967545] Bluetooth: Core ver 2.22\nSep  8 22:30:44 [    5.967768] NET: Registered PF_BLUETOOTH protocol family\nSep  8 22:30:44 [    5.967769] Bluetooth: HCI device and connection manager initialized\nSep  8 22:30:44 [    5.967773] Bluetooth: HCI socket layer initialized\nSep  8 22:30:44 [    5.967775] Bluetooth: L2CAP socket layer initialized\nSep  8 22:30:44 [    5.967779] Bluetooth: SCO socket layer initialized\nSep  8 22:30:44 [   13.083923] Bluetooth: BNEP (Ethernet Emulation) ver 1.3\nSep  8 22:30:44 [   13.083925] Bluetooth: BNEP filters: protocol multicast\nSep  8 22:30:44 [   13.083932] Bluetooth: BNEP socket layer initialized\nSep  8 22:30:48 [   16.275362] Bluetooth: hci0: Execution of wmt command timed out\nSep  8 22:30:48 [   16.275367] Bluetooth: hci0: Failed to send wmt func ctrl (-110)\n</code></pre> <p>The messages from September 7th indicate success, the messages from September 8 indicate failure to initialize the Bluetooth controller due to a command timing out. This seems conclusive and at this point I realize there may be a separate option in the UEFI BIOS to disable the Bluetooth controller.</p> <p>Indeed there is, right under it but I hadn\u2019t seen it because it was necessary to scroll down. So I disabled the Bluetooth Controller as well, under Advanced Mode(F7) \u2192 Advanced \u2192 Onboard Devices Configuration.</p> <p>Finally, booting into single-user mode is fast and shows no errors, and booting normally nearly everything works perfectly.</p> <p>I setup my PC with 2 static IPs and this setup is susceptible to changes in the names of network interfaces. Somehow disabling the Wi-Fi Controller had shifted the Ethernet card from <code>enp5s0</code> to <code>enp4s0</code>.</p> <p>Replacing that name in the config file under <code>/etc/netplan</code> and running netplan apply restored the network. We are back! \ud83d\ude0e</p>"},{"location":"blog/2023/09/09/the-death-of-wi-fi-and-bluetooth/#back-to-normal","title":"Back To Normal","text":"<p>With the system back to normal operation, we still have to plug all the other USB devices back in. Hopefully the Mediatek chip, being the only match for <code>usb 1-4</code> in the kernel logs, is the only device behind the disabled USB controller. And hopefully all other controllers will work.</p> <p>While running <code>dmesg -w</code> to capture logs in real time, I start plugging each USB cable on the back one by one, trying each cable on every USB port on the motherboard. That is, those directly in the I/O panel on the back of the computer, the only other USB ports are on the front panel which I\u2019ve been using for the keyboard and mouse without any problems.</p> <p>This process revealed a few things:</p> <ol> <li>My monitor     (Dell UltraSharp U3421WE)     has an RJ-45 port, which I never noticed until today.<ul> <li>When plugging it via USB, a <code>Realtek USB 10/100/1000 LAN</code>     shows up as interface <code>enx70b5e8f62f4d</code>.</li> </ul> </li> <li>No output on the USB ports closets to the Wireless controller \ud83d\ude41<ul> <li>Power is delivered, so at least the LED strips feeding off the     monitor\u2019s USB port do work.</li> </ul> </li> <li>There was no need to disable the entire USB controller     <code>0000:05:00.1</code></li> </ol> <p>Having disabled the Wi-Fi and Bluetooth controllers in the UEFI BIOS, it was no longer necessary to disable the entire USB controller.</p> <p>Prior to disabling the Bluetooth controller, it was possible to have USB devices detected into the USB ports closets to the Wireless controller. There were timeouts as seen above, but devices would eventually be detected and work normally. With the Bluetooth controller disabled, it was now possibly to remove the udev rules above to get those USB ports back to work.</p> <pre><code># rm -f /etc/udev/rules.d/10-disable-dead-usb-controller.rules\n</code></pre>"},{"location":"blog/2023/09/09/the-death-of-wi-fi-and-bluetooth/#root-cause-confirmation","title":"Root Cause Confirmation","text":"<p>To confirm that the Wi-Fi Controller was the root cause of the problem, I went back to the UEFI BIOS and enabled it again. This time, I had a plan: boot into single-user mode, watch out for different warnings from the kernel and very quickly check <code>dmesg</code> for matching entries. It was a close call, but it worked.</p> <p>The anticipated warning from the kernel showed up right after entering the root password to enter single-user mode:</p> <pre><code>[   16.593831] mt7921e 0000:04:00.0: driver own failed\n</code></pre> <p>Interestingly, that\u2019s on another USB controller (<code>0000:04:00.0</code>). Even more interestingly, this line alone confirms the MediaTek wireless controller is involved, as reported in multiple forums over the last year or so:</p> <ul> <li>Ubuntu 22.04 Mediatek mt7921e WiFi adaper not found (July 2023)</li> <li>Ubuntu 22.04 kernel update to 5.19 breaks Wi-Fi (mt7921e) (March 2023)</li> <li>WiFi problem on MSI Bravo 15 B5DD (July 2022)</li> <li>MediaTek MT7921 Driver (mt7921e) keeps crashing/failing, proceeds to not be detected for the next 1-2 boots (July 2022)</li> </ul> <p>A quick check for matching lines from dmesg shows a few more:</p> <pre><code>[    4.789341] mt7921e 0000:04:00.0: enabling device (0000 -&gt; 0002)\n[   16.593831] mt7921e 0000:04:00.0: driver own failed\n[   16.593831] mt7921e: probe of 0000:04:00.0 failed with error -5\n</code></pre> <p>Searching for that probe error yields even better results:</p> <ul> <li>[Solved] No WiFi with the Mediatek MT7922 adapter and MT7921e driver (July 2023)</li> <li>[PATCH] wifi: mt76: mt7921e: Perform FLR to recovery the device (June 2023)</li> <li>github.com/openwrt/mt76/issues/548     mt7921e: probe of 0000:02:00.0 failed with error -5 (June 2021)<ul> <li>A comment in July 2023 suggest updating the BIOS might help.</li> </ul> </li> <li>Reddit post What does \u201cdriver own failed\u201d mean? (April 2023)</li> </ul> <p>For now, enabling the Wi-Fi Controller on this motherboard inevitable crashes the system. In less than 2 minutes, more errors shoed up from the kernel and I had to reset the PC:</p> <pre><code>[  110.252478] igc 0000:05:00.0 enp5s0: PCIe link lost, device now detached\n[  110.860785] ahci 0000:07:00.0: AHCI controller unavailable!\n[  110.860786] ahci 0000:08:00.0: AHCI controller unavailable!\n[  114.716174] xhci_hcd: 0000:06:00.1: xHCI host controller not responding, assume dead\n[  114.716474] mt7921e: 0000:06:00.1: HC died; cleaning up\n</code></pre> <p>That looks like the Ethernet and probably most of the remaining USB controllers going offline. Nothing for it but to go back into the UEFI BIOS and disable the Wi-Fi Controller.</p> <p>Fortunately neither the Wi-Fi nor the Bluetooth controllers were in use in the first place, or are likely to be necessary. Disabling them feels like a bit of a loss, but not nearly as much as the hassle to replace the motherboard. Besides, there is a small chance that the issue might yet be fixed by a BIOS firmware update, or a Linux kernel patch, or a benevolent fairy. Either way, I\u2019m happy so long as everything else continues to work. May that last a really long time \ud83d\ude01</p> <p>After spending the whole morning troubleshooting this, and the rest of the day writing all the above, all I want now is to play a bit of Skyrim!</p> <p> </p>"},{"location":"blog/2023/09/16/migrating-a-plex-media-server-to-kubernetes/","title":"Migrating a Plex Media Server to Kubernetes","text":"<p>Running Plex Media Server on Linux is easy. Updating it is easy too. Re-using the library from an old server on a new one is also quite easy.</p> <p>That said, running anything in Kubernetes is only slightly harder once, and after that updates are entirely automatic and moving from one cluster to another would be even easier.</p>"},{"location":"blog/2023/09/16/migrating-a-plex-media-server-to-kubernetes/#prologue","title":"Prologue","text":"<p>I\u2019ve been using Plex Media Server for a few years, primarily to catch up with a bunch of podcasts I started listening from their beginning in the spring of 2020, and occasionally to share my Audible library with the family. The family doesn\u2019t really use any of this, specially since they got Spotify, but this library of Podcasts has been a faithful companion of mine for the last few years, at home and abroad.</p> <p>The Kubernetes cluster running on Lexicon has proven stable and convenient enough that I finally felt motivated to migrate the Plex Media Server, from the stand-alone setup into the Kubernetes cluster.</p>"},{"location":"blog/2023/09/16/migrating-a-plex-media-server-to-kubernetes/#kubernetes","title":"Kubernetes","text":"<p>Plex offers their official Docker image (plexinc/pms-docker) but somehow the better documentation, in Reddit and blog posts, seems to converge on the linuxserver/plex image from the LinuxServer.io team. For the most part, I followed Kubernetes Part 14: Deploy Plexserver from debontonline.com, except instead of NFS shares I used local paths where the data is already present. For that, I took inspiration from Greg Jeanmarts\u2019 article (K3S \u2013 5/8) Self-host your Media Center On Kubernetes with Plex, Sonarr, Radarr, Transmission and Jackett.</p> <p>I like to keep my Kubernetes deployments each in a single file, so this one is a bit long. The sections are in the order introduced by Kubernetes Part 14: Deploy Plexserver:</p> <ol> <li>Two <code>PersistentVolume</code>s for the data (media) and config (library).</li> <li>Two <code>PersistentVolumeClaim</code>s for these.</li> <li>A <code>Deployment</code> to run the Plex image using the above storage volumes.</li> <li>Two <code>Service</code>s to expose TCP and UDP on the same ports.</li> <li>No <code>Ingress</code> because I had no long-term need to access this Plex server directly. Having already configured similar Plex servers (e.g. on a Raspberry Pi), once the new server is claimed it will show up along with others at app.plex.tv.</li> </ol> <p>Notes about highlighted lines:</p> <ul> <li><code>14,64</code>: <code>1Gi</code> may seem excessive for the <code>/config</code> directory,      but with a large collection comes a large database under this      directory.</li> <li><code>19</code>: Local storage for Kubernetes pods goes under <code>/home/k8s</code>     (root partition is small, no separate <code>/var</code> partition).</li> <li><code>34</code>: Local storage for audio files goes under <code>/home/depot</code>     (including symlinks for a few folder in a SSD).</li> <li><code>49</code>: Local storage for video files goes under <code>/home/ssd</code>.</li> <li><code>29,44,79,94</code>: <code>500Gi</code> may seem even more excessive, but this is not     much more than the storage space taken by my collections already:<ul> <li>124 GB are taken by my Audible library (about 350 books).</li> <li>320 GB are taken by the Podcasts I\u2019ve downloaded, most of which I treasure.</li> </ul> </li> <li><code>132</code>: the token obtained from plex.tv/claim     \u2500 expires in just 4 minutes.</li> <li><code>134,136</code>: the old Plex server was running as user plex and     <code>998</code> was the UID and GID assigned to it.</li> <li><code>223,252</code>: this IP address comes from the range of reserved     when installing MetallLB and should be the same on both     services (TCP and UDP).</li> </ul> Kubernetes deployment: <code>plex-media-server.yaml</code> plex-media-server.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: plexserver\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: plexserver-pv-config\n  namespace: plexserver\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/plexmediaserver\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: plexserver-pv-data-depot\n  namespace: plexserver\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 500Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/depot\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: plexserver-pv-data-video\n  namespace: plexserver\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 500Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/ssd/video\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: plexserver-pvc-config\n  namespace: plexserver\nspec:\n  storageClassName: manual\n  volumeName: plexserver-pv-config\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: plexserver-pvc-data-depot\n  namespace: plexserver\nspec:\n  storageClassName: manual\n  volumeName: plexserver-pv-data-depot\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 500Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: plexserver-pvc-data-video\n  namespace: plexserver\nspec:\n  storageClassName: manual\n  volumeName: plexserver-pv-data-video\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 500Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: plexserver\n  name: plexserver\n  namespace: plexserver\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: plexserver\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: plexserver\n    spec:\n      volumes:\n      - name: plex-config\n        persistentVolumeClaim:\n          claimName: plexserver-pvc-config\n      - name: data-depot\n        persistentVolumeClaim:\n          claimName: plexserver-pvc-data-depot\n      - name: data-video\n        persistentVolumeClaim:\n          claimName: plexserver-pvc-data-video\n      containers:\n      - env:\n        - name: PLEX_CLAIM\n          value: claim-deDSmtULWyYbvwt_2xAu\n        - name: PGID\n          value: \"998\"\n        - name: PUID\n          value: \"998\"\n        - name: VERSION\n          value: latest\n        - name: TZ\n          value: Europe/Amsterdam\n        image: ghcr.io/linuxserver/plex\n        imagePullPolicy: Always\n        name: plexserver\n        ports:\n        - containerPort: 32400\n          name: pms-web\n          protocol: TCP\n        - containerPort: 32469\n          name: dlna-tcp\n          protocol: TCP\n        - containerPort: 1900\n          name: dlna-udp\n          protocol: UDP\n        - containerPort: 3005\n          name: plex-companion\n          protocol: TCP  \n        - containerPort: 5353\n          name: discovery-udp\n          protocol: UDP  \n        - containerPort: 8324\n          name: plex-roku\n          protocol: TCP  \n        - containerPort: 32410\n          name: gdm-32410\n          protocol: UDP\n        - containerPort: 32412\n          name: gdm-32412\n          protocol: UDP\n        - containerPort: 32413\n          name: gdm-32413\n          protocol: UDP\n        - containerPort: 32414\n          name: gdm-32414\n          protocol: UDP\n        resources: {}\n        stdin: true\n        tty: true\n        volumeMounts:\n        - mountPath: /config\n          name: plex-config\n        - mountPath: /home/depot\n          name: data-depot\n        - mountPath: /home/ssd/video\n          name: data-video\n      restartPolicy: Always\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: plex-udp\n  namespace: plexserver\n  annotations:\n    metallb.universe.tf/allow-shared-ip: plexserver\nspec:\n  selector:\n    app: plexserver\n  ports:\n  - port: 1900\n    targetPort: 1900\n    name: dlna-udp\n    protocol: UDP\n  - port: 5353\n    targetPort: 5353\n    name: discovery-udp\n    protocol: UDP\n  - port: 32410\n    targetPort: 32410\n    name: gdm-32410\n    protocol: UDP\n  - port: 32412\n    targetPort: 32412\n    name: gdm-32412\n    protocol: UDP\n  - port: 32413\n    targetPort: 32413\n    name: gdm-32413\n    protocol: UDP\n  - port: 32414\n    targetPort: 32414\n    name: gdm-32414\n    protocol: UDP\n  type: LoadBalancer\n  loadBalancerIP: 192.168.0.128  # Should be one from the MetalLB range and the same as the TCP service.\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: plex-tcp\n  namespace: plexserver\n  annotations:\n    metallb.universe.tf/allow-shared-ip: plexserver\nspec:\n  selector:\n    app: plexserver\n  ports:                      \n  - port: 32400\n    targetPort: 32400\n    name: pms-web\n    protocol: TCP\n  - port: 3005\n    targetPort: 3005\n    name: plex-companion\n  - port: 8324\n    name: plex-roku\n    targetPort: 8324  \n    protocol: TCP  \n  - port: 32469\n    targetPort: 32469\n    name: dlna-tcp\n    protocol: TCP\n  type: LoadBalancer\n  loadBalancerIP: 192.168.0.128  # Should be one from the MetalLB range and the same as the UDP service.\n</code></pre> <p>Having saved the above as <code>plex-media-server.yaml</code> all is left to do now is apply it:</p> <pre><code>$ kubectl apply -f plex-media-server.yaml \nnamespace/plexserver created\npersistentvolume/plexserver-pv-config created\npersistentvolume/plexserver-pv-data-depot created\npersistentvolume/plexserver-pv-data-video created\npersistentvolumeclaim/plexserver-pvc-config created\npersistentvolumeclaim/plexserver-pvc-data-depot created\npersistentvolumeclaim/plexserver-pvc-data-video created\ndeployment.apps/plexserver created\nservice/plex-udp created\nservice/plex-tcp created\n</code></pre> <p>After about a minute the pod finds the <code>PersistentVolumeClaim</code> and Plex is ready at 192.168.0.128:32400/web (LAN only, no SSL).</p> <p>When accessing the new server\u2019s web interface for the first time, log in and create a test library with a small body of media, just to confirm that Plex is able to scan the files. If there is no previous Plex server, this is already a good time to let it scan the entire media collection.</p>"},{"location":"blog/2023/09/16/migrating-a-plex-media-server-to-kubernetes/#addition-that-didnt-work","title":"Addition (that didn\u2019t work)","text":"<p>GPU in this server isn\u2019t big and video transcoding is not actually necessary, since this server is used almost exclusively for audio, but I tried following Plex on Kubernetes with intel iGPU passthrough \u2013 Small how to anyway to see how it\u2019d go. It didn\u2019t.</p> <p>First, I didn\u2019t even try tagging the server (single node) with the label <code>intel.feature.node.kubernetes.io/gpu=true</code> \u2500 this may have prevented pods from finding the GPU but I suspect something else did. Installing a certificate manager should not be necessary, because it\u2019s already installed.</p> <p>intel/intel-device-plugins-for-kubernetes requires NFD (Node Feature Discovery) for the Device Plugin operator, then the operator and GPU can be installed with their respective helm charts:</p> <ul> <li>intel/helm-charts/charts/device-plugin-operator</li> <li>intel/helm-charts/charts/gpu-device-plugin</li> </ul> <p>Installing all these through the Helm charts should be quite easy:</p> <pre><code>$ helm repo add nfd https://kubernetes-sigs.github.io/node-feature-discovery/charts\n$ helm repo add intel https://intel.github.io/helm-charts/\n$ helm repo update\n\n$ helm install nfd nfd/node-feature-discovery \\\n  --namespace node-feature-discovery --create-namespace --version 0.12.1 \\\n  --set 'master.extraLabelNs={gpu.intel.com,sgx.intel.com}' \\\n  --set 'master.resourceLabels={gpu.intel.com/millicores,gpu.intel.com/memory.max,gpu.intel.com/tiles,sgx.intel.com/epc}'\nNAME: nfd\nLAST DEPLOYED: Fri Sep 15 05:30:14 2023\nNAMESPACE: node-feature-discovery\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n\n$ helm install device-plugin-operator intel/intel-device-plugins-operator\nNAME: device-plugin-operator\nLAST DEPLOYED: Fri Sep 15 05:31:28 2023\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThank you for installing intel-device-plugins-operator.\n\nThe next step would be to install the device (plugin) specific chart.\n\n$ helm install gpu-device-plugin intel/intel-device-plugins-gpu\nNAME: gpu-device-plugin\nLAST DEPLOYED: Fri Sep 15 05:32:11 2023\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n\n$ helm show values intel/intel-device-plugins-operator\nmanager:\n  image:\n    hub: intel\n    tag: \"\"\n    pullPolicy: IfNotPresent\n\nkubeRbacProxy:\n  image:\n    hub: gcr.io\n    hubRepo: kubebuilder\n    tag: v0.14.1\n    pullPolicy: IfNotPresent\n\nprivateRegistry:\n  registryUrl: \"\"\n  registryUser: \"\"\n  registrySecret: \"\"\n\n$ helm show values intel/intel-device-plugins-gpu\nname: gpudeviceplugin-sample\n\nimage:\n  hub: intel\n  tag: \"\"\n\ninitImage:\n  hub: intel\n  tag: \"\"\n\nsharedDevNum: 1\nlogLevel: 2\nresourceManager: false\nenableMonitoring: true\nallocationPolicy: \"none\"\n\nnodeSelector:\n  intel.feature.node.kubernetes.io/gpu: 'true'\n\nnodeFeatureRule: false\n</code></pre> <p>I\u2019m not sure what is missing. Attempting to assign the GPU to Plex by adding the following lines to <code>plex-media-server.yaml</code> did not work:</p> plex-media-server.yaml<pre><code>        resources:\n            requests:\n                gpu.intel.com/i915: \"1\"\n            limits:\n                gpu.intel.com/i915: \"1\"\n</code></pre> <p>Made the deployment fail with because the GPU was not available:</p> <pre><code>0/1 nodes are available: 1\nInsufficient gpu.intel.com/i915.\npreemption: 0/1 nodes are available: 1\nNo preemption victims found for incoming pod.\n</code></pre>"},{"location":"blog/2023/09/16/migrating-a-plex-media-server-to-kubernetes/#alternative-that-didnt-work","title":"Alternative (that didn\u2019t work)","text":"<p>There are also Helm charts, at least 2:</p> <ul> <li>munnerz/kube-plex which     seems to be simpler</li> <li>ressu/kube-plex which is a     fork of munnerz/kube-plex</li> </ul> <p>I tried munnerz/kube-plex only; it didn\u2019t work.</p> <p>In all honestly, I didn\u2019t really try hard, and didn\u2019t quite see the motivation to use Helm charts instead a regular deployment (as seen above).</p> <p>Installing the chart was supposed to be this easy:</p> <pre><code>$ helm install plex ./charts/kube-plex \\\n    --create-namespace \\\n    --namespace plex \\\n    --set claimToken=claim-VqczS1yENc_F8J7zitZo\nNAME: plex\nLAST DEPLOYED: Wed Sep 13 22:40:48 2023\nNAMESPACE: plex\nSTATUS: deployed\nREVISION: 3\nTEST SUITE: None\nNOTES:\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace plex -l \"app=kube-plex,release=plex\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl port-forward $POD_NAME 8080:\n</code></pre> <p>However, this is resulted in the pod being</p> <pre><code>stuck waiting for a volume to be created,\neither by external provisioner \u201crancher.io/local-path\u201d\nor manually created by system administrator.\n</code></pre> <p>With no great motivation to get this to work, I decided to uninstall the chart and study the deployment options more closely to get one to work (as seen above).</p> <pre><code>$ helm uninstall -n plex plex\n$ kubectl delete namespace plex\n</code></pre>"},{"location":"blog/2023/09/16/migrating-a-plex-media-server-to-kubernetes/#migration","title":"Migration","text":"<p>With both Plex servers working, albeit with only a small test library in the new one, the next goal is to migrate the whole database from the old stand-alone Plex server to the new one running in Kubernetes. The path to each server\u2019s database is</p> <ul> <li><code>/home/depot/plexmediaserver/Library</code> for the old stand-alone server.</li> <li><code>/home/k8s/plexmediaserver/Library</code> for the new server in Kubernetes.</li> </ul> <p>The plan is simple: stop both servers, move the new database away, copy the old one as the new one, start the new server only.</p> <pre><code>$ kubectl delete -f plex/plex-media-server.yaml\n# systemctl stop plexmediaserver.service\n# cd /home/k8s/plexmediaserver\n# mv Library Library.backup\n# cp -a /home/depot/plexmediaserver/Library .\n</code></pre> <p>Most Plex deployments mount the media a generic path (<code>/data</code>). If that had been the case above, the new server would not recognize the files as already scanned, since the library would only refer to files under the old path (<code>/home/depot</code>). If this was the case, it would be enough to update the <code>mountPath</code> value in line <code>181</code> in <code>plex-media-server.yaml</code></p> plex-media-server.yaml<pre><code>        volumeMounts:\n        - mountPath: /config\n          name: plex-config\n        - mountPath: /home/depot\n          name: data\n</code></pre> <p>In addition to that, my deployment creates 2 separate <code>PhysicalVolume</code>s (and claims) because video files are in a separate disk:</p> plex-media-server.yaml<pre><code>        - mountPath: /home/depot\n          name: data-depot\n        - mountPath: /home/ssd/video\n          name: data-video\n</code></pre>"},{"location":"blog/2023/09/16/migrating-a-plex-media-server-to-kubernetes/#port-forwarding","title":"Port forwarding","text":"<p>Normally Plex is able to establish the necessary port forwarding via UPnP, but in this case that doesn't seem to work. This may be because it's not the only Plex server in the nework, so because it's running in a Kubernetes cluster. Either way, the port forwarding rule can be added manually to the router and then Manually specify public port in the Plex settings under Settings &gt; Remote Access.</p> <p>Note</p> <p>This step may require connecting directly to the web interface from the local network: 192.168.0.128:32400/web</p>"},{"location":"blog/2023/09/16/migrating-a-plex-media-server-to-kubernetes/#scheduled-tasks","title":"Scheduled Tasks","text":"<p>Normally Plex runs daily maintenance during the night, which is a good time. However, given the limited CPU in this server, even letting it run for 4 hours every day results in the CPU running quite hot for all that time:</p> <p> </p> <p>To get rid of this, I went to Settings &gt; Server &gt; Library to update Plex Library Settings and disable these which I don't really need, by setting their frequency to never:</p> <ul> <li>Generate chapter thumbnails seems relevant only for     TV series, which I don't use (or care about).</li> <li>Analyze audio tracks for loudness is clearly called out     as potentially CPU-intensive, and I don't think is useful     for podcasts, where each episode is a single track, and most     of the times one listens to a single one per \"album\". Even     if this could be useful for audiobooks, it shouldn't be     necessary given the rather high quality of their audio.</li> </ul> <p></p>"},{"location":"blog/2023/09/16/migrating-a-plex-media-server-to-kubernetes/#epilogue","title":"Epilogue","text":"<p>The migration had two critical requirements:</p> <ol> <li>Not re-scanning the library again. Although I take care to make    sure podcasts episodes have good    ID3 v2 tags,    adjusting them if necessary, Plex does not seem to understand them    all that well all the time, often leading to a single podcast    being split in 2 or 3 albums, authors showing as \u201cVarious Artists\u201d    despite correct ID3 tags, and a few other oddities. I spent time    adjusting metadata in Plex and would hate to lose that work.</li> <li>Preserve playlists. Catching up with over a dozen podcasts\u2019 back    catalogue is no small feat, specially since podcast apps (that I    know of) do not make this particularly easy; they probably assume    this is not what most users want. Here is where Plex comes in    handy: it was not too hard to create a playlist, open a browser    tab for each episode an add episodes in the right (release) order.    Again, although easy, this was quite a bit of work I would hate to lose.</li> </ol> <p>Note</p> <p>I did consider creating the playlist programmatically, and may yet do it. One option would be to create an M3U playlist and import it into Plex using DocDocDocDocDoc/PlexPlaylistImporter; the as-of-yet open question is how to create that playlist.</p> <p>The migration went smoothly and, I am happy to report, both goals where achieved. Here is my beloved Podcast collection:</p> <p></p>"},{"location":"blog/2023/09/21/starting-a-blog-with-jekyll-on-github-pages/","title":"Starting a blog with Jekyll on GitHub pages","text":"<p>Over the last couple of weekends I've been trying a couple of blogging platforms, namely WordPress.com and Blogger.</p> <p>Each have their pros and cons, but to cut a story short:</p> <ul> <li>WordPress.com is quick\u2019n\u2019easy to setup,     but the editor gets painfully slow with long (and not really all that long) articles, themes are very limited and can\u2019t be     customized.<ul> <li>On the plus side, its code highlight block is quite neat.</li> </ul> </li> <li>Blogger is also quick\u2019n\u2019easy to setup,     the editor works well enough, allows editing most of the content     as HTML and then uploading images and videos, and themes are     fully customizable (can be edited raw).<ul> <li>On the huge downside, it will unpublish, block or remove     posts even entire blogs, and there seems to be no way to     appeal.</li> </ul> </li> </ul>"},{"location":"blog/2023/09/21/starting-a-blog-with-jekyll-on-github-pages/#setup","title":"Setup","text":"<p>Jekyll on GitHub pages seems like a much better option, specially since Markdown offers a good balance between easy-to-write and close enough to HTML.</p> <p>Dang, I can even see italic and bold text in VIM!</p> <p>Anyway, setup was also relatively quick'n'easy:</p> <ol> <li>Create public repository    stibbons1990/hex.</li> <li>Follow docs.github.com/en/pages/quickstart    to publish the main branch to     stibbons1990.github.io/hex.</li> <li>Install Jekyll on Ubuntu (including Ruby):    <pre><code>$ sudo apt install -y ruby-full build-essential zlib1g-dev\n$ echo '# Install Ruby Gems to ~/gems' &gt;&gt; ~/.bashrc\n$ echo 'export GEM_HOME=\"$HOME/gems\"' &gt;&gt; ~/.bashrc\n$ echo 'export PATH=\"$HOME/gems/bin:$PATH\"' &gt;&gt; ~/.bashrc\n$ source ~/.bashrc\n$ gem install jekyll bundler\n...\nDone installing documentation for bundler after 0 seconds\n28 gems installed\n</code></pre></li> <li> <p>Follow up to steps 7 through 13 (1-6 involve the creation a    repo, already done) of Creating your site:    </p><pre><code>$ jekyll new --skip-bundle . --force\nNew jekyll site installed in /home/k8s/code-server/hex. \nBundle install skipped. \n\n$ bundle install\nFetching gem metadata from https://rubygems.org/............\nResolving dependencies...\nFetching webrick 1.8.1\nFetching sass-embedded 1.68.0\nInstalling webrick 1.8.1\nInstalling sass-embedded 1.68.0 with native extensions\nFetching jekyll-feed 0.17.0\nInstalling jekyll-feed 0.17.0\nBundle complete! 7 Gemfile dependencies, 34 gems now installed.\nUse `bundle info [gemname]` to see where a bundled gem is installed.\n</code></pre><p></p> </li> <li> <p>Commit the changes and uush the files to GitHub    </p><pre><code>$ git add .\n$ git commit -m \"bundle install\" -a\n$ git push\n</code></pre><p></p> </li> <li> <p>After a few minutes, the new blog is live at    stibbons1990.github.io/hex.</p> </li> </ol> <p>Note</p> <ul> <li>After <code>git push</code> it takes a few minutes for GitHub to generate     the content. Check out the latest     actions     to see its progress.</li> <li>If the new repository has any files in it (e.g <code>LICENSE</code>),     <code>jekyll</code> will report a conflict and ask that the directory be     empty or else try again with <code>--force</code> to proceed and     overwrite any files.</li> </ul>"},{"location":"blog/2023/09/21/starting-a-blog-with-jekyll-on-github-pages/#local-testing-on-pc","title":"Local Testing on PC","text":"<p>Testing locally on the PC was a little rockier, the first time failed with:</p> <code>bundle exec jekyll serve</code> <pre><code>$ bundle exec jekyll serve\nConfiguration file: /home/k8s/code-server/hex/_config.yml\nTo use retry middleware with Faraday v2.0+, install `faraday-retry` gem\n            Source: /home/k8s/code-server/hex\n      Destination: /home/k8s/code-server/hex/_site\nIncremental build: disabled. Enable with --incremental\n      Generating... \n      Jekyll Feed: Generating feed for posts\n                  done in 0.172 seconds.\nAuto-regeneration: enabled for '/home/k8s/code-server/hex'\nbundler: failed to load command: jekyll (/home/k8s/code-server/gems/bin/jekyll)\n/home/k8s/code-server/gems/gems/jekyll-3.9.3/lib/jekyll/commands/serve/servlet.rb:3:in `require': cannot load such file -- webrick (LoadError)\n     from /home/k8s/code-server/gems/gems/jekyll-3.9.3/lib/jekyll/commands/serve/servlet.rb:3:in `&lt;top (required)&gt;'\n     from /home/k8s/code-server/gems/gems/jekyll-3.9.3/lib/jekyll/commands/serve.rb:184:in `require_relative'\n     from /home/k8s/code-server/gems/gems/jekyll-3.9.3/lib/jekyll/commands/serve.rb:184:in `setup'\n     from /home/k8s/code-server/gems/gems/jekyll-3.9.3/lib/jekyll/commands/serve.rb:102:in `process'\n     from /home/k8s/code-server/gems/gems/jekyll-3.9.3/lib/jekyll/commands/serve.rb:93:in `block in start'\n     from /home/k8s/code-server/gems/gems/jekyll-3.9.3/lib/jekyll/commands/serve.rb:93:in `each'\n     from /home/k8s/code-server/gems/gems/jekyll-3.9.3/lib/jekyll/commands/serve.rb:93:in `start'\n     from /home/k8s/code-server/gems/gems/jekyll-3.9.3/lib/jekyll/commands/serve.rb:75:in `block (2 levels) in init_with_program'\n     from /home/k8s/code-server/gems/gems/mercenary-0.3.6/lib/mercenary/command.rb:220:in `block in execute'\n     from /home/k8s/code-server/gems/gems/mercenary-0.3.6/lib/mercenary/command.rb:220:in `each'\n     from /home/k8s/code-server/gems/gems/mercenary-0.3.6/lib/mercenary/command.rb:220:in `execute'\n     from /home/k8s/code-server/gems/gems/mercenary-0.3.6/lib/mercenary/program.rb:42:in `go'\n     from /home/k8s/code-server/gems/gems/mercenary-0.3.6/lib/mercenary.rb:19:in `program'\n     from /home/k8s/code-server/gems/gems/jekyll-3.9.3/exe/jekyll:15:in `&lt;top (required)&gt;'\n     from /home/k8s/code-server/gems/bin/jekyll:25:in `load'\n     from /home/k8s/code-server/gems/bin/jekyll:25:in `&lt;top (required)&gt;'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/lib/bundler/cli/exec.rb:58:in `load'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/lib/bundler/cli/exec.rb:58:in `kernel_load'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/lib/bundler/cli/exec.rb:23:in `run'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/lib/bundler/cli.rb:492:in `exec'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/lib/bundler/vendor/thor/lib/thor/command.rb:27:in `run'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/lib/bundler/vendor/thor/lib/thor/invocation.rb:127:in `invoke_command'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/lib/bundler/vendor/thor/lib/thor.rb:392:in `dispatch'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/lib/bundler/cli.rb:34:in `dispatch'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/lib/bundler/vendor/thor/lib/thor/base.rb:485:in `start'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/lib/bundler/cli.rb:28:in `start'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/exe/bundle:37:in `block in &lt;top (required)&gt;'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/lib/bundler/friendly_errors.rb:117:in `with_friendly_errors'\n     from /home/k8s/code-server/gems/gems/bundler-2.4.19/exe/bundle:29:in `&lt;top (required)&gt;'\n     from /home/k8s/code-server/gems/bin/bundle:25:in `load'\n     from /home/k8s/code-server/gems/bin/bundle:25:in `&lt;main&gt;'\n</code></pre> <p>Workaround from github.com/jekyll/jekyll/issues/8523 helped:</p> <pre><code>$ echo 'gem \"webrick\"' &gt;&gt; Gemfile\n$ bundle exec jekyll serve\nConfiguration file: /home/k8s/code-server/hex/_config.yml\nTo use retry middleware with Faraday v2.0+, install `faraday-retry` gem\n            Source: /home/k8s/code-server/hex\n       Destination: /home/k8s/code-server/hex/_site\n Incremental build: disabled. Enable with --incremental\n      Generating... \n       Jekyll Feed: Generating feed for posts\n                    done in 0.164 seconds.\n Auto-regeneration: enabled for '/home/k8s/code-server/hex'\n    Server address: http://127.0.0.1:4000/hex/\n  Server running... press ctrl-c to stop.\n</code></pre> <p>And this time it worked wonderfully, I couldn't tell the difference between the local site and the live site once deployed.</p>"},{"location":"blog/2023/09/21/starting-a-blog-with-jekyll-on-github-pages/#local-testing-on-server","title":"Local Testing on Server","text":"<p>Testing on the server (Lexicon) as a little bumpty to; the first time running the server didn't fail, but the server wouldn't respond to external hosts. The trick to make this work is adding <code>--host 0.0.0.0</code> so that the web server listens on all network interfaces:</p> <pre><code>$ bundle exec jekyll serve --host 0.0.0.0\nConfiguration file: /home/k8s/code-server/hex/_config.yml\nTo use retry middleware with Faraday v2.0+, install `faraday-retry` gem\n            Source: /home/k8s/code-server/hex\n       Destination: /home/k8s/code-server/hex/_site\n Incremental build: disabled. Enable with --incremental\n      Generating... \n       Jekyll Feed: Generating feed for posts\n                    done in 0.16 seconds.\n Auto-regeneration: enabled for '/home/k8s/code-server/hex'\n    Server address: http://0.0.0.0:4000/hex/\n  Server running... press ctrl-c to stop.\n</code></pre> <p>From there it's up to the local network's DNS (or your local <code>/etc/hosts</code> files) to resolve a handy hostname, e.g. http://lexicon:4000/hex/</p>"},{"location":"blog/2023/09/21/starting-a-blog-with-jekyll-on-github-pages/#adding-themes","title":"Adding Themes","text":"<p>Adding a theme turned out even trickier, in fact did not figure out how to add the first theme I tried.</p> <p>First I tried adding the Midnight theme and, following its instructions, got everything  messed up; the home page was empty and the one blog post had no style.</p> <p>Testing the site locally shows the required layouts (<code>home</code> and <code>post</code>) don't exist:</p> <pre><code>     Build Warning: Layout 'post' requested in _posts/2023-09-21-migrated-to-jekyll-on-github-pages.markdown does not exist.\n   GitHub Metadata: No GitHub API authentication could be found. Some fields may be missing or have incorrect data.\n     Build Warning: Layout 'page' requested in about.markdown does not exist.\n     Build Warning: Layout 'home' requested in index.markdown does not exist.\n</code></pre> <p>And indeed these layouts do not exist under the pages-themes/midnight/_layouts directory.</p> <p>Second, tried another theme found while searching for solutions for the problem above: minimal-mistakes</p> <p>It requires jekyll-include-cache so first installed that, by adding the required lines to <code>Gemfile</code> and <code>_config.yml</code> and then running <code>bundle</code> to install it:</p> <pre><code>$ bundle\nFetching gem metadata from https://rubygems.org/.........\nResolving dependencies...\nFetching minimal-mistakes-jekyll 4.24.0\nInstalling minimal-mistakes-jekyll 4.24.0\nBundle complete! 12 Gemfile dependencies, 91 gems now installed.\nUse `bundle info [gemname]` to see where a bundled gem is installed.\n</code></pre> <p>At this point following the instructions in the remote-theme-method the following should do:</p> _config.yml<pre><code>remote_theme: \"mmistakes/minimal-mistakes@4.24.0\"\nplugins:\n  - jekyll-feed\n  - jekyll-remote-theme\n  - jekyll-include-cache\n</code></pre> Gemfile<pre><code>group :jekyll_plugins do\n  gem \"jekyll-feed\", \"~&gt; 0.12\"\n  gem \"jekyll-remote-theme\"\n  gem \"minimal-mistakes-jekyll\"\nend\n</code></pre> <p>Based on the layouts available under the minimal-mistakes/_layouts directory, it seems blog posts should use <code>layout: single</code> while other pages should use <code>layout: default</code>.</p> <p>After updating those <code>layout</code> values, local testing works:</p> <pre><code>$ bundle exec jekyll serve --host 0.0.0.0\nConfiguration file: /home/k8s/code-server/hex/_config.yml\n      Remote Theme: Using theme mmistakes/minimal-mistakes\nTo use retry middleware with Faraday v2.0+, install `faraday-retry` gem\n            Source: /home/k8s/code-server/hex\n       Destination: /home/k8s/code-server/hex/_site\n Incremental build: disabled. Enable with --incremental\n      Generating... \n      Remote Theme: Using theme mmistakes/minimal-mistakes\n       Jekyll Feed: Generating feed for posts\n   GitHub Metadata: No GitHub API authentication could be found. Some fields may be missing or have incorrect data.\n                    done in 1.437 seconds.\n Auto-regeneration: enabled for '/home/k8s/code-server/hex'\n    Server address: http://0.0.0.0:4000/hex/\n  Server running... press ctrl-c to stop.\n</code></pre> <p>Finally, we can tweak the theme's configuration, e.g. adding to <code>_config.yml</code> the following line to set a different skin:</p> _config.yml<pre><code># Build settings\n# theme: minima\nremote_theme: \"mmistakes/minimal-mistakes@4.24.0\"\nminimal_mistakes_skin: \"neon\"\n</code></pre>"},{"location":"blog/2023/09/21/starting-a-blog-with-jekyll-on-github-pages/#customizing-minimal-mistakes","title":"Customizing Minimal Mistakes","text":"<p>I quite like this theme as it is, but I wish it used more space horizontally. Oddly enough, when the browser window is wider than 1280px the font size increases but the content width does not, resulting in narrower columns and, more annoyingly, more of the content in code blocks is hidden under the right-hand side so you have to scroll horizontally to see it. Not very convenient.</p> <p>One would expect it to be easy to change such a simple attribute. And it is... if you read the  Overriding Theme Defaults and Stylesheets pages carefully. If you don't, you might easily waste a few hours going through a number of issue reports and workarounds, most of which are at least a few years old and none of which worked for me.</p> <p>After reading those pages carefully, it seems the trick was to copy the entire mmistakes/minimal-mistakes/assets/css/main.scss file under the project folder, as <code>assets/css/main.scss</code>, then add the <code>$max-width</code> variable override:</p> <p>{% raw %} </p><pre><code>---\n# Only the main Sass file needs front matter (the dashes are enough)\nsearch: false\n---\n\n@charset \"utf-8\";\n\n$max-width: 1900px;\n\n@import \"minimal-mistakes/skins/{{ site.minimal_mistakes_skin | default: 'default' }}\"; // skin\n@import \"minimal-mistakes\"; // main partials\n</code></pre><p></p>"},{"location":"blog/2023/09/21/starting-a-blog-with-jekyll-on-github-pages/#install-google-analytics","title":"Install Google Analytics","text":"<p>It is as easy as following Jekyll's Analytics instructions to add the G-tag to <code>_config.yml</code> and, assuming Google Analytics itself has been already set up, it should just work.</p>"},{"location":"blog/2023/09/23/getting-started-with-dragon-age-origins---ultimate-edition/","title":"Getting started with Dragon Age: Origins - Ultimate Edition","text":"<p>Dragon Age: Origins - Ultimate Edition was on sale since last weekend, and today I fell for it.</p> <p>Before purchasing the game, I checked all recent Proton reports for Dragon Age: Origins - Ultimate Edition, because one does not simply run Windows games on Linux. At least, not always without a little tinkering.</p>"},{"location":"blog/2023/09/23/getting-started-with-dragon-age-origins---ultimate-edition/#launch-flags","title":"Launch flags","text":"<p>Among all the recent recommendations, this one worked for me:</p> <pre><code>__GLX_VENDOR_LIBRARY_NAME=nvidia PROTON_FORCE_LARGE_ADDRESS_AWARE=1 RADV_TEX_ANISO=16 PROTON_USE_D9VK=1 gamemoderun %command%\n</code></pre> <p>Adding <code>__NV_PRIME_RENDER_OFFLOAD=1</code> caused the game to get stuck in the splash screen, so I had to remove that.</p>"},{"location":"blog/2023/09/23/getting-started-with-dragon-age-origins---ultimate-edition/#update-2025-01-05","title":"Update (2025-01-05)","text":"<p>At this point more recent Proton reports suggest that it i best to stick with Proton 7.0-6. This does seem to help, although at this point the difference is only that the game crashes right after character selection rather than not starting.</p> <p>The same crash happens also with Proton version 6 and GE 8.16. With the latter the crash also left the mouse cursor of the game active and no other application reacted to (or received) click events, at least for several seconds. After the crash it is still necessary to ask Steam to stop the game, or even exit Steam.</p> <p>Proton Experimental, Hotfix and verion 9 had the same crash.</p>"},{"location":"blog/2023/09/23/getting-started-with-dragon-age-origins---ultimate-edition/#install-physx","title":"Install <code>physx</code>","text":"<p>Although I'm not sure it helped, I did try installing NVidia's Physx library:</p> <pre><code>$ winetricks physx\n</code></pre> <p>The installation seems to have been made only for the default <code>WINEPREFIX</code> in <code>$HOME</code> while the game uses the one in <code>$HOME/.local/share/Steam/steamapps/compatdata/47810/pfx</code> so the correct command would be</p> <pre><code>$ WINEPREFIX=$HOME/.local/share/Steam/steamapps/compatdata/47810/pfx winetricks physx\n</code></pre>"},{"location":"blog/2023/09/23/getting-started-with-dragon-age-origins---ultimate-edition/#update-2025-01-05_1","title":"Update (2025-01-05)","text":"<p>This worked out of the box in Ubuntu Studio 22.04, but later in Ubuntu Studio 24.04 it failed with this error:</p> <pre><code>$ WINEPREFIX=$HOME/.local/share/Steam/steamapps/compatdata/47810/pfx winetricks physx\nExecuting cd /usr/bin\n------------------------------------------------------\nwarning: Unknown file arch of /usr/bin/wine.\n------------------------------------------------------\n</code></pre> <p>Searching around for that error did not lead anywhere useful, but poking around did: <code>/usr/bin/wine</code> essentially symlinks to <code>/usr/bin/wine-stable</code> which is just a shell script to decide whether to launch <code>/usr/lib/wine/wine</code> or <code>/usr/lib/wine/wine64</code> but only the latter is installed so this can be bypassed with the <code>WINE</code> variable pointing directly to the binary:</p> <code>WINEPREFIX=$HOME/.local/share/Steam/steamapps/compatdata/47810/pfx winetricks physx</code> <pre><code>$ WINE=/usr/lib/wine/wine64 WINEPREFIX=$HOME/.local/share/Steam/steamapps/compatdata/47810/pfx winetricks physx\nExecuting cd /usr/bin\n------------------------------------------------------\nwarning: You are using a 64-bit WINEPREFIX. Note that many verbs only install 32-bit versions of packages. If you encounter problems, please retest in a clean 32-bit WINEPREFIX before reporting a bug.\n------------------------------------------------------\n------------------------------------------------------\nwarning: You apppear to be using Wine's new wow64 mode. Note that this is EXPERIMENTAL and not yet fully supported. If reporting an issue, be sure to mention this.\n------------------------------------------------------\nUsing winetricks 20240105 - sha256sum: 17da748ce874adb2ee9fed79d2550c0c58e57d5969cc779a8779301350625c55 with wine-9.0 (Ubuntu 9.0~repack-4build3) and WINEARCH=win64\nExecuting w_do_call physx\n------------------------------------------------------\nwarning: You are using a 64-bit WINEPREFIX. Note that many verbs only install 32-bit versions of packages. If you encounter problems, please retest in a clean 32-bit WINEPREFIX before reporting a bug.\n------------------------------------------------------\n------------------------------------------------------\nwarning: You apppear to be using Wine's new wow64 mode. Note that this is EXPERIMENTAL and not yet fully supported. If reporting an issue, be sure to mention this.\n------------------------------------------------------\nExecuting load_physx \n------------------------------------------------------\nwarning: /home/ponder/.local/share/Steam/steamapps/compatdata/47810/pfx/dosdevices/c:/Program Files (x86)/NVIDIA Corporation/PhysX/Engine/86C5F4F22ECD/APEX_Particles_x64.dll is not a regular file, not checking sha256sum\n------------------------------------------------------\nExecuting cd /home/ponder/.cache/winetricks/physx\nDownloading https://us.download.nvidia.com/Windows/9.21.0713/PhysX_9.21.0713_SystemSoftware.exe to /home/ponder/.cache/winetricks/physx\n--2025-01-05 23:25:47--  https://us.download.nvidia.com/Windows/9.21.0713/PhysX_9.21.0713_SystemSoftware.exe\nResolving us.download.nvidia.com (us.download.nvidia.com)... 192.229.221.58, 2606:2800:233:ef6:15dd:1ece:1d50:1e1\nConnecting to us.download.nvidia.com (us.download.nvidia.com)|192.229.221.58|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 27152680 (26M) [application/octet-stream]\nSaving to: \u2018PhysX_9.21.0713_SystemSoftware.exe\u2019\n\nPhysX_9.21.0713_Sys 100%[=================&gt;]  25.89M  55.4MB/s    in 0.5s    \n\n2025-01-05 23:25:48 (55.4 MB/s) - \u2018PhysX_9.21.0713_SystemSoftware.exe\u2019 saved [27152680/27152680]\n\nExecuting cd /home/ponder\nExecuting cd /home/ponder/.cache/winetricks/physx\nExecuting /usr/lib/wine/wine64 PhysX_9.21.0713_SystemSoftware.exe\n0238:err:environ:init_peb starting L\"Y:\\\\physx\\\\PhysX_9.21.0713_SystemSoftware.exe\" in experimental wow64 mode\nwine: failed to load L\"\\\\??\\\\C:\\\\windows\\\\syswow64\\\\ntdll.dll\" error c0000135\nApplication could not be started, or no application associated with the specif\nied file.\nShellExecuteEx failed: Internal error.\n\n------------------------------------------------------\nwarning: Note: command /usr/lib/wine/wine64 PhysX_9.21.0713_SystemSoftware.exe returned status 1. Aborting.\n------------------------------------------------------\n</code></pre>"},{"location":"blog/2023/09/23/getting-started-with-dragon-age-origins---ultimate-edition/#avoid-broken-launcher","title":"Avoid broken launcher","text":"<p>Also, as reported by thehoagie,</p> <p>The game tries to use a launcher. Bypass it because it doesn't work. Change the line in this xml file:</p> <pre><code>$ vi \"${HOME}/.local/share/Steam/steamapps/common/Dragon Age Ultimate Edition/data/DAOriginsLauncher.xml\"\n</code></pre> <p>and change line 247 from</p> <pre><code>&lt;xmlbutton name=\"play\" action=\"condition\" value=\"FirstRunCheck\"&gt;\n</code></pre> <p>to</p> <pre><code>&lt;xmlbutton name=\"play\" action=\"execute\" file=\"${BINARIES_DIR}\\DAOrigins.exe\" autoquit=\"true\"&gt;\n</code></pre> <p>With that, the game finally launches. There is a pop-up dialog about DLCs needing to be activated, which is one of those things that EA has broken by not maintaining things working.</p> <p>Years ago there was an option in Steam to obtain the \"CD keys\", but it appears that option is no longer feasible and there is no clear workaround that will work in 2023, except getting the games from GOG:</p> <ul> <li>What does the \"Dragon Age: Origins - Ultimate Edition DLC CD Key\" do?</li> <li>How to get all the DLC</li> <li>Downloadable content (Origins)</li> </ul>"},{"location":"blog/2023/09/23/getting-started-with-dragon-age-origins---ultimate-edition/#update-2025-01-11","title":"Update (2025-01-11)","text":"<p>In the off-chance that this game might have worked better with the Steam from snap, tried installing it again and tried all the above on it.</p> <p>To install <code>physx</code> the snap version of the plx directory was <code>$HOME/snap/steam/common/.local/share/Steam/steamapps/compatdata/47810/pfx</code> and the game was installed under <code>$HOME/snap/steam/common/.local/share/Steam/steamapps/common/Dragon Age Ultimate Edition</code>.</p> <p>Despite all the tweaks above and trying all versions of Proton 7.0-6 and newer, the game would not even show the launcher at all, so that's even worse than the recent update with non-snap Steam.</p>"},{"location":"blog/2023/09/29/recording-audio-in-a-loop/","title":"Recording audio in a loop","text":"<p>Sometimes I wish I could hear what someone just said. Most of the times, me, which is extra hard to recall with precision. We tend to remember what we meant to say, not the actual exact same words.</p> <p>Normally it takes only about an hour, maybe two, to realize of the desire to rewind and listen back.</p>"},{"location":"blog/2023/09/29/recording-audio-in-a-loop/#simple-audio-recording","title":"Simple audio recording","text":""},{"location":"blog/2023/09/29/recording-audio-in-a-loop/#analog-mic-from-gaming-headphones","title":"Analog mic from gaming headphones","text":"<p>Lets start with the basic: record from an old mic.</p> <p>Lexicon beign an Intel NUC11PAHi3, it has a front 2.5 mm connector for the 4-contact headphone+mic headset, which is precisely what I have left over from the old days before I got wireless headphones.</p> <p>The first question is whether the soundcard for that connector is supported and detected by the kernel:</p> <pre><code># dmesg | egrep -i 'audio|sound|snd' | grep -vi hdmi\n[    8.525267] snd_hda_intel 0000:00:1f.3: DSP detected with PCI class/subclass/prog-if info 0x040380\n[    8.525325] snd_hda_intel 0000:00:1f.3: enabling device (0000 -&gt; 0002)\n[    8.527228] snd_hda_intel 0000:00:1f.3: bound 0000:00:02.0 (ops i915_audio_component_bind_ops [i915])\n[    8.568725] snd_hda_codec_realtek hdaudioC0D0: autoconfig for ALC256: line_outs=1 (0x21/0x0/0x0/0x0/0x0) type:hp\n[    8.568731] snd_hda_codec_realtek hdaudioC0D0:    speaker_outs=0 (0x0/0x0/0x0/0x0/0x0)\n[    8.568732] snd_hda_codec_realtek hdaudioC0D0:    hp_outs=0 (0x0/0x0/0x0/0x0/0x0)\n[    8.568734] snd_hda_codec_realtek hdaudioC0D0:    mono: mono_out=0x0\n[    8.568735] snd_hda_codec_realtek hdaudioC0D0:    inputs:\n[    8.568736] snd_hda_codec_realtek hdaudioC0D0:      Internal Mic=0x13\n[    8.568737] snd_hda_codec_realtek hdaudioC0D0:      Internal Mic=0x12\n[    8.652690] input: HDA Intel PCH Headphone as /devices/pci0000:00/0000:00:1f.3/sound/card0/input4\n</code></pre> <p>Note: without <code>grep -vi hdmi</code> there are 12 additional inputs, not the ones we want now.</p> <p><code>/devices/pci0000:00/0000:00:1f.3/sound/card0/input4</code> looks like the one we want, but how do we record?</p> <p>I search for a while how to do this without ALSA or Pulseadio, but in the end gave in to ALSA:</p> <pre><code># apt install alsa-utils -y\n# arecord -L | grep -C1 'HDA Intel'\nhw:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    Direct hardware device without any conversions\nplughw:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    Hardware device with all software conversions\ndefault:CARD=PCH\n    HDA Intel PCH, ALC256 Analog\n    Default Audio Device\nsysdefault:CARD=PCH\n    HDA Intel PCH, ALC256 Analog\n    Default Audio Device\nfront:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    Front output / input\ndsnoop:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    Direct sample snooping device\n\n# aplay -L | grep -C1 'HDA Intel.*Analog'\nhw:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    Direct hardware device without any conversions\n--\nplughw:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    Hardware device with all software conversions\n--\ndefault:CARD=PCH\n    HDA Intel PCH, ALC256 Analog\n    Default Audio Device\nsysdefault:CARD=PCH\n    HDA Intel PCH, ALC256 Analog\n    Default Audio Device\nfront:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    Front output / input\nsurround21:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    2.1 Surround output to Front and Subwoofer speakers\nsurround40:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    4.0 Surround output to Front and Rear speakers\nsurround41:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    4.1 Surround output to Front, Rear and Subwoofer speakers\nsurround50:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    5.0 Surround output to Front, Center and Rear speakers\nsurround51:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    5.1 Surround output to Front, Center, Rear and Subwoofer speakers\nsurround71:CARD=PCH,DEV=0\n    HDA Intel PCH, ALC256 Analog\n    7.1 Surround output to Front, Center, Side, Rear and Woofer speakers\n--\n</code></pre> <p>To list the available input devices capable of capturing (recording) audio:</p> <pre><code># arecord -l\n**** List of CAPTURE Hardware Devices ****\ncard 0: PCH [HDA Intel PCH], device 0: ALC256 Analog [ALC256 Analog]\n  Subdevices: 1/1\n  Subdevice #0: subdevice #0\ncard 1: S7 [SteelSeries Arctis 7], device 0: USB Audio [USB Audio]\n  Subdevices: 1/1\n  Subdevice #0: subdevice #0\n</code></pre> <p>To record from the microphone in the onboard jack:</p> <pre><code># arecord \\\n  -D plughw:1,0 \\\n  -f S16_LE /tmp/test__.wav\nRecording WAVE '/tmp/test__.wav' : Signed 16 bit Little Endian, Rate 8000 Hz, Mono\n^CAborted by signal Interrupt...\n</code></pre> <p>To record from the microphone in the USB headphones:</p> <pre><code># arecord \\\n  -D plughw:CARD=S7,DEV=1 \\\n  -f S16_LE /tmp/test_S7_1.wav\nLittle Endian, Rate 8000 Hz, Mono\n^CAborted by signal Interrupt...\n</code></pre> <p>The problem with this is... well, these are headphone microphones, actually gaming headphones, so they only capture audio from the mouth a few milimeters from the microphone and very nearly nothing at all from anything else. This is very good performance for their intented use case, useless of today's goal.</p>"},{"location":"blog/2023/09/29/recording-audio-in-a-loop/#webcam-microphone-much-better","title":"Webcam microphone (much better)","text":"<p>The only other microphone available is an old webcam, never used otherwise. This begin a common Logitec C920 HD Pro, it is well supported:</p> <pre><code>[204236.991023] usb 3-2: new high-speed USB device number 4 using xhci_hcd\n[204237.709488] usb 3-2: New USB device found, idVendor=046d, idProduct=0892, bcdDevice= 0.19\n[204237.709496] usb 3-2: New USB device strings: Mfr=0, Product=2, SerialNumber=1\n[204237.709501] usb 3-2: Product: HD Pro Webcam C920\n[204237.709504] usb 3-2: SerialNumber: B9BAB6EF\n[204238.583154] videodev: Linux video capture interface: v2.00\n[204238.617035] gspca_main: v2.14.0 registered\n[204238.621950] gspca_main: vc032x-2.14.0 probing 046d:0892\n[204238.622360] gspca_vc032x: reg_r err -32\n[204238.622367] vc032x: probe of 3-2:1.0 failed with error -32\n[204238.622404] usbcore: registered new interface driver vc032x\n[204238.634075] usb 3-2: Found UVC 1.00 device HD Pro Webcam C920 (046d:0892)\n[204238.636766] input: HD Pro Webcam C920 as /devices/pci0000:00/0000:00:14.0/usb3/3-2/3-2:1.0/input/input20\n[204238.637655] usbcore: registered new interface driver uvcvideo\n</code></pre> <p>Once again, list capture capable devices and record:</p> <pre><code># arecord -l\n**** List of CAPTURE Hardware Devices ****\ncard 0: PCH [HDA Intel PCH], device 0: ALC256 Analog [ALC256 Analog]\n  Subdevices: 1/1\n  Subdevice #0: subdevice #0\ncard 1: C920 [HD Pro Webcam C920], device 0: USB Audio [USB Audio]\n  Subdevices: 1/1\n  Subdevice #0: subdevice #0\n\n# arecord \\\n  -D plughw:CARD=C920,DEV=0 \\\n  -f S16_LE /tmp/test_C920.wav\n</code></pre> <p>At first this produce barely audible sound, need to increase the capture volume, but not too much (needed to try a few values to find a good one). More importantly, use <code>-f cd</code> to capture audio in a better format:</p> <pre><code># amixer -c1 set Mic 70%\nSimple mixer control 'Mic',0\n  Capabilities: cvolume cvolume-joined cswitch cswitch-joined\n  Capture channels: Mono\n  Limits: Capture 0 - 60\n  Mono: Capture 42 [70%] [41.00dB] [on]\n\n# arecord \\\n  -D plughw:CARD=C920,DEV=0 \\\n  -f cd /tmp/test_C920.wav\n</code></pre>"},{"location":"blog/2023/09/29/recording-audio-in-a-loop/#clicky-noises-10-hz","title":"Clicky noises (~10 Hz)","text":"<p>Audio recording devices are affected by nearby electromagnetic waves, including those produced by cables. Power cables and power extensions in particular can be most problematic, so the webcam (or microphone) must be kept well away from those.</p>"},{"location":"blog/2023/09/29/recording-audio-in-a-loop/#loop-recording","title":"Loop recording","text":"<p>This example from the <code>arecord(1)</code> man page shows how to record indefinitely with a new file per hour:</p> <p><code>arecord -f cd -t wav --max-file-time 3600 --use-strftime %Y/%m/%d/listen-%H-%M-%v.wav</code></p> <p>Record in stereo from the default audio source. Create a new file every hour. The files are placed in directories based on their start dates and have names which include their start times and file numbers.</p> <pre><code># arecord \\\n  -f cd \\\n  -t wav \\\n  -D plughw:CARD=C920,DEV=0 \\\n  --max-file-time 3600 \\\n  --use-strftime %Y-%m-%d-%H-%M-%v.wav\n</code></pre> <p>A 5-minute recording in WAVE format takes about 50 MB, so each 1-hour file should be about 600 MB. Keeping a whole day of such files would take about 15 GB which is affordable.</p>"},{"location":"blog/2023/09/29/recording-audio-in-a-loop/#loop-recording-as-a-service","title":"Loop recording as a service","text":"<p>All that is left to do is keep that recording process running and making sure it starts again after each reboot.</p> <ol> <li> <p>Create the script <code>/root/bin/record-hourly</code> to run <code>arecord</code></p> <p></p>record-hourly<pre><code>#!/bin/bash\nD=/home/depot/audio/Hourly\nmkdir -p $D\narecord \\\n-f cd \\\n-t wav \\\n-D plughw:CARD=C920,DEV=0 \\\n--max-file-time 3600 \\\n--use-strftime $D/%Y-%m-%d-%H-%M-%v.wav\n</code></pre> 2. Create a service to run it on startup:    <code>/etc/systemd/system/record-hourly.service</code><p></p> record-hourly.service<pre><code>[Unit]\nDescription=Audio Recording\nAfter=influxd.service\nWants=influxd.service\n\n[Service]\nExecStart=/root/bin/record-hourly\nRestart=on-failure\nStandardOutput=null\nUser=root\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> </li> <li> <p>Load and start the new service</p> <pre><code># systemctl daemon-reload\n# systemctl enable record-hourly.service\nCreated symlink /etc/systemd/system/multi-user.target.wants/record-hourly.service \u2192 /etc/systemd/system/record-hourly.service.\n# systemctl start record-hourly.service\n</code></pre> </li> <li> <p>Run a daily job to generate spectrograms    from recordings, running <code>wav-to-png</code> as:</p> wav-to-png<pre><code>#!/bin/bash\nfor w in $*; do\n  png=${w/wav/png}\n  test -f $png || sox $w -n spectrogram -o $png\ndone\n</code></pre> </li> <li> <p>Run a daily job to remove recordings older than 3 days,    e.g. as <code>clean-up-recordings</code></p> clean-up-recordings<pre><code>#!/bin/bash\nD=/home/depot/audio/Hourly\ntest -d $D || exit 1\n\nN=3  # Number of days to keep\nnow=$(date +\"%s\")\n\nN_days_ago=$((now - N*24*3600))\nn=1\nwhile [[ $n -gt 0 ]]; do\n  date=$(date -d @$N_days_ago +\"%Y-%m-%d\")\n  n=$(ls $D | grep -c ${date}.*.wav) \n  rm -f $D/${date}-*.wav\n  date_ts=$(date -d @$N_days_ago +\"%s\")\n  N_days_ago=$((date_ts - 24*3600))\ndone\n</code></pre> </li> </ol>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/","title":"Modding a Steam game on Linux: The Elder Scrolls V: Skyrim Special Edition","text":"<p>The Elder Scrolls V: Skyrim Special Edition is admittedly one of my favorite games, having poured 200 hours and still wanted to go back and play it again.</p>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#vanilla-game","title":"Vanilla game","text":"<p>According to Proton reports this game runs mostly pretty well without tweaks,  so the first time I run it there was no tweaking or tinkering involved. The game spent 2 minutes compiling Vulkan shaders, detected optimal video settings and run.</p>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#audio-fix","title":"Audio fix","text":"<p>However, a few Proton reports indicate that <code>xact_x64</code> is required to fix the issue where voices are missing and indeed I experienced this myself. To fix this, install this with <code>winetricks</code>:</p> <pre><code>$ WINEARCH=win64 \\\n  WINEPREFIX=$HOME/.local/share/Steam/steamapps/compatdata/489830/pfx \\\n  winetricks xact\n$ WINEARCH=win64 \\\n  WINEPREFIX=$HOME/.local/share/Steam/steamapps/compatdata/489830/pfx \\\n  winetricks xact_x64\n</code></pre> <p>However, after installing these the game no longer runs and there is no clear error message even in <code>~/.xsession-errors</code>; forcing the game to run on Proton 7.0-6 did the trick and the Skyrim theme (vocal music) plays as soon as it starts.</p> <pre><code>.local/share/Steam/steamapps/common/Skyrim\\ Special\\ Edition/Skyrim\n</code></pre>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#screen-resolution","title":"Screen resolution","text":"<p>The game is old and conservative, so it runs at a mere 1920x1080 resolution despite having an ultrawide 3440x1440 display. The game is particularly known to work poorly with ultrawide screens, it is possible to set the game to run at 3440x1440 (full screen) but that will make UI elements at the bottom and top not accessible, including controller button hints and essential stats like gold and weight.</p> <p>To set the change resolution, edit the <code>SkyrimPrefs.ini</code> that is under your personal files, not the one that is part of the game files:</p> <pre><code>$ cd; find . -name SkyrimPrefs.ini\n./.local/share/Steam/steamapps/compatdata/489830/pfx/drive_c/users/steamuser/Documents/My Games/Skyrim Special Edition/SkyrimPrefs.ini\n./.local/share/Steam/steamapps/common/Skyrim Special Edition/Skyrim/SkyrimPrefs.ini\n</code></pre> <p>Edit the file under the <code>.../steamuser/Documents/...</code> path, otherwise the changes will not be effective. Change these lines</p> SkyrimPrefs.ini<pre><code>bFull Screen=0\niSize H=1200\niSize W=1920\n</code></pre> <p>To run on 3440x1440 full screen:</p> SkyrimPrefs.ini<pre><code>bFull Screen=1\niSize H=1440\niSize W=3440\n</code></pre> <p>Note</p> <p>See Guide:Skyrim Configuration Settings and the Guide:SkyrimPrefs INI for full details on how to configure Skyrim.</p>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#mods","title":"Mods","text":"<p>There is only one <code>Data</code> folder to install mods in:</p> <pre><code>~/.local/share/Steam/steamapps/common/Skyrim Special Edition/Data\n</code></pre>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#backup-skyrim-special-edition","title":"Backup Skyrim Special Edition","text":"<p>Through the journey of installing mods and other tools, is is recommended to make backups of the entire game folder (<code>Skyrim Special Edition</code>) to rollback changes when (not if) something breaks.</p> <p>To make multiple backups, each with a timestamp:</p> <pre><code>$ cd ~/.local/share/Steam/steamapps/common/\n$ rsync -ruta \\\n  \"Skyrim Special Edition\" \\\n  \"Backup of Skyrim Special Edition on $(date +\"%Y-%m-%d-%H-%M-%S\")\"\n</code></pre>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#vortex-mod-manager-failed","title":"Vortex mod manager (failed)","text":"<p>This Proton report recommends using Vortex mod manager installed through Lutris using the official install configuration.  That sounds like it should be easy enough, but even better there is a detailed walkthrough to  Manage Skyrim SE mods via Vortex Mod Manager in Linux.</p> <p>This walkthrough recommend installing an old version of Vortex via a YML file breated by rockerbacon</p> <pre><code>$ wget -O ~/Downloads/vortex.yml \\\n  https://github.com/rockerbacon/modorganizer2-linux-installer/releases/download/1.9.3/vortex.yml\n$ lutris -i ~/Downloads/vortex.yml\n</code></pre> <p>The installation is a bit rocky; after installing a few components the installer seems to fail with exit code 256, but hitting Back and then Continue/Install again it actually works. It shows the instructrions to disable the Internet connection and after that it installs and launches Vortex just fine.</p> <p>Add the Steam library under Settings =&gt; Games =&gt; Add Search Directory and enter the path based on / (root) as <code>/home/coder/.local/share/Steam/steamapps/common</code>, then do a full scan (Games =&gt; Scan =&gt; Scan:Full) and that should show all your (installed) Steam games.</p> <p>Add Skyrim Special Edition with the <code>+</code> sign on its thumbnail, then go to MODS section and drop a file in the Drop File(s) are to install mods manually.</p> <p>Warning</p> <p>DO NOT use the \"SSE Engine Fixes\" mod, it will mess up your audio.</p>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#skse","title":"SKSE","text":"<p>The Skyrim Script Extender (SKSE) is a tool used by many Skyrim mods that expands scripting capabilities and adds additional functionality to the game. Once installed, no additional steps are needed to launch Skyrim with SKSE's added functionality. You can start the game using SKSE from <code>skse64_loader.exe</code>.</p>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#using-vortex-failed","title":"Using Vortex (failed)","text":"<p>Download the version Compatible with Skyrim Special Edition 1.6.640 from Steam <code>Skyrim Script Extender (SKSE64)-30379-2-2-3-1665515370.7z</code> and drop the file into Vortex, then click Install.</p> <p>The whole Vortex window, or possibly a new, goes white and does nothing. After a while, it tries to go full screen, and still does nothing. No new files are installed.</p>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#manual-install-ok","title":"Manual install (ok)","text":"<p>Extract and copy all SKSE files and folders to the Skyrim SE game folder, then rename executables:</p> <pre><code>$ 7z x Skyrim\\ Script\\ Extender\\ \\(SKSE64\\)-30379-2-2-3-1665515370.7z \n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,16 CPUs AMD Ryzen 7 5800X 8-Core Processor              (A20F12),ASM,AES-NI)\n\nScanning the drive for archives:\n1 file, 748635 bytes (732 KiB)\n\nExtracting archive: Skyrim Script Extender (SKSE64)-30379-2-2-3-1665515370.7z\n--\nPath = Skyrim Script Extender (SKSE64)-30379-2-2-3-1665515370.7z\nType = 7z\nPhysical Size = 748635\nHeaders Size = 7306\nMethod = LZMA:5m BCJ2\nSolid = +\nBlocks = 2\n\nEverything is Ok\n\nFolders: 15\nFiles: 536\nSize:       4282404\nCompressed: 748635\n\n$ cp -a \\\n  skse64_2_02_03/skse64_* \\\n  ~/.local/share/Steam/steamapps/common/Skyrim\\ Special\\ Edition/\n$ cp -a \\\n  skse64_2_02_03/Data/Scripts/ \\\n  ~/.local/share/Steam/steamapps/common/Skyrim\\ Special\\ Edition/Data/\n</code></pre> <p>At this point there are 2 ways to launch Skyrim using the SKSE loader. This Proton report recommends these launch options to launch SKSE without having to rename executables (which seems to break some mods):</p> <pre><code>$(echo %command% | sed -r \"s/proton waitforexitandrun .*/proton waitforexitandrun/\") \"$STEAM_COMPAT_INSTALL_PATH/skse64_loader.exe\"\n</code></pre> <p>However, these launch options require a custom proton version.</p> <p>Also, in order to use the very desirable Sky UI mod, it is highly recommended to use Glorious Eggroll instead of the standard Proton release.</p> <p>The simpler alterantive si to simply rename files:</p> <pre><code>$ mv SkyrimSELauncher.exe SkyrimSELauncher.orig.exe\n$ mv skse64_loader.exe SkyrimSELauncher.exe\n</code></pre>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#glorious-eggroll","title":"Glorious Eggroll","text":"<p>Install Glorious Eggroll using the Manual method for Native Steam:</p> <pre><code># make temp working directory\nmkdir /tmp/proton-ge-custom\ncd /tmp/proton-ge-custom\n\n# download  tarball\ncurl -sLOJ \"$(curl -s https://api.github.com/repos/GloriousEggroll/proton-ge-custom/releases/latest | grep browser_download_url | cut -d\\\" -f4 | grep .tar.gz)\"\n\n# download checksum\ncurl -sLOJ \"$(curl -s https://api.github.com/repos/GloriousEggroll/proton-ge-custom/releases/latest | grep browser_download_url | cut -d\\\" -f4 | grep .sha512sum)\"\n\n# check tarball with checksum\nsha512sum -c ./*.sha512sum\n# if result is ok, continue\n\n# make steam directory if it does not exist\nmkdir -p ~/.steam/root/compatibilitytools.d\n\n# extract proton tarball to steam directory\ntar -xf GE-Proton*.tar.gz \\\n  -C ~/.steam/root/compatibilitytools.d/\n</code></pre>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#sky-ui","title":"Sky UI","text":"<p>Download the updated UI mod Sky_UI and install it manually (simple):</p> <pre><code>$ 7z x SkyUI_5_2_SE-12604-5-2SE.7z \n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,16 CPUs AMD Ryzen 7 5800X 8-Core Processor              (A20F12),ASM,AES-NI)\n\nScanning the drive for archives:\n1 file, 2783417 bytes (2719 KiB)\n\nExtracting archive: SkyUI_5_2_SE-12604-5-2SE.7z\n--\nPath = SkyUI_5_2_SE-12604-5-2SE.7z\nType = 7z\nPhysical Size = 2783417\nHeaders Size = 300\nMethod = LZMA2:3m\nSolid = +\nBlocks = 1\n\nEverything is Ok\n\nFolders: 1\nFiles: 6\nSize:       2931171\nCompressed: 2783417\n$ cp -a SkyUI_* \\\n  ~/.local/share/Steam/steamapps/common/Skyrim\\ Special\\ Edition/Data/\n</code></pre> <p>At this point it becomes clear, not only from the game UI but also from every single forum thread, that a Bethesda.net account is required to even load mods in this game.</p>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#sky-ui-keeps-auto-disabling","title":"Sky UI keeps auto-disabling","text":"<p>I seem to keep running into an old issue where <code>SkyUI_SE.esp</code> disables itself automatically because it has no masters listed, it's a bug with Skyrim Special Edition. Adding <code>Skyrim.esm</code> as a master to the file will fix it, and to do this you have to use xEdit.</p> <pre><code>$ 7z x TES5Edit\\ 4.0.4-25859-4-0-4-1636548544.7z \n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,16 CPUs AMD Ryzen 7 5800X 8-Core Processor              (A20F12),ASM,AES-NI)\n\nScanning the drive for archives:\n1 file, 14361776 bytes (14 MiB)\n\nExtracting archive: TES5Edit 4.0.4-25859-4-0-4-1636548544.7z\n--\nPath = TES5Edit 4.0.4-25859-4-0-4-1636548544.7z\nType = 7z\nPhysical Size = 14361776\nHeaders Size = 4031\nMethod = LZMA2:27 LZMA:20 BCJ2\nSolid = +\nBlocks = 2\n\nEverything is Ok                                 \n\nFolders: 3\nFiles: 180\nSize:       112996891\nCompressed: 14361776\n\n$ cp -a TES5Edit\\ 4.0.4 \\\n  ~/.local/share/Steam/steamapps/common/Skyrim\\ Special\\ Edition/TES5Edit\n\n$ cd ~/.local/share/Steam/steamapps/common/Skyrim\\ Special\\ Edition\n\n$ WINEARCH=win64 WINEPREFIX=$HOME/.local/share/Steam/steamapps/compatdata/489830/pfx wine \\\n  TES5Edit/TES5Edit.exe \\\n  -AllowMasterFilesEdit \\\n  -IKnowWhatImDoing \\\n  Data/SkyUI_SE.esp\n</code></pre> <p>This seems to fail, starts by showing an error message saying</p> <p>Fatal: Could not open registry key: \\SOFTWARE\\Bethesda Softworks\\Skyrim   This can happen after Steam updates, run the game's launcher to restore registry settings</p> <p>This keeps happening even right after launching the game from Steam. The UI does not show any hint of actually opening the file, not even  the expected first-time warning.</p>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#ultrawide-ui","title":"Ultrawide UI","text":"<p>Complete Widescreen Fix for Vanilla and SkyUI 2.2 and 5.2 SE</p> <pre><code>$ unrar x Complete\\ Widescreen\\ Fix\\ for\\ SkyUI\\ 5.2\\ SE\\ Alpha\\ -\\ 2560x1080-1778-2-0.rar \n\nUNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n\n\nExtracting from Complete Widescreen Fix for SkyUI 5.2 SE Alpha - 2560x1080-1778-2-0.rar\n\nCreating    interface                                                 OK\nExtracting  interface/bartermenu.swf                                  OK \nExtracting  interface/containermenu.swf                               OK \nExtracting  interface/craftingmenu.swf                                OK \nExtracting  interface/dialoguemenu.swf                                OK \nExtracting  interface/giftmenu.swf                                    OK \nExtracting  interface/inventorymenu.swf                               OK \nExtracting  interface/lockpickingmenu.swf                             OK \nExtracting  interface/magicmenu.swf                                   OK \nExtracting  interface/map.swf                                         OK \nExtracting  interface/messagebox.swf                                  OK \nExtracting  interface/quest_journal.swf                               OK \nCreating    interface/skyui                                           OK\nExtracting  interface/skyui/bottombar.swf                             OK \nExtracting  interface/skyui/configpanel.swf                           OK \nExtracting  interface/sleepwaitmenu.swf                               OK \nExtracting  interface/statsmenu.swf                                   OK \nExtracting  interface/trainingmenu.swf                                OK \nExtracting  interface/tweenmenu.swf                                   OK \nExtracting  widescreen_skyui_fix.esp                                  OK \nExtracting  widescreen_skyui_fix.ini                                  OK \nCreating    interface/exported                                        OK\nExtracting  interface/exported/racesex_menu.gfx                       OK \nAll OK\n\n$ cp -a interface \\\n  ~/.local/share/Steam/steamapps/common/Skyrim\\ Special\\ Edition/Data/\n</code></pre> <p>Then edit <code>SkyrimPrefs.ini</code> to change screen resolution.</p> <p>At this point it becomes clear, not only is theoretically bad that a Bethesda.net account is required to even load mods, it is actually terrible in practice when 10 out of 10 times trying to load mods all you get is the infamous Couldn\u2019t connect to the Bethesda.net servers error.</p> <p>accounts.bethesda.net/en/linked-accounts shows the Steam account is linked, yet the error persists and there is nothing you can do but wait... forever?</p>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#the-elder-scrolls-v-skyrim-original","title":"The Elder Scrolls V: Skyrim (original)","text":"<p>The Elder Scrolls V: Skyrim original game is no longer available in Steam, and it can't even be launched by its game id:</p> <pre><code>steam steam://rungameid/72850\n</code></pre> <p>This leads to a \"No licenses\" error pop-up and, maybe worse yet, Steam starts to check the game and re-download all or part of it. At least, it doesn't seem to have deleted mods.</p>"},{"location":"blog/2023/10/14/modding-a-steam-game-on-linux-the-elder-scrolls-v-skyrim-special-edition/#mod-manager","title":"Mod Manager","text":"<p>One recommendation to get modding with this game is to</p> <p>Install mods using MO2 through steam tinker launch and make your skse data folder into a mod and install through MO2. Also having skse installed first will help it get auto detected by MO2.</p>"},{"location":"blog/2023/10/15/calibrating-screen-color-with-displaycal/","title":"Calibrating screen color with DisplayCAL","text":"<p>Calibrating screen color is mostly optional these days, if you buy a good screen that comes out of factory with a good calibration profile. Nonetheless, it is recommended to re-calibrate every year or so to account for display ageing, so a few years ago I purchased a Datacolor Spyder5 Express which is known to work on Linux, with Argyll Color Management System.</p> <p>What doesn't always work so well is DisplayCal. In fact, the original project is dead and was dropped from Ubuntu 20.04 but was still possible to build with python2.7 packages. That is no longer possible in Ubuntu 22.04, but there is a Python 3 fork: eoyilmaz/displaycal-py3</p>"},{"location":"blog/2023/10/15/calibrating-screen-color-with-displaycal/#system-requirements","title":"System requirements","text":"<p>Running DisplayCAL requires building its depedencies (through PIP) so a few development packages are necessary, on top of ArgyllCMS:</p> <pre><code># apt install -y \\\n    build-essential dbus libglib2.0-dev pkg-config \\\n    libgtk-3-dev libxxf86vm-dev python3.10-venv argyll\n</code></pre>"},{"location":"blog/2023/10/15/calibrating-screen-color-with-displaycal/#python-requirements","title":"Python requirements","text":"<p>Building dependencies is necessary in either case, but unless you want to build DisplayCAL itself from source, you can skip directly to run the latest release.</p> <pre><code>$ git clone https://github.com/eoyilmaz/displaycal-py3\n$ python -m venv ./displaycal_venv\n$ source ./displaycal_venv/bin/activate\n$ cd ./displaycal-py3/\n$ pip install -r requirements.txt\n  Running setup.py install for wxPython ... |\n</code></pre> <p>The instalation of <code>wxPython</code> takes a while, it will keep the CPU busy and hot for several minutes (about 10 minutes on an AMD Ryzen 5 1600X).</p>"},{"location":"blog/2023/10/15/calibrating-screen-color-with-displaycal/#build-from-source","title":"Build from source","text":"<pre><code>$ python -m build\n$ pip install dist/DisplayCAL-3.9.*.whl\n</code></pre> <p>Sadly this fails to build, despite having installed all the above requirements:</p> <code>python -m build</code> <pre><code>$ python -m build\n* Creating venv isolated environment...\n* Installing packages in isolated environment... (setuptools)\n* Getting build dependencies for sdist...\nTrying to get git version information...\nTrying to get git information...\nGenerating __version__.py\nVersion 3.9.11\n['egg_info']\n*** /home/artist/Downloads/displaycal_venv/lib/python3.10/site-packages/pyproject_hooks/_in_process/_in_process.py egg_info\nusing distutils\n/tmp/build-env-uooasz97/lib/python3.10/site-packages/setuptools/config/setupcfg.py:293: _DeprecatedConfig: Deprecated config in `setup.cfg`\n!!\n\n        ********************************************************************************\n        The license_file parameter is deprecated, use license_files instead.\n\n        By 2023-Oct-30, you need to update your project and remove deprecated calls\n        or your builds will no longer be supported.\n\n        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n        ********************************************************************************\n\n!!\n  parsed = self.parsers.get(option_name, lambda x: x)(value)\nwarning: no files found matching 'MANIFEST'\nwarning: no files found matching 'use-distutils'\nwarning: no files found matching '_in_process.py'\nwarning: no files found matching '_in_process.cfg'\nwarning: no files found matching 'dist/copyright'\nwarning: no files found matching 'DisplayCAL/ref/DCDM'\nwarning: no files found matching 'X'Y'Z'.icm'\nwarning: no files found matching 'DisplayCAL/ref/XYZ'\nwarning: no files found matching 'D50'\nwarning: no files found matching '(ICC'\nwarning: no files found matching 'PCS'\nwarning: no files found matching 'encoding).icm'\nwarning: no files found matching 'DisplayCAL/ref/XYZ'\nwarning: no files found matching 'D50.icm'\nwarning: no files found matching 'dist/net.displaycal.DisplayCAL.appdata.xml'\nwarning: no files found matching 'misc/displaycal.desktop'\nwarning: no files found matching 'misc/z-displaycal-apply-profiles.desktop'\nwarning: no previously-included files matching '*~' found anywhere in distribution\nwarning: no previously-included files matching '*.backup' found anywhere in distribution\nwarning: no previously-included files matching '*.bak' found anywhere in distribution\n* Building sdist...\nTrying to get git version information...\nTrying to get git information...\nGenerating __version__.py\nVersion 3.9.11\n['sdist', '--formats', 'gztar', '--dist-dir', '/home/artist/Downloads/displaycal-py3/dist/.tmp-70quu7wi']\n*** /home/artist/Downloads/displaycal_venv/lib/python3.10/site-packages/pyproject_hooks/_in_process/_in_process.py sdist --formats gztar --dist-dir /home/artist/Downloads/displaycal-py3/dist/.tmp-70quu7wi\nusing distutils\n/tmp/build-env-uooasz97/lib/python3.10/site-packages/setuptools/config/setupcfg.py:293: _DeprecatedConfig: Deprecated config in `setup.cfg`\n!!\n\n        ********************************************************************************\n        The license_file parameter is deprecated, use license_files instead.\n\n        By 2023-Oct-30, you need to update your project and remove deprecated calls\n        or your builds will no longer be supported.\n\n        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n        ********************************************************************************\n\n!!\n  parsed = self.parsers.get(option_name, lambda x: x)(value)\nwarning: no files found matching 'MANIFEST'\nwarning: no files found matching 'use-distutils'\nwarning: no files found matching '_in_process.py'\nwarning: no files found matching '_in_process.cfg'\nwarning: no files found matching 'DisplayCAL/ref/DCDM'\nwarning: no files found matching 'X'Y'Z'.icm'\nwarning: no files found matching 'DisplayCAL/ref/XYZ'\nwarning: no files found matching 'D50'\nwarning: no files found matching '(ICC'\nwarning: no files found matching 'PCS'\nwarning: no files found matching 'encoding).icm'\nwarning: no files found matching 'DisplayCAL/ref/XYZ'\nwarning: no files found matching 'D50.icm'\nwarning: no files found matching 'misc/displaycal.desktop'\nwarning: no files found matching 'misc/z-displaycal-apply-profiles.desktop'\nwarning: no previously-included files matching '*~' found anywhere in distribution\nwarning: no previously-included files matching '*.backup' found anywhere in distribution\nwarning: no previously-included files matching '*.bak' found anywhere in distribution\n* Building wheel from sdist\n* Creating venv isolated environment...\n* Installing packages in isolated environment... (setuptools)\n* Getting build dependencies for wheel...\n['egg_info']\n*** /home/artist/Downloads/displaycal_venv/lib/python3.10/site-packages/pyproject_hooks/_in_process/_in_process.py egg_info\nusing distutils\n/tmp/build-env-po6qurti/lib/python3.10/site-packages/setuptools/config/setupcfg.py:293: _DeprecatedConfig: Deprecated config in `setup.cfg`\n!!\n\n        ********************************************************************************\n        The license_file parameter is deprecated, use license_files instead.\n\n        By 2023-Oct-30, you need to update your project and remove deprecated calls\n        or your builds will no longer be supported.\n\n        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n        ********************************************************************************\n\n!!\n  parsed = self.parsers.get(option_name, lambda x: x)(value)\nwarning: no files found matching 'MANIFEST'\nwarning: no files found matching 'use-distutils'\nwarning: no files found matching '_in_process.py'\nwarning: no files found matching '_in_process.cfg'\nwarning: no files found matching 'misc/displaycal.desktop'\nwarning: no files found matching 'misc/z-displaycal-apply-profiles.desktop'\nwarning: no previously-included files found matching 'misc/Argyll'\nwarning: no previously-included files found matching 'misc/*.rules'\nwarning: no previously-included files found matching 'misc/*.usermap'\nwarning: no previously-included files matching '*~' found anywhere in distribution\nwarning: no previously-included files matching '*.backup' found anywhere in distribution\nwarning: no previously-included files matching '*.bak' found anywhere in distribution\n* Installing packages in isolated environment... (wheel)\n* Building wheel...\n['bdist_wheel', '--dist-dir', '/home/artist/Downloads/displaycal-py3/dist/.tmp-1xx40_u9']\n*** /home/artist/Downloads/displaycal_venv/lib/python3.10/site-packages/pyproject_hooks/_in_process/_in_process.py bdist_wheel --dist-dir /home/artist/Downloads/displaycal-py3/dist/.tmp-1xx40_u9\nusing distutils\n/tmp/build-env-po6qurti/lib/python3.10/site-packages/setuptools/config/setupcfg.py:293: _DeprecatedConfig: Deprecated config in `setup.cfg`\n!!\n\n        ********************************************************************************\n        The license_file parameter is deprecated, use license_files instead.\n\n        By 2023-Oct-30, you need to update your project and remove deprecated calls\n        or your builds will no longer be supported.\n\n        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n        ********************************************************************************\n\n!!\n  parsed = self.parsers.get(option_name, lambda x: x)(value)\nDisplayCAL/RealDisplaySizeMM.c: In function \u2018get_displays\u2019:\nDisplayCAL/RealDisplaySizeMM.c:871:61: warning: pointer targets in passing argument 11 of \u2018XRRGetOutputProperty\u2019 differ in signedness [-Wpointer-sign]\n  871 |                                     &amp;ret_type, &amp;ret_format, &amp;ret_len, &amp;ret_togo, &amp;atomv) == Success\n      |                                                             ^~~~~~~~\n      |                                                             |\n      |                                                             long int *\nIn file included from DisplayCAL/RealDisplaySizeMM.c:33:\n/usr/include/X11/extensions/Xrandr.h:340:38: note: expected \u2018long unsigned int *\u2019 but argument is of type \u2018long int *\u2019\n  340 |                       unsigned long *nitems, unsigned long *bytes_after,\n      |                       ~~~~~~~~~~~~~~~^~~~~~\nDisplayCAL/RealDisplaySizeMM.c:871:71: warning: passing argument 12 of \u2018XRRGetOutputProperty\u2019 from incompatible pointer type [-Wincompatible-pointer-types]\n  871 |                                     &amp;ret_type, &amp;ret_format, &amp;ret_len, &amp;ret_togo, &amp;atomv) == Success\n      |                                                                       ^~~~~~~~~\n      |                                                                       |\n      |                                                                       long unsigned int **\nIn file included from DisplayCAL/RealDisplaySizeMM.c:33:\n/usr/include/X11/extensions/Xrandr.h:340:61: note: expected \u2018long unsigned int *\u2019 but argument is of type \u2018long unsigned int **\u2019\n  340 |                       unsigned long *nitems, unsigned long *bytes_after,\n      |                                              ~~~~~~~~~~~~~~~^~~~~~~~~~~\nDisplayCAL/RealDisplaySizeMM.c:1036:53: warning: pointer targets in passing argument 10 of \u2018XGetWindowProperty\u2019 differ in signedness [-Wpointer-sign]\n1036 |                             &amp;ret_type, &amp;ret_format, &amp;ret_len, &amp;ret_togo, &amp;atomv) == Success\n      |                                                     ^~~~~~~~\n      |                                                     |\n      |                                                     long int *\nIn file included from DisplayCAL/RealDisplaySizeMM.c:27:\n/usr/include/X11/Xlib.h:2696:5: note: expected \u2018long unsigned int *\u2019 but argument is of type \u2018long int *\u2019\n2696 |     unsigned long*      /* nitems_return */,\n      |     ^~~~~~~~~~~~~~\nDisplayCAL/RealDisplaySizeMM.c:1036:63: warning: pointer targets in passing argument 11 of \u2018XGetWindowProperty\u2019 differ in signedness [-Wpointer-sign]\n1036 |                             &amp;ret_type, &amp;ret_format, &amp;ret_len, &amp;ret_togo, &amp;atomv) == Success\n      |                                                               ^~~~~~~~~\n      |                                                               |\n      |                                                               long int *\nIn file included from DisplayCAL/RealDisplaySizeMM.c:27:\n/usr/include/X11/Xlib.h:2697:5: note: expected \u2018long unsigned int *\u2019 but argument is of type \u2018long int *\u2019\n2697 |     unsigned long*      /* bytes_after_return */,\n      |     ^~~~~~~~~~~~~~\nDisplayCAL/RealDisplaySizeMM.c: In function \u2018enumerate_displays\u2019:\nDisplayCAL/RealDisplaySizeMM.c:1364:56: warning: pointer targets in passing argument 1 of \u2018PyBytes_FromStringAndSize\u2019 differ in signedness [-Wpointer-sign]\n1364 |               (value = PyString_FromStringAndSize(dp[i]-&gt;edid, dp[i]-&gt;edid_len)) != NULL) {\n      |                                                   ~~~~~^~~~~~\n      |                                                        |\n      |                                                        unsigned char *\nIn file included from /usr/include/python3.10/Python.h:82,\n                from DisplayCAL/RealDisplaySizeMM.c:1:\n/usr/include/python3.10/bytesobject.h:34:50: note: expected \u2018const char *\u2019 but argument is of type \u2018unsigned char *\u2019\n  34 | PyAPI_FUNC(PyObject *) PyBytes_FromStringAndSize(const char *, Py_ssize_t);\n      |                                                  ^~~~~~~~~~~~\nerror: can't copy '/tmp/build-via-sdist-b6k1fhs3/DisplayCAL-3.9.11/DisplayCAL/../misc/displaycal.desktop': doesn't exist or not a regular file\n\nERROR Backend subprocess exited when trying to invoke build_wheel\n</code></pre> <p>Switching to the development branch (<code>git checkout develop</code>) leads to the same build error, and other branches don't seem likely to fix this either.</p>"},{"location":"blog/2023/10/15/calibrating-screen-color-with-displaycal/#run-latest-release","title":"Run latest release","text":"<p>Instead of building from source, you can simply run the latest release:</p> <pre><code>$ wget https://github.com/eoyilmaz/displaycal-py3/releases/download/3.9.11/DisplayCAL-3.9.11.tar.gz\n$ tar xfz DisplayCAL-3.9.11.tar.gz\n$ cd DisplayCAL-3.9.11/\n$ ./DisplayCAL.pyw \n</code></pre> <p>However, this won't work without first installing all the above requirements, it will fail because <code>send2trash</code> is missing and <code>wxPython</code> is too old:</p> <pre><code>$ ./DisplayCAL.pyw \nAcquired lock file: &lt;DisplayCAL.main.AppLock object at 0x7fb42b873970&gt;\nDisplayCAL.pyw 3.9.11 2023-06-05T17:07:58Z\nubuntu 22.04 jammy x86_64\nPython 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\nFaulthandler \nwxPython 4.0.7 gtk3 (phoenix) wxWidgets 3.0.5\nEncoding: utf-8\nFile system encoding: utf-8\nLoading /home/artist/.config/DisplayCAL/DisplayCAL.ini\nlistening\nwriting to lock file: port: 15411\n/home/artist/Downloads/DisplayCAL-3.9.11/DisplayCAL/edid.py:45: Warning: No module named 'DisplayCAL.lib64.python310.RealDisplaySizeMM'\n  warnings.warn(str(exception), Warning)\nTraceback (most recent call last):\n  File \"/home/artist/Downloads/DisplayCAL-3.9.11/DisplayCAL/main.py\", line 549, in main\n    _main(module, name, app_lock_file_name)\n  File \"/home/artist/Downloads/DisplayCAL-3.9.11/DisplayCAL/main.py\", line 534, in _main\n    from DisplayCAL.display_cal import main\n  File \"/home/artist/Downloads/DisplayCAL-3.9.11/DisplayCAL/display_cal.py\", line 45, in &lt;module&gt;\n    from send2trash import send2trash\nModuleNotFoundError: No module named 'send2trash'\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Traceback (most recent call last):                                           \u2502\n\u2502   File \"/home/artist/Downloads/DisplayCAL-3.9.11/DisplayCAL/main.py\", line   \u2502\n\u2502 549, in main                                                                 \u2502\n\u2502     _main(module, name, app_lock_file_name)                                  \u2502\n\u2502   File \"/home/artist/Downloads/DisplayCAL-3.9.11/DisplayCAL/main.py\", line   \u2502\n\u2502 534, in _main                                                                \u2502\n\u2502     from DisplayCAL.display_cal import main                                  \u2502\n\u2502   File \"/home/artist/Downloads/DisplayCAL-3.9.11/DisplayCAL/display_cal.py\", \u2502\n\u2502 line 45, in &lt;module&gt;                                                         \u2502\n\u2502     from send2trash import send2trash                                        \u2502\n\u2502 ModuleNotFoundError: No module named 'send2trash'                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n(DisplayCAL.pyw:943597): Gtk-WARNING **: 12:39:12.107: Theme directory places/128 of theme ubuntustudio-dark has no size field\n\n\n(DisplayCAL.pyw:943597): Gtk-WARNING **: 12:39:12.107: Theme directory places/scalable of theme ubuntustudio-dark has no size field\n\nExiting DisplayCAL\nRan application exit handlers\n</code></pre> <p>To install the requirements, use only these two steps from those required to build from source, without using a virtual environment:</p> <pre><code>$ cd ../displaycal-py3/\n$ pip install -r requirements.txt\n  Running setup.py install for wxPython ... |\n</code></pre> <p>Building <code>wxPython</code> takes a while, it will keep the CPU busy and hot for several minutes (about 10 minutes on an AMD Ryzen 5 1600X).</p> <p>Once the required libraries are installed, the latest release runs without problems:</p> <pre><code>$ ./DisplayCAL.pyw \n</code></pre> <p>There may be several pop-up windows, which may not be visible if the Spyder5 instrument is on the center of the screen. Once those are dealt with, the calibration process can be started.</p>"},{"location":"blog/2023/10/15/calibrating-screen-color-with-displaycal/#loading-the-latest-profile","title":"Loading the latest profile","text":"<p>Redshift adjusts the color temperature of your screen according to your surroundings. This may help your eyes hurt less if you are working in front of the screen at night.</p> <p>While Redshift is active, color profiles are replaced with the darker, warmer tone. When Redshift is temporarily disabled, you need to re-load the color profile to use it.</p> <p>To do this, simply run <code>xcalib</code> with the latest ICC profile. The one-line <code>xcalib-latest</code> makes this very easy:</p> xcalib-latest<pre><code>#!/bin/sh\nxcalib \"$(ls -t $HOME/.local/share/icc/*.icc | head -1)\"\n</code></pre> <p>Moreover, the point of having this simple command in a simple Bash script is to run this script even more easily by clicking on a Desktop launcher:</p> xcalib.desktop<pre><code>[Desktop Entry]\nName=Restore Color\nComment=Restore color calibratio profile\nExec=/home/artist/bin/xcalib-latest\nIcon=/home/artist/Desktop/.icons/DisplayCAL.png\nType=Application\nTerminal=false\nStartupNotify=false\nX-KDE-SubstituteUID=false\n</code></pre> <p></p> <p>Note</p> <p>Redshift must be disabled (or stopped) before using this launcher; otherwise it will very quickly override the color profile.</p>"},{"location":"blog/2024/01/10/encrypting-external-ssd/","title":"Encrypting external SSD","text":"<p>There are least two major approaches to encrypt external SSD drivers; using LUKS seems to be most convenient when using the drive only on Linux, while using VeryCrypt is the one that keeps the disk readable on all major OSes.</p> <p>Why not both?</p>"},{"location":"blog/2024/01/10/encrypting-external-ssd/#starting-point-samsung-t7-shield-4tb-ssd","title":"Starting point: Samsung T7 Shield 4TB SSD","text":"<p>Brand new, this SSD has 2 partitions:</p> <pre><code># fdisk -l /dev/sdf\nDisk /dev/sdf: 3.64 TiB, 4000787030016 bytes, 7814037168 sectors\nDisk model: PSSD T7 Shield  \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 33553920 bytes\nDisklabel type: gpt\nDisk identifier: 5D51395C-647D-4B86-83C1-BE9E1633491C\n\nDevice     Start        End    Sectors  Size Type\n/dev/sdf1     34      32767      32734   16M Microsoft reserved\n/dev/sdf2  32768 7814035455 7814002688  3.6T Microsoft basic data\n\n# mount | grep sdf\n/dev/sdf2 on /media/coder/T7 Shield type exfat (rw,nosuid,nodev,relatime,uid=1000,gid=1000,fmask=0022,dmask=0022,iocharset=utf8,errors=remount-ro,uhelper=udisks2)\n\n# df -h | grep sdf\n/dev/sdf2       3.7T   35M  3.7T   1% /media/coder/T7 Shield\n\n# ls -l /media/coder/T7\\ Shield/\ntotal 31488\n-rwxr-xr-x 1 coder coder 23890677 Sep 29  2021  SamsungPortableSSD_Setup_Mac_1.0.pkg\n-rwxr-xr-x 1 coder coder  8106168 Sep 29  2021  SamsungPortableSSD_Setup_Win_1.0.exe\n-rwxr-xr-x 1 coder coder      118 Dec 27  2019 'Samsung Portable SSD SW for Android.txt'\n</code></pre> <p>None of the above was going to be useful when using the disk from Linux systems most of the time, with perhaps the chance of needing to read a few files form others.</p>"},{"location":"blog/2024/01/10/encrypting-external-ssd/#luks","title":"LUKS","text":"<p>Most of the disk will be encrypted with LUKS and formatted with <code>ext4</code> (no need for <code>btrfs</code>).</p> <p>There is no real need for a multi-OS setup, we'll just create a small 32 GB partition at the start of the disk just in case this ever becomes necessary or desirable:</p> <p>For the most part, we follow the Encrypt USB Data Using cryptsetup section of the LinuxHint article to Encrypt Data on USB from Linux.</p> <p>Install <code>cryptsetup</code> (if not already installed):</p> <pre><code># apt install cryptsetup -y\n</code></pre> <p>The disk comes with 2 partitions but the first one is too small, so we replace those partitions with new ones so that the first one is a generous 32 GB:</p> <pre><code># fdisk -l /dev/sdf\nDisk /dev/sdf: 3.64 TiB, 4000787030016 bytes, 7814037168 sectors\nDisk model: PSSD T7 Shield  \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 33553920 bytes\nDisklabel type: gpt\nDisk identifier: 5D51395C-647D-4B86-83C1-BE9E1633491C\n\nDevice        Start        End    Sectors  Size Type\n/dev/sdf1      2048   67110911   67108864   32G Microsoft basic data\n/dev/sdf2  67110912 7814037134 7746926223  3.6T Linux filesystem\n</code></pre> <p>Format the 32 GB partition as NTFS (not encrypted):</p> <pre><code># mkfs -t ntfs /dev/sdf1\nCluster size has been automatically set to 4096 bytes.\nInitializing device with zeroes: 100% - Done.\nCreating NTFS volume structures.\nmkntfs completed successfully. Have a nice day.\n</code></pre> <p>Encrypt the rest of the disk (will format later):</p> <pre><code># cryptsetup --verbose --verify-passphrase luksFormat /dev/sdf2\n\nWARNING!\n========\nThis will overwrite data on /dev/sdf2 irrevocably.\n\nAre you sure? (Type 'yes' in capital letters): YES\nEnter passphrase for /dev/sdf2: \nVerify passphrase: \nIgnoring bogus optimal-io size for data device (33553920 bytes).\nKey slot 0 created.\nCommand successful.\n</code></pre> <pre><code># cryptsetup luksOpen /dev/sdf2 luks\nEnter passphrase for /dev/sdf2: \n\n# ls -l /dev/mapper/luks \nlrwxrwxrwx 1 root root 7 Nov 27 23:00 /dev/mapper/luks -&gt; ../dm-0\n\n# mkfs.ext4 /dev/mapper/luks\nmke2fs 1.46.5 (30-Dec-2021)\nCreating filesystem with 968361681 4k blocks and 242098176 inodes\nFilesystem UUID: bf17a8ca-56e7-4521-a9ba-2d75895251fd\nSuperblock backups stored on blocks: \n        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, \n        4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, \n        102400000, 214990848, 512000000, 550731776, 644972544\n\nAllocating group tables: done                            \nWriting inode tables: done                            \nCreating journal (262144 blocks): done\nWriting superblocks and filesystem accounting information: done       \n</code></pre> <p>Open the encrypted disk, mount the file system and setup permissions for the desired user/s:</p> <pre><code># mkdir /mnt/encrypted\n# mount /dev/mapper/luks /mnt/encrypted\n# touch /mnt/encrypted/file1.txt\n# chown -R `whoami` /mnt/encrypted\n# ls -la /mnt/encrypted\ntotal 24\ndrwxr-xr-x 3 coder root  4096 Nov 27 23:03 .\ndrwxr-xr-x 3 root   root  4096 Nov 27 23:02 ..\n-rw-r--r-- 1 coder root     0 Nov 27 23:03 file1.txt\ndrwx------ 2 coder root 16384 Nov 27 23:01 lost+found\n</code></pre> <p>Once done, umount the filesystem and close the encrypted disk:</p> <pre><code># umount /dev/mapper/luks\n# cryptsetup luksClose luks\n</code></pre>"},{"location":"blog/2024/01/10/encrypting-external-ssd/#everyday-use","title":"Everyday use","text":"<p>After this setup, the disk is recognized as an encrypted volume when plugged in and KDE Plasma will prompt for the password (and remember it, if desired). The disk will be mounted under <code>/media/coder</code> but somehow the option to Open with File Manager is not available for it. Still, the same option can be used in any other disk, and the File Manager (Dolphin) does show the mounted filesystem as 3.6 TiB Internal Drive (dm-0) and is ready to use.</p> <p>Copying 140 GB of Audiobooks, reading from a Crucial MX SSD (SATA) that should be able to keep up, takes about 8 minutes.</p>"},{"location":"blog/2024/01/10/encrypting-external-ssd/#veracrypt","title":"VeraCrypt","text":"<p>To create an Mac/Windows-friendly encrypted volume, download VeraCrypt for Ubuntu 22.04 an install it, then setup up the small 32 GB partition following the Encrypt USB Data Using VeraCrypt section of the same LinuxHint article to Encrypt Data on USB from Linux.</p>"},{"location":"blog/2024/01/10/encrypting-external-ssd/#follow-up-samsung-t5-2tb-ssd","title":"Follow up: Samsung T5 2TB SSD","text":"<p>After the above setup has been working very well on the new 4TB SSD, it was time to apply the same to the old 2TB SSD. In this case, there will be only one partition, encrypted with LUKS:</p> <pre><code># fdisk /dev/sdf\n\nWelcome to fdisk (util-linux 2.37.2).\nChanges will remain in memory only, until you decide to write them.\nBe careful before using the write command.\n\n\nCommand (m for help): p\nDisk /dev/sdf: 1.82 TiB, 2000398934016 bytes, 3907029168 sectors\nDisk model: Portable SSD T5 \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 33553920 bytes\nDisklabel type: dos\nDisk identifier: 0x9dfc017f\n\nDevice     Boot Start        End    Sectors  Size Id Type\n/dev/sdf1        2048 3907026112 3907024065  1.8T  7 HPFS/NTFS/exFAT\n\nCommand (m for help): l\n\n00 Empty            24 NEC DOS          81 Minix / old Lin  bf Solaris        \n01 FAT12            27 Hidden NTFS Win  82 Linux swap / So  c1 DRDOS/sec (FAT-\n02 XENIX root       39 Plan 9           83 Linux            c4 DRDOS/sec (FAT-\n03 XENIX usr        3c PartitionMagic   84 OS/2 hidden or   c6 DRDOS/sec (FAT-\n04 FAT16 &lt;32M       40 Venix 80286      85 Linux extended   c7 Syrinx         \n05 Extended         41 PPC PReP Boot    86 NTFS volume set  da Non-FS data    \n06 FAT16            42 SFS              87 NTFS volume set  db CP/M / CTOS / .\n07 HPFS/NTFS/exFAT  4d QNX4.x           88 Linux plaintext  de Dell Utility   \n08 AIX              4e QNX4.x 2nd part  8e Linux LVM        df BootIt         \n09 AIX bootable     4f QNX4.x 3rd part  93 Amoeba           e1 DOS access     \n0a OS/2 Boot Manag  50 OnTrack DM       94 Amoeba BBT       e3 DOS R/O        \n0b W95 FAT32        51 OnTrack DM6 Aux  9f BSD/OS           e4 SpeedStor      \n0c W95 FAT32 (LBA)  52 CP/M             a0 IBM Thinkpad hi  ea Linux extended \n0e W95 FAT16 (LBA)  53 OnTrack DM6 Aux  a5 FreeBSD          eb BeOS fs        \n0f W95 Ext'd (LBA)  54 OnTrackDM6       a6 OpenBSD          ee GPT            \n10 OPUS             55 EZ-Drive         a7 NeXTSTEP         ef EFI (FAT-12/16/\n11 Hidden FAT12     56 Golden Bow       a8 Darwin UFS       f0 Linux/PA-RISC b\n12 Compaq diagnost  5c Priam Edisk      a9 NetBSD           f1 SpeedStor      \n14 Hidden FAT16 &lt;3  61 SpeedStor        ab Darwin boot      f4 SpeedStor      \n16 Hidden FAT16     63 GNU HURD or Sys  af HFS / HFS+       f2 DOS secondary  \n17 Hidden HPFS/NTF  64 Novell Netware   b7 BSDI fs          fb VMware VMFS    \n18 AST SmartSleep   65 Novell Netware   b8 BSDI swap        fc VMware VMKCORE \n1b Hidden W95 FAT3  70 DiskSecure Mult  bb Boot Wizard hid  fd Linux raid auto\n1c Hidden W95 FAT3  75 PC/IX            bc Acronis FAT32 L  fe LANstep        \n1e Hidden W95 FAT1  80 Old Minix        be Solaris boot     ff BBT            \n\nAliases:\n   linux          - 83\n   swap           - 82\n   extended       - 05\n   uefi           - EF\n   raid           - FD\n   lvm            - 8E\n   linuxex        - 85\n\nCommand (m for help): p\nDisk /dev/sdf: 1.82 TiB, 2000398934016 bytes, 3907029168 sectors\nDisk model: Portable SSD T5 \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 33553920 bytes\nDisklabel type: dos\nDisk identifier: 0x9dfc017f\n\nDevice     Boot Start        End    Sectors  Size Id Type\n/dev/sdf1        2048 3907026112 3907024065  1.8T  7 HPFS/NTFS/exFAT\n\nCommand (m for help): d\nSelected partition 1\nPartition 1 has been deleted.\n\nCommand (m for help): n\nPartition type\n   p   primary (0 primary, 0 extended, 4 free)\n   e   extended (container for logical partitions)\nSelect (default p): p\nPartition number (1-4, default 1): \nFirst sector (2048-3907029167, default 2048): \nLast sector, +/-sectors or +/-size{K,M,G,T,P} (2048-3907029167, default 3907029167): \n\nCreated a new partition 1 of type 'Linux' and of size 1.8 TiB.\nPartition #1 contains a exfat signature.\n\nDo you want to remove the signature? [Y]es/[N]o: Y\n\nThe signature will be removed by a write command.\n\nCommand (m for help): p\nDisk /dev/sdf: 1.82 TiB, 2000398934016 bytes, 3907029168 sectors\nDisk model: Portable SSD T5 \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 33553920 bytes\nDisklabel type: dos\nDisk identifier: 0x9dfc017f\n\nDevice     Boot Start        End    Sectors  Size Id Type\n/dev/sdf1        2048 3907029167 3907027120  1.8T 83 Linux\n\nFilesystem/RAID signature on partition 1 will be wiped.\n\nCommand (m for help): w\nThe partition table has been altered.\nCalling ioctl() to re-read partition table.\nSyncing disks.\n</code></pre> <p>Carefully run the above commands on the new disk, making sure to apply the commands to the correct device, e.g. <code>/dev/sdf1</code> (not that <code>/dev/sdf2</code> would exist).</p> <pre><code># cryptsetup --verbose --verify-passphrase luksFormat /dev/sdf1\n# mkdir /mnt/encrypted\n# mount /dev/mapper/luks /mnt/encrypted\n# touch /mnt/encrypted/file1.txt\n# chown -R `whoami` /mnt/encrypted\n# ls -la /mnt/encrypted\ntotal 24\ndrwxr-xr-x 3 coder root  4096 Nov 27 23:03 .\ndrwxr-xr-x 3 root   root  4096 Nov 27 23:02 ..\n-rw-r--r-- 1 coder root     0 Nov 27 23:03 file1.txt\ndrwx------ 2 coder root 16384 Nov 27 23:01 lost+found\n</code></pre> <p>Once done, umount the filesystem and close the encrypted disk:</p> <pre><code># umount /dev/mapper/luks\n# cryptsetup luksClose luks\nDevice luks is still in use.\n\n# umount /dev/mapper/luks\n# cryptsetup luksClose luks\nDevice luks is still in use.\n# ls -la /mnt/encrypted\ntotal 8\ndrwxr-xr-x 2 root root 4096 Nov 27 23:02 .\ndrwxr-xr-x 3 root root 4096 Nov 27 23:02 ..\n# umount /mnt/encrypted\numount: /mnt/encrypted: not mounted.\n# cryptsetup luksClose luks\nDevice luks is still in use.\n# cryptsetup luksClose luks\nDevice luks is still in use.\n</code></pre> <p>At this point something weird happened, the disk went into a state were it kept producing <code>I/O error</code> messages even though it was essentially empty and ummounted.</p> <pre><code>[47088.440687] EXT4-fs (dm-0): mounted filesystem with ordered data mode. Opts: errors=remount-ro. Quota mode: none.\n\n[47199.602516] usb 4-4: USB disconnect, device number 7\n[47199.616305] sd 10:0:0:0: [sdf] Synchronizing SCSI cache\n[47199.627339] blk_update_request: I/O error, dev sdf, sector 356612464 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0\n[47199.627348] blk_update_request: I/O error, dev sdf, sector 356608368 op 0x1:(WRITE) flags 0x4000 phys_seg 128 prio class 0\n[47199.627352] blk_update_request: I/O error, dev sdf, sector 356609392 op 0x1:(WRITE) flags 0x0 phys_seg 128 prio class 0\n[47199.627381] blk_update_request: I/O error, dev sdf, sector 356610416 op 0x1:(WRITE) flags 0x4000 phys_seg 128 prio class 0\n[47199.627384] blk_update_request: I/O error, dev sdf, sector 356611440 op 0x1:(WRITE) flags 0x0 phys_seg 128 prio class 0\n[47199.737269] sd 10:0:0:0: [sdf] Synchronize Cache(10) failed: Result: hostbyte=DID_ERROR driverbyte=DRIVER_OK\n[47202.969163] Aborting journal on device dm-0-8.\n[47202.969184] Buffer I/O error on dev dm-0, logical block 243826688, lost sync page write\n[47202.969194] JBD2: Error -5 detected when updating journal superblock for dm-0-8.\n</code></pre> <p>After waiting a little, unplugged the disk and plugged it again:</p> <pre><code>[47222.937094] Buffer I/O error on dev dm-0, logical block 20, lost async page write\n[47222.937105] Buffer I/O error on dev dm-0, logical block 21, lost async page write\n\n[47343.977469] EXT4-fs (dm-1): recovery complete\n[47343.984397] EXT4-fs (dm-1): mounted filesystem with ordered data mode. Opts: errors=remount-ro. Quota mode: none.\n</code></pre> <p>A different spur of errors showed up later, although this was not caused by unplugging the disk:</p> <pre><code>[58660.961828] EXT4-fs error (device dm-0): __ext4_find_entry:1682: inode #2: comm winedevice.exe: reading directory lblock 0\n[58660.961872] Buffer I/O error on dev dm-0, logical block 0, lost sync page write\n[58660.961882] EXT4-fs (dm-0): I/O error while writing superblock\n[58660.961885] EXT4-fs (dm-0): Remounting filesystem read-only\n[58660.961904] EXT4-fs error (device dm-0): __ext4_find_entry:1682: inode #2: comm winedevice.exe: reading directory lblock 0\n[58660.961924] Buffer I/O error on dev dm-0, logical block 0, lost sync page write\n[58660.961930] EXT4-fs (dm-0): I/O error while writing superblock\n[58666.912824] EXT4-fs error (device dm-0): __ext4_find_entry:1682: inode #2: comm winedevice.exe: reading directory lblock 0\n[58666.912861] Buffer I/O error on dev dm-0, logical block 0, lost sync page write\n[58666.912868] EXT4-fs (dm-0): I/O error while writing superblock\n[58666.912887] EXT4-fs error (device dm-0): __ext4_find_entry:1682: inode #2: comm winedevice.exe: reading directory lblock 0\n[58666.912900] Buffer I/O error on dev dm-0, logical block 0, lost sync page write\n[58666.912904] EXT4-fs (dm-0): I/O error while writing superblock\n</code></pre>"},{"location":"blog/2024/01/10/encrypting-external-ssd/#appendix","title":"Appendix","text":"<p>Additional sources about the above setup and others to encrypt portable external disks:</p> <ul> <li>Protect external storage with this Linux encryption system</li> <li>jwillikers.com/encrypt-an-external-disk-on-linux</li> <li>How to Encrypt an External Hard Drive on Linux</li> <li>Encrypt an External SSD Hard Drive</li> <li>Encrypt Data on USB from Linux</li> <li>Encrypt an external hard drive with read+write access on both Windows and Linux</li> <li>Encryption of external drive (Linux and Windows)</li> <li>Encrypt external hard drives with Linux</li> <li>How to secure data on your external hard drives and USB peripherals</li> <li>How to encrypt a USB Drive on Linux Operating System?</li> </ul>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/","title":"Audiobookshelf on Kubernetes","text":"<p>Migrating a Plex Media Server to Kubernetes, was a significant improvement for the maintenance of the Plex Media Server I use to listen to podcasts and audiobooks, to keep me company while I play games, but after all these years Plex remains a very insufficient and deficient application for audiobooks.</p> <p>Enter audiobookshelf (because Emby and Jellyfin are also not great)</p> <p></p>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#installation-on-kubernetes","title":"Installation on Kubernetes","text":"<p>Audiobookshelf configuration requires writeable directories mounted at <code>/config</code> and <code>/metadata</code> for database, cache, etc.</p> <pre><code># useradd -d /home/k8s/audiobookshelf -s /usr/sbin/nologin audiobookshelf\n# mkdir /home/k8s/audiobookshelf/config /home/k8s/audiobookshelf/metadata\n# chown -R audiobookshelf.audiobookshelf /home/k8s/audiobookshelf\n# ls -dln /home/k8s/audiobookshelf/\ndrwxr-xr-x 1 1006 1006 28 Feb 27 22:47 /home/k8s/audiobookshelf/\n</code></pre> <p>Note the UID/GID (1006) to be used in the Kubernetes deployment <code>securityContext</code> later.</p> <p>Docker Compose suggests mounting audiobooks and podcasts as separate volumes, so the following Kubernetes deployment will do so, even though it would also work to have it all under a single volume.</p> <p>Create the following <code>audiobookshelf.yaml</code> and deploy it:</p> <pre><code>$ kubectl apply -f audiobookshelf.yaml\nnamespace/audiobookshelf created\npersistentvolume/audiobookshelf-pv-config created\npersistentvolume/audiobookshelf-pv-metadata created\npersistentvolume/audiobookshelf-pv-audiobooks created\npersistentvolume/audiobookshelf-pv-podcasts created\npersistentvolumeclaim/audiobookshelf-pvc-config created\npersistentvolumeclaim/audiobookshelf-pvc-metadata created\npersistentvolumeclaim/audiobookshelf-pvc-audiobooks created\npersistentvolumeclaim/audiobookshelf-pvc-podcasts created\ndeployment.apps/audiobookshelf created\nservice/audiobookshelf created\ningress.networking.k8s.io/audiobookshelf-ingress created\n\n$ kubectl -n audiobookshelf get service\nNAME             TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE\naudiobookshelf   NodePort   10.102.115.191   &lt;none&gt;        13378:31378/TCP   20s\ncm-acme-http-solver-jh67q   NodePort   10.106.142.67   &lt;none&gt;        8089:30126/TCP    2m39s\n\n$ kubectl -n audiobookshelf describe ingress audiobookshelf-ingress\nName:             audiobookshelf-ingress\nLabels:           &lt;none&gt;\nNamespace:        audiobookshelf\nAddress:          \nIngress Class:    nginx\nDefault backend:  &lt;default&gt;\nTLS:\n  tls-secret terminates aus.ssl.uu.am\nRules:\n  Host                Path  Backends\n  ----                ----  --------\n  aus.ssl.uu.am  \n                      /   audiobookshelf:31378 (10.244.0.202:13378)\nAnnotations:          acme.cert-manager.io/http01-edit-in-place: true\n                      cert-manager.io/cluster-issuer: letsencrypt-prod\n                      cert-manager.io/issue-temporary-certificate: true\nEvents:\n  Type    Reason             Age   From                       Message\n  ----    ------             ----  ----                       -------\n  Normal  Sync               26s   nginx-ingress-controller   Scheduled for sync\n  Normal  CreateCertificate  26s   cert-manager-ingress-shim  Successfully created Certificate \"tls-secret\"\n</code></pre> <p>The server is now available at http://192.168.0.6:31378</p> <p>NGinx should also make it available at https://aus.ssl.uu.am</p>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#deployment","title":"Deployment","text":"Kubernetes deployment: <code>audiobookshelf.yaml</code> audiobookshelf.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: audiobookshelf\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-config\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/audiobookshelf/config\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-metadata\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/audiobookshelf/metadata\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-audiobooks\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/depot/audio/Audiobooks\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-podcasts\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/depot/audio/Podcasts\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-config\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-config\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-metadata\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-metadata\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-audiobooks\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-audiobooks\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-podcasts\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-podcasts\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: audiobookshelf\n  name: audiobookshelf\n  namespace: audiobookshelf\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: audiobookshelf\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: audiobookshelf\n    spec:\n      containers:\n        - image: ghcr.io/advplyr/audiobookshelf:latest\n          imagePullPolicy: Always\n          name: audiobookshelf\n          env:\n          - name: PORT\n            value: \"13378\"\n          ports:\n          - containerPort: 13378\n          resources: {}\n          stdin: true\n          tty: true\n          volumeMounts:\n          - mountPath: /config\n            name: audiobookshelf-config\n          - mountPath: /metadata\n            name: audiobookshelf-metadata\n          - mountPath: /audiobooks\n            name: audiobookshelf-audiobooks\n          - mountPath: /podcasts\n            name: audiobookshelf-podcasts\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 1006\n            runAsGroup: 1006\n      restartPolicy: Always\n      volumes:\n      - name: audiobookshelf-config\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-config\n      - name: audiobookshelf-metadata\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-metadata\n      - name: audiobookshelf-audiobooks\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-audiobooks\n      - name: audiobookshelf-podcasts\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-podcasts\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: audiobookshelf-svc\n  namespace: audiobookshelf\nspec:\n  type: NodePort\n  ports:\n  - port: 13388\n    nodePort: 31378\n    targetPort: 13378\n  selector:\n    app: audiobookshelf\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: audiobookshelf-ingress\n  namespace: audiobookshelf\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: audiobookshelf-svc\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: abs.ssl.uu.am\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: audiobookshelf-svc\n                port:\n                  number: 13378\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - abs.ssl.uu.am\n</code></pre> <p>The above deployment is based on audiobookshelf Docker documentation and Audiobookshelf Helm Chart By TrueCharts.</p>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#troubleshooting","title":"Troubleshooting","text":"<p>Contrary to what Configuration would suggest, the server will by default try to listen on port 80. To override this the <code>env</code> variable <code>PORT</code> must be set:</p> audiobookshelf.yaml<pre><code>          env:\n          - name: PORT\n            value: \"13378\"\n</code></pre> <p>Otherwise, when running as a non-privileged user, this will cause it to crash-loop:</p> <pre><code>$ kubectl get all -n audiobookshelf\nNAME                                  READY   STATUS             RESTARTS      AGE\npod/audiobookshelf-754d55cd68-z4br6   0/1     CrashLoopBackOff   4 (59s ago)   3m9s\n\nNAME                         TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)           AGE\nservice/audiobookshelf-tcp   NodePort   10.97.42.11   &lt;none&gt;        13378:32164/TCP   10m\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/audiobookshelf   0/1     1            0           3m9s\n\nNAME                                        DESIRED   CURRENT   READY   AGE\nreplicaset.apps/audiobookshelf-754d55cd68   1         1         0       3m9s\n\n$ kubectl -n audiobookshelf describe pod audiobookshelf-754d55cd68-z4br6\nEvents:\n  Type     Reason     Age                    From               Message\n  ----     ------     ----                   ----               -------\n  Normal   Scheduled  5m56s                  default-scheduler  Successfully assigned audiobookshelf/audiobookshelf-754d55cd68-z4br6 to lexicon\n  Normal   Pulled     5m29s                  kubelet            Successfully pulled image \"ghcr.io/advplyr/audiobookshelf:latest\" in 25.769s (25.769s including waiting)\n  Normal   Pulled     5m26s                  kubelet            Successfully pulled image \"ghcr.io/advplyr/audiobookshelf:latest\" in 661ms (661ms including waiting)\n  Normal   Pulled     5m9s                   kubelet            Successfully pulled image \"ghcr.io/advplyr/audiobookshelf:latest\" in 659ms (659ms including waiting)\n  Normal   Created    4m39s (x4 over 5m29s)  kubelet            Created container audiobookshelf\n  Normal   Started    4m39s (x4 over 5m29s)  kubelet            Started container audiobookshelf\n  Normal   Pulled     4m39s                  kubelet            Successfully pulled image \"ghcr.io/advplyr/audiobookshelf:latest\" in 752ms (752ms including waiting)\n  Normal   Pulling    3m48s (x5 over 5m55s)  kubelet            Pulling image \"ghcr.io/advplyr/audiobookshelf:latest\"\n  Normal   Pulled     3m47s                  kubelet            Successfully pulled image \"ghcr.io/advplyr/audiobookshelf:latest\" in 634ms (634ms including waiting)\n  Warning  BackOff    47s (x22 over 5m25s)   kubelet            Back-off restarting failed container audiobookshelf in pod audiobookshelf-754d55cd68-z4br6_audiobookshelf(147c22bc-5f0b-4ed8-a881-478748234776)\n\n$ kubectl -n audiobookshelf logs audiobookshelf-754d55cd68-z4br6\nConfig /config /metadata\n[2024-02-27 22:29:14.925] INFO: === Starting Server ===\n[2024-02-27 22:29:14.939] INFO: [Server] Init v2.8.0\n[2024-02-27 22:29:14.967] INFO: [Database] Initializing db at \"/config/absdatabase.sqlite\"\n[2024-02-27 22:29:14.998] INFO: [Database] Db connection was successful\n[2024-02-27 22:29:15.065] INFO: [Database] Db initialized with models: user, library, libraryFolder, book, podcast, podcastEpisode, libraryItem, mediaProgress, series, bookSeries, author, bookAuthor, collection, collectionBook, playlist, playlistMediaItem, device, playbackSession, feed, feedEpisode, setting, customMetadataProvider\n[2024-02-27 22:29:15.080] INFO: [LogManager] Init current daily log filename: 2024-02-27.txt\n[2024-02-27 22:29:15.084] INFO: [BackupManager] 0 Backups Found\n[2024-02-27 22:29:15.085] INFO: [BackupManager] Auto Backups are disabled\nWarning: connect.session() MemoryStore is not\ndesigned for a production environment, as it will leak\nmemory, and will not scale past a single process.\n[2024-02-27 22:29:15.095] FATAL: [Server] Uncaught exception origin: uncaughtException, error: Error: listen EACCES: permission denied 0.0.0.0:80\n    at Server.setupListenHandle [as _listen2] (node:net:1855:21)\n    at listenInCluster (node:net:1920:12)\n    at Server.listen (node:net:2008:7)\n    at Server.start (/server/Server.js:319:17) {\n  code: 'EACCES',\n  errno: -13,\n  syscall: 'listen',\n  address: '0.0.0.0',\n  port: 80\n} (Server.js:160)\nnode:events:496\n      throw er; // Unhandled 'error' event\n      ^\n\nError: listen EACCES: permission denied 0.0.0.0:80\n    at Server.setupListenHandle [as _listen2] (node:net:1855:21)\n    at listenInCluster (node:net:1920:12)\n    at Server.listen (node:net:2008:7)\n    at Server.start (/server/Server.js:319:17)\nEmitted 'error' event on Server instance at:\n    at emitErrorNT (node:net:1899:8)\n    at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {\n  code: 'EACCES',\n  errno: -13,\n  syscall: 'listen',\n  address: '0.0.0.0',\n  port: 80\n}\n\nNode.js v20.11.1\n</code></pre>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#configuration","title":"Configuration","text":"<p>Once the service is running, accessing the web interface allows creating the first (<code>root</code>) user. Additional users can (and should) be created, so they can each have their own progress saved throughout books and podcatss. Users can also have access to different sets of libraries, although by default they will have access to all libraries.</p>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#mobile-app","title":"Mobile app","text":"<p>To complete the self-hosted audiobook service, there is also the mobile app advplyr/audiobookshelf-app.</p> <p></p> <p>Audiobookshelf Android app can easily be installed from Google Play directly. The iOS app cannot simply be installed, because Apple has a hard limit of 10k beta testers. Alternatively, plappa can be used to stream audiobooks and podcasts from  AudioBookShelf, Emby and Jellyfin. Downloading content and listening offline requires an in-app purchase.</p>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#better-features","title":"Better Features","text":"<p>There is a lot in Audiobookshelf that is better, or much better, than in Plex:</p> <ul> <li>Saving the progress through books and podcats, per user.</li> <li>Podcats with a separate feed for pre/after show are somehow    corrected merged together.</li> <li>REST API that may allow    implementing some of the missing features below.</li> </ul>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#matching-metadata","title":"Matching Metadata","text":"<p>One of my favorite features of audiobookshelf is the ability to fetch book metadata and thus more clearly tell different versions of a book a part, e.g. abridged vs unabridged:</p> <p></p> <p>After selecting one matching book, metadata can be edited or even left out. This is very useful when an exact match cannot be found, but a close enough match can be used to fetch the matching parts of its metadata while at the same time manually editing or adding as needed. After importing metadata, there are further options to add or edit book metadata in the Details tab:</p> <p></p> <p>I find it particularly useful to add or edit:</p> <ul> <li>Explict, because this is seem to never be provided by     matching metadata and in any case it can be a matter of opinion.</li> <li>Language, if not correctly matched (usually it is).</li> <li>Series. A book can have a different places in different     series or subseries, and you can create your own series too.</li> </ul> <p>The Series field is interesting to then navigate books by series:</p> <p></p>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#pdf-reader","title":"PDF Reader","text":"<p>When there is a PDF file along with audio files, this can be read from the browser by clicking the corresponding icon e.g. this is (half) a page of Nick Offerman's Good Clean Fun:</p> <p></p>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#podcast-publication-dates","title":"Podcast Publication Dates","text":"<p>At least when using a separate tool for downloading podcast episodes, they have have only a year of publication, from the <code>TYER</code> ID3 frame, so they show up with their release date being as YYYY-01-01 and they cannot be properly sorted by release date. This could be fixed by allowing Audiobookshelf to download RSS feeds directly (have not tried), but also by updating the separate tool to set the  <code>TDAT</code> and <code>TIME</code> frames. Once these are set in the MP3 files, audiobookshell will correctly parse and display them:</p> <p></p>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#podcast-progress","title":"Podcast Progress","text":"<p>Just like with Audiobooks, keep track of which podcasts episodes each user has listed to is a central feature. One little detail I really like about how this is done, is the yellow or black circle on the podcast cover indicating whether all episodes have been listed to already (black circle) or otherwise how many are yet to be listened to (yellow circle):</p> <p></p>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#missing-features","title":"Missing Features","text":"<p>After a few days listening to books and podcasts, a few features turn out to be missing or not working as expected. Most of these are already in the issue tracker:</p> <p>audiobookshelf</p> <ul> <li>Enhancement: Display podcast episode images #1573</li> <li>Enhancement: Ability to edit Series similar to tags #1604</li> <li>UI: Sort a Series by publication year if no sort sequence number present #1674</li> <li>Enhancement Set custom order for podcast library #1792</li> <li>Enhancement: Enable Custom datetime in Podcast episode File Name #1869</li> <li>Enhancement: More Flexible Library Structure #2208</li> <li>Bug: PDF Reader is flickering #2279</li> <li>Enhancement: Same book, different narrators #2396</li> <li>Enhancement: Display Chapter Art while playing. #2660</li> </ul> <p>audiobookshelf-app</p> <ul> <li>Provide ability to queue audiobooks and podcast episodes #416</li> <li>Ability to mark several Podcast episodes by first marking and then dragging down with two fingers in IOS #685</li> </ul>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#batch-updates","title":"Batch Updates","text":"<p>Not only is progress saved automatically while listening, podcasts can also be updated to mark episodes are finished. What is missing here is the ability to select multiple episodes easily and quickly, for instance with <code>Shift+click</code> the last episode of a range when the first one is already selected. It would help to even just be able to mark all episodes as finished, even if only to then mark a few as unfinished.</p>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#collections","title":"Collections","text":"<p>The option to remove books to collections seems to be missing and there doesn't seem to be any way to remove collections at all.</p>"},{"location":"blog/2024/02/28/audiobookshelf-on-kubernetes/#playlists","title":"Playlists","text":"<p>Playlists have the same problematic limitations as Collections: no way option to delete them, or remove episodes from them.</p> <p>This is the one and only thing I still use Plex for. I like to listen to podcasts in by their publication date, especially because I listen to several podcasts that cross-reference and even invite on each other. I have not found a good way to do this with Audiobookshelf, given the no-remove-option limitations, so I'm still using Plex.</p> <p></p>"},{"location":"blog/2024/03/05/updating-kubernetes-to-the-community-owned-package-repositories/","title":"Updating Kubernetes to the Community-Owned Package Repositories","text":""},{"location":"blog/2024/03/05/updating-kubernetes-to-the-community-owned-package-repositories/#an-unexpected-update","title":"An Unexpected Update","text":"<p>Most days I update my little server when I log into my PC, and today it gave quite an unexpected surprise:</p> <pre><code># apt update &amp;&amp; apt full-upgrade -y\n...\nE: The repository 'https://apt.kubernetes.io kubernetes-xenial Release' no longer has a Release file.\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\nN: See apt-secure(8) manpage for repository creation and user configuration details.\n</code></pre> <p>This was promptly reported already yesterday as Ubuntu kubernetes-xenial package repository issue #123673 and quick triaged pointing to the announcement from August 2023: pkgs.k8s.io: Introducing Kubernetes Community-Owned Package Repositories.</p> <p>Note that there is now a separate repository for each minor version, so we need to first check which version is currently running:</p> <pre><code>$ kubeadm version\nkubeadm version: &amp;version.Info{Major:\"1\", Minor:\"28\", GitVersion:\"v1.28.2\", GitCommit:\"89a4ea3e1e4ddd7f7572286090359983e0387b2f\", GitTreeState:\"clean\", BuildDate:\"2023-09-13T09:34:32Z\", GoVersion:\"go1.20.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n</code></pre> <p>That we migrate to the 1.28 repository:</p> <pre><code># echo \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /\" \\\n  | tee /etc/apt/sources.list.d/kubernetes.list\n# curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key \\\n  | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n# apt update &amp;&amp; apt full-upgrade -y\n</code></pre> <p>This replaced the old repository in <code>/etc/apt/sources.list.d/kubernetes.list</code> that was installed in March, when creating a Single-node Kubernetes cluster on Ubuntu Server (lexicon):</p> <pre><code>deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\n</code></pre>"},{"location":"blog/2024/03/05/updating-kubernetes-to-the-community-owned-package-repositories/#an-unexpected-breakage","title":"An Unexpected Breakage","text":"<p>Turns out updating to 1.28 broke everything: all services are unreachable. Can't read logs of pods either.</p> <pre><code>$ kubectl -n plexserver logs -f plexserver-85f7bf866-7c8gv\nError from server: Get \"https://10.0.0.6:10250/containerLogs/plexserver/plexserver-85f7bf866-7c8gv/plexserver?follow=true\": dial tcp 10.0.0.6:10250: connect: connection refused\n$ netstat -na | grep 10250\n(nothing)\n</code></pre> <p>Looking back at the process followed when creating the Single-node Kubernetes cluster on Ubuntu Server (lexicon) it was actually 1.26 that was installed, and it seems the update above put the <code>clientVersion</code> up to 1.28 but not the <code>serverVersion</code>:</p> <pre><code>$ kubectl version --output=yaml\nclientVersion:\n  buildDate: \"2024-02-14T10:40:48Z\"\n  compiler: gc\n  gitCommit: c8dcb00be9961ec36d141d2e4103f85f92bcf291\n  gitTreeState: clean\n  gitVersion: v1.28.7\n  goVersion: go1.21.7\n  major: \"1\"\n  minor: \"28\"\n  platform: linux/amd64\nkustomizeVersion: v5.0.4-0.20230601165947-6ce0bf390ce3\nserverVersion:\n  buildDate: \"2023-03-15T13:33:12Z\"\n  compiler: gc\n  gitCommit: 9e644106593f3f4aa98f8a84b23db5fa378900bd\n  gitTreeState: clean\n  gitVersion: v1.26.3\n  goVersion: go1.19.7\n  major: \"1\"\n  minor: \"26\"\n  platform: linux/amd64\n\nWARNING: version difference between client (1.28) and server (1.26) exceeds the supported minor version skew of +/-1\n\n$ kubeadm version\nkubeadm version: &amp;version.Info{Major:\"1\", Minor:\"28\", GitVersion:\"v1.28.7\", GitCommit:\"c8dcb00be9961ec36d141d2e4103f85f92bcf291\", GitTreeState:\"clean\", BuildDate:\"2024-02-14T10:39:01Z\", GoVersion:\"go1.21.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n</code></pre> <p>Downgraded to 1.26 but that was not enough.</p> <pre><code># echo \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.26/deb/ /\" \\\n  | tee /etc/apt/sources.list.d/kubernetes.list\n# curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.26/deb/Release.key \\\n  | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n# apt update\n# apt-get install --reinstall \\\n  --allow-downgrades \\\n  cri-tools=1.26.0-1.1 \\\n  kubelet=1.26.3-1.1 \\\n  kubectl=1.26.3-1.1 \\\n  kubeadm=1.26.3-1.1\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be DOWNGRADED:\n  cri-tools kubeadm kubectl kubelet\n0 upgraded, 0 newly installed, 4 downgraded, 0 to remove and 0 not upgraded.\nNeed to get 59.3 MB of archives.\nAfter this operation, 6,612 kB of additional disk space will be used.\nDo you want to continue? [Y/n]\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.26/deb  kubelet 1.26.3-1.1 [20.5 MB]\nGet:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.26/deb  kubectl 1.26.3-1.1 [10.1 MB]\nGet:3 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.26/deb  kubeadm 1.26.3-1.1 [9,750 kB]\nGet:4 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.26/deb  cri-tools 1.26.0-1.1 [19.0 MB]\nFetched 59.3 MB in 2s (34.9 MB/s)   \n</code></pre> <p>After this both versions are 1.26:</p> <pre><code>$ kubectl version --output=yaml\nclientVersion:\n  buildDate: \"2023-03-15T13:40:17Z\"\n  compiler: gc\n  gitCommit: 9e644106593f3f4aa98f8a84b23db5fa378900bd\n  gitTreeState: clean\n  gitVersion: v1.26.3\n  goVersion: go1.19.7\n  major: \"1\"\n  minor: \"26\"\n  platform: linux/amd64\nkustomizeVersion: v4.5.7\nserverVersion:\n  buildDate: \"2023-03-15T13:33:12Z\"\n  compiler: gc\n  gitCommit: 9e644106593f3f4aa98f8a84b23db5fa378900bd\n  gitTreeState: clean\n  gitVersion: v1.26.3\n  goVersion: go1.19.7\n  major: \"1\"\n  minor: \"26\"\n  platform: linux/amd64\n\n$ kubeadm version\nkubeadm version: &amp;version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:38:47Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n</code></pre> <p>Yet services remain unavailable and <code>kubelet</code> doesn't seem happy:</p> <pre><code># systemctl status kubelet\n\u25cb kubelet.service - kubelet: The Kubernetes Node Agent\n     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)\n    Drop-In: /etc/systemd/system/kubelet.service.d\n             \u2514\u250010-kubeadm.conf\n     Active: inactive (dead) since Tue 2024-03-05 22:34:03 CET; 22min ago\n       Docs: https://kubernetes.io/docs/\n   Main PID: 5986 (code=exited, status=0/SUCCESS)\n        CPU: 5h 37min 33.871s\n\nMar 05 22:30:23 lexicon kubelet[5986]: I0305 22:30:23.982864    5986 pod_container_deletor.go:53] \"DeleteContainer returned error\" containerID={\"Type\":\"containerd\",\"ID\":\"4f70a3adfdba7962f514d636b31ca983549d95d7f9acfd81931296eddba50ff7\"} err=\"failed to get container status \\\"4f70a3ad&gt;\nMar 05 22:30:23 lexicon kubelet[5986]: I0305 22:30:23.982877    5986 scope.go:117] \"RemoveContainer\" containerID=\"3449079cc4940249b6798267ee61f4a39639c53a4701cb04b19b084546fb450e\"\nMar 05 22:30:23 lexicon kubelet[5986]: E0305 22:30:23.983075    5986 remote_runtime.go:432] \"ContainerStatus from runtime service failed\" err=\"rpc error: code = NotFound desc = an error occurred when try to find container \\\"3449079cc4940249b6798267ee61f4a39639c53a4701cb04b19b084546f&gt;\nMar 05 22:30:23 lexicon kubelet[5986]: I0305 22:30:23.983095    5986 pod_container_deletor.go:53] \"DeleteContainer returned error\" containerID={\"Type\":\"containerd\",\"ID\":\"3449079cc4940249b6798267ee61f4a39639c53a4701cb04b19b084546fb450e\"} err=\"failed to get container status \\\"3449079c&gt;\nMar 05 22:30:25 lexicon kubelet[5986]: I0305 22:30:25.669377    5986 kubelet_volumes.go:161] \"Cleaned up orphaned pod volumes dir\" podUID=\"681ab091-7025-4590-918c-106aa17b63cb\" path=\"/var/lib/kubelet/pods/681ab091-7025-4590-918c-106aa17b63cb/volumes\"\nMar 05 22:34:03 lexicon systemd[1]: Stopping kubelet: The Kubernetes Node Agent...\nMar 05 22:34:03 lexicon kubelet[5986]: I0305 22:34:03.171189    5986 dynamic_cafile_content.go:171] \"Shutting down controller\" name=\"client-ca-bundle::/etc/kubernetes/pki/ca.crt\"\nMar 05 22:34:03 lexicon systemd[1]: kubelet.service: Deactivated successfully.\nMar 05 22:34:03 lexicon systemd[1]: Stopped kubelet: The Kubernetes Node Agent.\nMar 05 22:34:03 lexicon systemd[1]: kubelet.service: Consumed 5h 37min 33.871s CPU time.\n</code></pre> <p>Going back again to the Single-node Kubernetes cluster on Ubuntu Server (lexicon) and checking the current output from the first few <code>kubectl</code>,  turns out version 1.28 is still running:</p> <pre><code>$ kubectl get nodes -o wide\nNAME      STATUS     ROLES           AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nlexicon   NotReady   control-plane   349d   v1.28.2   10.0.0.6      &lt;none&gt;        Ubuntu 22.04.4 LTS   5.15.0-97-generic   containerd://1.6.28\n</code></pre> <p>Deployment are all not ready:</p> <pre><code>$ kubectl get deployments -A\nNAMESPACE                NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE\natuin-server             atuin                                   0/1     1            0           184d\naudiobookshelf           audiobookshelf                          0/1     1            0           4d12h\ncert-manager             cert-manager                            0/1     1            0           324d\ncert-manager             cert-manager-cainjector                 0/1     1            0           324d\ncert-manager             cert-manager-webhook                    0/1     1            0           324d\ncode-server              code-server                             0/1     1            0           321d\ndefault                  inteldeviceplugins-controller-manager   0/1     1            0           172d\ningress-nginx            ingress-nginx-controller                0/1     1            0           338d\nkube-system              coredns                                 0/2     2            0           349d\nkubernetes-dashboard     dashboard-metrics-scraper               0/1     1            0           338d\nkubernetes-dashboard     kubernetes-dashboard                    0/1     1            0           338d\nlocal-path-storage       local-path-provisioner                  0/1     1            0           324d\nmetallb-system           controller                              0/1     1            0           338d\nmetrics-server           metrics-server                          0/1     1            0           338d\nnode-feature-discovery   nfd-node-feature-discovery-master       0/1     1            0           172d\nplexserver               plexserver                              0/1     1            0           156d\ntelegraf                 telegraf                                0/1     1            0           320d\n</code></pre> <p>At this point it seems necessary to manually restart <code>kubelet</code>:</p> <pre><code># systemctl restart kubelet\n# systemctl status kubelet\n\u25cf kubelet.service - kubelet: The Kubernetes Node Agent\n     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)\n    Drop-In: /etc/systemd/system/kubelet.service.d\n             \u2514\u250010-kubeadm.conf\n     Active: active (running) since Tue 2024-03-05 23:13:17 CET; 5s ago\n       Docs: https://kubernetes.io/docs/\n   Main PID: 3869533 (kubelet)\n      Tasks: 13 (limit: 37926)\n     Memory: 122.6M\n        CPU: 701ms\n     CGroup: /system.slice/kubelet.service\n             \u2514\u25003869533 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:/run/containerd/containerd.sock --pod-infra-container-image=registr&gt;\n\nMar 05 23:13:21 lexicon kubelet[3869533]: I0305 23:13:21.200985 3869533 kubelet_volumes.go:160] \"Cleaned up orphaned pod volumes dir\" podUID=7f8e47b71a845b4eb634a44a94e20069 path=\"/var/lib/kubelet/pods/7f8e47b71a845b4eb634a44a94e20069/volumes\"\nMar 05 23:13:21 lexicon kubelet[3869533]: I0305 23:13:21.201640 3869533 kubelet_volumes.go:160] \"Cleaned up orphaned pod volumes dir\" podUID=b087540859a3a8090a0d1bf55891aa52 path=\"/var/lib/kubelet/pods/b087540859a3a8090a0d1bf55891aa52/volumes\"\nMar 05 23:13:21 lexicon kubelet[3869533]: W0305 23:13:21.333208 3869533 reflector.go:424] object-\"kubernetes-dashboard\"/\"kube-root-ca.crt\": failed to list *v1.ConfigMap: Get \"https://10.0.0.6:6443/api/v1/namespaces/kubernetes-dashboard/configmaps?fieldSelector=metadata.name%3Dkube-r&gt;\nMar 05 23:13:21 lexicon kubelet[3869533]: E0305 23:13:21.333302 3869533 reflector.go:140] object-\"kubernetes-dashboard\"/\"kube-root-ca.crt\": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://10.0.0.6:6443/api/v1/namespaces/kubernetes-dashboard/configmaps?field&gt;\nMar 05 23:13:21 lexicon kubelet[3869533]: W0305 23:13:21.533653 3869533 reflector.go:424] object-\"default\"/\"webhook-server-cert\": failed to list *v1.Secret: Get \"https://10.0.0.6:6443/api/v1/namespaces/default/secrets?fieldSelector=metadata.name%3Dwebhook-server-cert&amp;limit=500&amp;resou&gt;\nMar 05 23:13:21 lexicon kubelet[3869533]: E0305 23:13:21.533750 3869533 reflector.go:140] object-\"default\"/\"webhook-server-cert\": Failed to watch *v1.Secret: failed to list *v1.Secret: Get \"https://10.0.0.6:6443/api/v1/namespaces/default/secrets?fieldSelector=metadata.name%3Dwebhook&gt;\nMar 05 23:13:21 lexicon kubelet[3869533]: W0305 23:13:21.732794 3869533 reflector.go:424] object-\"default\"/\"kube-root-ca.crt\": failed to list *v1.ConfigMap: Get \"https://10.0.0.6:6443/api/v1/namespaces/default/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&amp;limit=500&amp;resou&gt;\nMar 05 23:13:21 lexicon kubelet[3869533]: E0305 23:13:21.732877 3869533 reflector.go:140] object-\"default\"/\"kube-root-ca.crt\": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://10.0.0.6:6443/api/v1/namespaces/default/configmaps?fieldSelector=metadata.name%3Dk&gt;\nMar 05 23:13:21 lexicon kubelet[3869533]: W0305 23:13:21.933227 3869533 reflector.go:424] object-\"metrics-server\"/\"kube-root-ca.crt\": failed to list *v1.ConfigMap: Get \"https://10.0.0.6:6443/api/v1/namespaces/metrics-server/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&amp;l&gt;\nMar 05 23:13:21 lexicon kubelet[3869533]: E0305 23:13:21.933273 3869533 reflector.go:140] object-\"metrics-server\"/\"kube-root-ca.crt\": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://10.0.0.6:6443/api/v1/namespaces/metrics-server/configmaps?fieldSelector=met&gt;\n</code></pre> <p>Those logs entries look bad and yet everything seems to work now (and deployment are all ready):</p> <pre><code>$ kubectl get deployment -A \nNAMESPACE                NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE\natuin-server             atuin                                   1/1     1            1           184d\naudiobookshelf           audiobookshelf                          1/1     1            1           4d13h\ncert-manager             cert-manager                            1/1     1            1           324d\ncert-manager             cert-manager-cainjector                 1/1     1            1           324d\ncert-manager             cert-manager-webhook                    1/1     1            1           324d\ncode-server              code-server                             1/1     1            1           321d\ndefault                  inteldeviceplugins-controller-manager   1/1     1            1           172d\ningress-nginx            ingress-nginx-controller                1/1     1            1           338d\nkube-system              coredns                                 2/2     2            2           349d\nkubernetes-dashboard     dashboard-metrics-scraper               1/1     1            1           338d\nkubernetes-dashboard     kubernetes-dashboard                    1/1     1            1           338d\nlocal-path-storage       local-path-provisioner                  1/1     1            1           324d\nmetallb-system           controller                              1/1     1            1           338d\nmetrics-server           metrics-server                          1/1     1            1           338d\nnode-feature-discovery   nfd-node-feature-discovery-master       1/1     1            1           172d\nplexserver               plexserver                              1/1     1            1           156d\ntelegraf                 telegraf                                1/1     1            1           320d\n</code></pre>"},{"location":"blog/2024/03/15/getbukkit-expired/","title":"Getbukkit Expired","text":""},{"location":"blog/2024/03/15/getbukkit-expired/#an-unexpected-minecraft-update","title":"An Unexpected Minecraft \"Update\"","text":"<p>One day while looking at the monitoring in lexicon I noticed there was something big missing: the minecraft server that normally takes over 4GB of RAM was not running:</p> <pre><code>$ kubectl get all -n minecraft-server\nNAME                                   READY   STATUS             RESTARTS        AGE\npod/minecraft-server-88f84b5fc-5kjr2   0/1     CrashLoopBackOff   152 (30s ago)   12h\n\nNAME                       TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                           AGE\nservice/minecraft-server   NodePort   10.110.215.139   &lt;none&gt;        25565:32565/TCP,19132:32132/UDP   291d\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/minecraft-server   0/1     1            0           12h\n\nNAME                                         DESIRED   CURRENT   READY   AGE\nreplicaset.apps/minecraft-server-88f84b5fc   1         1         0       12h\n\n$ kubectl -n minecraft-server logs minecraft-server-88f84b5fc-5kjr2\n[init] Running as uid=1003 gid=1003 with /data as 'drwxrwxr-x 1 1003 1003 722 Dec 17 05:10 /data'\n[init] Resolving type given SPIGOT\n2024/03/15 17:44:59 Unable to find an element with attribute matcher property=og:title\n[init] ERROR: failed to retrieve latest version from https://getbukkit.org/download/spigot -- site might be down\n\n$ curl https://getbukkit.org/download/spigot\n&lt;!doctype html&gt;\n&lt;html data-adblockkey=\"MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBANDrp2lz7AOmADaN8tA50LsWcjLFyQFcb/P2Txc58oYOeILb3vBw7J6f4pamkAQVSQuqYsKx3YzdUHCvbVZvFUsCAwEAAQ==_UL89QGTogxdwKHwZzilx913GmK75KOL2kLgPnkgb9dD1Tc/wjgiP2tuKwPeUMm3vEXLjUWOarjD7XgGHgmalBg==\" lang=\"en\" style=\"background: #2B2B2B;\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"utf-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n    &lt;link rel=\"icon\" href=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAIAAACQd1PeAAAADElEQVQI12P4//8/AAX+Av7czFnnAAAAAElFTkSuQmCC\"&gt;\n    &lt;link rel=\"preconnect\" href=\"https://www.google.com\" crossorigin&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;div id=\"target\" style=\"opacity: 0\"&gt;&lt;/div&gt;\n&lt;script&gt;window.park = \"eyJ1dWlkIjoiNDk5MDE1NDQtMTJlZi00YWQzLWI3YmQtMjA5Y2YwYzlmZjFmIiwicGFnZV90aW1lIjoxNzEwNTI2ODY5LCJwYWdlX3VybCI6Imh0dHBzOi8vZ2V0YnVra2l0Lm9yZy9kb3dubG9hZC9zcGlnb3QiLCJwYWdlX21ldGhvZCI6IkdFVCIsInBhZ2VfcmVxdWVzdCI6e30sInBhZ2VfaGVhZGVycyI6e30sImhvc3QiOiJnZXRidWtraXQub3JnIiwiaXAiOiIyMTcuMTYyLjU3LjY0In0K\";&lt;/script&gt;\n&lt;script src=\"/bNjGNXnzR.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"blog/2024/03/15/getbukkit-expired/#short-term-workaround","title":"Short-term Workaround","text":"<p>The server fails to start because it bails out when it fails to download the latest version, but a few versions are already downloaded and can be used by switching from <code>TYPE=SPIGOT</code> to <code>TYPE=CUSTOM</code> and specifying <code>CUSTOM_SERVER=/data/spigot_server-1.20.4.jar</code> as explained in Issue #2521: Updated Bukkit download URL https://getbukkit.org/get/</p> minecraft-server.yaml<pre><code>          env:\n            - name: EULA\n              value: \"TRUE\"\n            - name: TYPE\n              value: CUSTOM\n            - name: CUSTOM_SERVER\n              value: /data/spigot_server-1.20.4.jar\n            - name: MEMORY\n              value: 4G\n</code></pre> <p>Reapplying the deployment brings the server back up.</p> <pre><code>$ kubectl apply -f minecraft-server.yaml\n</code></pre>"},{"location":"blog/2024/03/15/getbukkit-expired/#long-term-workaround","title":"Long-term Workaround","text":"<p>A day or two after the server was back up, it was noted in Issue #2721: Fallback to existing server file when getbukkit retrieval fails that the whole getbukkit.com site seems to be completely shut down. A few days later the site was up again.</p> <p>Nevertheless, itzg seems to consistently recommend switching to Paper, stating that Spigot/Bukkit should no longer be used and Paper used instead.</p> <p>This should be a simple as changing the <code>TYPE</code> value, since Paper also supports GeyserMC (to allows Bedrock clients to join Java servers).</p> minecraft-server.yaml<pre><code>          env:\n            - name: EULA\n              value: \"TRUE\"\n            - name: TYPE\n              value: PAPER\n            - name: MEMORY\n              value: 4G\n</code></pre> <p>Applying this changes the replicaset, brings RAM usage down from 5.14 GB to 2.69 GB, but also shows the Geyser plugin fails to load:</p> <pre><code>$ kubectl apply -f minecraft-server.yaml\nnamespace/minecraft-server unchanged\nservice/minecraft-server unchanged\npersistentvolume/minecraft-server-pv unchanged\npersistentvolumeclaim/minecraft-server-pv-claim unchanged\ndeployment.apps/minecraft-server configured\n\n$ kubectl get all -n minecraft-server\nNAME                                    READY   STATUS        RESTARTS   AGE\npod/minecraft-server-545c4b56f5-sz5j8   1/1     Running       0          9s\npod/minecraft-server-7f847b6b7-tv6tw    1/1     Terminating   0          4h35m\n\nNAME                       TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                           AGE\nservice/minecraft-server   NodePort   10.110.215.139   &lt;none&gt;        25565:32565/TCP,19132:32132/UDP   299d\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/minecraft-server   1/1     1            1           4h47m\n\nNAME                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/minecraft-server-545c4b56f5   1         1         1       9s\nreplicaset.apps/minecraft-server-7f847b6b7    0         0         0       4h35m\n\n$ kubectl get all -n minecraft-server\nNAME                                    READY   STATUS    RESTARTS   AGE\npod/minecraft-server-545c4b56f5-sz5j8   1/1     Running   0          106s\n\nNAME                       TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                           AGE\nservice/minecraft-server   NodePort   10.110.215.139   &lt;none&gt;        25565:32565/TCP,19132:32132/UDP   299d\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/minecraft-server   1/1     1            1           4h49m\n\nNAME                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/minecraft-server-545c4b56f5   1         1         1       106s\nreplicaset.apps/minecraft-server-7f847b6b7    0         0         0       4h37m\n\n$ kubectl -n minecraft-server logs minecraft-server-545c4b56f5-sz5j8\n[init] Running as uid=1003 gid=1003 with /data as 'drwxrwxr-x 1 1003 1003 750 Mar 15 18:33 /data'\n[init] Resolving type given PAPER\n[mc-image-helper] 14:43:03.576 INFO  : Resolved paper to version 1.20.4 build 459\n[mc-image-helper] 14:43:04.869 INFO  : Downloaded /data/paper-1.20.4-459.jar\n[mc-image-helper] 14:43:07.295 INFO  : Created/updated 1 property in /data/server.properties\n[init] Setting initial memory to 4G and max to 4G\n[init] Starting the Minecraft server...\nDownloading mojang_1.20.4.jar\nApplying patches\nStarting org.bukkit.craftbukkit.Main\nSystem Info: Java 17 (OpenJDK 64-Bit Server VM 17.0.10+7) Host: Linux 5.15.0-101-generic (amd64)\nLoading libraries, please wait...\n2024-03-23 14:43:20,628 ServerMain WARN Advanced terminal features are not available in this environment\n[14:43:25 INFO]: Environment: Environment[sessionHost=https://sessionserver.mojang.com, servicesHost=https://api.minecraftservices.com, name=PROD]\n[14:43:26 INFO]: Loaded 1174 recipes\n[14:43:27 INFO]: Loaded 1271 advancements\n[14:43:27 INFO]: Starting minecraft server version 1.20.4\n[14:43:27 INFO]: Loading properties\n[14:43:27 INFO]: This server is running Paper version git-Paper-459 (MC: 1.20.4) (Implementing API version 1.20.4-R0.1-SNAPSHOT) (Git: 88419b2)\n[14:43:28 INFO]: Server Ping Player Sample Count: 12\n[14:43:28 INFO]: Using 4 threads for Netty based IO\n[14:43:28 INFO]: [ChunkTaskScheduler] Chunk system is using 1 I/O threads, 1 worker threads, and gen parallelism of 1 threads\n[14:43:28 WARN]: [!] The timings profiler has been enabled but has been scheduled for removal from Paper in the future.\n    We recommend installing the spark profiler as a replacement: https://spark.lucko.me/\n    For more information please visit: https://github.com/PaperMC/Paper/issues/8948\n[14:43:28 INFO]: Default game type: CREATIVE\n[14:43:28 INFO]: Generating keypair\n[14:43:28 INFO]: Starting Minecraft server on *:25565\n[14:43:28 INFO]: Using epoll channel type\n[14:43:28 INFO]: Paper: Using libdeflate (Linux x86_64) compression from Velocity.\n[14:43:28 INFO]: Paper: Using OpenSSL 1.1.x (Linux x86_64) cipher from Velocity.\n[14:43:29 INFO]: [Geyser-Spigot] Loading server plugin Geyser-Spigot v2.1.0-SNAPSHOT\n[14:43:29 ERROR]: [Geyser-Spigot] Error initializing plugin 'Geyser-Spigot.jar' in folder 'plugins' (Is it up to date?)\njava.lang.NoSuchMethodError: 'void org.yaml.snakeyaml.parser.ParserImpl.&lt;init&gt;(org.yaml.snakeyaml.reader.StreamReader)'\n        at org.geysermc.geyser.platform.spigot.shaded.com.fasterxml.jackson.dataformat.yaml.YAMLParser.&lt;init&gt;(YAMLParser.java:191) ~[Geyser-Spigot.jar:?]\n        at org.geysermc.geyser.platform.spigot.shaded.com.fasterxml.jackson.dataformat.yaml.YAMLFactory._createParser(YAMLFactory.java:504) ~[Geyser-Spigot.jar:?]\n        at org.geysermc.geyser.platform.spigot.shaded.com.fasterxml.jackson.dataformat.yaml.YAMLFactory.createParser(YAMLFactory.java:392) ~[Geyser-Spigot.jar:?]\n        at org.geysermc.geyser.platform.spigot.shaded.com.fasterxml.jackson.dataformat.yaml.YAMLFactory.createParser(YAMLFactory.java:15) ~[Geyser-Spigot.jar:?]\n        at org.geysermc.geyser.platform.spigot.shaded.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3542) ~[Geyser-Spigot.jar:?]\n        at org.geysermc.geyser.util.FileUtils.loadConfig(FileUtils.java:57) ~[Geyser-Spigot.jar:?]\n        at org.geysermc.geyser.platform.spigot.GeyserSpigotPlugin.onLoad(GeyserSpigotPlugin.java:144) ~[Geyser-Spigot.jar:?]\n        at io.papermc.paper.plugin.storage.ServerPluginProviderStorage.processProvided(ServerPluginProviderStorage.java:59) ~[paper-1.20.4.jar:git-Paper-459]\n        at io.papermc.paper.plugin.storage.ServerPluginProviderStorage.processProvided(ServerPluginProviderStorage.java:18) ~[paper-1.20.4.jar:git-Paper-459]\n        at io.papermc.paper.plugin.storage.SimpleProviderStorage.enter(SimpleProviderStorage.java:39) ~[paper-1.20.4.jar:git-Paper-459]\n        at io.papermc.paper.plugin.entrypoint.LaunchEntryPointHandler.enter(LaunchEntryPointHandler.java:36) ~[paper-1.20.4.jar:git-Paper-459]\n        at org.bukkit.craftbukkit.v1_20_R3.CraftServer.loadPlugins(CraftServer.java:507) ~[paper-1.20.4.jar:git-Paper-459]\n        at net.minecraft.server.dedicated.DedicatedServer.initServer(DedicatedServer.java:274) ~[paper-1.20.4.jar:git-Paper-459]\n        at net.minecraft.server.MinecraftServer.runServer(MinecraftServer.java:1131) ~[paper-1.20.4.jar:git-Paper-459]\n        at net.minecraft.server.MinecraftServer.lambda$spin$0(MinecraftServer.java:319) ~[paper-1.20.4.jar:git-Paper-459]\n        at java.lang.Thread.run(Unknown Source) ~[?:?]\n[14:43:29 INFO]: Server permissions file permissions.yml is empty, ignoring it\n[14:43:29 INFO]: Preparing level \"world\"\n[14:43:29 INFO]: -------- World Settings For [world] --------\n[14:43:29 INFO]: Item Merge Radius: 2.5\n[14:43:29 INFO]: Experience Merge Radius: 3.0\n[14:43:29 INFO]: Mob Spawn Range: 6\n[14:43:29 INFO]: Item Despawn Rate: 6000\n[14:43:29 INFO]: Arrow Despawn Rate: 1200 Trident Respawn Rate:1200\n[14:43:29 INFO]: Zombie Aggressive Towards Villager: true\n[14:43:29 INFO]: Nerfing mobs spawned from spawners: false\n[14:43:29 INFO]: Allow Zombie Pigmen to spawn from portal blocks: true\n[14:43:29 INFO]: Entity Activation Range: An 32 / Mo 32 / Ra 48 / Mi 16 / Tiv true / Isa false\n[14:43:29 INFO]: Entity Tracking Range: Pl 48 / An 48 / Mo 48 / Mi 32 / Di 128 / Other 64\n[14:43:29 INFO]: Hopper Transfer: 8 Hopper Check: 1 Hopper Amount: 1 Hopper Can Load Chunks: false\n[14:43:29 INFO]: View Distance: 10\n[14:43:29 INFO]: Simulation Distance: 10\n[14:43:29 INFO]: Custom Map Seeds:  Village: 10387312 Desert: 14357617 Igloo: 14357618 Jungle: 14357619 Swamp: 14357620 Monument: 10387313 Ocean: 14357621 Shipwreck: 165745295 End City: 10387313 Slime: 987234911 Nether: 30084232 Mansion: 10387319 Fossil: 14357921 Portal: 34222645\n[14:43:29 INFO]: Max TNT Explosions: 100\n[14:43:29 INFO]: Tile Max Tick Time: 50ms Entity max Tick Time: 50ms\n[14:43:29 INFO]: Cactus Growth Modifier: 100%\n[14:43:29 INFO]: Cane Growth Modifier: 100%\n[14:43:29 INFO]: Melon Growth Modifier: 100%\n[14:43:29 INFO]: Mushroom Growth Modifier: 100%\n[14:43:29 INFO]: Pumpkin Growth Modifier: 100%\n[14:43:29 INFO]: Sapling Growth Modifier: 100%\n[14:43:29 INFO]: Beetroot Growth Modifier: 100%\n[14:43:29 INFO]: Carrot Growth Modifier: 100%\n[14:43:29 INFO]: Potato Growth Modifier: 100%\n[14:43:29 INFO]: TorchFlower Growth Modifier: 100%\n[14:43:29 INFO]: Wheat Growth Modifier: 100%\n[14:43:29 INFO]: NetherWart Growth Modifier: 100%\n[14:43:29 INFO]: Vine Growth Modifier: 100%\n[14:43:29 INFO]: Cocoa Growth Modifier: 100%\n[14:43:29 INFO]: Bamboo Growth Modifier: 100%\n[14:43:29 INFO]: SweetBerry Growth Modifier: 100%\n[14:43:29 INFO]: Kelp Growth Modifier: 100%\n[14:43:29 INFO]: TwistingVines Growth Modifier: 100%\n[14:43:29 INFO]: WeepingVines Growth Modifier: 100%\n[14:43:29 INFO]: CaveVines Growth Modifier: 100%\n[14:43:29 INFO]: GlowBerry Growth Modifier: 100%\n[14:43:29 INFO]: PitcherPlant Growth Modifier: 100%\n[14:43:30 INFO]: -------- World Settings For [world_nether] --------\n[14:43:30 INFO]: Item Merge Radius: 2.5\n[14:43:30 INFO]: Experience Merge Radius: 3.0\n[14:43:30 INFO]: Mob Spawn Range: 6\n[14:43:30 INFO]: Item Despawn Rate: 6000\n[14:43:30 INFO]: Arrow Despawn Rate: 1200 Trident Respawn Rate:1200\n[14:43:30 INFO]: Zombie Aggressive Towards Villager: true\n[14:43:30 INFO]: Nerfing mobs spawned from spawners: false\n[14:43:30 INFO]: Allow Zombie Pigmen to spawn from portal blocks: true\n[14:43:30 INFO]: Entity Activation Range: An 32 / Mo 32 / Ra 48 / Mi 16 / Tiv true / Isa false\n[14:43:30 INFO]: Entity Tracking Range: Pl 48 / An 48 / Mo 48 / Mi 32 / Di 128 / Other 64\n[14:43:30 INFO]: Hopper Transfer: 8 Hopper Check: 1 Hopper Amount: 1 Hopper Can Load Chunks: false\n[14:43:30 INFO]: View Distance: 10\n[14:43:30 INFO]: Simulation Distance: 10\n[14:43:30 INFO]: Custom Map Seeds:  Village: 10387312 Desert: 14357617 Igloo: 14357618 Jungle: 14357619 Swamp: 14357620 Monument: 10387313 Ocean: 14357621 Shipwreck: 165745295 End City: 10387313 Slime: 987234911 Nether: 30084232 Mansion: 10387319 Fossil: 14357921 Portal: 34222645\n[14:43:30 INFO]: Max TNT Explosions: 100\n[14:43:30 INFO]: Tile Max Tick Time: 50ms Entity max Tick Time: 50ms\n[14:43:30 INFO]: Cactus Growth Modifier: 100%\n[14:43:30 INFO]: Cane Growth Modifier: 100%\n[14:43:30 INFO]: Melon Growth Modifier: 100%\n[14:43:30 INFO]: Mushroom Growth Modifier: 100%\n[14:43:30 INFO]: Pumpkin Growth Modifier: 100%\n[14:43:30 INFO]: Sapling Growth Modifier: 100%\n[14:43:30 INFO]: Beetroot Growth Modifier: 100%\n[14:43:30 INFO]: Carrot Growth Modifier: 100%\n[14:43:30 INFO]: Potato Growth Modifier: 100%\n[14:43:30 INFO]: TorchFlower Growth Modifier: 100%\n[14:43:30 INFO]: Wheat Growth Modifier: 100%\n[14:43:30 INFO]: NetherWart Growth Modifier: 100%\n[14:43:30 INFO]: Vine Growth Modifier: 100%\n[14:43:30 INFO]: Cocoa Growth Modifier: 100%\n[14:43:30 INFO]: Bamboo Growth Modifier: 100%\n[14:43:30 INFO]: SweetBerry Growth Modifier: 100%\n[14:43:30 INFO]: Kelp Growth Modifier: 100%\n[14:43:30 INFO]: TwistingVines Growth Modifier: 100%\n[14:43:30 INFO]: WeepingVines Growth Modifier: 100%\n[14:43:30 INFO]: CaveVines Growth Modifier: 100%\n[14:43:30 INFO]: GlowBerry Growth Modifier: 100%\n[14:43:30 INFO]: PitcherPlant Growth Modifier: 100%\n[14:43:30 INFO]: -------- World Settings For [world_the_end] --------\n[14:43:30 INFO]: Item Merge Radius: 2.5\n[14:43:30 INFO]: Experience Merge Radius: 3.0\n[14:43:30 INFO]: Mob Spawn Range: 6\n[14:43:30 INFO]: Item Despawn Rate: 6000\n[14:43:30 INFO]: Arrow Despawn Rate: 1200 Trident Respawn Rate:1200\n[14:43:30 INFO]: Zombie Aggressive Towards Villager: true\n[14:43:30 INFO]: Nerfing mobs spawned from spawners: false\n[14:43:30 INFO]: Allow Zombie Pigmen to spawn from portal blocks: true\n[14:43:30 INFO]: Entity Activation Range: An 32 / Mo 32 / Ra 48 / Mi 16 / Tiv true / Isa false\n[14:43:30 INFO]: Entity Tracking Range: Pl 48 / An 48 / Mo 48 / Mi 32 / Di 128 / Other 64\n[14:43:30 INFO]: Hopper Transfer: 8 Hopper Check: 1 Hopper Amount: 1 Hopper Can Load Chunks: false\n[14:43:30 INFO]: View Distance: 10\n[14:43:30 INFO]: Simulation Distance: 10\n[14:43:30 INFO]: Custom Map Seeds:  Village: 10387312 Desert: 14357617 Igloo: 14357618 Jungle: 14357619 Swamp: 14357620 Monument: 10387313 Ocean: 14357621 Shipwreck: 165745295 End City: 10387313 Slime: 987234911 Nether: 30084232 Mansion: 10387319 Fossil: 14357921 Portal: 34222645\n[14:43:30 INFO]: Max TNT Explosions: 100\n[14:43:30 INFO]: Tile Max Tick Time: 50ms Entity max Tick Time: 50ms\n[14:43:30 INFO]: Cactus Growth Modifier: 100%\n[14:43:30 INFO]: Cane Growth Modifier: 100%\n[14:43:30 INFO]: Melon Growth Modifier: 100%\n[14:43:30 INFO]: Mushroom Growth Modifier: 100%\n[14:43:30 INFO]: Pumpkin Growth Modifier: 100%\n[14:43:30 INFO]: Sapling Growth Modifier: 100%\n[14:43:30 INFO]: Beetroot Growth Modifier: 100%\n[14:43:30 INFO]: Carrot Growth Modifier: 100%\n[14:43:30 INFO]: Potato Growth Modifier: 100%\n[14:43:30 INFO]: TorchFlower Growth Modifier: 100%\n[14:43:30 INFO]: Wheat Growth Modifier: 100%\n[14:43:30 INFO]: NetherWart Growth Modifier: 100%\n[14:43:30 INFO]: Vine Growth Modifier: 100%\n[14:43:30 INFO]: Cocoa Growth Modifier: 100%\n[14:43:30 INFO]: Bamboo Growth Modifier: 100%\n[14:43:30 INFO]: SweetBerry Growth Modifier: 100%\n[14:43:30 INFO]: Kelp Growth Modifier: 100%\n[14:43:30 INFO]: TwistingVines Growth Modifier: 100%\n[14:43:30 INFO]: WeepingVines Growth Modifier: 100%\n[14:43:30 INFO]: CaveVines Growth Modifier: 100%\n[14:43:30 INFO]: GlowBerry Growth Modifier: 100%\n[14:43:30 INFO]: PitcherPlant Growth Modifier: 100%\n[14:43:30 INFO]: Preparing start region for dimension minecraft:overworld\n[14:43:30 INFO]: Time elapsed: 266 ms\n[14:43:30 INFO]: Preparing start region for dimension minecraft:the_nether\n[14:43:30 INFO]: Time elapsed: 64 ms\n[14:43:30 INFO]: Preparing start region for dimension minecraft:the_end\n[14:43:30 INFO]: Time elapsed: 56 ms\n[14:43:30 INFO]: [Geyser-Spigot] Enabling Geyser-Spigot v2.1.0-SNAPSHOT\n[14:43:30 INFO]: [Geyser-Spigot] Disabling Geyser-Spigot v2.1.0-SNAPSHOT\n[14:43:31 INFO]: Starting remote control listener\n[14:43:31 INFO]: Thread RCON Listener started\n[14:43:31 INFO]: RCON running on 0.0.0.0:25575\n[14:43:31 INFO]: Running delayed init tasks\n[14:43:31 INFO]: Done (3.866s)! For help, type \"help\"\n[14:43:31 INFO]: *************************************************************************************\n[14:43:31 INFO]: This is the first time you're starting this server.\n[14:43:31 INFO]: It's recommended you read our 'Getting Started' documentation for guidance.\n[14:43:31 INFO]: View this and more helpful information here: https://docs.papermc.io/paper/next-steps\n[14:43:31 INFO]: *************************************************************************************\n[14:43:31 INFO]: Timings Reset\n</code></pre> <p>It apperas the Geyser-Spigot.jar from May 27, 2023 is too outdated:</p> <pre><code>[14:43:29 ERROR]: [Geyser-Spigot] Error initializing plugin 'Geyser-Spigot.jar' in folder 'plugins' (Is it up to date?)\n</code></pre> <p>To Update Plugins in a PaperMC server, they need only be downloaded into the server's <code>plugins/update</code> folder, and the next time the server is restarted it will automatically update them.</p> <ol> <li>Download the latest Paper version from https://hangar.papermc.io/GeyserMC/Geyser</li> <li>Copy it into <code>/home/k8s/minecraft-server/plugins/update</code></li> <li>Restart the server with the     start/stop scripts</li> </ol> <p>This time, the Geyser plugin starts successfully:</p> <pre><code>$ minecraft-stop-k8s &amp;&amp; sleep 10 &amp;&amp; minecraft-start-k8s\ndeployment.apps \"minecraft-server\" deleted\nremote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Compressing objects: 100% (1/1), done.\nremote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\nUnpacking objects: 100% (3/3), 297 bytes | 297.00 KiB/s, done.\nFrom github.com:stibbons1990/lexicon-deployments\n   b584f18..f0308df  main       -&gt; origin/main\nUpdating b584f18..f0308df\nFast-forward\n minecraft-server.yaml | 4 +---\n 1 file changed, 1 insertion(+), 3 deletions(-)\nnamespace/minecraft-server unchanged\nservice/minecraft-server unchanged\npersistentvolume/minecraft-server-pv unchanged\npersistentvolumeclaim/minecraft-server-pv-claim unchanged\ndeployment.apps/minecraft-server created\n\n$ kubectl -n minecraft-server logs -f minecraft-server-545c4b56f5-6xmw5\n...\n[15:30:57 INFO]: [Geyser-Spigot] Enabling Geyser-Spigot v2.2.2-SNAPSHOT\n[15:30:57 INFO]: [Geyser-Spigot] ******************************************\n[15:30:57 INFO]: [Geyser-Spigot] \n[15:30:57 INFO]: [Geyser-Spigot] Loading Geyser version 2.2.2-SNAPSHOT (git-master-c64e8af)\n[15:30:57 INFO]: [Geyser-Spigot] \n[15:30:57 INFO]: [Geyser-Spigot] ******************************************\n[15:31:03 INFO]: [Geyser-Spigot] Started Geyser on 0.0.0.0:19132\n[15:31:03 INFO]: [Geyser-Spigot] Done (5.952s)! Run /geyser help for help!\n[15:31:03 INFO]: Starting remote control listener\n[15:31:03 INFO]: Thread RCON Listener started\n[15:31:03 INFO]: RCON running on 0.0.0.0:25575\n[15:31:03 INFO]: Running delayed init tasks\n[15:31:03 INFO]: Done (11.204s)! For help, type \"help\"\n[15:31:03 INFO]: Timings Reset\n[15:31:04 INFO]: [Geyser-Spigot] Downloading Minecraft JAR to extract required files, please wait... (this may take some time depending on the speed of your internet connection)\n[15:31:05 INFO]: [Geyser-Spigot] Minecraft JAR has been successfully downloaded and loaded!\n</code></pre> <p>Reminder: the server is listening on port 25565 in the container, but the service is mapping (<code>NodePort</code>) port 32565 to it, so that is the port that must be used when connecting to it from Java clients. For Bedrock clients, the correct (UDP) port is 32132.</p>"},{"location":"blog/2024/03/15/getbukkit-expired/#fix-the-scripts","title":"Fix The Scripts","text":"<p>With the Spigot server, to send commands to the server the scripts were sending those via a pipeline. With the Paper server, this no longer works:</p> <pre><code>$ kubectl -n minecraft-server exec deploy/minecraft-server -- mc-send-to-console \"say hi\"\nERROR: console pipe needs to be enabled by setting CREATE_CONSOLE_IN_PIPE to true\nERROR: named pipe /tmp/minecraft-console-in is missing\ncommand terminated with exit code 1\n</code></pre> <p>There seems to be nothing about <code>CREATE_CONSOLE_IN_PIPE</code> but there is Issue #2485: ERROR: named pipe /tmp/minecraft-console-in is missing where the recommendation is to use <code>rcon-cli</code> instead:</p> <pre><code>$ kubectl -n minecraft-server exec deploy/minecraft-server -- rcon-cli \"say hi\"\n</code></pre> <p>These show in the logs as</p> <pre><code>[17:38:28 INFO]: Thread RCON Client /0:0:0:0:0:0:0:1 started\n[17:38:28 INFO]: Thread RCON Client /0:0:0:0:0:0:0:1 shutting down\n[17:38:28 INFO]: [Not Secure] [Rcon] hi\n</code></pre>"},{"location":"blog/2024/03/15/getbukkit-expired/#further-improvements","title":"Further Improvements","text":""},{"location":"blog/2024/03/15/getbukkit-expired/#tweak-flags","title":"Tweak Flags","text":"<p>Recommended JVM Startup Flags include allocating 10 GB of RAM to the server. This should be fine since the node has a total of 32 GB.</p> <p>We recommend using at least 6-10GB, no matter how few players! If you can't afford 10GB of memory, give as much as you can, but ensure you leave the operating system some memory too. G1GC operates better with more memory.</p> <pre><code>java \\\n    -Xms10G \\\n    -Xmx10G \\\n    -XX:+UseG1GC \\\n    -XX:+ParallelRefProcEnabled \\\n    -XX:MaxGCPauseMillis=200 \\\n    -XX:+UnlockExperimentalVMOptions \\\n    -XX:+DisableExplicitGC \\\n    -XX:+AlwaysPreTouch \\\n    -XX:G1NewSizePercent=30 \\\n    -XX:G1MaxNewSizePercent=40 \\\n    -XX:G1HeapRegionSize=8M \\\n    -XX:G1ReservePercent=20 \\\n    -XX:G1HeapWastePercent=5 \\\n    -XX:G1MixedGCCountTarget=4 \\\n    -XX:InitiatingHeapOccupancyPercent=15 \\\n    -XX:G1MixedGCLiveThresholdPercent=90 \\\n    -XX:G1RSetUpdatingPauseTimePercent=5 \\\n    -XX:SurvivorRatio=32 \\\n    -XX:+PerfDisableSharedMem \\\n    -XX:MaxTenuringThreshold=1 \\\n    -Dusing.aikars.flags=https://mcflags.emc.gs \\\n    -Daikars.new.flags=true \\\n    -jar paper.jar \\\n    --nogui\n</code></pre> <p>The RAM allocation flags <code>-Xms10G -Xmx10G</code> are already set from the <code>MEMORY</code> variable in the deployment:</p> minecraft-server.yaml<pre><code>          env:\n            - name: MEMORY\n              value: 10G\n</code></pre> <p>Other <code>XX</code> flags must be set via the <code>env</code> variable <code>JVM_XX_OPTS</code> in the deployment, as explained in Variables &gt; General options:</p> minecraft-server.yaml<pre><code>          env:\n            - name: MEMORY\n              value: 10G\n            - name: JVM_XX_OPTS\n              value: &gt;\n                  -XX:+UseG1GC\n                  -XX:+ParallelRefProcEnabled\n                  -XX:MaxGCPauseMillis=200\n                  -XX:+UnlockExperimentalVMOptions\n                  -XX:+DisableExplicitGC\n                  -XX:+AlwaysPreTouch\n                  -XX:G1NewSizePercent=30\n                  -XX:G1MaxNewSizePercent=40\n                  -XX:G1HeapRegionSize=8M\n                  -XX:G1ReservePercent=20\n                  -XX:G1HeapWastePercent=5\n                  -XX:G1MixedGCCountTarget=4\n                  -XX:InitiatingHeapOccupancyPercent=15\n                  -XX:G1MixedGCLiveThresholdPercent=90\n                  -XX:G1RSetUpdatingPauseTimePercent=5\n                  -XX:SurvivorRatio=32\n                  -XX:+PerfDisableSharedMem\n                  -XX:MaxTenuringThreshold=1\n                  -Dusing.aikars.flags=https://mcflags.emc.gs\n                  -Daikars.new.flags=true\n</code></pre> <p>Note: the <code>--nogui</code> flag does not seem to be necessary.</p>"},{"location":"blog/2024/03/15/getbukkit-expired/#floodgate","title":"Floodgate","text":"<p>The companion to Geyser, Floodgate is a hybrid mode plugin which allows Minecraft: Bedrock Edition connections to join online mode Minecraft: Java Edition servers without the need for a Java Edition account. Bedrock clients are still required to be logged in through Microsoft.</p> <p>Adding Plugins should be a simple as downloading the latest version, dropping it under <code>/home/k8s/minecraft-server/plugins/</code> (not <code>update</code>), and then restarting the server:</p> <pre><code>$ minecraft-stop-k8s &amp;&amp; sleep 10 &amp;&amp; \\\n  minecraft-start-k8s &amp;&amp; sleep 20 &amp;&amp; \\\n  minecraft-logs\ndeployment.apps \"minecraft-server\" deleted\nAlready up to date.\nnamespace/minecraft-server unchanged\nservice/minecraft-server unchanged\npersistentvolume/minecraft-server-pv unchanged\npersistentvolumeclaim/minecraft-server-pv-claim unchanged\ndeployment.apps/minecraft-server created\n...\n[16:02:43 INFO]: [floodgate] Loading server plugin floodgate v2.2.2-SNAPSHOT (b96-7f38765)\n[16:02:44 INFO]: [floodgate] Took 1,508ms to boot Floodgate\n...\n[15:58:19 INFO]: [floodgate] Enabling floodgate v2.2.2-SNAPSHOT (b96-7f38765)\n[15:58:20 INFO]: [Geyser-Spigot] Enabling Geyser-Spigot v2.2.2-SNAPSHOT\n[15:58:20 INFO]: [Geyser-Spigot] ******************************************\n[15:58:20 INFO]: [Geyser-Spigot] \n[15:58:20 INFO]: [Geyser-Spigot] Loading Geyser version 2.2.2-SNAPSHOT (git-master-c64e8af)\n[15:58:20 INFO]: [Geyser-Spigot] \n[15:58:20 INFO]: [Geyser-Spigot] ******************************************\n</code></pre> <p>Note: it apperas that a <code>.</code> is prepended to the username of users logging via Floodgate; this is important to use with server commands:</p> <pre><code>[17:14:49 INFO]: [Geyser-Spigot] /10.244.0.1:61211 tried to connect!\n[17:14:50 INFO]: [Geyser-Spigot] Player connected with username Isa________45\n[17:14:50 INFO]: [Geyser-Spigot] Isa________45 (logged in as: Isa________45) has connected to the Java server\n[17:14:51 INFO]: [floodgate] Floodgate player logged in as .Isa________45 joined (UUID: 00000000-0000-0000-0009-0__________8)\n</code></pre> <p>To send a command for this user, the <code>.</code> must be in front of the username:</p> <pre><code>$ kubectl -n minecraft-server exec deploy/minecraft-server -- rcon-cli \"gamemode survival Isa________45\"\nNo player was found\n\n$ kubectl -n minecraft-server exec deploy/minecraft-server -- rcon-cli \"gamemode survival .Isa________45\"\nSet .Isa________45's game mode to Survival Mode\n</code></pre>"},{"location":"blog/2024/03/15/getbukkit-expired/#perms","title":"Perms","text":"<p>LuckPerms is a permissions plugin for Minecraft servers.</p> <p>Once again, download the latest version, drop it under <code>/home/k8s/minecraft-server/plugins/</code>  and restart the server:</p> <pre><code>$ minecraft-stop-k8s &amp;&amp; sleep 10 &amp;&amp; \\\n  minecraft-start-k8s &amp;&amp; sleep 20 &amp;&amp; \\\n  minecraft-logs\ndeployment.apps \"minecraft-server\" deleted\nAlready up to date.\nnamespace/minecraft-server unchanged\nservice/minecraft-server unchanged\npersistentvolume/minecraft-server-pv unchanged\npersistentvolumeclaim/minecraft-server-pv-claim unchanged\ndeployment.apps/minecraft-server created\n...\n[16:02:47 INFO]: Server permissions file permissions.yml is empty, ignoring it\n[16:02:47 INFO]: [LuckPerms] Enabling LuckPerms v5.4.121\n[16:02:47 INFO]:         __    \n[16:02:47 INFO]:   |    |__)   LuckPerms v5.4.121\n[16:02:47 INFO]:   |___ |      Running on Bukkit - Paper\n[16:02:47 INFO]: \n[16:02:47 INFO]: [LuckPerms] Loading configuration...\n[16:02:48 INFO]: [LuckPerms] Loading storage provider... [H2]\n[16:02:48 INFO]: [LuckPerms] Loading internal permission managers...\n[16:02:48 INFO]: [LuckPerms] Performing initial data load...\n[16:02:48 INFO]: [LuckPerms] Successfully enabled. (took 1319ms)\n...\n</code></pre> <p>Next, decide what permissions to set on each user/s and continue from  Step 4: Configuring LuckPerms.</p>"},{"location":"blog/2024/03/15/getbukkit-expired/#install-plugins-from-deployment","title":"Install Plugins From Deployment","text":"<p>Plugins can also be defined installed by simply adding their download URLs in the <code>PLUGINS</code> variable from, as in /examples/geyser/docker-compose.yml:</p> minecraft-server.yaml<pre><code>          env:\n            - name: EULA\n              value: \"TRUE\"\n            - name: TYPE\n              value: PAPER\n            - name: PLUGINS\n              value: |\n                https://download.geysermc.org/v2/projects/geyser/versions/latest/builds/latest/downloads/spigot\n                https://download.geysermc.org/v2/projects/floodgate/versions/latest/builds/latest/downloads/spigot\n                https://download.luckperms.net/1534/bukkit/loader/LuckPerms-Bukkit-5.4.121.jar\n            - name: MEMORY\n              value: 10G\n</code></pre>"},{"location":"blog/2024/03/15/getbukkit-expired/#replace-timings-profiler-with-spark","title":"Replace Timings Profiler with Spark","text":"<p>Follow the recommendation from the logs:</p> <pre><code>[15:30:53 WARN]: [!] The timings profiler has been enabled but has been scheduled for removal from Paper in the future.\n    We recommend installing the spark profiler as a replacement: https://spark.lucko.me/\n    For more information please visit: https://github.com/PaperMC/Paper/issues/8948\n</code></pre> <p>Having read the start and end of PaperMC issue #8948: Replace Timings with Spark as the discussion stands now, this will be a place to revisit in the future once their plans have solidified.</p>"},{"location":"blog/2024/03/22/kubernetes-certificate-expired/","title":"Kubernetes Certificate Expired","text":""},{"location":"blog/2024/03/22/kubernetes-certificate-expired/#yet-another-unexpected-hump","title":"Yet Another Unexpected Hump","text":"<p>The regular user is unable to connect to the Kubernetes API server because the x509 certificate expired on 2024-03-21T21:37:37Z (nearly 24 hours ago):</p> <pre><code>$ kubectl get all -n minecraft-server\nE0322 20:57:59.141510 3545623 memcache.go:265] couldn't get current server API group list: Get \"https://10.0.0.6:6443/api?timeout=32s\": tls: failed to verify certificate: x509: certificate has expired or is not yet valid: current time 2024-03-22T20:57:59+01:00 is after 2024-03-21T21:37:37Z\nE0322 20:57:59.143467 3545623 memcache.go:265] couldn't get current server API group list: Get \"https://10.0.0.6:6443/api?timeout=32s\": tls: failed to verify certificate: x509: certificate has expired or is not yet valid: current time 2024-03-22T20:57:59+01:00 is after 2024-03-21T21:37:37Z\nE0322 20:57:59.145339 3545623 memcache.go:265] couldn't get current server API group list: Get \"https://10.0.0.6:6443/api?timeout=32s\": tls: failed to verify certificate: x509: certificate has expired or is not yet valid: current time 2024-03-22T20:57:59+01:00 is after 2024-03-21T21:37:37Z\nE0322 20:57:59.147141 3545623 memcache.go:265] couldn't get current server API group list: Get \"https://10.0.0.6:6443/api?timeout=32s\": tls: failed to verify certificate: x509: certificate has expired or is not yet valid: current time 2024-03-22T20:57:59+01:00 is after 2024-03-21T21:37:37Z\nE0322 20:57:59.148895 3545623 memcache.go:265] couldn't get current server API group list: Get \"https://10.0.0.6:6443/api?timeout=32s\": tls: failed to verify certificate: x509: certificate has expired or is not yet valid: current time 2024-03-22T20:57:59+01:00 is after 2024-03-21T21:37:37Z\nUnable to connect to the server: tls: failed to verify certificate: x509: certificate has expired or is not yet valid: current time 2024-03-22T20:57:59+01:00 is after 2024-03-21T21:37:37Z\n</code></pre> <p>OTOH the <code>root</code> user is trying to connect to the wrong port (8443 instead of 6443) and hitting instead the UniFi server.</p> <pre><code># kubectl get all -n minecraft-server\nE0322 20:59:13.297860 3571458 memcache.go:265] couldn't get current server API group list: Get \"https://localhost:8443/api\": tls: failed to verify certificate: x509: certificate is valid for UniFi, not localhost\nE0322 20:59:13.300723 3571458 memcache.go:265] couldn't get current server API group list: Get \"https://localhost:8443/api\": tls: failed to verify certificate: x509: certificate is valid for UniFi, not localhost\nE0322 20:59:13.304184 3571458 memcache.go:265] couldn't get current server API group list: Get \"https://localhost:8443/api\": tls: failed to verify certificate: x509: certificate is valid for UniFi, not localhost\nE0322 20:59:13.306950 3571458 memcache.go:265] couldn't get current server API group list: Get \"https://localhost:8443/api\": tls: failed to verify certificate: x509: certificate is valid for UniFi, not localhost\nE0322 20:59:13.309929 3571458 memcache.go:265] couldn't get current server API group list: Get \"https://localhost:8443/api\": tls: failed to verify certificate: x509: certificate is valid for UniFi, not localhost\nUnable to connect to the server: tls: failed to verify certificate: x509: certificate is valid for UniFi, not localhost\n</code></pre> <p>This happens to <code>root</code> because it is missing <code>.kube/config</code>:</p> <pre><code># kubectl config view\napiVersion: v1\nclusters: null\ncontexts: null\ncurrent-context: \"\"\nkind: Config\npreferences: {}\nusers: null\n\n# ls .kube\ncache\n# cp /etc/kubernetes/admin.conf .kube/config\n\n# kubectl config view\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://10.0.0.6:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: kubernetes-admin\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: kubernetes-admin\n  user:\n    client-certificate-data: DATA+OMITTED\n    client-key-data: DATA+OMITTED\n</code></pre> <p>The Kubelet config has not changed in about a year:</p> <pre><code># ls -l /etc/kubernetes/kubelet.conf \n-rw------- 1 root root 1960 Mar 22  2023 /etc/kubernetes/kubelet.conf\n\n# cat /etc/kubernetes/kubelet.conf \napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1ETXlNakl4TXpjek5sb1hEVE16TURNeE9USXhNemN6Tmxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS29LCkNMNThMQ1l5Ym5lck95UUFlNUtqamZJS3BZSmhEU2x1aUNZSUVBdjgzcjZPWlZoajF4bHhNcSs4N05YTEp1KzIKSHhSSDdNcXd6T3FTMk9SVGRWcmJOTzNOZHdSRUJ6WTFNRkNxMnhzUFdBeFVBemZHMERSMUdBVUhpYStSZjFkLwp3Tm9BeG9qWWFrYzk5N0psbVJvS1NYWkdSSC9yQnNyV0w3bnpZaTkwVGNjSnpZSytOckM2dE53OEcrSm1vRzM2ClFwamhEVHJSQW9JaG5PSkc2dzB0ZmkxZG02Q29yZ2h1M2owNFJEM1dvbnpnTTZqejdRZ25TSGJKNTRkeVJjWkwKZ1NSdlZiTkF2Y0RVRFNjcGJDTmdVUHgySFBJdUV4UFRSNnpiMmlSaXRadTJiQm1WRDhoYzExczN4NGtDWURGawpvQURRSXI2cGpTUmFRLzBKRWJNQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZFK2JxU3NDRlZkT3hmempzeU5sb0dBaHJjaEtNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBR2JaVWRXTVVpbTRBeUFoUmI2bwpNMWVOdmdYVUZVcER5WHFrdCt0MWN5bGtiQlRPMDY4NzZ2MGFpU2NKaXhNS2x1bHBwYzF4WStXbEpnQVV1VW9xCkZtQ2pvcUVaWWdyY2FhRjBlMTBNU3h0cjRaMWRTUFdhL2szdUN3cEEra2VYQ3VhaTlqODh3OW54azZjbnVlTC8KdVBTNzQ2MW8xL29HUEpIbmRMOTc4emJLUWU2NEEvb2xvclFEdzZvVVZaU3RvNWRzNGVha3QzVGc0Y1N1TklSbgoyNE9HMEhNMXBKdjMyRjFEdnpJazRPYzZEU0JmeDdZZGN2K1JZdC9ob0lLVGl0WnFFV3ZjSlJSUlMxNmc4VHRwCkRzMGdqK0lRY29yd0FLM1E4UU1DNDg4T3RwVmh5Y2JaSXJSanZPZnBxeTNsQXIwQng4Ykl6ZkJQTTNycUdnTHMKeFhZPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n    server: https://10.0.0.6:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: system:node:lexicon\n  name: system:node:lexicon@kubernetes\ncurrent-context: system:node:lexicon@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: system:node:lexicon\n  user:\n    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem\n    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem\n\n# ls -l /var/lib/kubelet/pki/kubelet-client-current.pem /var/lib/kubelet/pki/kubelet-client-current.pem\nlrwxrwxrwx 1 root root 59 Jan 15 16:44 /var/lib/kubelet/pki/kubelet-client-current.pem -&gt; /var/lib/kubelet/pki/kubelet-client-2024-01-15-16-44-55.pem\nlrwxrwxrwx 1 root root 59 Jan 15 16:44 /var/lib/kubelet/pki/kubelet-client-current.pem -&gt; /var/lib/kubelet/pki/kubelet-client-2024-01-15-16-44-55.pem\n\n# ls -l /var/lib/kubelet/pki/\ntotal 20\n-rw------- 1 root root 2826 Mar 22  2023 kubelet-client-2023-03-22-22-37-39.pem\n-rw------- 1 root root 1110 Jan 15 16:44 kubelet-client-2024-01-15-16-44-55.pem\nlrwxrwxrwx 1 root root   59 Jan 15 16:44 kubelet-client-current.pem -&gt; /var/lib/kubelet/pki/kubelet-client-2024-01-15-16-44-55.pem\n-rw-r--r-- 1 root root 2246 Mar 22  2023 kubelet.crt\n-rw------- 1 root root 1675 Mar 22  2023 kubelet.key\n</code></pre> <p>A search for kubectl \"couldn't get current server API group list\" \"tls: failed to verify certificate: x509: certificate has expired or is not yet valid\" returns a single result, which doesn't help.</p> <p>Unable to connect to the server GKE/GCP x509: certificate has expired or is not yet valid claims that running <code>hwclock -s</code> helped, but it didn't</p> <pre><code># date -R\nFri, 22 Mar 2024 21:12:39 +0100\n# hwclock -s\n# date -R\nFri, 22 Mar 2024 21:13:19 +0100\n# timedatectl\n               Local time: Fri 2024-03-22 21:19:44 CET\n           Universal time: Fri 2024-03-22 20:19:44 UTC\n                 RTC time: Fri 2024-03-22 20:19:44\n                Time zone: Europe/Paris (CET, +0100)\nSystem clock synchronized: no\n              NTP service: n/a\n          RTC in local TZ: no\n</code></pre> <p>A search for just kubectl \"tls: failed to verify certificate: x509: certificate has expired or is not yet valid\" returns more results, including a few that finally offer hints in the right direction.</p> <p>TL;DR: By default, the certificate expires after 365 days. That tracks, because the  Kubernetex cluster in lexicon was created exactly a little over one year ago, on Mar 22, 2023:</p> <pre><code># ls -l /etc/kubernetes/kubelet.conf \n-rw------- 1 root root 1960 Mar 22  2023 /etc/kubernetes/kubelet.conf\n</code></pre> <p>Most other search results seem to be specific to platforms other than the current set (<code>kubelet</code>) but not, knowing that this is the most likely root cause, a more direct search for renew kubelet certificate produces more relevant results, including Certificate Management with kubeadm and a more digestiable answer in Server Fault: /var/lib/kubelet/pki/kubelet.crt is expired, How to renew it?.</p> <p>Indeed that is the non-rotateable certificate that expired:</p> <pre><code># ls -l /var/lib/kubelet/pki/kubelet.crt \n-rw-r--r-- 1 root root 2246 Mar 22  2023 /var/lib/kubelet/pki/kubelet.crt\n\n# openssl x509 -in /var/lib/kubelet/pki/kubelet.crt -text -noout  | grep -A 2 Validity\n        Validity\n            Not Before: Mar 22 20:37:39 2023 GMT\n            Not After : Mar 21 20:37:39 2024 GMT\n\n# ls -l /var/lib/kubelet/pki/kubelet-server-current.pem\nls: cannot access '/var/lib/kubelet/pki/kubelet-server-current.pem': No such file or directory\n</code></pre>"},{"location":"blog/2024/03/22/kubernetes-certificate-expired/#manual-certificate-renewal","title":"Manual Certificate Renewal","text":"<p>Kubernetes documentation for Manual certificate renewal seem to suggest one can simply run <code>kubeadm certs renew</code> to renew specific certificate or, with the subcommand <code>all</code>, renew all of them:</p> <pre><code># kubeadm certs renew all\n</code></pre> <p>Clusters built with <code>kubeadm</code> often copy the <code>admin.conf</code> certificate into <code>$HOME/.kube/config</code>. On such a system, to update the contents of <code>$HOME/.kube/config</code> after renewing the <code>admin.conf</code>, run the following commands:</p> <pre><code># cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n# chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p>Before running <code>kubeadm certs renew all</code>:</p> <pre><code># kubeadm certs check-expiration\n[check-expiration] Reading configuration from the cluster...\n[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[check-expiration] Error reading configuration from the Cluster. Falling back to default configuration\n\nCERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED\nadmin.conf                 Mar 21, 2024 21:37 UTC   &lt;invalid&gt;       ca                      no      \napiserver                  Mar 21, 2024 21:37 UTC   &lt;invalid&gt;       ca                      no      \napiserver-etcd-client      Mar 21, 2024 21:37 UTC   &lt;invalid&gt;       etcd-ca                 no      \napiserver-kubelet-client   Mar 21, 2024 21:37 UTC   &lt;invalid&gt;       ca                      no      \ncontroller-manager.conf    Mar 21, 2024 21:37 UTC   &lt;invalid&gt;       ca                      no      \netcd-healthcheck-client    Mar 21, 2024 21:37 UTC   &lt;invalid&gt;       etcd-ca                 no      \netcd-peer                  Mar 21, 2024 21:37 UTC   &lt;invalid&gt;       etcd-ca                 no      \netcd-server                Mar 21, 2024 21:37 UTC   &lt;invalid&gt;       etcd-ca                 no      \nfront-proxy-client         Mar 21, 2024 21:37 UTC   &lt;invalid&gt;       front-proxy-ca          no      \nscheduler.conf             Mar 21, 2024 21:37 UTC   &lt;invalid&gt;       ca                      no      \n\nCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED\nca                      Mar 19, 2033 21:37 UTC   8y              no      \netcd-ca                 Mar 19, 2033 21:37 UTC   8y              no      \nfront-proxy-ca          Mar 19, 2033 21:37 UTC   8y              no      \n</code></pre> <p>YouTube video unable to connect to the Server and having x509 certificate has expired issue by Cyber Legum show that that <code>kubeadm certs renew all</code> does indeed renew all the certificates, and that it then requires restarting the 4 components (), which the video doesn't quite show. </p> <p>Manual certificate renewal also does not say exactly how one should restart the control plane Pods, there seem to be 2 ways:</p> <pre><code># kubectl -n kube-system delete pod -l 'component=kube-apiserver'\n# kubectl -n kube-system delete pod -l 'component=kube-controller-manager'\n# kubectl -n kube-system delete pod -l 'component=kube-scheduler'\n# kubectl -n kube-system delete pod -l 'component=etcd'\n</code></pre> <p>which would require copying again:</p> <p><code>.kube/config</code>:</p> <pre><code># cp /etc/kubernetes/admin.conf .kube/config\n</code></pre> <p>Or possibly just reload and restart <code>kubelet</code> with:</p> <pre><code># systemctl daemon-reload\n# service kubelet restart\n</code></pre> <p>After running <code>kubeadm certs renew all</code>:</p> <pre><code># kubeadm certs renew all\n[renew] Reading configuration from the cluster...\n[renew] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[renew] Error reading configuration from the Cluster. Falling back to default configuration\n\ncertificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed\ncertificate for serving the Kubernetes API renewed\ncertificate the apiserver uses to access etcd renewed\ncertificate for the API server to connect to kubelet renewed\ncertificate embedded in the kubeconfig file for the controller manager to use renewed\ncertificate for liveness probes to healthcheck etcd renewed\ncertificate for etcd nodes to communicate with each other renewed\ncertificate for serving etcd renewed\ncertificate for the front proxy client renewed\ncertificate embedded in the kubeconfig file for the scheduler manager to use renewed\n\nDone renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the new certificates.\n\n# kubeadm certs check-expiration\n[check-expiration] Reading configuration from the cluster...\n[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n\nCERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED\nadmin.conf                 Mar 23, 2025 09:41 UTC   364d            ca                      no      \napiserver                  Mar 23, 2025 09:41 UTC   364d            ca                      no      \napiserver-etcd-client      Mar 23, 2025 09:41 UTC   364d            etcd-ca                 no      \napiserver-kubelet-client   Mar 23, 2025 09:41 UTC   364d            ca                      no      \ncontroller-manager.conf    Mar 23, 2025 09:41 UTC   364d            ca                      no      \netcd-healthcheck-client    Mar 23, 2025 09:41 UTC   364d            etcd-ca                 no      \netcd-peer                  Mar 23, 2025 09:41 UTC   364d            etcd-ca                 no      \netcd-server                Mar 23, 2025 09:41 UTC   364d            etcd-ca                 no      \nfront-proxy-client         Mar 23, 2025 09:41 UTC   364d            front-proxy-ca          no      \nscheduler.conf             Mar 23, 2025 09:41 UTC   364d            ca                      no      \n\nCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED\nca                      Mar 19, 2033 21:37 UTC   8y              no      \netcd-ca                 Mar 19, 2033 21:37 UTC   8y              no      \nfront-proxy-ca          Mar 19, 2033 21:37 UTC   8y              no  \n\n# openssl x509 -in /var/lib/kubelet/pki/kubelet.crt -text -noout  | grep -A 2 Validity\n        Validity\n            Not Before: Mar 22 20:37:39 2023 GMT\n            Not After : Mar 21 20:37:39 2024 GMT\n\n# ls -l /var/lib/kubelet/pki/kubelet-server-current.pem\nls: cannot access '/var/lib/kubelet/pki/kubelet-server-current.pem': No such file or directory\n\n# cp /etc/kubernetes/admin.conf .kube/config\n# kubectl cluster-info\nKubernetes control plane is running at https://10.0.0.6:6443\nCoreDNS is running at https://10.0.0.6:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\n# kubectl get nodes\nNAME      STATUS   ROLES           AGE    VERSION\nlexicon   Ready    control-plane   366d   v1.26.15\n\n# kubectl describe node lexicon\nName:               lexicon\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    feature.node.kubernetes.io/cpu-cpuid.ADX=true\n                    feature.node.kubernetes.io/cpu-cpuid.AESNI=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX2=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512BITALG=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512BW=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512CD=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512DQ=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512F=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512IFMA=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512VBMI=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512VBMI2=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512VL=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512VNNI=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512VP2INTERSECT=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512VPOPCNTDQ=true\n                    feature.node.kubernetes.io/cpu-cpuid.CETIBT=true\n                    feature.node.kubernetes.io/cpu-cpuid.CETSS=true\n                    feature.node.kubernetes.io/cpu-cpuid.CMPXCHG8=true\n                    feature.node.kubernetes.io/cpu-cpuid.FLUSH_L1D=true\n                    feature.node.kubernetes.io/cpu-cpuid.FMA3=true\n                    feature.node.kubernetes.io/cpu-cpuid.FSRM=true\n                    feature.node.kubernetes.io/cpu-cpuid.FXSR=true\n                    feature.node.kubernetes.io/cpu-cpuid.FXSROPT=true\n                    feature.node.kubernetes.io/cpu-cpuid.GFNI=true\n                    feature.node.kubernetes.io/cpu-cpuid.IA32_ARCH_CAP=true\n                    feature.node.kubernetes.io/cpu-cpuid.IA32_CORE_CAP=true\n                    feature.node.kubernetes.io/cpu-cpuid.IBPB=true\n                    feature.node.kubernetes.io/cpu-cpuid.LAHF=true\n                    feature.node.kubernetes.io/cpu-cpuid.MD_CLEAR=true\n                    feature.node.kubernetes.io/cpu-cpuid.MOVBE=true\n                    feature.node.kubernetes.io/cpu-cpuid.MOVDIR64B=true\n                    feature.node.kubernetes.io/cpu-cpuid.MOVDIRI=true\n                    feature.node.kubernetes.io/cpu-cpuid.OSXSAVE=true\n                    feature.node.kubernetes.io/cpu-cpuid.SHA=true\n                    feature.node.kubernetes.io/cpu-cpuid.SPEC_CTRL_SSBD=true\n                    feature.node.kubernetes.io/cpu-cpuid.SRBDS_CTRL=true\n                    feature.node.kubernetes.io/cpu-cpuid.STIBP=true\n                    feature.node.kubernetes.io/cpu-cpuid.SYSCALL=true\n                    feature.node.kubernetes.io/cpu-cpuid.SYSEE=true\n                    feature.node.kubernetes.io/cpu-cpuid.VAES=true\n                    feature.node.kubernetes.io/cpu-cpuid.VMX=true\n                    feature.node.kubernetes.io/cpu-cpuid.VPCLMULQDQ=true\n                    feature.node.kubernetes.io/cpu-cpuid.X87=true\n                    feature.node.kubernetes.io/cpu-cpuid.XGETBV1=true\n                    feature.node.kubernetes.io/cpu-cpuid.XSAVE=true\n                    feature.node.kubernetes.io/cpu-cpuid.XSAVEC=true\n                    feature.node.kubernetes.io/cpu-cpuid.XSAVEOPT=true\n                    feature.node.kubernetes.io/cpu-cpuid.XSAVES=true\n                    feature.node.kubernetes.io/cpu-cstate.enabled=true\n                    feature.node.kubernetes.io/cpu-hardware_multithreading=true\n                    feature.node.kubernetes.io/cpu-model.family=6\n                    feature.node.kubernetes.io/cpu-model.id=140\n                    feature.node.kubernetes.io/cpu-model.vendor_id=Intel\n                    feature.node.kubernetes.io/cpu-pstate.scaling_governor=powersave\n                    feature.node.kubernetes.io/cpu-pstate.status=active\n                    feature.node.kubernetes.io/cpu-pstate.turbo=true\n                    feature.node.kubernetes.io/cpu-rdt.RDTL2CA=true\n                    feature.node.kubernetes.io/kernel-config.NO_HZ=true\n                    feature.node.kubernetes.io/kernel-config.NO_HZ_IDLE=true\n                    feature.node.kubernetes.io/kernel-version.full=5.15.0-101-generic\n                    feature.node.kubernetes.io/kernel-version.major=5\n                    feature.node.kubernetes.io/kernel-version.minor=15\n                    feature.node.kubernetes.io/kernel-version.revision=0\n                    feature.node.kubernetes.io/pci-0300_8086.present=true\n                    feature.node.kubernetes.io/pci-0300_8086.sriov.capable=true\n                    feature.node.kubernetes.io/storage-nonrotationaldisk=true\n                    feature.node.kubernetes.io/system-os_release.ID=ubuntu\n                    feature.node.kubernetes.io/system-os_release.VERSION_ID=22.04\n                    feature.node.kubernetes.io/system-os_release.VERSION_ID.major=22\n                    feature.node.kubernetes.io/system-os_release.VERSION_ID.minor=04\n                    feature.node.kubernetes.io/usb-ef_046d_0892.present=true\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=lexicon\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"26:f0:f4:2a:36:8a\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.0.0.6\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:/run/containerd/containerd.sock\n                    nfd.node.kubernetes.io/extended-resources: \n                    nfd.node.kubernetes.io/feature-labels:\n                      cpu-cpuid.ADX,cpu-cpuid.AESNI,cpu-cpuid.AVX,cpu-cpuid.AVX2,cpu-cpuid.AVX512BITALG,cpu-cpuid.AVX512BW,cpu-cpuid.AVX512CD,cpu-cpuid.AVX512DQ...\n                    nfd.node.kubernetes.io/master.version: v0.12.1\n                    nfd.node.kubernetes.io/worker.version: v0.12.1\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 22 Mar 2023 22:37:43 +0100\nTaints:             &lt;none&gt;\nUnschedulable:      false\nLease:\n  HolderIdentity:  lexicon\n  AcquireTime:     &lt;unset&gt;\n  RenewTime:       Sat, 23 Mar 2024 10:45:03 +0100\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 18 Mar 2024 21:03:00 +0100   Mon, 18 Mar 2024 21:03:00 +0100   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Sat, 23 Mar 2024 10:41:21 +0100   Mon, 18 Mar 2024 21:02:41 +0100   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 23 Mar 2024 10:41:21 +0100   Mon, 18 Mar 2024 21:02:41 +0100   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 23 Mar 2024 10:41:21 +0100   Mon, 18 Mar 2024 21:02:41 +0100   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 23 Mar 2024 10:41:21 +0100   Mon, 18 Mar 2024 21:02:41 +0100   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.0.6\n  Hostname:    lexicon\nCapacity:\n  cpu:                4\n  ephemeral-storage:  47684Mi\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32479888Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  45000268112\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32377488Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 2056272131924fa6bd1ad9d2922ee4cc\n  System UUID:                3214261f-3ee2-1169-841c-88aedd021b0e\n  Boot ID:                    6db4e730-968b-4864-a85c-a1de937de9a3\n  Kernel Version:             5.15.0-101-generic\n  OS Image:                   Ubuntu 22.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.28\n  Kubelet Version:            v1.26.15\n  Kube-Proxy Version:         v1.26.15\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (26 in total)\n  Namespace                   Name                                                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                      ------------  ----------  ---------------  -------------  ---\n  atuin-server                atuin-585c6b96f8-htnvj                                    500m (12%)    500m (12%)  2Gi (6%)         2Gi (6%)       201d\n  audiobookshelf              audiobookshelf-867f885b49-rpbpr                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         38h\n  cert-manager                cert-manager-64f9f45d6f-qx4hs                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         341d\n  cert-manager                cert-manager-cainjector-56bbdd5c47-ltjgx                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         341d\n  cert-manager                cert-manager-webhook-d4f4545d7-tf4l6                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         341d\n  code-server                 code-server-5768c9d997-cr858                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         192d\n  default                     inteldeviceplugins-controller-manager-7994555cdb-gjmfm    100m (2%)     100m (2%)   100Mi (0%)       120Mi (0%)     190d\n  ingress-nginx               ingress-nginx-controller-6b58ffdc97-rt5lm                 100m (2%)     0 (0%)      90Mi (0%)        0 (0%)         355d\n  kube-flannel                kube-flannel-ds-nrrg6                                     100m (2%)     0 (0%)      50Mi (0%)        0 (0%)         356d\n  kube-system                 coredns-787d4945fb-67z8g                                  100m (2%)     0 (0%)      70Mi (0%)        170Mi (0%)     366d\n  kube-system                 coredns-787d4945fb-gsx6h                                  100m (2%)     0 (0%)      70Mi (0%)        170Mi (0%)     366d\n  kube-system                 etcd-lexicon                                              100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         17d\n  kube-system                 kube-apiserver-lexicon                                    250m (6%)     0 (0%)      0 (0%)           0 (0%)         17d\n  kube-system                 kube-controller-manager-lexicon                           200m (5%)     0 (0%)      0 (0%)           0 (0%)         17d\n  kube-system                 kube-proxy-qlt8c                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         366d\n  kube-system                 kube-scheduler-lexicon                                    100m (2%)     0 (0%)      0 (0%)           0 (0%)         17d\n  kubernetes-dashboard        dashboard-metrics-scraper-7bc864c59-p27cg                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         355d\n  kubernetes-dashboard        kubernetes-dashboard-6c7ccbcf87-vwxnt                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         355d\n  local-path-storage          local-path-provisioner-8bc8875b-lbjh8                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         342d\n  metallb-system              controller-68bf958bf9-79mpk                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         355d\n  metallb-system              speaker-t8f7k                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         355d\n  metrics-server              metrics-server-74c749979-wd278                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         355d\n  node-feature-discovery      nfd-node-feature-discovery-master-5f56c75d-xjkb8          0 (0%)        0 (0%)      0 (0%)           0 (0%)         190d\n  node-feature-discovery      nfd-node-feature-discovery-worker-xmzwk                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         190d\n  plexserver                  plexserver-85f7bf866-7c8gv                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         174d\n  telegraf                    telegraf-7f7c9db469-fmkd7                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         313d\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1650m (41%)  600m (15%)\n  memory             2528Mi (7%)  2508Mi (7%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              &lt;none&gt;\n</code></pre> <p>This looks like at least <code>kubectl</code> finally works again, so now may be the time to 1. upgrade and 2. enable automatic certificate renewal.</p> <p>Now we try to restart the control plane Pods:</p> <pre><code># kubectl -n kube-system delete pod -l 'component=kube-apiserver'\n# kubectl -n kube-system delete pod -l 'component=kube-controller-manager'\n# kubectl -n kube-system delete pod -l 'component=kube-scheduler'\n# kubectl -n kube-system delete pod -l 'component=etcd'\n</code></pre> <p>It's not entirely clear what may have changed, but now the Kubernetes console and everything else seems to work, except that one job shows as pending:</p> <pre><code># kubectl get jobs -n kube-system\nNAME                   COMPLETIONS   DURATION   AGE\nupgrade-health-check   0/1                      9m43s\n</code></pre> <p>After a node reboot, the minecraft server was working again (and all clients were able to connect to it) but the <code>metrics-server</code> is now failed:</p> <pre><code>$ kubectl get all -n metrics-server\nNAME                                 READY   STATUS    RESTARTS        AGE\npod/metrics-server-74c749979-wd278   0/1     Running   31 (118m ago)   355d\n\nNAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice/metrics-server   ClusterIP   10.101.79.149   &lt;none&gt;        443/TCP   355d\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/metrics-server   0/1     1            0           355d\n\nNAME                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/metrics-server-74c749979   1         1         0       355d\n\n$ kubectl logs metrics-server-74c749979-wd278 -n metrics-server\nError from server: Get \"https://10.0.0.6:10250/containerLogs/metrics-server/metrics-server-74c749979-wd278/metrics-server\": remote error: tls: internal error\n</code></pre> <p>Kubernetes dashboard shows event <code>metrics-server-74c749979-wd278.17bf5cdf3efdd640</code> with message <code>Readiness probe failed: HTTP probe failed with statuscode: 500</code></p> <p>The above error to see the logs is happening everywhere, also to the minecraft server, both when trying to show logs and when trying to communicate with the server to send messages:</p> <pre><code>$ kubectl -n minecraft-server get pods\nNAME                               READY   STATUS    RESTARTS   AGE\nminecraft-server-7f847b6b7-tv6tw   1/1     Running   0          124m\n\n$ kubectl -n minecraft-server logs minecraft-server-7f847b6b7-tv6tw\nError from server: Get \"https://10.0.0.6:10250/containerLogs/minecraft-server/minecraft-server-7f847b6b7-tv6tw/minecraft-server\": remote error: tls: internal error\n\n$ kubectl -n minecraft-server exec deploy/minecraft-server -- mc-send-to-console \"Hello\"\nError from server: error dialing backend: remote error: tls: internal error\n</code></pre> <p>Journal logs from kubelet show a TLS handshake error several times per minute:</p> <pre><code># journalctl -xeu kubelet | grep -Ev 'Nameserver limits exceeded' | head -3\nMar 23 11:46:36 lexicon kubelet[6055]: I0323 11:46:36.036164    6055 log.go:245] http: TLS handshake error from 10.244.0.44:45274: no serving certificate available for the kubelet\nMar 23 11:46:51 lexicon kubelet[6055]: I0323 11:46:51.042664    6055 log.go:245] http: TLS handshake error from 10.244.0.44:40012: no serving certificate available for the kubelet\nMar 23 11:47:06 lexicon kubelet[6055]: I0323 11:47:06.042088    6055 log.go:245] http: TLS handshake error from 10.244.0.44:50504: no serving certificate available for the kubelet\n\n# journalctl -xeu kubelet | grep -Ev 'Nameserver limits exceeded' | grep -c 'http: TLS handshake error'\n366\n</code></pre> <p>Filtering those out, there are signs that rotating the certs is stuck:</p> <pre><code># journalctl -xeu kubelet | grep -Ev 'Nameserver limits exceeded' | grep -v 'http: TLS handshake error'\nMar 23 11:51:57 lexicon kubelet[6055]: E0323 11:51:57.493069    6055 certificate_manager.go:488] kubernetes.io/kubelet-serving: certificate request was not signed: timed out waiting for the condition\nMar 23 12:07:05 lexicon kubelet[6055]: E0323 12:07:05.531775    6055 certificate_manager.go:488] kubernetes.io/kubelet-serving: certificate request was not signed: timed out waiting for the condition\nMar 23 12:22:21 lexicon kubelet[6055]: E0323 12:22:21.802759    6055 certificate_manager.go:488] kubernetes.io/kubelet-serving: certificate request was not signed: timed out waiting for the condition\nMar 23 12:22:21 lexicon kubelet[6055]: E0323 12:22:21.802782    6055 certificate_manager.go:354] kubernetes.io/kubelet-serving: Reached backoff limit, still unable to rotate certs: timed out waiting for the condition\nMar 23 12:37:53 lexicon kubelet[6055]: E0323 12:37:53.816829    6055 certificate_manager.go:488] kubernetes.io/kubelet-serving: certificate request was not signed: timed out waiting for the condition\nMar 23 12:53:21 lexicon kubelet[6055]: E0323 12:53:21.816677    6055 certificate_manager.go:488] kubernetes.io/kubelet-serving: certificate request was not signed: timed out waiting for the condition\nMar 23 13:08:49 lexicon kubelet[6055]: E0323 13:08:49.816195    6055 certificate_manager.go:488] kubernetes.io/kubelet-serving: certificate request was not signed: timed out waiting for the condition\n</code></pre> <p>This is because, after restart, one needs to approve the most recent <code>csr</code> from kubernetes:</p> <pre><code># kubectl get csr\nNAME        AGE     SIGNERNAME                      REQUESTOR             REQUESTEDDURATION   CONDITION\ncsr-454fv   7m21s   kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-4mtnx   69m     kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-fljjd   22m     kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-n5bvr   114m    kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-plnf8   99m     kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-rg4cs   38m     kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-wzg9g   53m     kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-xlmlk   129m    kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-zj52z   84m     kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\n\n# kubectl certificate approve csr-454fv\ncertificatesigningrequest.certificates.k8s.io/csr-454fv approved\n\n# kubectl get csr\nNAME        AGE    SIGNERNAME                      REQUESTOR             REQUESTEDDURATION   CONDITION\ncsr-454fv   10m    kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Approved,Issued\ncsr-4mtnx   72m    kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-fljjd   25m    kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-n5bvr   117m   kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-plnf8   102m   kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-rg4cs   41m    kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-wzg9g   56m    kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-xlmlk   132m   kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\ncsr-zj52z   87m    kubernetes.io/kubelet-serving   system:node:lexicon   &lt;none&gt;              Pending\n</code></pre> <p>After approving the most recent CSR (and it's Issued), <code>kubectl</code> operations are back to work and the <code>metrics-server</code> deployment is functional again:</p> <pre><code>$ kubectl get all -n metrics-server\n                      READY   STATUS    RESTARTS        AGE\npod/metrics-server-74c749979-wd278   1/1     Running   31 (132m ago)   355d\n\nNAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice/metrics-server   ClusterIP   10.101.79.149   &lt;none&gt;        443/TCP   355d\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/metrics-server   1/1     1            1           355d\n\nNAME                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/metrics-server-74c749979   1         1         1       355d\n\n$ kubectl -n minecraft-server logs minecraft-server-7f847b6b7-tv6tw | tail -4\n[11:23:46] [User Authenticator #2/INFO]: UUID of player M________t is bbe841f4-06de-4e86-86cd-de0061cf8db1\n[11:23:47] [Server thread/INFO]: M________t joined the game\n[11:23:47] [Server thread/INFO]: M________t[/10.244.0.1:12945] logged in with entity id 327 at ([world]-179.5, 71.0, -312.5)\n[11:23:55] [Server thread/INFO]: M________t lost connection: Disconnected\n[11:23:55] [Server thread/INFO]: M________t left the game\n</code></pre>"},{"location":"blog/2024/03/22/kubernetes-certificate-expired/#maybe-update","title":"Maybe Update","text":"<pre><code># kubeadm upgrade plan\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks.\n[upgrade] Running cluster health checks\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: v1.26.3\n[upgrade/versions] kubeadm version: v1.26.15\nI0323 10:44:05.356608 3686397 version.go:256] remote version is much newer: v1.29.3; falling back to: stable-1.26\n[upgrade/versions] Target version: v1.26.15\n[upgrade/versions] Latest version in the v1.26 series: v1.26.15\n\nUpgrade to the latest version in the v1.26 series:\n\nCOMPONENT                 CURRENT   TARGET\nkube-apiserver            v1.26.3   v1.26.15\nkube-controller-manager   v1.26.3   v1.26.15\nkube-scheduler            v1.26.3   v1.26.15\nkube-proxy                v1.26.3   v1.26.15\nCoreDNS                   v1.9.3    v1.9.3\netcd                      3.5.6-0   3.5.10-0\n\nYou can now apply the upgrade by executing the following command:\n\n        kubeadm upgrade apply v1.26.15\n\n_____________________________________________________________________\n\n\nThe table below shows the current state of component configs as understood by this version of kubeadm.\nConfigs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or\nresetting to kubeadm defaults before a successful upgrade can be performed. The version to manually\nupgrade to is denoted in the \"PREFERRED VERSION\" column.\n\nAPI GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED\nkubeproxy.config.k8s.io   v1alpha1          v1alpha1            no\nkubelet.config.k8s.io     v1beta1           v1beta1             no\n_____________________________________________________________________\n</code></pre>"},{"location":"blog/2024/03/22/kubernetes-certificate-expired/#automatic-certificate-renewal","title":"Automatic Certificate Renewal","text":"<p>Automatic certificate renewal suggests that the certificates should have been renewed automatically already, but the system show symptoms of Kubelet client certificate rotation fails.</p> <p>The most likely reason for this is that there has been no control plane upgrade for just a bit over a year, and that's precisely the issue because the automatic certificate renewal is designed for addressing the simplest use cases; if you don't have specific requirements on certificate renewal and perform Kubernetes version upgrades regularly (less than 1 year in between each upgrade).</p> <p>The troubleshoot steps to deal with the case of Kubelet client certificate rotation fails seem to require a multi-node cluster (a working control plane node in the cluster) which is not available here.</p> <p>Note that the client certificates are correctly renewed (apparently every 2 months), but the server certificate is still the static one:</p> <pre><code># ls -l /var/lib/kubelet/pki/kubelet.crt\n-rw-r--r-- 1 root root 2246 Mar 22  2023 /var/lib/kubelet/pki/kubelet.crt\n\n# ls -l /var/lib/kubelet/pki/kubelet-server-current.pem\nls: cannot access '/var/lib/kubelet/pki/kubelet-server-current.pem': No such file or directory\n\n# ls -l /var/lib/kubelet/pki/kubelet-client-current.pem\nlrwxrwxrwx 1 root root 59 Jan 15 16:44 /var/lib/kubelet/pki/kubelet-client-current.pem -&gt; /var/lib/kubelet/pki/kubelet-client-2024-01-15-16-44-55.pem\n</code></pre>"},{"location":"blog/2024/03/22/kubernetes-certificate-expired/#manual-recovery-and-certificate-renewal","title":"Manual Recovery And Certificate Renewal","text":"<p>It appears the only solution available may be the one in /var/lib/kubelet/pki/kubelet.crt is expired, How to renew it?.</p> <p>Edit <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> and add the following line to set <code>KUBELET_EXTRA_ARGS</code>:</p> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf<pre><code>Environment=\"KUBELET_EXTRA_ARGS=--rotate-certificates=true --rotate-server-certificates=true --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\"\n</code></pre> Full content of <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf<pre><code># Note: This dropin only works with kubeadm and kubelet v1.11+\n[Service]\nEnvironment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\"\nEnvironment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\"\nEnvironment=\"KUBELET_EXTRA_ARGS=--rotate-certificates=true --rotate-server-certificates=true --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\"\n# This is a file that \"kubeadm init\" and \"kubeadm join\" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically\nEnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env\n# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use\n# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.\nEnvironmentFile=-/etc/default/kubelet\nExecStart=\nExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS\n</code></pre> <p>Reload and restart <code>kubelet</code> with:</p> <pre><code># systemctl daemon-reload\n# service kubelet restart\n</code></pre> <p>After this restart, one needs to approve the <code>csr</code> from kubernetes, but this requires using <code>kubectl</code> which is precisely what was broken:</p> <pre><code>$ kubectl get csr\n</code></pre> <p>There will see the certificate waiting to be approved:</p> <pre><code>$ kubectl certificate approve csr-dlcf6\n</code></pre> <p>References:</p> <ul> <li>Certificate Management with kubeadm</li> <li>Kubernetes Certificate Rotation</li> <li>Configure Certificate Rotation for the Kubelet</li> <li>Enabling signed kubelet serving certificates</li> </ul>"},{"location":"blog/2024/03/22/kubernetes-certificate-expired/#time-to-upgrade-maybe","title":"Time To Upgrade Maybe","text":"<p>Another reason to manually renew the server certificate is that the node cannot be upgraded without it:</p> <pre><code># kubeadm upgrade plan\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[upgrade/config] FATAL: failed to get config map: Get \"https://10.0.0.6:6443/api/v1/namespaces/kube-system/configmaps/kubeadm-config?timeout=10s\": tls: failed to verify certificate: x509: certificate has expired or is not yet valid: current time 2024-03-23T10:16:27+01:00 is after 2024-03-21T21:37:37Z\nTo see the stack trace of this error execute with --v=5 or higher\n</code></pre>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/","title":"Monitoring with InfluxDB and Grafana on Kubernetes","text":"<p>Four years later, I still have not gotten the hang of telegraf, I'm still running my own home-made detailed system and process monitoring reporting to InfluxDB running container-lessly in lexicon and I feel the time is up for moving these services into the Kubernetes cluster. Besides keeping them updated, what I'm most looking forward is leveraging the cluster's infrastructure to expose these services (only) over HTTPS with automatically renewed SSL certs.</p>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#current-setup","title":"Current Setup","text":"<p>Continuous Monitoring describes the current, complete setup with the OSS versions of InfluxDB and Grafana.</p>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>There quite a few articles out there explaining how to run all 3 components (Telegraf, InfluxDB, Grafana) on Docker, on some of which the following <code>monitoring.yaml</code> deployment is (very loosly) based:</p> Kubernetes deployment: <code>monitoring.yaml</code> monitoring.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: monitoring\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: influxdb-pv\n  labels:\n    type: local\n  namespace: monitoring\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 30Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/k8s/influxdb\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: influxdb-pv-claim\n  namespace: monitoring\nspec:\n  storageClassName: manual\n  volumeName: influxdb-pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 30Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  labels:\n    app: influxdb\n  name: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      hostname: influxdb\n      containers:\n      - image: docker.io/influxdb:1.8\n        name: influxdb\n        volumeMounts:\n        - mountPath: /var/lib/influxdb\n          name: influxdb-data\n      securityContext:\n        runAsUser: 114\n        runAsGroup: 114\n      volumes:\n      - name: influxdb-data\n        persistentVolumeClaim:\n          claimName: influxdb-pv-claim\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: influxdb\n  name: influxdb-svc\n  namespace: monitoring\nspec:\n  ports:\n  - port: 18086\n    protocol: TCP\n    targetPort: 8086\n    nodePort: 30086\n  selector:\n    app: influxdb\n  type: NodePort\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: telegraf\n  namespace: monitoring\n  labels:\n    app: telegraf\ndata:\n  telegraf.conf: |+\n    [agent]\n      hostname = \"influxdb\"\n    [[outputs.influxdb]]\n      urls = [\"http://influxdb-svc:18086/\"]\n      database = \"telegraf\"\n      timeout = \"5s\"\n    [[inputs.cpu]]\n      percpu = true\n      totalcpu = true\n      collect_cpu_time = false\n      report_active = false\n    [[inputs.disk]]\n      ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\"]\n    [[inputs.diskio]]\n    [[inputs.kernel]]\n    [[inputs.mem]]\n    [[inputs.processes]]\n    [[inputs.swap]]\n    [[inputs.system]]\n    [[inputs.docker]]\n      endpoint = \"unix:///var/run/docker.sock\"\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: telegraf\n  namespace: monitoring\n  labels:\n    app: telegraf\nspec:\n  selector:\n    matchLabels:\n      name: telegraf\n  template:\n    metadata:\n      labels:\n        name: telegraf\n    spec:\n      containers:\n      - name: telegraf\n        image: docker.io/telegraf:1.30.1\n        env:\n        - name: HOSTNAME\n          value: \"influxdb\"\n        - name: \"HOST_PROC\"\n          value: \"/rootfs/proc\"\n        - name: \"HOST_SYS\"\n          value: \"/rootfs/sys\"\n        - name: INFLUXDB_DB\n          value: \"telegraf\"\n        volumeMounts:\n        - name: sys\n          mountPath: /rootfs/sys\n          readOnly: true\n        - name: proc\n          mountPath: /rootfs/proc\n          readOnly: true\n        - name: docker-socket\n          mountPath: /var/run/docker.sock\n        - name: utmp\n          mountPath: /var/run/utmp\n          readOnly: true\n        - name: config\n          mountPath: /etc/telegraf\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: utmp\n        hostPath:\n          path: /var/run/utmp\n      - name: config\n        configMap:\n          name: telegraf\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: grafana-pv\n  labels:\n    type: local\n  namespace: monitoring\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 3Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/k8s/grafana\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: grafana-pv-claim\n  namespace: monitoring\nspec:\n  storageClassName: manual\n  volumeName: grafana-pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  labels:\n    app: grafana\n  name: grafana\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - image: docker.io/grafana/grafana:10.4.2\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: \"GF_AUTH_ANONYMOUS_ENABLED\"\n          value: \"true\"\n        - name: \"GF_SECURITY_ADMIN_USER\"\n          value: \"admin\"\n        - name: \"GF_SECURITY_ADMIN_PASSWORD\"\n          value: \"PLEASE_CHOOSE_A_SENSIBLE_PASSWORD\"\n        name: grafana\n        volumeMounts:\n          - name: grafana-data\n            mountPath: /var/lib/grafana\n      securityContext:\n        runAsUser: 115\n        runAsGroup: 115\n        fsGroup: 115\n      volumes:\n      - name: grafana-data\n        persistentVolumeClaim:\n          claimName: grafana-pv-claim\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: grafana\n  name: grafana-svc\n  namespace: monitoring\nspec:\n  ports:\n  - port: 13000\n    protocol: TCP\n    targetPort: 3000\n    nodePort: 30300\n  selector:\n    app: grafana\n  type: NodePort\n</code></pre> <p>This setup reuses the existing dedicated users <code>influxdb</code> (114) and <code>grafana</code> (115) and requires new directories owned by these users:</p> <pre><code>$ ls -ld /home/k8s/influxdb/ /home/k8s/grafana/\ndrwxr-xr-x 1 grafana  grafana  0 Mar 28 23:25 /home/k8s/grafana/\ndrwxr-xr-x 1 influxdb influxdb 0 Mar 28 21:54 /home/k8s/influxdb/\n\n$ kubectl apply -f monitoring.yaml \nnamespace/monitoring created\npersistentvolume/influxdb-pv created\npersistentvolumeclaim/influxdb-pv-claim created\ndeployment.apps/influxdb created\nservice/influxdb-svc created\nconfigmap/telegraf created\ndaemonset.apps/telegraf created\npersistentvolume/grafana-pv created\npersistentvolumeclaim/grafana-pv-claim created\ndeployment.apps/grafana created\nservice/grafana-svc created\n\n$ kubectl -n monitoring get all\nNAME                            READY   STATUS    RESTARTS   AGE\npod/grafana-6c49f96c47-hx7kd    1/1     Running   0          73s\npod/influxdb-6c86444bb7-kt4sx   1/1     Running   0          73s\npod/telegraf-pwtkh              1/1     Running   0          73s\n\nNAME               TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nservice/grafana    NodePort   10.111.217.211   &lt;none&gt;        3000:30300/TCP   73s\nservice/influxdb   NodePort   10.109.61.156    &lt;none&gt;        8086:30086/TCP   73s\n\nNAME                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/telegraf   1         1         1       1            1           &lt;none&gt;          73s\n\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana    1/1     1            1           73s\ndeployment.apps/influxdb   1/1     1            1           73s\n\nNAME                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-6c49f96c47    1         1         1       73s\nreplicaset.apps/influxdb-6c86444bb7   1         1         1       73s\n</code></pre> <p>Grafana is able to query InfluxDB at  http://influxdb:8086/ and the following steps will be to enable accessing both Grafana and InfluxDB over HTTPS externally.</p>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#grafana-setup","title":"Grafana Setup","text":"<p>Once InfluxDB is ready and Telegraf is feeding data into it, setup Grafana by creating a Data source:</p> <ul> <li>Type: InfluxDB</li> <li>Name: <code>telegraf</code></li> <li>Query language: InfluxQL</li> <li>URL: http://influxdb-svc:18086 (see Pod Hostname DNS)</li> <li>Database: <code>telegraf</code></li> </ul>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#secure-influxdb","title":"Secure InfluxDB","text":"<p>The next step is to add authentication to InfluxDB. This will require updating Telegraf and Grafana.</p>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#influxdb-authentication","title":"InfluxDB Authentication","text":"<p>Authentication and authorization in InfluxDB starts by enabling authentication.</p> <p>Create at least one <code>admin</code> user:</p> <pre><code>$ influx -host localhost -port 30086\nConnected to http://localhost:30086 version 1.8.10\nInfluxDB shell version: 1.6.7~rc0\n\n&gt; USE telegraf\nUsing database telegraf\n\n&gt; CREATE USER admin WITH PASSWORD '**********' WITH ALL PRIVILEGES\n</code></pre> <p>Warning</p> <p>The password must be enclosed in single quotes (<code>'</code>).</p> <p>Enable authentication in the deployment by setting the <code>INFLUXDB_HTTP_AUTH_ENABLED</code> variable:</p> monitoring.yaml<pre><code>    spec:\n      containers:\n      - image: docker.io/influxdb:1.8\n        env:\n        - name: \"INFLUXDB_HTTP_AUTH_ENABLED\"\n          value: \"true\"\n        name: influxdb\n</code></pre> <p>Restart InfluxDB:</p> <pre><code>$ kubectl apply -f monitoring.yaml\n...\ndeployment.apps/influxdb configured\n...\n</code></pre> <p>The result is not that connections are rejected, but access to the database is denied:</p> <pre><code>$ kubectl  -n monitoring logs telegraf-2k8hb | tail -1\n2024-04-20T18:24:16Z E! [outputs.influxdb] E! [outputs.influxdb] Failed to write metric (will be dropped: 401 Unauthorized): unable to parse authentication credentials\n\n$ influx -host localhost -port 30086\nConnected to http://localhost:30086 version 1.8.10\nInfluxDB shell version: 1.6.7~rc0\n&gt; USE telegraf\nERR: unable to parse authentication credentials\nDB does not exist!\n</code></pre>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#update-grafana","title":"Update Grafana","text":"<p>Updating the InfluxDB connection under Data soures in Grafana, by adding the username (<code>admin</code>) and password, is enough to get the connextion restored.</p>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#update-telegraf","title":"Update Telegraf","text":"<p>To restore access to Telegraf, add the credentials to the <code>ConfigMap</code> as follows:</p> monitoring.yaml<pre><code>data:\n  telegraf.conf: |+\n    [[outputs.influxdb]]\n      urls = [\"http://influxdb-svc:18086/\"]\n      database = \"telegraf\"\n      username = \"admin\"\n      password = \"*********************\"\n</code></pre> <p>And restart <code>telegraf</code>:</p> <pre><code>$ kubectl delete -n monitoring daemonset telegraf\n$ kubectl apply -f monitoring.yaml\n</code></pre>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#https-access","title":"HTTPS Access","text":"<p>To do this, add an <code>Ingress</code> pointing to each service:</p> monitoring.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana-ingress\n  namespace: monitoring\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: gra.ssl.uu.am\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: grafana-svc\n                port:\n                  number: 3000\n  tls:\n    - secretName: grafana-tls-secret\n      hosts:\n        - gra.ssl.uu.am\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: influxdb-ingress\n  namespace: monitoring\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: inf.ssl.uu.am\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: influxdb-svc\n                port:\n                  number: 8086\n  tls:\n    - secretName: influxdb-tls-secret\n      hosts:\n        - inf.ssl.uu.am\n</code></pre> <pre><code>$ kubectl apply -f monitoring.yaml \n...\ningress.networking.k8s.io/grafana-ingress created\ningress.networking.k8s.io/influxdb-ingress created\n</code></pre> <p>Each Ingress will need to obtain its own certificate, which requires patching each ACME solver to listen on port 32080 (set up in router), leveraging the script for Monthly renewal of certificates (automated):</p> <pre><code># /root/bin/cert-renewal-port-fwd.sh\nservice/cm-acme-http-solver-rnrs5 patched\n</code></pre> <p>This script runs every 30 minutes via <code>crontab</code> but can also be run manually to speed things up.</p> <p>If the external port 80 seems to be timing out, it may be necessary to remove the port forwarding rule in the router and add it again.</p> <p>Once DNS records are updated, Grafana should be available at https://gra.ssl.uu.am and InfluxDB should be available at https://inf.ssl.uu.am</p>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#troubleshooting","title":"Troubleshooting","text":""},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#persistent-volumes","title":"Persistent Volumes","text":"<p>Because there are multiple <code>PersistentVolume</code> and <code>PersistentVolumeClaim</code>, it is necessary to link them explicitly by adding <code>volumeName</code> to each <code>PersistentVolumeClaim</code>, otherwise volatile volumes are used which are then discarded each time the deployment is deleted.</p>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#ingress-multiple-ssl-certificates","title":"Ingress Multiple SSL Certificates","text":"<p>When multiple <code>Ingress</code> are created in the same namespace, it is also necessary to give each a different <code>tls.secretName</code> for each <code>tls.hosts</code> value; otherwise only one SSL certificate will be created (and signed) and it won't be valid for all subdomains in <code>tls.hosts</code>.</p>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#pod-hostname-dns","title":"Pod Hostname DNS","text":"<p>For this setup to work, <code>telegraf</code> and <code>grafana</code> need to send HTTP requests to <code>influxdb</code>. At first, it was enough to set the <code>hostname</code> value in the <code>influxdb</code> deployment, so that other services can connect to it via the internal port and DNS: http://influxdb:8086</p> <p>However, after the deployment was deleted and applied a few times, <code>telegraf</code> and <code>grafana</code> were no longer able to connect; the internal DNS would no longer return an IP address for the <code>influxdb</code> hostname:</p> <pre><code>$ kubectl -n monitoring exec -i -t telegraf-zzs52 -- ping -c 1 kube-dns.kube-system.svc.cluster.local\nPING kube-dns.kube-system.svc.cluster.local (10.96.0.10) 56(84) bytes of data.\n\n$ kubectl -n monitoring exec -i -t telegraf-zzs52 -- ping -c 1 influxdb.monitoring.svc.cluster.local\nping: influxdb.monitoring.svc.cluster.local: Name or service not known\ncommand terminated with exit code 2\n</code></pre> <p>Telegraf cannot write to InfluxDB:</p> <pre><code>$ kubectl -n monitoring logs telegraf-5rv6z | tail -2\n2024-04-20T16:05:50Z E! [outputs.influxdb] When writing to [http://influxdb:8086/]: failed doing req: Post \"http://influxdb:8086/write?db=telegraf\": dial tcp: lookup influxdb on 10.96.0.10:53: no such host\n2024-04-20T16:05:50Z E! [agent] Error writing to outputs.influxdb: could not write any address\n</code></pre> <p>Grafana cannot query InfluxDB:</p> <pre><code>Get \"http://influxdb:8086/query?db=telegraf&amp;epoch=ms&amp;q=SELECT++FROM+%22%22+WHERE+time+%3E%3D+1713606668691ms+and+time+%3C%3D+1713628268691ms\": dial tcp: lookup influxdb on 10.96.0.10:53: no such host\n</code></pre> <p>After getting tired of not finding relevant information to troubleshoot this, updated the deployment and Grafana to query InfluxDB via its <code>NodePort</code> at 10.0.0.6:30086</p> <p>After updating dhe deployement, restart <code>telegraf</code>:</p> <pre><code>$ kubectl delete -n monitoring daemonset telegraf\n$ kubectl apply -f monitoring.yaml\n</code></pre> <p>Still went to to check whether DNS queries are being received/processed? and after adding logging of queries and trying again:</p> <pre><code>$ kubectl -n monitoring exec -i -t telegraf-zzs52 -- ping -c 1 kube-dns.kube-system.svc.cluster.local\nPING kube-dns.kube-system.svc.cluster.local (10.96.0.10) 56(84) bytes of data.\n\n$ kubectl -n monitoring exec -i -t telegraf-zzs52 -- ping -c 1 kube-dns.kube-system\nPING kube-dns.kube-system.svc.cluster.local (10.96.0.10) 56(84) bytes of data.\n\n$ kubectl -n monitoring exec -i -t telegraf-zzs52 -- ping -c 1 influxdb.monitoring\nping: influxdb.monitoring: Name or service not known\ncommand terminated with exit code 2\n\n$ kubectl -n monitoring exec -i -t telegraf-zzs52 -- ping -c 1 influxdb.monitoring.svc.cluster.local\nping: influxdb.monitoring.svc.cluster.local: Name or service not known\ncommand terminated with exit code 2\n\n$ kubectl logs --namespace=kube-system -l k8s-app=kube-dns\n[INFO] 10.244.0.216:34419 - 28080 \"AAAA IN kube-dns.kube-system.monitoring.svc.cluster.local. udp 67 false 512\" NXDOMAIN qr,aa,rd 160 0.00013723s\n[INFO] 10.244.0.216:34419 - 8637 \"A IN kube-dns.kube-system.monitoring.svc.cluster.local. udp 67 false 512\" NXDOMAIN qr,aa,rd 160 0.000129401s\n[INFO] 10.244.0.216:43649 - 36293 \"A IN kube-dns.kube-system.svc.cluster.local. udp 56 false 512\" NOERROR qr,aa,rd 110 0.000073868s\n[INFO] 10.244.0.216:43649 - 62915 \"AAAA IN kube-dns.kube-system.svc.cluster.local. udp 56 false 512\" NOERROR qr,aa,rd 149 0.000107266s\n[INFO] 10.244.0.216:42170 - 53476 \"A IN influxdb.monitoring.svc.cluster.local. udp 55 false 512\" NXDOMAIN qr,aa,rd 148 0.000116052s\n[INFO] 10.244.0.216:42170 - 27874 \"AAAA IN influxdb.monitoring.svc.cluster.local. udp 55 false 512\" NXDOMAIN qr,aa,rd 148 0.000141714s\n[INFO] 10.244.0.216:51203 - 31196 \"A IN influxdb.monitoring.cluster.local. udp 51 false 512\" NXDOMAIN qr,aa,rd 144 0.000056868s\n[INFO] 10.244.0.216:51203 - 64478 \"AAAA IN influxdb.monitoring.cluster.local. udp 51 false 512\" NXDOMAIN qr,aa,rd 144 0.000128173s\n[INFO] 10.244.0.216:51687 - 28937 \"A IN influxdb.monitoring.v.cablecom.net. udp 52 false 512\" NXDOMAIN qr,rd,ra 139 0.020606519s\n[INFO] 10.244.0.216:51687 - 65290 \"AAAA IN influxdb.monitoring.v.cablecom.net. udp 52 false 512\" NXDOMAIN qr,rd,ra 139 0.021962102s\n</code></pre> <p>NXDOMAIN means that the domain is non-existent, providing a DNS error message that is received by the client (Recursive DNS server). This happens when the domain has been requested but cannot be resolved to a valid IP address. All in all, NXDOMAIN error messages simply mean that the domain does not exist.</p> <p>This only happens with the pod's hostname, but we can resolve its <code>Service</code>:</p> <pre><code>$ kubectl -n monitoring exec -i -t telegraf-zzs52 -- ping -c 1 grafana-svc.monitoring\nPING grafana-svc.monitoring.svc.cluster.local (10.109.127.41) 56(84) bytes of data.\n</code></pre> <p>All this is because there's no A record for a Pod born of a Deployment so the DNS service will not resolve pods, but instead services, and the port exposed is that of the service (<code>port</code>) instead of that of the pod (<code>targetPort</code>), so the correct URL to reach InfluxDB is http://influxdb-svc:18086</p> <p>The change of service name and <code>port</code> happened while creating the <code>Ingress</code> for HTTP access:</p> <pre><code>$ kubectl -n monitoring get all \nNAME                            READY   STATUS    RESTARTS   AGE\npod/grafana-7647f97d64-k8h5m    1/1     Running   0          19h\npod/influxdb-84dd8bc664-5m9fx   1/1     Running   0          17h\npod/telegraf-c59gg              1/1     Running   0          17h\n\nNAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE\nservice/grafana-svc    NodePort   10.109.127.41    &lt;none&gt;        13000:30300/TCP   19h\nservice/influxdb-svc   NodePort   10.109.191.140   &lt;none&gt;        18086:30086/TCP   19h\n\nNAME                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/telegraf   1         1         1       1            1           &lt;none&gt;          17h\n\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana    1/1     1            1           19h\ndeployment.apps/influxdb   1/1     1            1           19h\n\nNAME                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-7647f97d64    1         1         1       19h\nreplicaset.apps/influxdb-6c86444bb7   0         0         0       19h\nreplicaset.apps/influxdb-84dd8bc664   1         1         1       17h\nreplicaset.apps/influxdb-87c66ff6     0         0         0       17h\n</code></pre> <p>Further reading: Connecting the Dots: Understanding How Pods Talk in Kubernetes Networks.</p>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#conmon-migration","title":"Conmon Migration","text":"<p>Continuous Monitoring can now be migrated to report metrics to a (new) database in the new InfluxDB and serve dashboards securely over HTTPS from the new Grafana.</p> <p>Create a <code>monitoring</code> database (separate from the <code>telegraf</code> database) and set its retencion period to 30 days:</p> <pre><code>$ influx -host localhost -port 30086\nConnected to http://localhost:30086 version 1.8.10\nInfluxDB shell version: 1.6.7~rc0\n\n&gt; auth\nusername: admin\npassword: \n\n&gt; CREATE DATABASE monitoring\n&gt; USE monitoring\nUsing database monitoring\n\n&gt; CREATE RETENTION POLICY \"30_days\" ON \"monitoring\" DURATION 30d REPLICATION 1\n&gt; ALTER RETENTION POLICY \"30_days\" on \"monitoring\" DURATION 30d REPLICATION 1 DEFAULT\n</code></pre> <p>In Grafana, create a new InfluxDB connection under Data sources pointing to this database.</p> <p>In the <code>conmon</code> scripts, update the <code>curl</code> domain to use HTTP Basic authentication, and update the <code>TARGET</code> to point at the new InfluxDB service; the value can be any of the following:</p> <ul> <li>http://localhost:30086 only on the server itself</li> <li>http://lexicon:30086 when running in the same LAN</li> <li>https://inf.ssl.uu.am when running out of LAN</li> </ul> <p>To accommodate for a transition period, scripts can report to both InfluxDB instances until the migration is over.</p> <p>First, store the InfluxDB credentials in <code>/etc/conmon/influxdb-auth</code> and make the file readable only to the <code>root</code> user:</p> <pre><code># echo 'admin:*************' &gt; /etc/conmon/influxdb-auth\n# chmod 400 /etc/conmon/influxdb-auth\n</code></pre> conmon.sh<pre><code>DBNAME=monitoring\nTARGET='http://lexicon:8086'\nTARGET2='http://lexicon:30086'\n...\n      curl &gt;/dev/null 2&gt;/dev/null \\\n        -i -XPOST \"${TARGET}/write?db=${DBNAME}\" \\\n        --data-binary @\"${DATA}.POST\"\n      curl &gt;/dev/null 2&gt;/dev/null \\\n        -u $(cat /etc/conmon/influxdb-auth) \\\n        -i -XPOST \"${TARGET2}/write?db=${DBNAME}\" \\\n        --data-binary @\"${DATA}.POST\"\n</code></pre> <p>The <code>conmon</code> scripts will need to be updated later to optionally send authentication (user and password) only when necessary, and store the password somewhere safe (definitely outside of the running script).</p> <p>For each host reporting metrics, migrating the dashboard/s from the old Grafana to the old new should be as easy as:</p> <ol> <li>In (each one of) the old dashboards, go to      Dashboard settings &gt; JSON Model     and copy the JSON model into a local file.</li> <li>In (any of) the new old dashboards, go to      Dashboard settings &gt; JSON Model     and copy the <code>uid</code> of (any) <code>datasource</code> object.</li> <li>In the local file, replace the <code>uid</code> of the <code>influxdb</code> <code>datasource</code> objects (one per panel) with the value copied     from the old dashboard.</li> <li>In the new Grafana, go to Home &gt; Dashboards then     New &gt; Import and upload the local file.</li> </ol> <p>Finally, once all the dashboards are working, one can go to Administration &gt; General &gt; Default preferences and set a specific dashboard as Home Dashboard.</p>"},{"location":"blog/2024/04/20/monitoring-with-influxdb-and-grafana-on-kubernetes/#clean-up","title":"Clean Up","text":"<p>After some time dual-reporting with no regressions observed, reporting to the old InfluxDB was removed and a few days later the service could be disabled:</p> <pre><code># systemctl stop grafana-server.service\n# systemctl stop influxdb.service \n# systemctl disable grafana-server.service\nSynchronizing state of grafana-server.service with SysV service script with /lib/systemd/systemd-sysv-install.\nExecuting: /lib/systemd/systemd-sysv-install disable grafana-server\nRemoved /etc/systemd/system/multi-user.target.wants/grafana-server.service.\n# systemctl disable influxdb.service \nSynchronizing state of influxdb.service with SysV service script with /lib/systemd/systemd-sysv-install.\nExecuting: /lib/systemd/systemd-sysv-install disable influxdb\nRemoved /etc/systemd/system/multi-user.target.wants/influxdb.service.\nRemoved /etc/systemd/system/influxd.service.\n</code></pre> <p>This was rather necessary to keep the server cooler and quieter, if not completely cool and quiet:</p> <p></p>"},{"location":"blog/2024/05/04/getting-started-with-fallout-3-and-4-goty/","title":"Getting started with Fallout 3 and 4 GOTY","text":"<p>A few weeks ago Fallout 4: Game of the Year Edition was on sale (75% off) and so I finally pulled the trigger on it. It had been in my wishlist for a few years, since I spent nearly 150 hours on Fallout 3, but I had been reluctant on account of many people complaining that its story was weaker and it was more focused on exploration and combat. Well, people say the same (and I'd agree) about The Elder Scrolls V: Skyrim and I spent already more than 200 hours on it and still want to go back.</p>"},{"location":"blog/2024/05/04/getting-started-with-fallout-3-and-4-goty/#fallout-4","title":"Fallout 4","text":"<p>As usualy, before purchasing the game I checked all recent Proton reports for Fallout 4, because one does not simply run Windows games on Linux. At least, not always without a little tinkering.</p> <p>Among all the recent recommendations, this one worked for me:</p> <ul> <li>Switch to GE-Proton8-16 (not experimental)</li> <li>Limit framerate to 60: <code>DXVK_FRAME_RATE=60 %command%</code></li> </ul> <p>Screen resolution was limited to 1920x1200, but that was easy to change.</p> <p>Other recommendations I found not (yet?) necessary, but seemed likely to be useful (have not tried yet):</p> <ul> <li><code>gamemoderun ENABLE_VKBASALT=1 DXVK_ASYNC=1 WINEDLLOVERRIDES=\"xaudio2_7=n,b\" %command%</code></li> <li>Install <code>faudio</code> from <code>protontricks</code></li> <li>Emulate virtual desktop</li> </ul>"},{"location":"blog/2024/05/04/getting-started-with-fallout-3-and-4-goty/#screen-resolution","title":"Screen Resolution","text":"<p>My only slight discontent with the initial setup was that 1920x1200 looks a little too small on a 3440x1440 screen. I wouldn't terribly mind 2560x1440, but at least I'd rather use all the screen's vertical space.</p> <p>This is just because the game defaults to running windowed. At least with the current (recent) version, disabling the windows mode in the launcher (before launching the game) will correctly default to glorious 3440x1440.</p> <p>In window mode, updatding the values in <code>Fallout4Prefs.ini</code> under <code>.local/share/Steam/steamapps/common/Fallout 4/Fallout4/</code> had no effect:</p> <pre><code>bBorderless=1\nbFull Screen=0\niSize H=1440\niSize W=2560\n</code></pre>"},{"location":"blog/2024/05/04/getting-started-with-fallout-3-and-4-goty/#controller-support","title":"Controller Support","text":"<p>Initially the game would not detect my DualShock controller, but this wasn't the first game to show such problem (I had seen the same with Dirt Rally when running on Proton) so I searched a bit a found a simple workaround here:</p> <p>Solved it by going to \"Properties\" on the game, \"Controllers\", then \"Enable Steam input\" under the \"Override for\"-option.</p>"},{"location":"blog/2024/05/04/getting-started-with-fallout-3-and-4-goty/#emulate-virtual-desktop","title":"Emulate virtual desktop","text":"<p>Use protontricks 377160 winecfg to get to winecfg, then enable Graphics &gt; Emulate virtual desktop and set the size to 1920x1080 (or 2560x1440 in my case). This fixes the infamous <code>Alt-Tab</code> bug.</p>"},{"location":"blog/2024/05/04/getting-started-with-fallout-3-and-4-goty/#mods","title":"Mods","text":"<p>A user reports </p> <p>The game played as expected under Proton. I was able to install F4SE and otherwise mod the game using Mod Organizer 2, which functioned similarly to Vortex (Vortex does not run in WINE). Performance was comparable to Windows, with no major boon or deficit.</p> <p>Another user reports that Mods need a little tweaks:</p> <p>For mods i used the Vortex (mod manager), downloaded from the official website (also download dotnet 6.0 from there).</p> <ol> <li>Install dotnet6 and Vortex (in that order) on a Wine PREFIX outside the Proton prefix.</li> <li>Look over the proton PREFIX location of your game, because you need the folder \"My Games/Fallout4\".</li> <li>Link your <code>.../Documents/My Games/Fallout4</code> to your actual home:    <pre><code>$ ln -sf  \\\n  $path/SteamLibrary/steamapps/compatdata/377160/pfx/drive_c/users/steamuser/Documents/My\\ Games/Fallout4 \\\n  /home/$User/Documents/My\\ Games/Fallout 4\n</code></pre></li> <li>Open Vortex, setup Fallout 4 and manually add the install location of the game.</li> <li>Vortex should look for your <code>/Documents/My Games/Fallout 4</code> that is linked, but you could need to set it manually.</li> <li>Everything should work now.</li> </ol> <p>Note: Change <code>$path</code> and <code>$User</code> to your actual.</p>"},{"location":"blog/2024/05/04/getting-started-with-fallout-3-and-4-goty/#may-2024-update","title":"May 2024 Update","text":"<p>Nothing appears to have broken, except perhaps my hope to experience the game as it used to be.</p>"},{"location":"blog/2024/05/04/getting-started-with-fallout-3-and-4-goty/#fallout-3","title":"Fallout 3","text":"<p>While working on this, I realize I never took note of similar tweaks I used to play Fallout 3. I went back and updated its configuration, with what seems to work best (for me) now:</p> <ul> <li>Switch to GE-Proton8-16 (not experimental)</li> <li><code>PROTON_USE_WINED3D=1 PROTON_FORCE_LARGE_ADDRESS_AWARE=1 DXVK_FRAME_RATE=60 %command%</code></li> </ul> <p>First attempt to run shows </p> <p>You must use \"Turn Windows features on of off\" in the Control Panel to install or configure Microsoft .NET Framework 3.0 x64.</p> <p>But then it just works. What is even better, it actually works well at full screen on ultra-wide 3440x1440.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/","title":"Single-node Kubernetes cluster on Ubuntu Studio desktop (rapture)","text":"<p>For a bit more than a year, I've been running self-hosted services on a single-node Kubernetes cluster on Ubuntu server and, while it has presented some troubles to shoot, I have grown used to the advantages of deploying services far more easily, without having to worry too much about their dependencies, getting automatic updates, and even making many of them available over HTTPS with good SSL certificates. Now I want to enjoy some of those advantages in my desktop PC: Rapture.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#motivation","title":"Motivation","text":"<p>In the last couple of months, a couple of incidents have left me thinking it may be time to upgrade the cluster.</p> <p>First, following the removal of Legacy Package Repositories, I had to update Kubernetes to the Community-Owned Package Repositories, which didn't directly break anything but there was an unexpected breakage due to a minor version mismatch, after which the cluster was left at version 1.26 as this was the one originall installed for the single-node Kubernetes cluster on.</p> <p>Then, just a few weeks later, the Kubernetes Certificate Expired and eventually the solution was to perform a cumbersome manual certificate renewal. While I'm still not entirely convince that it will work, apparently certificates should be automatically renewed when the Kubernetes cluster is upgraded to a higher minor version.</p> <p>These left me with the nagging feeling that Kubernetes clusters need to be periodically upgraded, but the stark impression that one does not simply upgrade, even though at least  Philpep's Kubernetes the self-hosted single node way claims it wasn't so bad:</p> <p>Handling upgrade with kubeadm is quite simple as long as you read the upgrade notes carefully. For the latest releases I just ran: </p><pre><code>$ export v=1.14.1\n$ apt-get update\n$ apt-get install kubeadm=$v-00 kubelet=$v-00\n$ kubeadm upgrade plan\n[...]\n$ kubeadm upgrade apply v$v\n</code></pre><p></p> <p>At any rate, it would be better to have a second single-node cluster that is not running critical services, which is only to say services I would be rather inconvenienced without, if only to try things out with the peace of mind of not riksing breaking production the cluster.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#more-motivation","title":"More Motivation","text":"<p>On the other hand, other incidents went rather well, e.g. when Getbukkit Expired the solutions where relativly easy to apply and I suspect it was easier to update a single YAML file and <code>kubectl apply</code> than applying the same changes on a bespoke environment would have been.</p> <p>I have also been wanting to migrate Plex Media Server to Kubernetes on my desktop PC, even though I barely use it. Precisely because I barely use this one, it would be nice to deploy it in Kubernetes and forget about it, letting Kubernetes handle updates which I'm not bothering with (not using it).</p> <p>I might also want to make more risky experiments with a Minecraft Java Server for Bedrock clients, something I'd feel better doing on my PC with a <code>world</code> nobody care about.</p> <p>But what I think would be more interesting is trying some other applications I have not been able to run in Kubernetes so far, mostly because they are much more CPU-hungry than what a low-end Intel NUC can handle:</p> <ul> <li>Audiobookshelf is surprisingly CPU    intensive, not sure why but it is the one service that makes    the CPU run hotter on a regular basis, although always in    short spikes and never much above 100% (a single CPU core).</li> <li>GitLab Helm chart    requires enough CPU to be allocated that, together with    just the basic single-node cluster allocations, it exceeded    the limit of the available 4 CPU cores. Besides, running    CI/CD pipelines    would likely take too long on a system with few CPU cores.</li> <li>PhotoPrism    is quite CPU hungry too, as is only to be expected given the    features it offers, so it would only run well on a high-end    CPU and that's one I only have on my desktop PC. Besides,    that PC is the one where the files (photos and videos) live.</li> </ul>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#github-repository","title":"GitHub Repository","text":"<p>From this point on, adding deployments to the cluster will be mostly based on YAML files and these are best stored in a revision controlled environment. A GitHub repository (e.g. <code>kubernetes-deployments</code>) will do just fine, and with the idea of later upgrading the cluster to newer versions of Kubernetes, it may be a good idea to keep deployments in separate directories for each Kubernetes version:</p> <pre><code>$ git clone git@github.com:xxxx/kubernetes-deployments.git\n$ cd kubernetes-deployments/\n$ mkdir 1.26\n$ cd 1.26\n</code></pre>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#install-kubernetes","title":"Install Kubernetes","text":"<p>Kubernetes installation in Lexicon was based on  Install Kubernetes Cluster on Ubuntu 22.04 using kubeadm, How to install Kubernetes on Ubuntu 22.04 Jammy Jellyfish Linux and How to Install Kubernetes Cluster on Ubuntu 22.04. These article were all very helpful and I would highly recommend following them, or their updated versions whenever available, to anyone getting started with Kubernetes for the first time.</p> <p>Being now somewhat more familiar with Kubernetes, this time I will be following the Kubernetes Documentation at kubernetes.io/docs to learn more details first-hand from the official documentation.</p> <p>To install the older 1.26 version, which is the one running in Lexicon, so that the future upgrade/s to newer versions can be tested in Rapture, the first step is to jump from the Available Documentation Versions page to the 1.26 documentation and from there to the starting point: Getting started.</p> <p>Right from the start, things get confusing: Download Kubernetes lists a number of Container Images and points to downloadkubernetes.com to find the binaries, but there is no clear indication as to which binaries are actually required, or recommended.</p> <p>There is not even a mention of the possiblity to install these binaries through more convenient means such as the native package management which is explained only later, in the documentation to Install and Set Up kubectl on Linux, in particular using native package management for Debian-based distributions:</p> <pre><code># curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.26/deb/Release.key \\\n  | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.26/deb/ /' \\\n  | tee /etc/apt/sources.list.d/kubernetes.list\ndeb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.26/deb/ /\n\n# apt-get update\n...\nGet:4 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.26/deb  InRelease [1,192 B]\nGet:11 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.26/deb  Packages [22.1 kB]\n\n# apt-get install -y kubectl\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  kubectl\n0 upgraded, 1 newly installed, 0 to remove and 13 not upgraded.\nNeed to get 10.2 MB of archives.\nAfter this operation, 48.2 MB of additional disk space will be used.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.26/deb  kubectl 1.26.15-1.1 [10.2 MB]\nFetched 10.2 MB in 1s (16.1 MB/s)\nSelecting previously unselected package kubectl.\n(Reading database ... 636365 files and directories currently installed.)\nPreparing to unpack .../kubectl_1.26.15-1.1_amd64.deb ...\nUnpacking kubectl (1.26.15-1.1) ...\nSetting up kubectl (1.26.15-1.1) ...\n</code></pre> <p>Additional commands may be necessary in a brand-new Ubuntu desktop install, but in my case <code>/etc/apt/keyrings</code> akready existed and <code>apt-transport-https ca-certificates curl</code> were all already installed.</p> <p>With only <code>kubectl</code> installed, pretty much anything that one can try will (inevitably) fail because there is no cluster to interact with:</p> <pre><code># kubectl version --output=yaml\nclientVersion:\n  buildDate: \"2024-03-14T01:05:39Z\"\n  compiler: gc\n  gitCommit: 1649f592f1909b97aa3c2a0a8f968a3fd05a7b8b\n  gitTreeState: clean\n  gitVersion: v1.26.15\n  goVersion: go1.21.8\n  major: \"1\"\n  minor: \"26\"\n  platform: linux/amd64\nkustomizeVersion: v4.5.7\n\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n\n# kubectl cluster-info\nE0510 20:41:21.723009 2656643 memcache.go:265] couldn't get current server API group list: Get \"http://localhost:8080/api?timeout=32s\": dial tcp [::1]:8080: connect: connection refused\nE0510 20:41:21.723354 2656643 memcache.go:265] couldn't get current server API group list: Get \"http://localhost:8080/api?timeout=32s\": dial tcp [::1]:8080: connect: connection refused\nE0510 20:41:21.724591 2656643 memcache.go:265] couldn't get current server API group list: Get \"http://localhost:8080/api?timeout=32s\": dial tcp [::1]:8080: connect: connection refused\nE0510 20:41:21.724761 2656643 memcache.go:265] couldn't get current server API group list: Get \"http://localhost:8080/api?timeout=32s\": dial tcp [::1]:8080: connect: connection refused\nE0510 20:41:21.726089 2656643 memcache.go:265] couldn't get current server API group list: Get \"http://localhost:8080/api?timeout=32s\": dial tcp [::1]:8080: connect: connection refused\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n\n# kubectl cluster-info dump\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n</code></pre> <p>In addition to <code>kubectl</code> at least the following 2 packages are required to start and run a Kubernetes cluster:</p> <ul> <li><code>kubeadm</code>: Command-line utility for administering a Kubernetes cluster.</li> <li><code>kubectl</code>: Command-line utility for interacting with a Kubernetes cluster.</li> <li><code>kubelet</code>: Node agent for Kubernetes clusters.</li> </ul> <p>Although far from being clearly explained, or even mentioned, anywhere under docs.kubernetes.io, these components can be installed as simply <code>kubectl</code>:</p> <code># apt install -y kubelet kubeadm kubectl</code> <pre><code># apt install -y kubelet kubeadm kubectl\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nkubectl is already the newest version (1.26.15-1.1).\nThe following additional packages will be installed:\n  conntrack cri-tools ebtables ethtool kubernetes-cni\nThe following NEW packages will be installed:\n  conntrack cri-tools ebtables ethtool kubeadm kubelet kubernetes-cni\n0 upgraded, 7 newly installed, 0 to remove and 13 not upgraded.\nNeed to get 77.8 MB of archives.\nAfter this operation, 296 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 conntrack amd64 1:1.4.6-2build2 [33.5 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 ebtables amd64 2.0.11-4build2 [84.9 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ethtool amd64 1:5.16-1ubuntu0.1 [207 kB]\nGet:3 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.26/deb  cri-tools 1.26.0-1.1 [19.0 MB]\nGet:5 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.26/deb  kubernetes-cni 1.2.0-2.1 [27.6 MB]\nGet:6 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.26/deb  kubelet 1.26.15-1.1 [21.1 MB]\nGet:7 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.26/deb  kubeadm 1.26.15-1.1 [9,842 kB]\nFetched 77.8 MB in 1s (54.5 MB/s)  \nSelecting previously unselected package conntrack.\n(Reading database ... 636369 files and directories currently installed.)\nPreparing to unpack .../0-conntrack_1%3a1.4.6-2build2_amd64.deb ...\nUnpacking conntrack (1:1.4.6-2build2) ...\nSelecting previously unselected package cri-tools.\nPreparing to unpack .../1-cri-tools_1.26.0-1.1_amd64.deb ...\nUnpacking cri-tools (1.26.0-1.1) ...\nSelecting previously unselected package ebtables.\nPreparing to unpack .../2-ebtables_2.0.11-4build2_amd64.deb ...\nUnpacking ebtables (2.0.11-4build2) ...\nSelecting previously unselected package ethtool.\nPreparing to unpack .../3-ethtool_1%3a5.16-1ubuntu0.1_amd64.deb ...\nUnpacking ethtool (1:5.16-1ubuntu0.1) ...\nSelecting previously unselected package kubernetes-cni.\nPreparing to unpack .../4-kubernetes-cni_1.2.0-2.1_amd64.deb ...\nUnpacking kubernetes-cni (1.2.0-2.1) ...\nSelecting previously unselected package kubelet.\nPreparing to unpack .../5-kubelet_1.26.15-1.1_amd64.deb ...\nUnpacking kubelet (1.26.15-1.1) ...\nSelecting previously unselected package kubeadm.\nPreparing to unpack .../6-kubeadm_1.26.15-1.1_amd64.deb ...\nUnpacking kubeadm (1.26.15-1.1) ...\nSetting up conntrack (1:1.4.6-2build2) ...\nSetting up ebtables (2.0.11-4build2) ...\nSetting up cri-tools (1.26.0-1.1) ...\nSetting up kubernetes-cni (1.2.0-2.1) ...\nSetting up ethtool (1:5.16-1ubuntu0.1) ...\nSetting up kubelet (1.26.15-1.1) ...\nSetting up kubeadm (1.26.15-1.1) ...\nProcessing triggers for man-db (2.10.2-1) ...\n</code></pre> <p>Among other tools, this installs other critical components to run a Kubernetes cluster</p> <ul> <li><code>cri-tools</code>: Command-line utility for interacting with a    container runtime.</li> <li><code>kubernetes-cni</code>: Binaries required to provision kubernetes    container networking.</li> </ul>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#install-container-runtime","title":"Install container runtime","text":"<p>And now we come to the next major steps in the installation of a new Kubernetes cluster: select a container runtime.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#prerequisites","title":"Prerequisites","text":"<p>Forwarding IPv4 and letting iptables see bridged traffic are already enabled in Ubuntu:</p> <pre><code># lsmod | egrep 'overlay|bridge'\nbridge                311296  1 br_netfilter\nstp                    16384  1 bridge\nllc                    16384  2 bridge,stp\noverlay               151552  1\n\n# sysctl -a | egrep 'net.ipv4.ip_forward|net.bridge.bridge-nf-call-ip'\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\n</code></pre> <p>Even so, these must be enabled explicitly to avoid issues later, ask me how I know.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#install-containerd","title":"Install containerd","text":"<p>To install containerd, follow the instructions on  getting started with containerd, in particular Option 2: From <code>apt-get</code> which leads to the instructions to Install Docker Engine on Ubuntu.</p> <p>Normally this starts by uninstalling conflicting packages, that is those provided by Ubuntu, before installing the official version of Docker Engine. In the case of Rapture, there are a few packages installed from learning excercises over a year ago:</p> <pre><code># for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do \n  dpkg -l $pkg 2&gt;/dev/null | grep ^ii\ndone\nii  docker.io      24.0.5-0ubuntu1~22.04.1 amd64        Linux container runtime\nii  docker-compose 1.29.2-1                all          define and run multi-container Docker applications with YAML\nii  containerd     1.7.2-0ubuntu1~22.04.1  amd64        daemon to control runC\nii  runc           1.1.7-0ubuntu1~22.04.2  amd64        Open Container Project - runtime\n\n# docker ps\nCONTAINER ID   IMAGE           COMMAND                  CREATED        STATUS        PORTS      NAMES\ne6a7446a39d5   mariadb:10.11   \"docker-entrypoint.s\u2026\"   8 months ago   Up 26 hours   3306/tcp   photoprism_mariadb_1\n\n# docker images\nREPOSITORY              TAG       IMAGE ID       CREATED         SIZE\nmariadb                 10.11     58df8de36e1c   8 months ago    403MB\nphotoprism/photoprism   latest    c7bf390f2ce9   9 months ago    1.84GB\nwebsite                 latest    e4ff4dff6768   14 months ago   57.5MB\n&lt;none&gt;                  &lt;none&gt;    e000dde7140f   14 months ago   57.5MB\nnginx                   alpine    2bc7edbc3cf2   15 months ago   40.7MB\n</code></pre> <p>These all begin very old leftovers, they are all to be thoroughly removed:</p> <pre><code># apt-get purge -y docker.io docker-compose containerd runc\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages were automatically installed and are no longer required:\n  bridge-utils pigz python3-docker python3-dockerpty python3-dotenv python3-jsonschema\n  python3-pyrsistent python3-texttable python3-websocket ubuntu-fan\nUse 'apt autoremove' to remove them.\nThe following packages will be REMOVED:\n  containerd* docker-compose* docker.io* runc*\n0 upgraded, 0 newly installed, 4 to remove and 2 not upgraded.\nAfter this operation, 266 MB disk space will be freed.\n(Reading database ... 636455 files and directories currently installed.)\nRemoving docker.io (24.0.5-0ubuntu1~22.04.1) ...\n'/usr/share/docker.io/contrib/nuke-graph-directory.sh' -&gt; '/var/lib/docker/nuke-graph-directory.sh'\nWarning: Stopping docker.service, but it can still be activated by:\n  docker.socket\nRemoving containerd (1.7.2-0ubuntu1~22.04.1) ...\nRemoving docker-compose (1.29.2-1) ...\nRemoving runc (1.1.7-0ubuntu1~22.04.2) ...\nProcessing triggers for man-db (2.10.2-1) ...\n(Reading database ... 636121 files and directories currently installed.)\nPurging configuration files for docker.io (24.0.5-0ubuntu1~22.04.1) ...\n\nNuking /var/lib/docker ...\n  (if this is wrong, press Ctrl+C NOW!)\n\n+ sleep 10\n\n+ rm -rf /var/lib/docker/buildkit /var/lib/docker/containers /var/lib/docker/engine-id /var/lib/docker/image /var/lib/docker/network /var/lib/docker/nuke-graph-directory.sh /var/lib/docker/overlay2 /var/lib/docker/plugins /var/lib/docker/runtimes /var/lib/docker/swarm /var/lib/docker/tmp /var/lib/docker/trust /var/lib/docker/volumes\ndpkg: warning: while removing docker.io, directory '/etc/docker' not empty so not removed\nPurging configuration files for containerd (1.7.2-0ubuntu1~22.04.1) ...\n\n# rm -rf /var/lib/containerd /var/lib/docker/\n\n# apt autoremove -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  bridge-utils pigz python3-docker python3-dockerpty python3-dotenv python3-jsonschema\n  python3-pyrsistent python3-texttable python3-websocket ubuntu-fan\n0 upgraded, 0 newly installed, 10 to remove and 2 not upgraded.\nAfter this operation, 1,896 kB disk space will be freed.\n(Reading database ... 636120 files and directories currently installed.)\nRemoving ubuntu-fan (0.12.16) ...\nubuntu-fan: removing default /etc/network/fan configuration\nRemoving bridge-utils (1.7-1ubuntu3) ...\nRemoving pigz (2.6-1) ...\nRemoving python3-docker (5.0.3-1) ...\nRemoving python3-dockerpty (0.4.1-2) ...\nRemoving python3-dotenv (0.19.2-1) ...\nRemoving python3-jsonschema (3.2.0-0ubuntu2) ...\nRemoving python3-pyrsistent:amd64 (0.18.1-1build1) ...\nRemoving python3-texttable (1.6.4-1) ...\nRemoving python3-websocket (1.2.3-1) ...\nProcessing triggers for man-db (2.10.2-1) ...\n</code></pre> <p>With all those old packages out of the we, we proceed to set up and install Docker Engine from  Docker's apt repository.</p> <pre><code># curl \\\n  -fsSL https://download.docker.com/linux/ubuntu/gpg \\\n  -o /etc/apt/keyrings/docker.asc\n\n# echo \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  tee /etc/apt/sources.list.d/docker.list\ndeb [arch=amd64 signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu   jammy stable\n\n# apt-get update\n...\nGet:10 https://esm.ubuntu.com/apps/ubuntu jammy-apps-security InRelease [7,553 B]\nGet:14 https://esm.ubuntu.com/infra/ubuntu jammy-infra-updates InRelease [7,449 B]\n</code></pre> <code># apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</code> <pre><code># apt-get install -y \\\n  docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  docker-ce-rootless-extras libslirp0 pigz slirp4netns\nSuggested packages:\n  aufs-tools cgroupfs-mount | cgroup-lite\nThe following NEW packages will be installed:\n  containerd.io docker-buildx-plugin docker-ce docker-ce-cli docker-ce-rootless-extras\n  docker-compose-plugin libslirp0 pigz slirp4netns\n0 upgraded, 9 newly installed, 0 to remove and 2 not upgraded.\nNeed to get 121 MB of archives.\nAfter this operation, 434 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pigz amd64 2.6-1 [63.6 kB]\nGet:2 https://download.docker.com/linux/ubuntu jammy/stable amd64 containerd.io amd64 1.6.31-1 [29.8 MB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libslirp0 amd64 4.6.1-1build1 [61.5 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 slirp4netns amd64 1.0.1-2 [28.2 kB]\nGet:5 https://download.docker.com/linux/ubuntu jammy/stable amd64 docker-buildx-plugin amd64 0.14.0-1~ubuntu.22.04~jammy [29.7 MB]\nGet:6 https://download.docker.com/linux/ubuntu jammy/stable amd64 docker-ce-cli amd64 5:26.1.2-1~ubuntu.22.04~jammy [14.6 MB]\nGet:7 https://download.docker.com/linux/ubuntu jammy/stable amd64 docker-ce amd64 5:26.1.2-1~ubuntu.22.04~jammy [25.3 MB]\nGet:8 https://download.docker.com/linux/ubuntu jammy/stable amd64 docker-ce-rootless-extras amd64 5:26.1.2-1~ubuntu.22.04~jammy [9,319 kB]\nGet:9 https://download.docker.com/linux/ubuntu jammy/stable amd64 docker-compose-plugin amd64 2.27.0-1~ubuntu.22.04~jammy [12.5 MB]\nFetched 121 MB in 1s (95.8 MB/s)                \nSelecting previously unselected package pigz.\n(Reading database ... 635829 files and directories currently installed.)\nPreparing to unpack .../0-pigz_2.6-1_amd64.deb ...\nUnpacking pigz (2.6-1) ...\nSelecting previously unselected package containerd.io.\nPreparing to unpack .../1-containerd.io_1.6.31-1_amd64.deb ...\nUnpacking containerd.io (1.6.31-1) ...\nSelecting previously unselected package docker-buildx-plugin.\nPreparing to unpack .../2-docker-buildx-plugin_0.14.0-1~ubuntu.22.04~jammy_amd64.deb ...\nUnpacking docker-buildx-plugin (0.14.0-1~ubuntu.22.04~jammy) ...\nSelecting previously unselected package docker-ce-cli.\nPreparing to unpack .../3-docker-ce-cli_5%3a26.1.2-1~ubuntu.22.04~jammy_amd64.deb ...\nUnpacking docker-ce-cli (5:26.1.2-1~ubuntu.22.04~jammy) ...\nSelecting previously unselected package docker-ce.\nPreparing to unpack .../4-docker-ce_5%3a26.1.2-1~ubuntu.22.04~jammy_amd64.deb ...\nUnpacking docker-ce (5:26.1.2-1~ubuntu.22.04~jammy) ...\nSelecting previously unselected package docker-ce-rootless-extras.\nPreparing to unpack .../5-docker-ce-rootless-extras_5%3a26.1.2-1~ubuntu.22.04~jammy_amd64.deb ...\nUnpacking docker-ce-rootless-extras (5:26.1.2-1~ubuntu.22.04~jammy) ...\nSelecting previously unselected package docker-compose-plugin.\nPreparing to unpack .../6-docker-compose-plugin_2.27.0-1~ubuntu.22.04~jammy_amd64.deb ...\nUnpacking docker-compose-plugin (2.27.0-1~ubuntu.22.04~jammy) ...\nSelecting previously unselected package libslirp0:amd64.\nPreparing to unpack .../7-libslirp0_4.6.1-1build1_amd64.deb ...\nUnpacking libslirp0:amd64 (4.6.1-1build1) ...\nSelecting previously unselected package slirp4netns.\nPreparing to unpack .../8-slirp4netns_1.0.1-2_amd64.deb ...\nUnpacking slirp4netns (1.0.1-2) ...\nSetting up docker-buildx-plugin (0.14.0-1~ubuntu.22.04~jammy) ...\nSetting up containerd.io (1.6.31-1) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/containerd.service \u2192 /lib/systemd/system/containerd.service.\nSetting up docker-compose-plugin (2.27.0-1~ubuntu.22.04~jammy) ...\nSetting up docker-ce-cli (5:26.1.2-1~ubuntu.22.04~jammy) ...\nSetting up libslirp0:amd64 (4.6.1-1build1) ...\nSetting up pigz (2.6-1) ...\nSetting up docker-ce-rootless-extras (5:26.1.2-1~ubuntu.22.04~jammy) ...\nSetting up slirp4netns (1.0.1-2) ...\nSetting up docker-ce (5:26.1.2-1~ubuntu.22.04~jammy) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/docker.service \u2192 /lib/systemd/system/docker.service.\nCreated symlink /etc/systemd/system/sockets.target.wants/docker.socket \u2192 /lib/systemd/system/docker.socket.\nCould not execute systemctl:  at /usr/bin/deb-systemd-invoke line 142.\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.7) ...\n</code></pre> <p>Aftet this the following packages are newly installed, from Docker's official distribution:</p> <pre><code># for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras; do \\\n   dpkg -l $pkg 2&gt;/dev/null | grep ^ii\\\ndone\nii  docker-ce      5:26.1.2-1~ubuntu.22.04~jammy amd64        Docker: the open-source application container engine\nii  docker-ce-cli  5:26.1.2-1~ubuntu.22.04~jammy amd64        Docker CLI: the open-source application container engine\nii  containerd.io  1.6.31-1     amd64        An open and reliable container runtime\nii  docker-buildx-plugin 0.14.0-1~ubuntu.22.04~jammy amd64        Docker Buildx cli plugin.\nii  docker-compose-plugin 2.27.0-1~ubuntu.22.04~jammy amd64        Docker Compose (V2) plugin for the Docker CLI.\nii  docker-ce-rootless-extras 5:26.1.2-1~ubuntu.22.04~jammy amd64        Rootless support for Docker.\n</code></pre> <p>However, we're not done yet; Docker Engine is not actually working at this point.</p> <p>Trying to verify that the Docker Engine installation is successful by running the <code>hello-world</code> image, turns out the Docker daemon is not yet running:</p> <pre><code># docker run hello-world\ndocker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.\nSee 'docker run --help'.\n</code></pre> <p>Indeed the service is not running:</p> <pre><code># systemctl status docker\n\u00d7 docker.service - Docker Application Container Engine\n     Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)\n     Active: failed (Result: exit-code) since Sun 2024-05-12 09:18:02 CEST; 5min ago\nTriggeredBy: \u00d7 docker.socket\n       Docs: https://docs.docker.com\n    Process: 1402955 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock (code=exited, status=1/FAILURE)\n   Main PID: 1402955 (code=exited, status=1/FAILURE)\n        CPU: 41ms\n\nMay 12 09:18:02 rapture systemd[1]: docker.service: Scheduled restart job, restart counter is at 3.\nMay 12 09:18:02 rapture systemd[1]: Stopped Docker Application Container Engine.\nMay 12 09:18:02 rapture systemd[1]: docker.service: Start request repeated too quickly.\nMay 12 09:18:02 rapture systemd[1]: docker.service: Failed with result 'exit-code'.\nMay 12 09:18:02 rapture systemd[1]: Failed to start Docker Application Container Engine.\n\n# ls -l /run/containerd/containerd.sock /var/run/docker.sock\nsrw-rw---- 1 root root   0 May 12 09:17 /run/containerd/containerd.sock\nsrw-rw---- 1 root docker 0 May 11 22:29 /var/run/docker.sock\n</code></pre> <p>Both sockets exist, but apparently <code>dockerd</code> has a hard time start the firs time and needs to be kicked up:</p> <pre><code># systemctl restart docker\n# systemctl status docker\n\u25cf docker.service - Docker Application Container Engine\n     Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)\n     Active: active (running) since Sun 2024-05-12 09:33:02 CEST; 1s ago\nTriggeredBy: \u25cf docker.socket\n       Docs: https://docs.docker.com\n   Main PID: 1690331 (dockerd)\n      Tasks: 22\n     Memory: 30.6M\n        CPU: 152ms\n     CGroup: /system.slice/docker.service\n             \u2514\u25001690331 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n\n# docker version\nClient: Docker Engine - Community\n Version:           26.1.2\n API version:       1.45\n Go version:        go1.21.10\n Git commit:        211e74b\n Built:             Wed May  8 13:59:59 2024\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          26.1.2\n  API version:      1.45 (minimum version 1.24)\n  Go version:       go1.21.10\n  Git commit:       ef1912d\n  Built:            Wed May  8 13:59:59 2024\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.6.31\n  GitCommit:        e377cd56a71523140ca6ae87e30244719194a521\n runc:\n  Version:          1.1.12\n  GitCommit:        v1.1.12-0-g51d5e94\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre> <p>Now, finally, the <code>hello-world</code> example does work:</p> <pre><code># docker run hello-world\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\nc1ec31eb5944: Pull complete \nDigest: sha256:a26bff933ddc26d5cdf7faa98b4ae1e3ec20c4985e6f87ac0973052224d24302\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\n\n# docker images \nREPOSITORY    TAG       IMAGE ID       CREATED         SIZE\nhello-world   latest    d2c94e258dcb   12 months ago   13.3kB\n</code></pre>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#configuration-tweaks-for-containerd-and-docker","title":"Configuration tweaks for containerd and Docker","text":"<p>With <code>containerd</code> installed and running, the next step is to update its configuration to</p> <ul> <li>Enable the use of <code>systemd</code> cgroup driver.</li> <li>Enable CRI integration, which is disabled by default     when installing <code>containerd</code> from Ubuntu packages,     but is needed to use <code>containerd</code> with Kubernetes.</li> <li>Move <code>/var/lib/containerd</code> to a bigger file system,     this PC has most disk space mounted under <code>/home</code>.</li> </ul> <pre><code># systemctl stop containerd\n# cp -a /var/lib/containerd/ /home/lib/\n# containerd config default \\\n | sed 's:root = \"/var/lib/:root = \"/home/lib/:' \\\n | sed 's/disabled_plugins.*/disabled_plugins = []/' \\\n | sed 's/SystemdCgroup = false/SystemdCgroup = true/' \\\n &gt; /etc/containerd/config.toml\n# mv /var/lib/containerd  /var/lib/DO_NOT_USE_containerd\n# systemctl start containerd\n</code></pre> <p>Warning</p> <p>To create a good <code>/etc/containerd/config.toml</code> one must do so by modifying the default configuration. Using the running configuration (with <code>containerd config dump</code>) will produce an invalid configuration with an empty <code>runtime_type</code> where that is not allowed. This leads to containerd issue #6964: default configuration isn't working, which makes <code>kubelet</code> failing to running any images, and <code>kubeadm</code> fails to initialize the control pane.</p> <p>Note</p> <p>Despite a misleading note in the 1.26 documentation, there is no need to manually configure the  cgroup driver for kubelet. because, already since v1.22, <code>kubeadm</code> defaults it to <code>systemd</code>.</p> <p>This would be a good time to do the same with  <code>/var/lib/docker</code> and, when using BTRFS, add this Important Tweak for BTRFS.</p> <p>Fist, create <code>/etc/docker/daemon.json</code> with this:</p> <pre><code>{\n  \"data-root\": \"/home/lib/docker\",\n  \"storage-driver\": \"overlay2\"\n}\n</code></pre> <p>Then, stop all <code>docker</code> and <code>containerd</code> services, move <code>/var/lib/docker</code> and start services again:</p> <pre><code># docker info | grep -E 'Storage Driver|Docker Root Dir'\n Storage Driver: overlay2\n Docker Root Dir: /var/lib/docker\n\n# systemctl stop docker.service\n# systemctl stop docker.socket\n# systemctl stop containerd.service\n\n# cp -a /var/lib/docker/ /home/lib/\n# mv /var/lib/docker /var/lib/DO_NOT_USE_docker\n\n# systemctl start docker.socket\n# systemctl start docker.service\n# systemctl start containerd.service\n\n# docker info | grep -E 'Storage Driver|Docker Root Dir'\n Storage Driver: overlay2\n Docker Root Dir: /home/lib/docker\n</code></pre> <p>Possibly because Docker was already using <code>overlay2</code>, possibly because the root partition is not using BTRFS, Docker images were not lost as it happened in Lexicon.</p> <p>Warning</p> <p>At the end of the Container runtimes documentation, it is suggested that the next step is to install a network plugin. However, this should not be done before bootstrapping the cluster (see next).</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#bootstrap-the-cluster-with-kubeadm","title":"Bootstrap the cluster with <code>kubeadm</code>","text":"<p>Having already installed kubeadm, kubelet and kubectl the next step is to initialize the cluster: Creating acluster with kubeadm.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#initialize-control-plane","title":"Initialize control-plane","text":"<p>Having reviewed the requirements and having installed all the components already (see above), we proceed to Initializing your control-plane node with:</p> <ul> <li><code>--cri-socket=/run/containerd/containerd.sock</code> to make    sure Kubernetes uses the <code>containerd</code> runtime.</li> <li><code>--pod-network-cidr=10.244.0.0/16</code> as    required by flannel,    which is the network plugin to be    installed later.</li> </ul> <p>Note that before initializing the control panel, the <code>kubelet</code> service is not running:</p> <pre><code># systemctl status kubelet\n\u25cf kubelet.service - kubelet: The Kubernetes Node Agent\n     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)\n    Drop-In: /usr/lib/systemd/system/kubelet.service.d\n             \u2514\u250010-kubeadm.conf\n     Active: activating (auto-restart) (Result: exit-code) since Sun 2024-05-12 11:57:52 CEST; 4s ago\n       Docs: https://kubernetes.io/docs/\n    Process: 243927 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=1/FAILURE)\n   Main PID: 243927 (code=exited, status=1/FAILURE)\n        CPU: 43ms\n\n# kubeadm init \\\n  --cri-socket=/run/containerd/containerd.sock \\\n  --pod-network-cidr=10.244.0.0/16\n\nW0512 12:51:35.811501 1168278 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme \"unix\" to the \"criSocket\" with value \"/run/containerd/containerd.sock\". Please update your configuration!\nI0512 12:51:36.359676 1168278 version.go:256] remote version is much newer: v1.30.0; falling back to: stable-1.26\n[init] Using Kubernetes version: v1.26.15\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local rapture] and IPs [10.96.0.1 10.0.0.2]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [localhost rapture] and IPs [10.0.0.2 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [localhost rapture] and IPs [10.0.0.2 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\n[apiclient] All control plane components are healthy after 5.501819 seconds\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Skipping phase. Please see --upload-certs\n[mark-control-plane] Marking the node rapture as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n[mark-control-plane] Marking the node rapture as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]\n[bootstrap-token] Using token: wjnord.g9lpm4heieilydn7\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 10.0.0.2:6443 --token wjnord.g9lpm4heieilydn7 \\\n        --discovery-token-ca-cert-hash sha256:bfe7582cfdf2d60c74c18da6aedfa7d5943b314cedd0ee25af996aecbd5a0c0f \n</code></pre> <p>Now <code>kubelet</code> is running, if only reporting that Container runtime network not ready:</p> <pre><code># systemctl status kubelet\n\u25cf kubelet.service - kubelet: The Kubernetes Node Agent\n     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)\n    Drop-In: /usr/lib/systemd/system/kubelet.service.d\n             \u2514\u250010-kubeadm.conf\n     Active: active (running) since Sun 2024-05-12 12:52:00 CEST; 3min 27s ago\n       Docs: https://kubernetes.io/docs/\n   Main PID: 1176576 (kubelet)\n      Tasks: 23 (limit: 38142)\n     Memory: 40.8M\n        CPU: 3.311s\n     CGroup: /system.slice/kubelet.service\n             \u2514\u25001176576 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.9\n\nMay 12 12:54:41 rapture kubelet[1176576]: E0512 12:54:41.015172 1176576 kubelet.go:2475] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\n</code></pre> <p>We can see all the containers already running:</p> <pre><code># crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps -a \nCONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD\nd87c94df74a28       6c84132270a33       5 minutes ago       Running             kube-proxy                0                   51688b22b87e1       kube-proxy-tszr7\n7513bd021911b       a0eed15eed449       5 minutes ago       Running             etcd                      0                   96b9a9ca6948e       etcd-rapture\n641df12c76b68       5626d764f1326       5 minutes ago       Running             kube-scheduler            0                   6fd11b2b44804       kube-scheduler-rapture\nef7ad9f56205e       61669ae28d85c       5 minutes ago       Running             kube-apiserver            0                   e460715dc3a14       kube-apiserver-rapture\n4244471dc3642       3b418eef6821c       5 minutes ago       Running             kube-controller-manager   0                   3f3e222d2483a       kube-controller-manager-rapture\n</code></pre> <p>Confirm the flag is sent in <code>/var/lib/kubelet/kubeadm-flags.env</code> to use the desired container runtime:</p> <pre><code># grep container-runtime /var/lib/kubelet/kubeadm-flags.env\nKUBELET_KUBEADM_ARGS=\"--container-runtime-endpoint=unix:/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.9\"\n</code></pre> <p>And check the cluster status. As <code>root</code> one can simply point <code>KUBECONFIG</code> to the cluster's <code>admin.conf</code>:</p> <pre><code># export KUBECONFIG=/etc/kubernetes/admin.conf\n# kubectl cluster-info\nKubernetes control plane is running at https://10.0.0.6:6443\nCoreDNS is running at https://10.0.0.6:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre> <p>To run <code>kubectl</code> as non-root, make a copy of that file under your own <code>~/.kube</code> directory:</p> <pre><code>$ mkdir $HOME/.kube\n$ sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config\n$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n$ ls -l $HOME/.kube/config\n-rw------- 1 coder coder 5659 Feb 19   2023 /home/coder/.kube/config\n$ kubectl cluster-info\nKubernetes control plane is running at https://10.0.0.6:6443\nCoreDNS is running at https://10.0.0.6:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre> <p>And now we can take care of the container runtime network.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#network-plugin","title":"Network plugin","text":"<p>Installing a Pod network add-on is indeed the very next step after initializing the control-plane node.</p> <p>Once again, far from being clear and offering a direction to follow, the documentation simply points to a non-exhaustive list of networking addons supported by Kubernetes, without much of a hint as to which one/s may be a good choice for each scenario.</p> <p>Having no desire to study multiple network plugins, lets just install Flannel, following the recommendation from  Computing for Geeks and LinuxConfig.</p> <pre><code>$ wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n\n$ kubectl apply -f kube-flannel.yml\nnamespace/kube-flannel created\nserviceaccount/flannel created\nclusterrole.rbac.authorization.k8s.io/flannel created\nclusterrolebinding.rbac.authorization.k8s.io/flannel created\nconfigmap/kube-flannel-cfg created\ndaemonset.apps/kube-flannel-ds created\n\n$ kubectl get all -n kube-flannel\nNAME                        READY   STATUS    RESTARTS   AGE\npod/kube-flannel-ds-bbhn8   1/1     Running   0          14s\n\nNAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/kube-flannel-ds   1         1         1       1            1           &lt;none&gt;          14s\n</code></pre>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#enable-single-node-cluster-as-worker-node","title":"Enable single-node cluster as Worker node","text":"<p>Before anything else is allowed to run in this cluster, Control plane node isolation must be setup. By default, for security reasons, the cluster will not schedule Pods to run in this node because it is running the control plane. This is reflected by the Taints:</p> <pre><code>$ kubectl get nodes\nNAME      STATUS   ROLES           AGE   VERSION\nrapture   Ready    control-plane   38m   v1.26.15\n\n$ kubectl describe node rapture\nName:               rapture\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=rapture\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"d6:46:a2:fe:00:f8\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.0.0.2\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 12 May 2024 12:51:57 +0200\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\n...\n</code></pre> <p>Remove this taint to allow other pods to be scheduled:</p> <pre><code>$ kubectl taint nodes --all node-role.kubernetes.io/control-plane-\nnode/rapture untainted\n\n$ kubectl describe node rapture | grep -i taint\nTaints:             &lt;none&gt;\n</code></pre>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#test-pod-scheduling","title":"Test Pod Scheduling","text":"<p>At this point the cluster is ready to run pods, here is a simple test:</p> <pre><code>$ kubectl apply -f https://k8s.io/examples/pods/commands.yaml\npod/command-demo created\n\n$ kubectl get all \nNAME               READY   STATUS      RESTARTS   AGE\npod/command-demo   0/1     Completed   0          10s\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   64m\n\n$ kubectl events pods\nLAST SEEN   TYPE     REASON      OBJECT             MESSAGE\n35m         Normal   NodeReady   Node/rapture       Node rapture status is now: NodeReady\n20s         Normal   Scheduled   Pod/command-demo   Successfully assigned default/command-demo to rapture\n19s         Normal   Pulling     Pod/command-demo   Pulling image \"debian\"\n15s         Normal   Pulled      Pod/command-demo   Successfully pulled image \"debian\" in 4.165178746s (4.165185238s including waiting)\n15s         Normal   Created     Pod/command-demo   Created container command-demo-container\n15s         Normal   Started     Pod/command-demo   Started container command-demo-container\n</code></pre> <p>With the cluster now ready to run pods and services, we can move on to installing more components that will be used by the actual services: MetalLB Load Balancer, Kubernets Dashboard, Ingress Controller, with HTTPS with Let\u2019s Encrypt and a LocalPath PV provisioner for simple persistent storage in local file systems.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#do-not-kubeadm-join","title":"DO NOT <code>kubeadm join</code>","text":"<p>Joining your nodes is only to be done by other nodes, no the master. The master node is already joined and <code>kubeadm</code> will not allow joining itself again.</p> <p>Trying to do so, by running the <code>kubeadm join</code> command provided by <code>kubeadm init</code> will fail:</p> <pre><code># kubeadm join 10.0.0.2:6443 --token wjnord.g9lpm4heieilydn7 \\\n        --discovery-token-ca-cert-hash sha256:bfe7582cfdf2d60c74c18da6aedfa7d5943b314cedd0ee25af996aecbd5a0c0f\n\n[preflight] Running pre-flight checks\nerror execution phase preflight: [preflight] Some fatal errors occurred:\n        [ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists\n        [ERROR Port-10250]: Port 10250 is in use\n        [ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists\n[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`\nTo see the stack trace of this error execute with --v=5 or higher\n</code></pre> <p>In order to resolve those pre-flight checks one would have to <code>kubeadm reset</code> which would only undo the work!</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#metallb-load-balancer","title":"MetalLB Load Balancer","text":"<p>A Load Balancer is going to be necessary for the Dashboard and other services, to expose individual services via open ports on the server (<code>NodePort</code>) or virtual IP addresses. Installation By Manifest is as simple as applying the provided manifest:</p> <pre><code>$ wget https://raw.githubusercontent.com/metallb/metallb/v0.14.5/config/manifests/metallb-native.yaml\n\n$ kubectl apply -f metallb-native.yaml\nnamespace/metallb-system created\ncustomresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/communities.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/servicel2statuses.metallb.io created\nserviceaccount/controller created\nserviceaccount/speaker created\nrole.rbac.authorization.k8s.io/controller created\nrole.rbac.authorization.k8s.io/pod-lister created\nclusterrole.rbac.authorization.k8s.io/metallb-system:controller created\nclusterrole.rbac.authorization.k8s.io/metallb-system:speaker created\nrolebinding.rbac.authorization.k8s.io/controller created\nrolebinding.rbac.authorization.k8s.io/pod-lister created\nclusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created\nclusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created\nconfigmap/metallb-excludel2 created\nsecret/metallb-webhook-cert created\nservice/metallb-webhook-service created\ndeployment.apps/controller created\ndaemonset.apps/speaker created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/metallb-webhook-configuration created\n</code></pre> <p>Note</p> <p>YAML files for MetalLB will be stored for future reference, under <code>1.26/metallb</code> in the GitHub repository.</p> <p>MetalLB remains idle until configured, which is done by deploying resources into its namespace (<code>metallb-system</code>). In this PC, a small range of IP addresses is advertised via Layer 2 Configuration, which does not not require the IPs to be bound to the network interfaces:</p> metallb/ipaddress_pools.yaml<pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: rapture-pool\n  namespace: metallb-system\nspec:\n  addresses:\n  - 192.168.0.220-192.168.0.229\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: l2-advert\n  namespace: metallb-system\n</code></pre> <p>Store this configuration in a separate, host-specific file (<code>ipaddress-pool-rapture.yaml</code>) and apply it:</p> <pre><code>$ kubectl apply -f ipaddress-pool-rapture.yaml\nipaddresspool.metallb.io/rapture-pool created\nl2advertisement.metallb.io/l2-advert created\n\n$ kubectl get l2advertisements.metallb.io -n metallb-system\nNAME        IPADDRESSPOOLS   IPADDRESSPOOL SELECTORS   INTERFACES\nl2-advert\n\n$ kubectl get ipaddresspools.metallb.io -n metallb-system\nNAME           AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES\nrapture-pool   true          false             [\"192.168.0.220-192.168.0.229\"]\n\n$ kubectl describe ipaddresspools.metallb.io rapture-pool -n metallb-system\nName:         rapture-pool\nNamespace:    metallb-system\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  metallb.io/v1beta1\nKind:         IPAddressPool\nMetadata:\n  Creation Timestamp:  2024-05-12T13:34:20Z\n  Generation:          1\n  Managed Fields:\n    API Version:  metallb.io/v1beta1\n    Fields Type:  FieldsV1\n    fieldsV1:\n      f:metadata:\n        f:annotations:\n          .:\n          f:kubectl.kubernetes.io/last-applied-configuration:\n      f:spec:\n        .:\n        f:addresses:\n        f:autoAssign:\n        f:avoidBuggyIPs:\n    Manager:         kubectl-client-side-apply\n    Operation:       Update\n    Time:            2024-05-12T13:34:20Z\n  Resource Version:  13194\n  UID:               979ae8fc-7cad-48df-9c2c-744c242a5075\nSpec:\n  Addresses:\n    192.168.0.220-192.168.0.229\n  Auto Assign:       true\n  Avoid Buggy I Ps:  false\nEvents:              &lt;none&gt;\n</code></pre> <p>The range 192.168.0.220-192.168.0.229 is based on the local DHCP server being configured to lease 228 addresses starting with 192.168.0.2. The current active leases are reserved so they don\u2019t change, and the range 122-140 are just not leased so far. The reason to use IPs from the leased range is that the router only allows adding port forwarding rules for those. This range is intentionally on the same network range and subnet as the DHCP server so that no routing is required to reach MetalLB IP addresses.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#kubernets-dashboard","title":"Kubernets Dashboard","text":"<p>The first service to leverage all the above infrastructure, including virtual IP addresses from the MetalLB Load Balancer, is the Kubernetes Dashboard.</p> <p>Once again, this service is deployed by manifest:</p> <pre><code>$ wget -O kubernetes-dashboard.yaml \\\n  https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n</code></pre> <p>Note</p> <p>YAML files for MetalLB will be stored for future reference, under <code>1.26/dashboard</code> in the GitHub repository.</p> <p>In this case to make the dashboard easily available in the local network, we edit the deployment to add <code>type: LoadBalancer</code> to the <code>kubernetes-dashboard</code> service (line 30):</p> kubernetes-dashboard.yaml<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 443\n      targetPort: 8443\n  selector:\n    k8s-app: kubernetes-dashboard\n</code></pre> <p>After applying this deployment, a virtual IP address will be assign to it, where the dashboard can be accessed:</p> <pre><code>$ kubectl apply -f kubernetes-dashboard.yaml\nnamespace/kubernetes-dashboard created\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n\n$ kubectl get svc -n kubernetes-dashboard\nNAME                        TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)         AGE\ndashboard-metrics-scraper   ClusterIP      10.107.131.104   &lt;none&gt;          8000/TCP        32s\nkubernetes-dashboard        LoadBalancer   10.107.235.155   192.168.0.229   443:30480/TCP   32s\n</code></pre> <p>The dashboard is accessible at https://192.168.0.229 but not exactly accessible without a login token:</p> <p></p> <p>The Authentication link is broken; the documentation that seems most relevant to authenticating Service Accounts to acess the dashboard is the Authenticating page, which does not quite explain how to create a Service Account, grant it access to the dashboard and then obtain a token. Neither does the documentation to Deploy and Access the Kubernetes Dashboard.</p> <p>For this, I really had to resort to the Creating Kubernetes Admin User for Accessing Dashboard article. To create a Service Account and grant it access to everything (the <code>cluster-admin</code> roles), apply the following manifest as <code>admin-sa-rbac.yaml</code></p> admin-sa-rbac.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: k8sadmin\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  namespace: kube-system\n  name: k8sadmin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: k8sadmin\n    namespace: kube-system\n</code></pre> <pre><code>$ kubectl apply -f admin-sa-rbac.yaml\nserviceaccount/k8sadmin created\nclusterrolebinding.rbac.authorization.k8s.io/k8sadmin created\n</code></pre> <p>Now one can create a token for the <code>k8sadmin</code> user and use it to access the dashboard:</p> <pre><code>$ kubectl create token k8sadmin -n kube-system\neyJhbGciOiJSUzI1NiIsImtpZCI6IkktZ05rdlNzcVNzb1F0S3VYYmo3TXlpMjdTU2ktUUhpSzQwcUNLOTBRMXMifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzE1NTMwNTQyLCJpYXQiOjE3MTU1MjY5NDIsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJrOHNhZG1pbiIsInVpZCI6IjJjMDliOTU3LTBhYzItNDQyNy05YjA2LWFiZDBhMmI5OTJhNyJ9fSwibmJmIjoxNzE1NTI2OTQyLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06azhzYWRtaW4ifQ.e6aAjUZvHvtZCN-QJAwFVt0I9pDklXC0La8RA-SjWTguEuAm0YbTgeOkRUKzb1q5_lkYIvpVyFN_E5z5IH1s6ygWdcWqS_RYtsmdQGr7iBTbpk-2cZb3bmt-mZkfDkYGgThNbzYoQ2gqXHSYnRtQKH2Spz6j_xt3G42IL1WGrfZIrdfBcLnkRBe1-l76qt1i0mq1wDwcSR2XAi3EpaOF-65f19__DYY4nt2JkhgxixpC-TZCLOXTfOAmbfbLoR07BmLZxw97Y85tvtYj1un7vQmgeDTBmWqtHIV4hYBlgH4ljUNIZg11GWECkOcd6UrHHtG8IG3REh87dAF2eD5Ysw\n</code></pre> <p></p> <p>Note</p> <p>At this point the SSL certificate is not yet valid, this will be addressed later by accessing the dashboard via an Ingress Controller with HTTPS.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#ingress-controller","title":"Ingress Controller","text":"<p>An Nginx Ingress Controller will be used to redirect HTTPS requests to different services depending on the <code>Host:</code> header, while all those requests will be hitting the same IP address.</p> <p>Download the deployment manifest following the Installation Guide from kubernetes/ingress-nginx</p> <pre><code>$ wget -O nginx-ingress-controller.yaml \\\n  https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/cloud/deploy.yaml\n</code></pre> <p>To serve HTTPS requests on a single IP address, the <code>ingress-nginx-controller</code> service must be created with <code>type: LoadBalancer</code>. As it happens, the <code>v1.10.1</code> manifest already has this property set in line 366:</p> nginx-ingress-controller.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/version: 1.10.1\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\nspec:\n  externalTrafficPolicy: Local\n  ipFamilies:\n  - IPv4\n  ipFamilyPolicy: SingleStack\n  ports:\n  - appProtocol: http\n    name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - appProtocol: https\n    name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n  type: LoadBalancer\n</code></pre> <p>After applying this deployment, another virtual IP address is assigned to the <code>ingress-nginx-controller</code> service and there is NGinx happily returning 404 Not found:</p> <pre><code>$ kubectl apply -f  nginx-ingress-controller.yaml\nnamespace/ingress-nginx created\nserviceaccount/ingress-nginx created\nserviceaccount/ingress-nginx-admission created\nrole.rbac.authorization.k8s.io/ingress-nginx created\nrole.rbac.authorization.k8s.io/ingress-nginx-admission created\nclusterrole.rbac.authorization.k8s.io/ingress-nginx created\nclusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created\nrolebinding.rbac.authorization.k8s.io/ingress-nginx created\nrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\nconfigmap/ingress-nginx-controller created\nservice/ingress-nginx-controller created\nservice/ingress-nginx-controller-admission created\ndeployment.apps/ingress-nginx-controller created\njob.batch/ingress-nginx-admission-create created\njob.batch/ingress-nginx-admission-patch created\ningressclass.networking.k8s.io/nginx created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created\n\n$ kubectl get svc -n ingress-nginx\nNAME                                 TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                      AGE\ningress-nginx-controller             LoadBalancer   10.97.59.176   192.168.0.228   80:30706/TCP,443:30199/TCP   28s\ningress-nginx-controller-admission   ClusterIP      10.98.208.0    &lt;none&gt;          443/TCP                      28s\n\n$ curl http://192.168.0.228/\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#https-with-lets-encrypt","title":"HTTPS with Let\u2019s Encrypt","text":"<p>The Kubernetes setup for Let\u2019s Encrypt in Lexicon was riddled with first-time issues, but I was able to Add Ingress for Kubernetes Dashboard and setup en entirely automated monthly renewal of certificates. However, the entire Let\u2019s Encrypt system to obtain and renew certificates relies on external requests reaching the NGinx controller, which is precisely some to avoid here.</p> <p>If and when it becomes necessary, the setup can be replicated even if that involves manually renewing certifidates each month. While any other external requests would not be routed to this PC, <code>/etc/hosts</code> can be used to map rapture.uu.am to the NGinx LoadBalancer IP (192.168.0.228) so that external-looking URLs can be used for this internal traffic.</p> <p>In the meantime, <code>/etc/hosts</code> can be used to map specific subdomains to the LoadBalancer IPs of individual services, e.g. k8s.rapture.uu.am can be mapped to 192.168.0.229 so that the dashboard is accessible at https://k8s.rapture.uu.am/.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#localpath-pv-provisioner","title":"LocalPath PV provisioner","text":"<p>By default, a Kubernetes cluster is not set up to provide storage to pods. This was problematic for a while, especially when trying to  deploy GitLab with Helm without first taking the time to Configure storage for the GitLab chart.</p> <p>Before finding out that the GitLab Helm chart requires more CPU to be allocated than available, I spent some time to setup a LocalPath PV provisioner in Lexicon but this was never really used. In the end, all services deployed in Lexicon are using <code>storageClassName: manual</code> with <code>hostPath</code> pointing to specific directories in local file systems.</p> <p>This may become necessary in the future, e.g. when installing GitLab, at which point the setup can be replicated in this PC, but that may be more appropriate to capture along with the work to fullfil all other GitLab chart prerequisites.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#audiobookshelf","title":"Audiobookshelf","text":"<p>Audiobookshelf may be a good example to check much better a service runs having many more CPU cores available. For this test, the deployment is limited to just audiobooks (no podcasts), stored under a different path (<code>/home/new-ssd</code>) for faster storage (see monitoring below), and served via <code>LoadBalancer</code> IP:</p> Kubernetes deployment: <code>audiobookshelf.yaml</code> audiobookshelf.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: audiobookshelf\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-config\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/audiobookshelf/config\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-metadata\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/audiobookshelf/metadata\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-audiobooks\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/new-ssd/audio/Audiobooks\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-config\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-config\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-metadata\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-metadata\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-audiobooks\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-audiobooks\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: audiobookshelf\n  name: audiobookshelf\n  namespace: audiobookshelf\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: audiobookshelf\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: audiobookshelf\n    spec:\n      containers:\n        - image: ghcr.io/advplyr/audiobookshelf:latest\n          imagePullPolicy: Always\n          name: audiobookshelf\n          env:\n          - name: PORT\n            value: \"8888\"\n          ports:\n          - containerPort: 8888\n          resources: {}\n          stdin: true\n          tty: true\n          volumeMounts:\n          - mountPath: /config\n            name: audiobookshelf-config\n          - mountPath: /metadata\n            name: audiobookshelf-metadata\n          - mountPath: /audiobooks\n            name: audiobookshelf-audiobooks\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 1006\n            runAsGroup: 1006\n      restartPolicy: Always\n      volumes:\n      - name: audiobookshelf-config\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-config\n      - name: audiobookshelf-metadata\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-metadata\n      - name: audiobookshelf-audiobooks\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-audiobooks\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: audiobookshelf-svc\n  namespace: audiobookshelf\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 80\n      targetPort: 8888\n  selector:\n    app: audiobookshelf\n</code></pre> <p>Note</p> <p>YAML files for Rapture-specific deployments will be stored for future reference, under <code>1.26/rapture</code> in the GitHub repository, while the original deployment for Lexicon will be under <code>1.26/lexicon</code>.</p> <p>As in Lexicon, the service runs as a dedicated <code>audiobookshelf</code> user with its own persistent storage under <code>/home/k8s/audiobookshelf</code> which must be created manually:</p> <pre><code># useradd -u 1006 -d /home/k8s/audiobookshelf -s /usr/sbin/nologin audiobookshelf\n# mkdir -p /home/k8s/audiobookshelf/config /home/k8s/audiobookshelf/metadata\n# chown -R audiobookshelf.audiobookshelf /home/k8s/audiobookshelf\n# ls -lan /home/k8s/audiobookshelf\ntotal 0\ndrwxr-xr-x 1 1006 1006 28 May 12 20:51 .\ndrwxr-xr-x 1    0    0 28 May 12 20:51 ..\ndrwxr-xr-x 1 1006 1006  0 May 12 20:51 config\ndrwxr-xr-x 1 1006 1006  0 May 12 20:51 metadata\n</code></pre> <p>Once the dedicate user is ready, apply the manifest:</p> <pre><code>$ kubectl apply -f audiobookshelf.yaml\nnamespace/audiobookshelf created\npersistentvolume/audiobookshelf-pv-config created\npersistentvolume/audiobookshelf-pv-metadata created\npersistentvolume/audiobookshelf-pv-audiobooks created\npersistentvolumeclaim/audiobookshelf-pvc-config created\npersistentvolumeclaim/audiobookshelf-pvc-metadata created\npersistentvolumeclaim/audiobookshelf-pvc-audiobooks created\ndeployment.apps/audiobookshelf created\nservice/audiobookshelf-svc created\n\n$ kubectl get svc -n audiobookshelf\nNAME                 TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE\naudiobookshelf-svc   LoadBalancer   10.102.116.166   192.168.0.221   80:31059/TCP   47s\n</code></pre> <p>The service is soon available at http://192.168.0.221/, or more conveniently at http://abs.rapture.uu.am/ after mapping this in <code>/etc/hosts</code>. After setting the password for the <code>root</code> user and logging in, create a new library (Audiobooks) and scan it:</p> <p></p> <p></p> <p>At this point something interesting happened: originally the audiobooks were read from <code>/home/depot</code> which is slow storage, so I stopped the deployment with <code>kubectl delete</code>, updated the path to read audiobooks from the faster SSD storage, applied the manifest again and the scan finished much faster.</p> <p>Monitoring shows that scanning a library is mostly IO-bound, with CPU usage going only near 400% (at a short spike) for <code>ffprobe</code> but never above 100% for the service itself (<code>node</code>):</p> <p></p> <p>Showing the list of books seems to go a lot faster, even though it does not seem to require much of the CPU. Perhaps the big difference is in the amount of metadata to fetch from storage: Audiobookshelf in Lexicon has much more detailed metadata, Audiobookshelf in Rapture has only what is embeded in the audio files.</p>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#migration-from-lexicon","title":"Migration from Lexicon","text":"<p>Copying all that metadata from Lexicon to Rapture could be an interesting exercise, and possibly a way to back it up. It is possible to replace the entire <code>/home/k8s/audiobookshelf</code> in Rapture with that from Lexicon, so long as the original  contents are entirely removed:</p> <pre><code># kubectl delete -f audiobookshelf.yaml \nnamespace \"audiobookshelf\" deleted\npersistentvolume \"audiobookshelf-pv-config\" deleted\npersistentvolume \"audiobookshelf-pv-metadata\" deleted\npersistentvolume \"audiobookshelf-pv-audiobooks\" deleted\npersistentvolumeclaim \"audiobookshelf-pvc-config\" deleted\npersistentvolumeclaim \"audiobookshelf-pvc-metadata\" deleted\npersistentvolumeclaim \"audiobookshelf-pvc-audiobooks\" deleted\ndeployment.apps \"audiobookshelf\" deleted\nservice \"audiobookshelf-svc\" deleted\n\n# cp -a /home/k8s/audiobookshelf/ /home/k8s/audiobookshelf_backup\n# rm -rf /home/k8s/audiobookshelf/*/*\n# rsync -uva lexicon:/home/k8s/audiobookshelf/* /home/k8s/audiobookshelf/\n\n# kubectl apply -f audiobookshelf.yaml \nnamespace/audiobookshelf created\npersistentvolume/audiobookshelf-pv-config created\npersistentvolume/audiobookshelf-pv-metadata created\npersistentvolume/audiobookshelf-pv-audiobooks created\npersistentvolumeclaim/audiobookshelf-pvc-config created\npersistentvolumeclaim/audiobookshelf-pvc-metadata created\npersistentvolumeclaim/audiobookshelf-pvc-audiobooks created\ndeployment.apps/audiobookshelf created\nservice/audiobookshelf-svc created\n</code></pre> <p>This migration leaves the service in Rapture no longer accessible for the <code>root</code> user, while others keep their access and even reading process. The service runs faster on Rapture, especially when listing Authors with all their photos, but even then Rapture does take a while to load all the photos and runs hotter even though the service barely reaches 150% of CPU utilization.</p> <p>The process can be used in case of need, albeit having to reset the <code>root</code> password by editing the SQLite database:</p> <pre><code># kubectl delete -f audiobookshelf.yaml \n# cd /home/k8s/audiobookshelf/config/\n# cp absdatabase.sqlite absdatabase.sqlite.bak\n# sqlite3 absdatabase.sqlite\n</code></pre> <p>In the <code>sqlite</code> prompt:</p> <pre><code>update users set pash=NULL where username='root';\n.exit\n</code></pre> <p>Then <code>kubectl apply</code> again and the user <code>root</code> can login without entering a password; but should then set one immediately.</p> <p>The database and metadata can be re-synced again, so long as the service is not running:</p> <pre><code># kubectl delete -f audiobookshelf.yaml \n# rsync -uva lexicon:/home/k8s/audiobookshelf/* /home/k8s/audiobookshelf/\n# kubectl apply -f audiobookshelf.yaml \n</code></pre>"},{"location":"blog/2024/05/12/single-node-kubernetes-cluster-on-ubuntu-studio-desktop-rapture/#plex-media-server","title":"Plex Media Server","text":"<p>Migrating the Plex Media Server in Rapture to Kubernetes would be a repetition of the same exercise already done for the  Plex Media Server deployment in Lexicon, with only a few changes:</p> <ul> <li>Audio files are stored under <code>/home/raid/audio</code> but some     of the libraries are pointing to <code>/home/depot/audio</code></li> <li>Video files are stored under <code>/home/raid/video</code> but some     of the libraries are pointing to <code>/home/depot/video</code> while     others are pointing to <code>/media/video</code> (a very old path).</li> <li>The <code>loadBalancerIP</code> must be set to the next available IP     from MetalLB. Run     <code>kubectl get svc -A</code> and check the <code>EXTERNAL-IP</code> column     to find which ones are in use already, then pick the next.</li> <li>Adjust the <code>PUID</code> variable to match the UID of the <code>plex</code>     user (999).</li> <li>A fresh new <code>PLEX_CLAIM</code> token has to be obtained from     plex.tv/claim as it expires     in just 4 minutes.</li> </ul> <p>Note</p> <p>This setup assumes the <code>plex</code> user (and group) already exists with ID 998; otherwise either create them with that ID or update the <code>PGID</code> and <code>PUID</code> variables in the manifest to match the actual <code>plex</code> user (and group) IDs.</p> <p>Create the plex-owned directory for its database (<code>/config</code>), stop the old server and copy over its database:</p> <pre><code># systemctl stop plexmediaserver.service\n# systemctl disable plexmediaserver.service\nRemoved /etc/systemd/system/multi-user.target.wants/plexmediaserver.service.\n\n# mkdir -p /home/k8s/plexmediaserver\n# cp -a /var/lib/plexmediaserver/Library /home/k8s/plexmediaserver/\n# chown -R plex.plex /home/k8s/plexmediaserver\n</code></pre> <p>And now, just before deploying the new server, go to plex.tv/claim to get a fresh new claim token and set the <code>PLEX_CLAIM</code> variable to it; then apply the deployment:</p> Kubernetes deployment: <code>plex-media-server.yaml</code> plex-media-server.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: plexserver\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: plexserver-pv-config\n  namespace: plexserver\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/plexmediaserver\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: plexserver-pv-data-audio\n  namespace: plexserver\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 500Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/raid/audio\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: plexserver-pv-data-video\n  namespace: plexserver\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 500Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/raid/video\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: plexserver-pvc-config\n  namespace: plexserver\nspec:\n  storageClassName: manual\n  volumeName: plexserver-pv-config\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: plexserver-pvc-data-audio\n  namespace: plexserver\nspec:\n  storageClassName: manual\n  volumeName: plexserver-pv-data-audio\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 500Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: plexserver-pvc-data-video\n  namespace: plexserver\nspec:\n  storageClassName: manual\n  volumeName: plexserver-pv-data-video\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 500Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: plexserver\n  name: plexserver\n  namespace: plexserver\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: plexserver\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: plexserver\n    spec:\n      volumes:\n      - name: plex-config\n        persistentVolumeClaim:\n          claimName: plexserver-pvc-config\n      - name: data-audio\n        persistentVolumeClaim:\n          claimName: plexserver-pvc-data-audio\n      - name: data-video\n        persistentVolumeClaim:\n          claimName: plexserver-pvc-data-video\n      containers:\n      - env:\n        - name: PLEX_CLAIM\n          value: claim-vwSJ5pLyxPQVKKRPvVDk\n        - name: PGID\n          value: \"998\"\n        - name: PUID\n          value: \"999\"\n        - name: VERSION\n          value: latest\n        - name: TZ\n          value: Europe/Amsterdam\n        image: ghcr.io/linuxserver/plex\n        imagePullPolicy: Always\n        name: plexserver\n        ports:\n        - containerPort: 32400\n          name: pms-web\n          protocol: TCP\n        - containerPort: 32469\n          name: dlna-tcp\n          protocol: TCP\n        - containerPort: 1900\n          name: dlna-udp\n          protocol: UDP\n        - containerPort: 3005\n          name: plex-companion\n          protocol: TCP  \n        - containerPort: 5353\n          name: discovery-udp\n          protocol: UDP  \n        - containerPort: 8324\n          name: plex-roku\n          protocol: TCP  \n        - containerPort: 32410\n          name: gdm-32410\n          protocol: UDP\n        - containerPort: 32412\n          name: gdm-32412\n          protocol: UDP\n        - containerPort: 32413\n          name: gdm-32413\n          protocol: UDP\n        - containerPort: 32414\n          name: gdm-32414\n          protocol: UDP\n        resources: {}\n        stdin: true\n        tty: true\n        volumeMounts:\n        - mountPath: /config\n          name: plex-config\n        - mountPath: /home/depot/audio\n          name: data-audio\n        - mountPath: /home/depot/video\n          name: data-video\n        - mountPath: /home/raid/audio\n          name: data-audio\n        - mountPath: /home/raid/video\n          name: data-video\n        - mountPath: /media/video\n          name: data-video\n      restartPolicy: Always\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: plex-udp\n  namespace: plexserver\n  annotations:\n    metallb.universe.tf/allow-shared-ip: plexserver\nspec:\n  selector:\n    app: plexserver\n  ports:\n  - port: 1900\n    targetPort: 1900\n    name: dlna-udp\n    protocol: UDP\n  - port: 5353\n    targetPort: 5353\n    name: discovery-udp\n    protocol: UDP\n  - port: 32410\n    targetPort: 32410\n    name: gdm-32410\n    protocol: UDP\n  - port: 32412\n    targetPort: 32412\n    name: gdm-32412\n    protocol: UDP\n  - port: 32413\n    targetPort: 32413\n    name: gdm-32413\n    protocol: UDP\n  - port: 32414\n    targetPort: 32414\n    name: gdm-32414\n    protocol: UDP\n  type: LoadBalancer\n  loadBalancerIP: 192.168.0.221  # Should be one from the MetalLB range and the same as the TCP service.\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: plex-tcp\n  namespace: plexserver\n  annotations:\n    metallb.universe.tf/allow-shared-ip: plexserver\nspec:\n  selector:\n    app: plexserver\n  ports:                      \n  - port: 32400\n    targetPort: 32400\n    name: pms-web\n    protocol: TCP\n  - port: 3005\n    targetPort: 3005\n    name: plex-companion\n  - port: 8324\n    name: plex-roku\n    targetPort: 8324  \n    protocol: TCP  \n  - port: 32469\n    targetPort: 32469\n    name: dlna-tcp\n    protocol: TCP\n  type: LoadBalancer\n  loadBalancerIP: 192.168.0.221  # Should be one from the MetalLB range and the same as the UDP service.\n</code></pre> <pre><code>$ kubectl apply -f plex-media-server.yaml \nnamespace/plexserver created\npersistentvolume/plexserver-pv-config created\npersistentvolume/plexserver-pv-data-audio created\npersistentvolume/plexserver-pv-data-video created\npersistentvolumeclaim/plexserver-pvc-config created\npersistentvolumeclaim/plexserver-pvc-data-audio created\npersistentvolumeclaim/plexserver-pvc-data-video created\ndeployment.apps/plexserver created\nservice/plex-udp created\nservice/plex-tcp created\n\n$ kubectl get svc -n plexserver\nNAME       TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                                                                                         AGE\nplex-tcp   LoadBalancer   10.100.110.149   192.168.0.221   32400:30939/TCP,3005:30159/TCP,8324:30278/TCP,32469:30767/TCP                                   7m19s\nplex-udp   LoadBalancer   10.97.4.51       192.168.0.221   1900:32446/UDP,5353:31607/UDP,32410:31569/UDP,32412:32246/UDP,32413:31588/UDP,32414:32331/UDP   7m19s\n</code></pre> <p>Plex web app is now available at http://192.168.0.221:32400/web and the next steps are to</p> <ol> <li>Log in with the same Plex account from the old server.</li> <li>Enable remote access.</li> </ol> <p>Normally Plex is able to establish the necessary port forwarding via UPnP, but in this case that doesn\u2019t seem to work. The port forwarding rule can be added manually to the router and then Manually specify public port in the Plex settings under Settings &gt; Remote Access.</p> <p>Note</p> <p>This step may require connecting directly to the web interface from the local network, via http://192.168.0.221:32400/web.</p> <p>However, to do this MetalLB must be updated to include at least a few IP addresses overlapping with the range supported by the router (e.g. 192.168.0.221) so that it can be reached via port 23240.</p> <pre><code>$ kubectl delete -f rapture/plex-media-server.yaml\n\n$ kubectl apply -f metallb/ipaddress-pool-rapture.yaml \nipaddresspool.metallb.io/rapture-pool configured\nl2advertisement.metallb.io/l2-advert unchanged\n\n$ kubectl apply -f rapture/plex-media-server.yaml\n\n$ kubectl get svc -A | grep -v '&lt;none&gt;'\nNAMESPACE              NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                                                                                         AGE\naudiobookshelf         audiobookshelf-svc                   LoadBalancer   10.100.173.108   192.168.0.221   80:32696/TCP                                                                                    4h58m\ningress-nginx          ingress-nginx-controller             LoadBalancer   10.97.59.176     192.168.0.228   80:30706/TCP,443:30199/TCP                                                                      6d23h\nkubernetes-dashboard   kubernetes-dashboard                 LoadBalancer   10.107.235.155   192.168.0.229   443:30480/TCP                                                                                   7d\nplexserver             plex-tcp                             LoadBalancer   10.97.83.71      192.168.0.222   32400:30505/TCP,3005:30299/TCP,8324:31277/TCP,32469:31987/TCP                                   6d16h\nplexserver             plex-udp                             LoadBalancer   10.100.30.101    192.168.0.222   1900:31197/UDP,5353:32649/UDP,32410:32214/UDP,32412:32151/UDP,32413:32713/UDP,32414:32543/UDP   6d16h\n</code></pre>"},{"location":"blog/2024/05/19/self-hosted-accountancy-with-firefly-iii/","title":"Self-hosted accountancy with Firefly III","text":"<p>Keep track of expenses and stuff is hard, thankless work.</p> <p>Over the years I've done it, with varying degrees of success, using a variety of solutions including my first ever LAMP project, right after learning PHP and MySQL, and once my bank's own built-in solutions until they unceremonously took it away with no notice.</p> <p>After this last disappointment, I decided to go the self-hosted way taking inspiration from the list of Money, Budgeting &amp; Management solutions by Awesome-Selfhosted. Based on comments in several forums, I decided to first try with Firefly III.</p>"},{"location":"blog/2024/05/19/self-hosted-accountancy-with-firefly-iii/#deployment","title":"Deployment","text":"<p>Before deploying this applications, persistant storage needs to be prepared for the database and the application itself:</p> <pre><code># mkdir -p /home/k8s/firefly-iii/mysql /home/k8s/firefly-iii/upload\n# chown -R 33.33 /home/k8s/firefly-iii/mysql\n# chown -R www-data.www-data /home/k8s/firefly-iii/upload\n# ls -ln /home/k8s/firefly-iii\ntotal 0\ndrwxr-xr-x 1 100 101 390 May 19 19:49 mysql\ndrwxrwxr-x 1  33  33   0 May 19 16:18 upload\n</code></pre> <p>Note</p> <p>User and groupd IDs are enforced by the docker images. Enforcing different users seems to be too much of a headache.</p> <p>Firefly III on Kubernetes. includes deployments for each component, of which I will be using the most basic ones:</p> <ul> <li><code>mysql.yaml</code> for the database.</li> <li><code>firefly-iii.yaml</code> for the web application.</li> <li><code>ingress-firefly-iii.yaml</code> for the ingress.</li> </ul> <p>While these are meant to be used with <code>kustomize</code>, I will keep it simpler by putting it all together in my own <code>firefly-iii.yaml</code>:</p> Kubernetes deployment: <code>firefly-iii.yaml</code> firefly-iii.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: firefly-iii\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: firefly-iii-pv-mysql\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 20Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/firefly-iii/mysql\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: firefly-iii-pvc-mysql\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  storageClassName: manual\n  volumeName: firefly-iii-pv-mysql\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 20Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: firefly-iii-mysql\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  selector:\n    matchLabels:\n      app: firefly-iii\n      tier: mysql\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: firefly-iii\n        tier: mysql\n    spec:\n      containers:\n      - image: yobasystems/alpine-mariadb:latest\n        imagePullPolicy: Always\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"**************************\"\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: firefly-iii-pvc-mysql\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: firefly-iii-mysql-svc\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  type: NodePort\n  ports:\n  - port: 3306\n    nodePort: 30306\n    targetPort: 3306\n  selector:\n    app: firefly-iii\n    tier: mysql\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: firefly-iii-pv-upload\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/firefly-iii/upload\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: firefly-iii-pvc-upload\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  storageClassName: manual\n  volumeName: firefly-iii-pv-upload\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 10Gi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: firefly-iii-svc\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  type: NodePort\n  ports:\n  - port: 8080\n    nodePort: 30080\n    targetPort: 8080\n  selector:\n    app: firefly-iii\n    tier: frontend\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: firefly-iii\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  selector:\n    matchLabels:\n      app: firefly-iii\n      tier: frontend\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: firefly-iii\n        tier: frontend\n    spec:\n      containers:\n      - image: fireflyiii/core\n        imagePullPolicy: Always\n        name: firefly-iii\n        env:\n        - name: APP_ENV\n          value: \"local\"\n        - name: APP_KEY\n          value: \"********************************\"\n        - name: DB_HOST\n          value: firefly-iii-mysql-svc\n        - name: DB_CONNECTION\n          value: mysql\n        - name: DB_DATABASE\n          value: \"fireflyiii\"\n        - name: DB_USERNAME\n          value: \"root\"\n        - name: DB_PASSWORD\n          value: \"**************************\"\n        - name: TRUSTED_PROXIES\n          value: \"**\"\n        ports:\n        - containerPort: 8080\n          name: firefly-iii\n        volumeMounts:\n        - mountPath: \"/var/www/html/storage/upload\"\n          name: firefly-iii-upload \n      volumes:\n        - name: firefly-iii-upload\n          persistentVolumeClaim:\n            claimName: firefly-iii-pvc-upload\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: firefly-iii-ingress\n  namespace: firefly-iii\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: firefly-iii-svc\n    nginx.ingress.kubernetes.io/proxy-buffer-size: \"16k\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: ffi.ssl.uu.am\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: firefly-iii-svc\n            port:\n              number: 8080\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - ffi.ssl.uu.am\n</code></pre> <p>Note</p> <p>The <code>APP_KEY</code> value must have exactly 32 characters, as noted in  #2193: Can't get started - hitting an \"encryption key not specified error\", also better explained in monicahq/monica #6449.</p> <pre><code>$ kubectl apply -f firefly-iii.yaml \nnamespace/firefly-iii created\npersistentvolume/firefly-iii-pv-mysql created\npersistentvolumeclaim/firefly-iii-pvc-mysql created\ndeployment.apps/firefly-iii-mysql created\nservice/firefly-iii-mysql-svc created\npersistentvolume/firefly-iii-pv-upload created\npersistentvolumeclaim/firefly-iii-pvc-upload created\nservice/firefly-iii-svc created\ndeployment.apps/firefly-iii created\ningress.networking.k8s.io/firefly-iii-ingress created\n\n$ kubectl -n firefly-iii get all\nNAME                                    READY   STATUS    RESTARTS   AGE\npod/cm-acme-http-solver-vfwlp           1/1     Running   0          16s\npod/firefly-iii-6c8dbdd45f-jqdsv        1/1     Running   0          16s\npod/firefly-iii-mysql-68f59d48f-chhbw   1/1     Running   0          16s\n\nNAME                            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nservice/firefly-iii-mysql-svc   NodePort   10.97.148.234   &lt;none&gt;        3306:30306/TCP   16s\nservice/firefly-iii-svc         NodePort   10.99.161.143   &lt;none&gt;        8080:30080/TCP   16s\n\nNAME                                READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/firefly-iii         1/1     1            1           16s\ndeployment.apps/firefly-iii-mysql   1/1     1            1           16s\n\nNAME                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/firefly-iii-6c8dbdd45f        1         1         1       16s\nreplicaset.apps/firefly-iii-mysql-68f59d48f   1         1         1       16s\n</code></pre> <p>Once the service has started up, initialized the database and everything else, one can finally visit  https://ffi.ssl.uu.am/ to create an account and get started:</p> <p>Note</p> <p>The <code>cm-acme-http-solver</code> is responsible for obtaining a valid certificated for this service; the HTTPS connection will be secure only after this pod has finished its job. </p> <p></p> <p>Note</p> <p>The password must have at least 16 characters, which is more than Chrome will use when suggesting a strong password.</p> <p>From this point on, RTFM will be probably the best way to go.</p>"},{"location":"blog/2024/05/26/self-hosted-ebook-library-with-komga/","title":"Self-hosted eBook library with Komga","text":"<p>After weeks of using Audiobookshelf to listen to audiobooks daily, it dawned on me that the PDF reader was probably not the best I could be using.</p> <p>Then is also dawned on me that Audible is not my only source of eBooks; I have a few from HumbleBundle deals and a few indipendent authors who sell PDF files directly, as well as a small collection of appliance manuals and electronics datasheets. All these files have been scattered all over the place, never having a common home where they could all be conveniently navigated and read.</p> <p>Until now. Enter... Komga.</p> <p></p> <p>Komga is described as a media server for your comics, mangas, BDs, magazines and eBooks. Most importantly, for me, is that it handles individual books, file names and metadata better than a few others.</p>"},{"location":"blog/2024/05/26/self-hosted-ebook-library-with-komga/#installation","title":"Installation","text":"<p>To deploy Komga in Kubernetes, the setup is essentially a fork of Audiobookshelf deployment.</p> <p>A single phisical volume to store the application's databases is needed, while the books are read from <code>/home/depot/books/</code>:</p> <pre><code># mkdir /home/k8s/komga/config\n# chown -R audiobookshelf.audiobookshelf /home/k8s/komga/\n</code></pre> <p>Once the directories and files a ready, we run Komga as the same non-privileged user as Audiobookshelf, based on Komga's <code>docker-compose</code>:</p> Kubernetes deployment: <code>komga.yaml</code> komga.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: komga\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: komga-pv-config\n  namespace: komga\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/komga/config\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: komga-pv-books\n  namespace: komga\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/depot/books\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: komga-pvc-config\n  namespace: komga\nspec:\n  storageClassName: manual\n  volumeName: komga-pv-config\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: komga-pvc-books\n  namespace: komga\nspec:\n  storageClassName: manual\n  volumeName: komga-pv-books\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: komga\n  name: komga\n  namespace: komga\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: komga\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: komga\n    spec:\n      containers:\n        - image: gotson/komga\n          imagePullPolicy: Always\n          name: komga\n          args: [\"user\", \"1006:1006\"]\n          env:\n          - name: TZ\n            value: \"Europe/Madrid\"\n          ports:\n          - containerPort: 25600\n          resources: {}\n          stdin: true\n          tty: true\n          volumeMounts:\n          - mountPath: /config\n            name: komga-config\n          - mountPath: /data\n            name: komga-books\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 1006\n            runAsGroup: 1006\n      restartPolicy: Always\n      volumes:\n      - name: komga-config\n        persistentVolumeClaim:\n          claimName: komga-pvc-config\n      - name: komga-books\n        persistentVolumeClaim:\n          claimName: komga-pvc-books\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: komga-svc\n  namespace: komga\nspec:\n  type: NodePort\n  ports:\n  - port: 25600\n    nodePort: 30600\n    targetPort: 25600\n  selector:\n    app: komga\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: komga-ingress\n  namespace: komga\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: komga-svc\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: komga.ssl.uu.am\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: komga-svc\n                port:\n                  number: 25600\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - komga.ssl.uu.am\n</code></pre> <pre><code>$ kubectl apply -f komga.yaml\nnamespace/komga created\npersistentvolume/komga-pv-config created\npersistentvolume/komga-pv-books created\npersistentvolumeclaim/komga-pvc-config created\npersistentvolumeclaim/komga-pvc-books created\ndeployment.apps/komga created\nservice/komga-svc created\ningress.networking.k8s.io/komga-ingress created\n\n$ kubectl -n komga get all\nNAME                         READY   STATUS    RESTARTS   AGE\npod/komga-5c865d6587-tvlt2   1/1     Running   0          28s\n\nNAME                TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE\nservice/komga-svc   NodePort   10.103.226.126   &lt;none&gt;        25600:30600/TCP   8s\n\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/komga   1/1     1            1           29s\n\nNAME                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/komga-5c865d6587   1         1         1       28s\n</code></pre>"},{"location":"blog/2024/05/26/self-hosted-ebook-library-with-komga/#configuration","title":"Configuration","text":"<p>Once the application is running for the first time, register an account (which will be the administrator) and start creating libraries based on the files available under <code>/data</code>.</p> <p>When creating libraries in Komga, an important step if there are individual books, is to put then under one (or more) directories that will be recognized as One-Shots.</p> <p>My libray is rather small and exclusively made ouf of one-shots, although I found it useful to store a few files as a kind of series:</p> <ul> <li>Art and LEGO: eBooks from HumbleBundle deals.</li> <li>Audible: visual aids (PDF files) from Audible.</li> <li>Cosplay: books from    Kamui Cosplay and   Punished Props</li> <li>Including a couple of sets of templates, which make sense to     store as kind of small series.</li> <li>Terry Pratchett: a small collection of   Discworld Fanfics,   mirrored at home just for convenience.</li> </ul> <p>These are organized under the <code>/data</code> volume as follows:</p> <pre><code>/data/Art/books/\n/data/Audible/books/\n/data/Cosplay/books/\n/data/Cosplay/templates/Jade Rabbit/\n/data/Cosplay/templates/Ripper Axe/\n/data/LEGO/books/\n/data/Terry.Pratchett/books/\n</code></pre> <p>All individual books are stored under a <code>books</code> subdirectory under each library directory, and then this name is set as the One-Shots directory for every library:</p> <p></p>"},{"location":"blog/2024/05/26/self-hosted-ebook-library-with-komga/#ereaders","title":"eReaders","text":"<p>While Komga comes with a good web-based eBook reader, this is only good when used on a laptop or desktop PC with a big enough screen. To read books on a tablet or phone, this web UI is hard to use because once it is zoomed in enough to make the text readable turning pages is akward, confusing and sometimes frustrating. A much better experience is often available through several eReaders devices or applications.</p> <p>Warning</p> <p>If access to the Komga web UI is gated behind an access control layer, such as Cloudflare Tunnels or  Pomerium, additional configuration will be necessary to allow unauthenticated access to Komga API endpoints (relying only on Komga's own authentication).</p>"},{"location":"blog/2024/05/26/self-hosted-ebook-library-with-komga/#kobo","title":"Kobo","text":"<p>Kobo eReaders may well be the best, open-source friendly eReaders out there. They can be easily tweaked to sync with a Komga server instead of the official Kobo servers, and Komga has the ability to proxy syncing on to the official Kobo servers, so the reader has access to both a private collection on a Komga server and books and audiobooks purchased from Kobo.</p> <p>As anticipated in the Troubleshooting section of the Komga guide for Kobo sync, Sync fails with <code>Invalid character found in the request target</code>, which is resolved by adding an environment variable in the above deployment:</p> komga.yaml<pre><code>    spec:\n      containers:\n        - image: gotson/komga\n          imagePullPolicy: Always\n          name: komga\n          args: [\"user\", \"1006:1006\"]\n          env:\n          - name: TZ\n            value: \"Europe/Madrid\"\n          - name: SERVER_TOMCAT_RELAXEDQUERYCHARS\n            value: \"[,]\"\n</code></pre>"},{"location":"blog/2024/05/26/self-hosted-ebook-library-with-komga/#komic-for-ios","title":"Komic for iOS","text":"<p>Komic seems to be the best, free to use (and not limited behind paywall or subscription) eReader iOS app for iPad. Although slightly hidden, it hes the option to filter books by library:</p> <p></p>"},{"location":"blog/2024/05/26/self-hosted-ebook-library-with-komga/#ereaders-for-android","title":"eReaders for Android","text":"<p>There are only a few eReader Android apps that are actually easy to install on a non-rooted Android phone, through the Google Play store, without side-loading an APK, and actually work. Of those few, there is maybe one that is not too annoying to use.</p> <p>All of them require first that the Komga catalog is available as an OPDS feed.</p>"},{"location":"blog/2024/05/26/self-hosted-ebook-library-with-komga/#enable-opds-catalog","title":"Enable OPDS catalog","text":"<p>Komga works with OPDS eReaders, whether they use OPDS v1 or v2, but getting Komga to produce a well-formed OPDS catalog requires a few adjustments to the aboove deployment to make it use the correct URL in the ODPS feed by adding two environment variables:</p> <ol> <li> <p><code>KOMGA_SERVER_BASE_URL</code> set to the external URL Komga is reachable at.</p> </li> <li> <p><code>SERVER_FORWARD_HEADERS_STRATEGY</code> set to <code>FRAMEWORK</code> to make Komga trust the headers     coming from the reverse proxy gating access to it.</p> </li> </ol> komga.yaml<pre><code>    spec:\n      containers:\n        - image: gotson/komga\n          imagePullPolicy: Always\n          name: komga\n          args: [\"user\", \"1006:1006\"]\n          env:\n          - name: TZ\n            value: \"Europe/Madrid\"\n          - name: KOMGA_SERVER_BASE_URL\n            value: \"https://komga.ssl.uu.am\"\n          - name: SERVER_FORWARD_HEADERS_STRATEGY\n            value: \"FRAMEWORK\"\n          - name: SERVER_TOMCAT_RELAXEDQUERYCHARS\n            value: \"[,]\"\n</code></pre> Why is <code>SERVER_FORWARD_HEADERS_STRATEGY</code> necessary. <p>Even with the <code>KOMGA_SERVER_BASE_URL</code> set to the external URL, Komga's OPDS generator uses dynamic link building. When unaware of a reverse proxy gating access, Komga sees incoming requests from an internal IP address and uses that IP to construct the \"absolute\" URLs found in the XML feed. Setting <code>SERVER_FORWARD_HEADERS_STRATEGY</code> to <code>FRAMEWORK</code> tells it to \"unwrap\" the proxy headers and treat <code>komga.ssl.uu.am</code> as its own identity.</p> <p>Apply the changes to the deployment and check the variables are set:</p> <pre><code>$ kubectl apply -f komga.yaml \nnamespace/komga unchanged\npersistentvolume/komga-pv-config unchanged\npersistentvolume/komga-pv-books unchanged\npersistentvolumeclaim/komga-pvc-config unchanged\npersistentvolumeclaim/komga-pvc-books unchanged\ndeployment.apps/komga configured\nservice/komga-svc unchanged\n\n$ kubectl describe pod komga -n komga | grep -A3 Environment\n    Environment:\n      KOMGA_SERVER_BASE_URL:            https://komga.ssl.uu.am\n      SERVER_FORWARD_HEADERS_STRATEGY:  FRAMEWORK\n      SERVER_TOMCAT_RELAXEDQUERYCHARS:  [,]\n</code></pre> <p>Reverse proxies gating access to Komga also must be adjusted to set the required headers.</p> <p>For the original Nginx ingress above, this can be done by adding these annotations:</p> komga.yaml<pre><code>metadata:\n  name: komga-ingress\n  namespace: komga\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: komga-svc\n    nginx.ingress.kubernetes.io/proxy-set-headers: \"true\"\n    nginx.ingress.kubernetes.io/x-forwarded-prefix: \"/\"\n    # Ensure the Host header is passed correctly\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      proxy_set_header Host $host;\n      proxy_set_header X-Forwarded-Proto $scheme;\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n</code></pre> <p>After replacing Nginx with Pomerium, just one annotation is needed to set the required headers in the new Pomerium ingress:</p> pomerium-ingress/komga.yaml<pre><code>metadata:\n  name: komga-pomerium-ingress\n  namespace: komga\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.pomerium.io/pass_identity_headers: true\n    ingress.pomerium.io/set_request_headers: |\n      X-Forwarded-Proto: \"https\"\n      X-Forwarded-Host: \"komga.ssl.uu.am\"\nspec:\n  ingressClassName: pomerium\n</code></pre> <p>Once all the above changes are applied to Komga and the relevant reverse proxy, the OPDS catalog should be available at the following URLs:</p> <ul> <li>https://komga.ssl.uu.am/opds/v1.2/catalog</li> <li>https://komga.ssl.uu.am/opds/v2/catalog</li> </ul> <p>Both feeds must be using the externally facing URL rather than an internal IP address:</p> v1.2 catalog (XML)v2 catalog (JSON) <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;feed xmlns=\"http://www.w3.org/2005/Atom\"&gt;\n  &lt;id&gt;root&lt;/id&gt;\n  &lt;title&gt;Komga OPDS catalog&lt;/title&gt;\n  &lt;updated&gt;2026-01-04T13:55:36.737358992+01:00&lt;/updated&gt;\n  &lt;author&gt;\n    &lt;name&gt;Komga&lt;/name&gt;\n    &lt;uri&gt;https://github.com/gotson/komga&lt;/uri&gt;\n  &lt;/author&gt;\n  &lt;link type=\"application/atom+xml;profile=opds-catalog;kind=navigation\" rel=\"self\" href=\"https://komga.ssl.uu.am/opds/v1.2/catalog\"/&gt;\n  &lt;link type=\"application/atom+xml;profile=opds-catalog;kind=navigation\" rel=\"start\" href=\"https://komga.ssl.uu.am/opds/v1.2/catalog\"/&gt;\n  &lt;link type=\"application/opensearchdescription+xml\" rel=\"search\" href=\"https://komga.ssl.uu.am/opds/v1.2/search\"/&gt;\n  &lt;link type=\"application/opds+json\" rel=\"alternate\" href=\"https://komga.ssl.uu.am/opds/v2/catalog\"/&gt;\n  &lt;entry&gt;\n    &lt;title&gt;Keep Reading&lt;/title&gt;\n    ...\n</code></pre> <pre><code>{\n  \"metadata\": {\n    \"title\": \"All libraries - Recommended\",\n    \"modified\": \"2026-01-04T15:44:57.858656045+01:00\"\n  },\n  \"links\": [\n    {\n      \"rel\": \"self\",\n      \"href\": \"https://komga.ssl.uu.am/opds/v2/libraries\"\n    },\n    {\n      \"title\": \"Home\",\n      \"rel\": \"start\",\n      \"href\": \"https://komga.ssl.uu.am/opds/v2/catalog\",\n      \"type\": \"application/opds+json\"\n    },\n    ...\n</code></pre> <p>Either of these can be used to add the Komga server as a remote (network) library in eReader applications, although the v1.2 feed is recommended for maximum compatibility.</p>"},{"location":"blog/2024/05/26/self-hosted-ebook-library-with-komga/#pocketbook","title":"PocketBook","text":"<p>PocketBook seems to be the only Android eReader app that is acceptable; easy to install, actually works, and is not obnoxiously ridden by disgusting ads.</p> <p>To connect the app to the Komga library:</p> <ol> <li>Tap the Menu icon () in the top-left corner.</li> <li>Tap on Network Libraries and then tap the <code>+</code> icon usually found inside a     folder icon () at the top of the screen.</li> <li>Enter the Komga librart details:<ul> <li>URL: https://komga.ssl.uu.am/opds/v1.2/catalog     (for maximum compatibility).</li> <li>Catalog Name: enter a name (e.g., \"My Komga\").</li> </ul> </li> <li>When prompted, enter the Komga username and password.</li> </ol> <p>Once the Komga server has been added as a network library, books can be downloaded to the phone and they will later show up on the initial view when opening the app later.</p> Network libraries Komga libraries Art library Deck"},{"location":"blog/2024/05/26/self-hosted-ebook-library-with-komga/#discarded","title":"Discarded","text":"<p>The few other Android eReader apps that are actually easy to install on a non-rooted Android phone, through the Google Play store, without side-loading an APK, and actually work, fail miserably in the user experience they chose to offer; they chose to be obnoxiously annoying.</p> <ul> <li>Librera starts with the most annoying \"Manage your data\" dialog with a long list     of \"partners\" who claim to make \"legitimate use\" of your data (yeah, right), then     displays adds a big button blended into the ui in a blatant attempt to confuse users     into tapping on the ad when looking for the option to add a network library, which is     hidden behind a tiny <code>+</code> icon on the top right, and then (disgusting) ads are     displayed all the time taking away valuable screen space.</li> <li>Moon+ Reader not only hides the UI controls to add a network library a few levels     deep in the UI, it also makes those books not available anywhere else than in that     off-the-main-track section. Not does this app contain ads, it's also quite horrible     about it; upon closing a book, it displays multiple, non-skippable, disgusting     ads. Being bombarded with ads from the some of the worst, beyond borderline illegal apps; the only sane way to use this app is to avoid leaving a book and     instead close the app entirely each time.</li> </ul>"},{"location":"blog/2024/05/26/self-hosted-ebook-library-with-komga/#alternatives","title":"Alternatives","text":"<p>Komga is not the only Free Software application available for this purpose, so it is worth mentioning why it was chosen over the alternatives.</p> <p>Priot to setting up Komga, I spent some time trying the same with kavitareader.com. The Kubernetes deployment was essentially the same, based on linuxserver.io/images/docker-kavita. The main drawback that kept me from using this one long-term was that it really is built for series and really not for individual books.</p> <p>Admitedly, that was the only one I tried among other alternatives mentioned here. Others include:</p> <ul> <li>Calibre Web, available as janeczku/calibre-web   or   linuxserver/calibre-web.   It only supports a single library and requires using the   Calibre desktop application. While this wouldn't be a problem   for myself, it would prevent kids from reading the books because   they only have school laptops where Calibre cannot be installed.</li> <li>Librum   could be a good option, were it not for a similar requirement   to install and run a client application. This is not a web app.</li> </ul>"},{"location":"blog/2024/06/01/installing-utau-on-ubuntu-studio-2204/","title":"Installing Utau on Ubuntu Studio 22.04","text":"<p>The young artist wanted to try Utau, which as of May 24 was on version v0.4.19\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30e9\u30fc\u4fee\u6b63\u7248.</p> <p>Downloaded the installer utau0419cInstaller.zip and followed more or less steps found in the (now defunct) Utau forum at utaforum.net/threads/utau-in-ubuntu.620.</p> <p>Run PlayOnLinux with the <code>LANG</code> set to Japanese, as required by Utau:</p> <pre><code>LANG=ja_JP.utf8 playonlinux\n</code></pre> <p>Created a 64-bit environment running Windows 10 and run the installer, clicked on an N button because everything else was unreadable. Eventually the installer finished, created a shortcut for <code>utau.exe</code>.</p> <p>Sadly, the result of running the program after installation was only a UI full of unreadable blocky characters.</p> <p>Tried to fix this by intalling <code>allfonts</code> following steps in activating Winetricks:</p> <pre><code>WINEPREFIX=~/.PlayOnLinux/wineprefix/UTAU winetricks allfonts\n</code></pre> <p>After this, running the problem with <code>playonlinux</code> was OK.</p> <p>Installing UTAU in Linux are other instructions that might work (better), possibly to be tested in the future.</p>"},{"location":"blog/2024/06/30/self-hosted-time-tracking-with-activitywatch/","title":"Self-hosted time tracking with ActivityWatch","text":"<p>A big chunk of my time is spent at the computer, also during my downtime, and there is no clear separation between study, chores, entertainment, etc. Work happens at other computers, where time flies by sometimes at ridiculous speeds. I often find myself wondering where did my day/week go?</p> <p>For some time I've been using a badly-cobbled-together solution with Bash scripts doing a few basic operations, all the time:</p> <ol> <li>Detect when the screen saver is active (AFk).</li> <li>Capture the id and title of the active windown (when not AFK).</li> <li>Store those details in plain-text log files.</li> <li>Aggregate those by window id into CSV files.</li> <li>Import CSV files into a spreadsheet to clean it up.</li> </ol> <p>The results have been barely enough to keep track of where my weeks go, which has already been a relief; when someone (often me) asks \"why so little progress on X?\", I can check the spreadsheet and answer with numbers: because this week, out of 40 hours, ...</p> <p>At home, however, the results have been very underwhelming. This is due to completely different behaviour patterns, which is where I hope ActivityWatch will help.</p>"},{"location":"blog/2024/06/30/self-hosted-time-tracking-with-activitywatch/#installation","title":"Installation","text":"<p>To install ActivityWatch on Ubuntu (and other Debian-based systems) one can simply install the <code>.deb</code> package provided as part of ActivityWatch releases:</p> <pre><code>$ wget -S \\\n    https://github.com/ActivityWatch/activitywatch/releases/download/v0.13.1/activitywatch-v0.13.1-linux-x86_64.deb\n$ sudo dpkg -i activitywatch-v0.13.1-linux-x86_64.deb\n</code></pre> <p>Once installed, make sure <code>/opt/activitywatch/aw-qt</code> is run when logging into the desktop session. For instance, in KDE Plasma, add ActivityWatch in the Autostart section of System Setting: click + Add... then Add Application... then enter the name (ActivityWatch) and it should be available under Utilities.</p> <p>While ActivityWatch is running, the web UI is available at http://localhost:5600/. After about 2 hours of having this running, while  going about my business, I can see it's already capturing my activities just as messing about with audiobooks and learning languages:</p> <p></p>"},{"location":"blog/2024/06/30/self-hosted-time-tracking-with-activitywatch/#configuration","title":"Configuration","text":"<p>As noted by ActivityWatch itself, 65% of the time today is uncategorized, and this can be addressed by using the Category Builder.</p> <p>At first nothing shows up in the Categorization Helper, because it defauls to the unknown hostname. Click on Show options and select the appropriate hostname, then a list of common words show with their associated times:</p> <p></p> <p>New categories can be added here to match these words, but in this section the UI does not allow creating new parent categories:</p> <p></p> <p>Following the link to the Settings page one can create a whole new branch, e.g. here a new Learning root category is created for all activites related to learning and studying:</p> <p></p> <p>To create additional categories, it is useful to follow the Uncategorized link under Top Categories in the dashboard, to see what makes up most of the uncategorized time. For instance, here it is clear that most of the time is spent in activities related to Audiobooks:</p> <p></p> <p>After a few more tweaks and, of course, finding the dark theme in the Setting, this is finally looking much better:</p> <p></p>"},{"location":"blog/2024/06/30/self-hosted-time-tracking-with-activitywatch/#additional-wathers","title":"Additional Wathers","text":"<p>A number of Watchers are available for more accurate tracking. Sadly, none of the following worked out.</p>"},{"location":"blog/2024/06/30/self-hosted-time-tracking-with-activitywatch/#steam","title":"Steam","text":"<p>aw-watcher-steam attempts to log the timeline of Steam game activity by fetching play time from the Steam API.</p> <p>Installating is easy enough:</p> <pre><code>$ git clone https://github.com/Edwardsoen/aw-watcher-steam.git\n$ cd aw-watcher-steam/\n$ pip install .\n...\nSuccessfully installed aw-client-0.5.13 aw-core-0.5.16 aw-watcher-steam-0.0.1 charset-normalizer-3.3.2 deprecation-2.1.0 iso8601-1.1.0 peewee-3.17.5 persist-queue-0.8.1 platformdirs-3.10.0 requests-2.32.3 rfc3339-validator-0.1.4 strict-rfc3339-0.7 timeslot-0.1.2\n</code></pre> <p>Or may be note, see Issue #4: How to install? where no conclusive answer has been found. In particular, marcinsmialek's comment points out this watcher does not making any easier to group all games under a parent category (can't find a way to add a rule that would match the Steam games in general). Thus, this watcher seem to only provide better names for the same activities.</p>"},{"location":"blog/2024/06/30/self-hosted-time-tracking-with-activitywatch/#media-player","title":"Media Player","text":"<p>Media Player watcher captures information about media playback. It supports any player which can report its status to the system and be controllable by tray or standard multimedia keys.</p> <p>This is an interesting watcher, just not so much if one is more interested about traking time \"watching movies\" or \"listining to X\" rather than tracking time spent on each piece of video or audio.</p>"},{"location":"blog/2024/06/30/self-hosted-time-tracking-with-activitywatch/#vs-code","title":"VS Code","text":"<p>It would be good to have better reporting in some areas, for instance working in VS Code. Sadly, aw-watcher-vscode is not available in code-server for the Web, and ActivityWatchVS is not even found at all.</p>"},{"location":"blog/2024/06/30/self-hosted-time-tracking-with-activitywatch/#appendix","title":"Appendix","text":"<p>This is the script that does steps 1-3 above, added here for  reference because it has a few interesting bits. Aggregating this data is rather trivial so not really interesting.</p> <p>``` bash numlines=\"1\"</p>"},{"location":"blog/2024/06/30/self-hosted-time-tracking-with-activitywatch/#binbash","title":"!/bin/bash","text":""},{"location":"blog/2024/06/30/self-hosted-time-tracking-with-activitywatch/#_1","title":"Self-hosted time tracking with ActivityWatch","text":""},{"location":"blog/2024/06/30/self-hosted-time-tracking-with-activitywatch/#poll-every-second-for-the-name-of-the-active-window","title":"Poll every second for the name of the active window.","text":"<p>LDIR=\"${HOME}/activitylogs\" TSTM=\"$(date +'%Y-%m-%d-%H-%M-%S')\" LOGF=\"${LDIR}/${TSTM}.txt\"</p> <p>mkdir -p \"${LDIR}\" while true; do   # Skip if screen is locked (KDE Plasma)   if qdbus org.freedesktop.ScreenSaver /ScreenSaver org.freedesktop.ScreenSaver.GetActive 2&gt;/dev/null |     grep -q true; then     continue   fi   # Take a screenshot, but only if the screen is not locked.   if dbus-send --session --dest=org.freedesktop.ScreenSaver --type=method_call --print-reply /org/freedesktop/ScreenSaver org.freedesktop.ScreenSaver.GetActive 2&gt;/dev/null |     grep -q 'boolean false'; then     continue   fi   # Skip if screen is locked (XFCE)   if dbus-send --session --dest=org.xfce.ScreenSaver --type=method_call --print-reply /org/xfce/ScreenSaver org.xfce.ScreenSaver.GetActive 2&gt;/dev/null |     grep -q true; then     continue   fi   wid=$(xdotool getwindowfocus -f)   if [[ -z \"${wid}\" ]]; then     continue   fi   wname=$(xdotool getwindowname \"${wid}\")   echo \"${wid} ${wname}\" &gt;&gt;\"${LOGF}\"   sleep 1 done ```</p>"},{"location":"blog/2024/07/10/self-hosted-inventory-with-homebox/","title":"Self-hosted inventory with Homebox","text":"<p>Homebox is the inventory and organization system built for the Home User which sounds exactly like what I want, to keep track of  where things go.</p>"},{"location":"blog/2024/07/10/self-hosted-inventory-with-homebox/#installation","title":"Installation","text":"<p>Note: the original project from  hay-kot was archived last month, so this post is based on the currently active fork at github.com/sysadminsmedia/homebox.</p> <p>To deploy Homebox in Kubernetes, the setup is yet another fork of the Audiobookshelf deployment. A single phisical volume to store the application's databases is needed, while the books are read from <code>/home/depot/books/</code>:</p> <pre><code># mkdir /home/k8s/homebox\n# chown -R 65532:65532 /home/k8s/homebox/\n</code></pre> <p>This simple setup is based on Homebox's Docker-Dompose documentation:</p> Kubernetes deployment: <code>homebox.yaml</code> homebox.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: homebox\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: homebox-pv-data\n  namespace: homebox\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/homebox\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: homebox-pvc-data\n  namespace: homebox\nspec:\n  storageClassName: manual\n  volumeName: homebox-pv-data\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: homebox\n  name: homebox\n  namespace: homebox\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: homebox\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: homebox\n    spec:\n      containers:\n        - image: ghcr.io/sysadminsmedia/homebox:latest-rootless\n          imagePullPolicy: Always\n          name: homebox\n          env:\n          - name: TZ\n            value: \"Europe/Madrid\"\n          ports:\n          - containerPort: 7745\n          resources: {}\n          stdin: true\n          tty: true\n          volumeMounts:\n          - mountPath: /data\n            name: homebox-data\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 65532\n            runAsGroup: 65532\n      restartPolicy: Always\n      volumes:\n      - name: homebox-data\n        persistentVolumeClaim:\n          claimName: homebox-pvc-data\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: homebox-svc\n  namespace: homebox\nspec:\n  type: NodePort\n  ports:\n  - port: 7745\n    nodePort: 30745\n    targetPort: 7745\n  selector:\n    app: homebox\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: homebox-ingress\n  namespace: homebox\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: homebox-svc\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: homebox.ssl.uu.am\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: homebox-svc\n                port:\n                  number: 7745\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - homebox.ssl.uu.am\n</code></pre> <pre><code>$ kubectl apply -f homebox.yaml\nnamespace/homebox created\npersistentvolume/homebox-pv-data created\npersistentvolumeclaim/homebox-pvc-data created\ndeployment.apps/homebox created\nservice/homebox-svc created\ningress.networking.k8s.io/homebox-ingress created\n\n$ kubectl -n homebox get all\nNAME                            READY   STATUS    RESTARTS   AGE\npod/cm-acme-http-solver-jk4cr   1/1     Running   0          31s\npod/homebox-7fb9d44d48-wxj6q    1/1     Running   0          29m\n\nNAME                                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nservice/cm-acme-http-solver-2xqr8   NodePort   10.102.88.72    &lt;none&gt;        8089:32649/TCP   31s\nservice/homebox-svc                 NodePort   10.96.235.150   &lt;none&gt;        7745:30745/TCP   91m\n\nNAME                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/homebox   1/1     1            1           91m\n\nNAME                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/homebox-7fb9d44d48   1         1         1       29m\n</code></pre>"},{"location":"blog/2024/07/10/self-hosted-inventory-with-homebox/#configuration","title":"Configuration","text":"<p>Homebox is available on  lexicon:30745 locally, and at https://homebox.ssl.uu.am remotely. The initial login screen allows self-registering an account and beyon that there is not much more to explain.</p> <p></p>"},{"location":"blog/2024/08/16/router-over-heating-issues/","title":"Router over-heating issues","text":"<p>One day, completely out of nowhere, both download and upload speeds dropped to about 12 Mb/s flat, suggesting the speed was being throttled very much intentionally:</p> <p></p> <p>It turned out the broadband router, despite being designed to be standing vertically on a flat surface, was suffering from dust clogging its vertical airflow. This was eventually bad enough that its fan was running at full speed constantly, at which point the noise was unmistakable. After cleaning the air intake at the bottom, which had barely a single milimeter to let air flow in, the router was able to cool down and download speeds were restored sharply:</p> <p></p> <p>The restricted airflow had also compounded with abnormaly high room temperature. This was promptly addressed and the air around the router and lexicon went down several (Celcius) degrees, but still room temperature retains a tendency to slowly climp up:</p> <p></p> <p>To alleviate the issue on the router and lexicon, a Silverstone Air Penetrator AP181, 180 mm. fan, combined with a  LM2577 DC-DC Voltage Step-Up (boost) module to power it from a regular USB port (lexicon). The fan speed can be adjusted in 3 steps, at its maximum speed it lowers lexicon CPU temperatures by about 10\u00baC:</p> <p></p> <p></p>"},{"location":"blog/2024/08/22/checking-deployments-before-upgrading-kubeadm-clusters/","title":"Checking deployments before upgrading kubeadm clusters","text":"<p>I've been meaning to upgrade Kubernetes ever since  Kubernetes Certificate Expired about a year after setting up the Kubernetes cluster in lexicon.</p> <p>Upgrading kubeadm clusters is a bit of an involved process and a little intimidating at first, so it pays to use the tools available to anticipate problems with API deprecations from one Kubernetes version to the next.</p>"},{"location":"blog/2024/08/22/checking-deployments-before-upgrading-kubeadm-clusters/#install-go","title":"Install Go","text":"<p>Several of the following tools require Go and in some cases a fairly recent version, e.g. <code>kubeconform</code> requires v1.22. Ubuntu Server 22.04 packages only provide v1.18, to install newer versions use <code>snap</code> instead:</p> <pre><code># snap install --classic --channel=latest/stable go\ngo 1.23.0 from Canonical\u2713 installed\n</code></pre> <p>For convenience, add <code>~/go/bin/</code> to your <code>PATH</code>.</p>"},{"location":"blog/2024/08/22/checking-deployments-before-upgrading-kubeadm-clusters/#kubepug","title":"Kubepug","text":"<p>Deprecations AKA KubePug - Pre UpGrade (Checker) is one of the simplest tools to use.</p> <pre><code>$ go install github.com/kubepug/kubepug@latest\n\n$ kubepug 2&gt;/dev/null \\\n  --k8s-version=v1.27 \\\n  --input-file ~/src/lexicon-deployments/\n\nNo deprecated or deleted APIs found\n\nKubepug validates the APIs using Kubernetes markers. To know what are the deprecated and deleted APIS it checks, please go to https://kubepug.xyz/status/\n\n$ kubepug 2&gt;/dev/null \\\n  --k8s-version=v1.27 \\\n  --input-file ~/src/kubernetes-deployments/\n\nNo deprecated or deleted APIs found\n\nKubepug validates the APIs using Kubernetes markers. To know what are the deprecated and deleted APIS it checks, please go to https://kubepug.xyz/status/\n</code></pre> <p>Same with <code>--k8s-version=v1.32</code>.</p>"},{"location":"blog/2024/08/22/checking-deployments-before-upgrading-kubeadm-clusters/#kubent","title":"Kubent","text":"<p>kube-no-trouble</p> <pre><code># sh -c \"$(curl -sSL https://git.io/install-kubent)\"\n&gt;&gt;&gt; kubent installation script &lt;&lt;&lt;\n&gt; Detecting latest version\n&gt; Downloading version 0.7.3\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 13.2M  100 13.2M    0     0  10.6M      0  0:00:01  0:00:01 --:--:-- 44.7M\n&gt; Done. kubent was installed to /usr/local/bin/.\n</code></pre> <p>To check the running environment:</p> <pre><code>$ kubent -t 1.27 \n12:29AM INF &gt;&gt;&gt; Kube No Trouble `kubent` &lt;&lt;&lt;\n12:29AM INF version 0.7.3 (git sha 57480c07b3f91238f12a35d0ec88d9368aae99aa)\n12:29AM INF Initializing collectors and retrieving data\n12:29AM INF Target K8s version is 1.27.0\n12:29AM INF Retrieved 41 resources from collector name=Cluster\n12:29AM INF Retrieved 0 resources from collector name=\"Helm v3\"\n12:29AM INF Loaded ruleset name=custom.rego.tmpl\n12:29AM INF Loaded ruleset name=deprecated-1-16.rego\n12:29AM INF Loaded ruleset name=deprecated-1-22.rego\n12:29AM INF Loaded ruleset name=deprecated-1-25.rego\n12:29AM INF Loaded ruleset name=deprecated-1-26.rego\n12:29AM INF Loaded ruleset name=deprecated-1-27.rego\n12:29AM INF Loaded ruleset name=deprecated-1-29.rego\n12:29AM INF Loaded ruleset name=deprecated-1-32.rego\n12:29AM INF Loaded ruleset name=deprecated-future.rego\n</code></pre> <p>To check manifest files:</p> <pre><code>$ kubent -t 1.27 $(find -name \"*.yaml\")\n12:34AM INF &gt;&gt;&gt; Kube No Trouble `kubent` &lt;&lt;&lt;\n12:34AM INF version 0.7.3 (git sha 57480c07b3f91238f12a35d0ec88d9368aae99aa)\n12:34AM INF Initializing collectors and retrieving data\n12:34AM INF Target K8s version is 1.27.0\n12:34AM INF Retrieved 41 resources from collector name=Cluster\n12:34AM INF Retrieved 0 resources from collector name=\"Helm v3\"\n12:34AM INF Loaded ruleset name=custom.rego.tmpl\n12:34AM INF Loaded ruleset name=deprecated-1-16.rego\n12:34AM INF Loaded ruleset name=deprecated-1-22.rego\n12:34AM INF Loaded ruleset name=deprecated-1-25.rego\n12:34AM INF Loaded ruleset name=deprecated-1-26.rego\n12:34AM INF Loaded ruleset name=deprecated-1-27.rego\n12:34AM INF Loaded ruleset name=deprecated-1-29.rego\n12:34AM INF Loaded ruleset name=deprecated-1-32.rego\n12:34AM INF Loaded ruleset name=deprecated-future.rego\n</code></pre> <p>Same in <code>kubernetes-deployments</code> and <code>lexicon-deployments</code>, with <code>-t 1.27</code> and <code>-t 1.32</code>.</p>"},{"location":"blog/2024/08/22/checking-deployments-before-upgrading-kubeadm-clusters/#kubeconform","title":"kubeconform","text":"<p>kubeconform seems to show the opposite picture: errors are found even for the currently running Kubernetes version, which is running all these deployments just fine.</p> <pre><code>$ go install github.com/yannh/kubeconform/cmd/kubeconform@latest\n\n$ kubectl  version --output=yaml\nclientVersion:\n  buildDate: \"2024-03-14T01:05:39Z\"\n  compiler: gc\n  gitCommit: 1649f592f1909b97aa3c2a0a8f968a3fd05a7b8b\n  gitTreeState: clean\n  gitVersion: v1.26.15\n  goVersion: go1.21.8\n  major: \"1\"\n  minor: \"26\"\n  platform: linux/amd64\n\n$ cd ~/src/lexicon-deployments\n\n$ kubeconform -summary -strict -kubernetes-version 1.26.15 $(find -name \"*.yaml\")\n./metallb/ipaddress_pools.yaml - L2Advertisement l2-advert failed validation: could not find schema for L2Advertisement\n./metallb/ipaddress_pools.yaml - IPAddressPool production failed validation: could not find schema for IPAddressPool\n./metallb/metallb-native.yaml - CustomResourceDefinition addresspools.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./metallb/metallb-native.yaml - CustomResourceDefinition bgppeers.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./metallb/metallb-native.yaml - CustomResourceDefinition communities.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./metallb/metallb-native.yaml - CustomResourceDefinition ipaddresspools.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./metallb/metallb-native.yaml - CustomResourceDefinition l2advertisements.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./metallb/metallb-native.yaml - CustomResourceDefinition bfdprofiles.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./metallb/metallb-native.yaml - CustomResourceDefinition bgpadvertisements.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./gitea/values-with-ingress.yaml - failed validation: error while parsing: missing 'kind' key\n./gitea/values.yaml - failed validation: error while parsing: missing 'kind' key\nSummary: 71 resources found in 9 files - Valid: 60, Invalid: 0, Errors: 11, Skipped: 0\n\n$ cd ~/src/kubernetes-deployments\n\n$ kubeconform \\\n  -summary -strict \\\n  -kubernetes-version 1.26.15 \\\n  $(find -name \"*.yaml\")\n./1.26/metallb/metallb-native.yaml - CustomResourceDefinition bfdprofiles.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./1.26/metallb/metallb-native.yaml - CustomResourceDefinition bgpadvertisements.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./1.26/metallb/metallb-native.yaml - CustomResourceDefinition bgppeers.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./1.26/metallb/metallb-native.yaml - CustomResourceDefinition communities.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./1.26/metallb/metallb-native.yaml - CustomResourceDefinition ipaddresspools.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./1.26/metallb/metallb-native.yaml - CustomResourceDefinition servicel2statuses.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./1.26/metallb/metallb-native.yaml - CustomResourceDefinition l2advertisements.metallb.io failed validation: could not find schema for CustomResourceDefinition\n./1.26/metallb/ipaddress-pool-lexicon.yaml - IPAddressPool production failed validation: could not find schema for IPAddressPool\n./1.26/metallb/ipaddress-pool-rapture.yaml - IPAddressPool rapture-pool failed validation: could not find schema for IPAddressPool\n./1.26/metallb/ipaddress-pool-lexicon.yaml - L2Advertisement l2-advert failed validation: could not find schema for L2Advertisement\n./1.26/metallb/ipaddress-pool-rapture.yaml - L2Advertisement l2-advert failed validation: could not find schema for L2Advertisement\nSummary: 152 resources found in 16 files - Valid: 141, Invalid: 0, Errors: 11, Skipped: 0\n</code></pre> <p>Even if the above errors can be safely ignored, many more are returned when running with a higher target version, e.g. 1.32:</p> <pre><code>$ cd ~/src/lexicon-deployments\n\n$ kubeconform -summary -strict -kubernetes-version 1.32.2 */*.yaml\ndashboard/kubernetes-dashboard.yaml - Service kubernetes-dashboard failed validation: could not find schema for Service\ndashboard/kubernetes-dashboard-ingress.yaml - Ingress kubernetes-dashboard-ingress failed validation: could not find schema for Ingress\ndashboard/kubernetes-dashboard.yaml - Namespace kubernetes-dashboard failed validation: could not find schema for Namespace\ndashboard/kubernetes-dashboard.yaml - ServiceAccount kubernetes-dashboard failed validation: could not find schema for ServiceAccount\ndashboard/kubernetes-dashboard.yaml - Secret kubernetes-dashboard-certs failed validation: could not find schema for Secret\ndashboard/kubernetes-dashboard.yaml - Secret kubernetes-dashboard-key-holder failed validation: could not find schema for Secret\ndashboard/kubernetes-dashboard.yaml - Secret kubernetes-dashboard-csrf failed validation: could not find schema for Secret\ndashboard/kubernetes-dashboard.yaml - ConfigMap kubernetes-dashboard-settings failed validation: could not find schema for ConfigMap\ndashboard/kubernetes-dashboard.yaml - Role kubernetes-dashboard failed validation: could not find schema for Role\ndashboard/kubernetes-dashboard.yaml - RoleBinding kubernetes-dashboard failed validation: could not find schema for RoleBinding\ndashboard/kubernetes-dashboard.yaml - Service dashboard-metrics-scraper failed validation: could not find schema for Service\ndashboard/kubernetes-dashboard.yaml - ClusterRole kubernetes-dashboard failed validation: could not find schema for ClusterRole\ngitea/values-with-ingress.yaml - failed validation: error while parsing: missing 'kind' key\ngitea/values.yaml - failed validation: error while parsing: missing 'kind' key\ningress-nginx/nginx-ingress-controller-deploy.yaml - Namespace ingress-nginx failed validation: could not find schema for Namespace\ningress-nginx/nginx-ingress-controller-deploy.yaml - ServiceAccount ingress-nginx failed validation: could not find schema for ServiceAccount\ningress-nginx/nginx-ingress-controller-deploy.yaml - ServiceAccount ingress-nginx-admission failed validation: could not find schema for ServiceAccount\ningress-nginx/nginx-ingress-controller-deploy.yaml - Role ingress-nginx failed validation: could not find schema for Role\ningress-nginx/nginx-ingress-controller-deploy.yaml - Role ingress-nginx-admission failed validation: could not find schema for Role\ningress-nginx/nginx-ingress-controller-deploy.yaml - ClusterRole ingress-nginx failed validation: could not find schema for ClusterRole\ningress-nginx/nginx-ingress-controller-deploy.yaml - ClusterRole ingress-nginx-admission failed validation: could not find schema for ClusterRole\ningress-nginx/nginx-ingress-controller-deploy.yaml - RoleBinding ingress-nginx failed validation: could not find schema for RoleBinding\ningress-nginx/nginx-ingress-controller-deploy.yaml - RoleBinding ingress-nginx-admission failed validation: could not find schema for RoleBinding\ndashboard/kubernetes-dashboard.yaml - ClusterRoleBinding kubernetes-dashboard failed validation: could not find schema for ClusterRoleBinding\ningress-nginx/nginx-ingress-controller-deploy.yaml - ClusterRoleBinding ingress-nginx-admission failed validation: could not find schema for ClusterRoleBinding\ningress-nginx/nginx-ingress-controller-deploy.yaml - ConfigMap ingress-nginx-controller failed validation: could not find schema for ConfigMap\ningress-nginx/nginx-ingress-controller-deploy.yaml - Service ingress-nginx-controller failed validation: could not find schema for Service\ningress-nginx/nginx-ingress-controller-deploy.yaml - ClusterRoleBinding ingress-nginx failed validation: could not find schema for ClusterRoleBinding\ningress-nginx/nginx-ingress-controller-deploy.yaml - Service ingress-nginx-controller-admission failed validation: could not find schema for Service\ndashboard/kubernetes-dashboard.yaml - Deployment kubernetes-dashboard failed validation: could not find schema for Deployment\ningress-nginx/nginx-ingress-controller-deploy.yaml - Deployment ingress-nginx-controller failed validation: could not find schema for Deployment\ndashboard/kubernetes-dashboard.yaml - Deployment dashboard-metrics-scraper failed validation: could not find schema for Deployment\ningress-nginx/nginx-ingress-controller-deploy.yaml - Job ingress-nginx-admission-create failed validation: could not find schema for Job\ningress-nginx/nginx-ingress-controller-deploy.yaml - Job ingress-nginx-admission-patch failed validation: could not find schema for Job\ningress-nginx/nginx-ingress-controller-deploy.yaml - ValidatingWebhookConfiguration ingress-nginx-admission failed validation: could not find schema for ValidatingWebhookConfiguration\nmetallb/metallb-native.yaml - Namespace metallb-system failed validation: could not find schema for Namespace\ningress-nginx/nginx-ingress-controller-deploy.yaml - IngressClass nginx failed validation: could not find schema for IngressClass\nmetallb/ipaddress_pools.yaml - L2Advertisement l2-advert failed validation: could not find schema for L2Advertisement\nmetallb/ipaddress_pools.yaml - IPAddressPool production failed validation: could not find schema for IPAddressPool\nmetallb/metallb-native.yaml - CustomResourceDefinition addresspools.metallb.io failed validation: could not find schema for CustomResourceDefinition\nmetallb/metallb-native.yaml - CustomResourceDefinition communities.metallb.io failed validation: could not find schema for CustomResourceDefinition\nmetallb/metallb-native.yaml - CustomResourceDefinition bgpadvertisements.metallb.io failed validation: could not find schema for CustomResourceDefinition\nmetallb/metallb-native.yaml - CustomResourceDefinition bgppeers.metallb.io failed validation: could not find schema for CustomResourceDefinition\nmetallb/metallb-native.yaml - CustomResourceDefinition bfdprofiles.metallb.io failed validation: could not find schema for CustomResourceDefinition\nmetallb/metallb-native.yaml - ServiceAccount controller failed validation: could not find schema for ServiceAccount\nmetallb/metallb-native.yaml - ServiceAccount speaker failed validation: could not find schema for ServiceAccount\nmetallb/metallb-native.yaml - CustomResourceDefinition ipaddresspools.metallb.io failed validation: could not find schema for CustomResourceDefinition\nmetallb/metallb-native.yaml - Role controller failed validation: could not find schema for Role\nmetallb/metallb-native.yaml - Role pod-lister failed validation: could not find schema for Role\nmetallb/metallb-native.yaml - CustomResourceDefinition l2advertisements.metallb.io failed validation: could not find schema for CustomResourceDefinition\nmetallb/metallb-native.yaml - ClusterRole metallb-system:controller failed validation: could not find schema for ClusterRole\nmetallb/metallb-native.yaml - RoleBinding controller failed validation: could not find schema for RoleBinding\nmetallb/metallb-native.yaml - ClusterRole metallb-system:speaker failed validation: could not find schema for ClusterRole\nmetallb/metallb-native.yaml - RoleBinding pod-lister failed validation: could not find schema for RoleBinding\nmetallb/metallb-native.yaml - ClusterRoleBinding metallb-system:controller failed validation: could not find schema for ClusterRoleBinding\nmetallb/metallb-native.yaml - ClusterRoleBinding metallb-system:speaker failed validation: could not find schema for ClusterRoleBinding\nmetallb/metallb-native.yaml - Secret webhook-server-cert failed validation: could not find schema for Secret\nmetallb/metallb-native.yaml - Service webhook-service failed validation: could not find schema for Service\nmetallb/metallb-native.yaml - Deployment controller failed validation: could not find schema for Deployment\nmetallb/web-app-demo.yaml - Namespace web failed validation: could not find schema for Namespace\nmetallb/web-app-demo.yaml - Service web-server-service failed validation: could not find schema for Service\nmetallb/web-app-demo.yaml - Deployment web-server failed validation: could not find schema for Deployment\nmetallb/metallb-native.yaml - ValidatingWebhookConfiguration metallb-webhook-configuration failed validation: could not find schema for ValidatingWebhookConfiguration\nmetallb/metallb-native.yaml - DaemonSet speaker failed validation: could not find schema for DaemonSet\nSummary: 64 resources found in 8 files - Valid: 0, Invalid: 0, Errors: 64, Skipped: 0\n\n$ cd ~/src/kubernetes-deployments\n\n$ kubeconform -summary -strict -kubernetes-version 1.32.2 */*/*.yaml\n1.26/dashboard/admin-sa-rbac.yaml - ClusterRoleBinding k8sadmin failed validation: could not find schema for ClusterRoleBinding\n1.26/dashboard/admin-sa-rbac.yaml - ServiceAccount k8sadmin failed validation: could not find schema for ServiceAccount\n1.26/dashboard/admin-sa-rbac.yaml - Secret k8sadmin-token failed validation: could not find schema for Secret\n1.26/dashboard/kubernetes-dashboard.yaml - Secret kubernetes-dashboard-certs failed validation: could not find schema for Secret\n1.26/dashboard/kubernetes-dashboard.yaml - Secret kubernetes-dashboard-csrf failed validation: could not find schema for Secret\n1.26/dashboard/kubernetes-dashboard.yaml - Secret kubernetes-dashboard-key-holder failed validation: could not find schema for Secret\n1.26/dashboard/kubernetes-dashboard.yaml - Namespace kubernetes-dashboard failed validation: could not find schema for Namespace\n1.26/dashboard/kubernetes-dashboard.yaml - ServiceAccount kubernetes-dashboard failed validation: could not find schema for ServiceAccount\n1.26/dashboard/kubernetes-dashboard.yaml - Service kubernetes-dashboard failed validation: could not find schema for Service\n1.26/dashboard/kubernetes-dashboard.yaml - Role kubernetes-dashboard failed validation: could not find schema for Role\n1.26/dashboard/kubernetes-dashboard.yaml - ClusterRoleBinding kubernetes-dashboard failed validation: could not find schema for ClusterRoleBinding\n1.26/dashboard/kubernetes-dashboard.yaml - ClusterRole kubernetes-dashboard failed validation: could not find schema for ClusterRole\n1.26/dashboard/kubernetes-dashboard.yaml - Service dashboard-metrics-scraper failed validation: could not find schema for Service\n1.26/dashboard/kubernetes-dashboard.yaml - ConfigMap kubernetes-dashboard-settings failed validation: could not find schema for ConfigMap\n1.26/lexicon/audiobookshelf.yaml - Namespace audiobookshelf failed validation: could not find schema for Namespace\n1.26/dashboard/kubernetes-dashboard.yaml - Deployment kubernetes-dashboard failed validation: could not find schema for Deployment\n1.26/dashboard/kubernetes-dashboard.yaml - Deployment dashboard-metrics-scraper failed validation: could not find schema for Deployment\n1.26/dashboard/kubernetes-dashboard.yaml - RoleBinding kubernetes-dashboard failed validation: could not find schema for RoleBinding\n1.26/lexicon/audiobookshelf.yaml - PersistentVolume audiobookshelf-pv-config failed validation: could not find schema for PersistentVolume\n1.26/lexicon/audiobookshelf.yaml - PersistentVolume audiobookshelf-pv-metadata failed validation: could not find schema for PersistentVolume\n1.26/lexicon/audiobookshelf.yaml - PersistentVolume audiobookshelf-pv-audiobooks failed validation: could not find schema for PersistentVolume\n1.26/lexicon/audiobookshelf.yaml - PersistentVolume audiobookshelf-pv-podcasts failed validation: could not find schema for PersistentVolume\n1.26/lexicon/audiobookshelf.yaml - PersistentVolumeClaim audiobookshelf-pvc-config failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/audiobookshelf.yaml - Deployment audiobookshelf failed validation: could not find schema for Deployment\n1.26/lexicon/audiobookshelf.yaml - Service audiobookshelf-svc failed validation: could not find schema for Service\n1.26/lexicon/audiobookshelf.yaml - PersistentVolumeClaim audiobookshelf-pvc-metadata failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/firefly-iii.yaml - Namespace firefly-iii failed validation: could not find schema for Namespace\n1.26/lexicon/audiobookshelf.yaml - PersistentVolumeClaim audiobookshelf-pvc-audiobooks failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/audiobookshelf.yaml - PersistentVolumeClaim audiobookshelf-pvc-podcasts failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/firefly-iii.yaml - PersistentVolume firefly-iii-pv-mysql failed validation: could not find schema for PersistentVolume\n1.26/lexicon/firefly-iii.yaml - PersistentVolumeClaim firefly-iii-pvc-mysql failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/firefly-iii.yaml - Deployment firefly-iii-mysql failed validation: could not find schema for Deployment\n1.26/lexicon/firefly-iii.yaml - PersistentVolume firefly-iii-pv-upload failed validation: could not find schema for PersistentVolume\n1.26/lexicon/firefly-iii.yaml - PersistentVolumeClaim firefly-iii-pvc-upload failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/firefly-iii.yaml - Service firefly-iii-mysql-svc failed validation: could not find schema for Service\n1.26/lexicon/firefly-iii.yaml - Service firefly-iii-svc failed validation: could not find schema for Service\n1.26/lexicon/homebox.yaml - Namespace homebox failed validation: could not find schema for Namespace\n1.26/lexicon/homebox.yaml - PersistentVolume homebox-pv-data failed validation: could not find schema for PersistentVolume\n1.26/lexicon/firefly-iii.yaml - Deployment firefly-iii failed validation: could not find schema for Deployment\n1.26/lexicon/homebox.yaml - PersistentVolumeClaim homebox-pvc-data failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/homebox.yaml - Service homebox-svc failed validation: could not find schema for Service\n1.26/lexicon/homebox.yaml - Deployment homebox failed validation: could not find schema for Deployment\n1.26/lexicon/kavita.yaml - Namespace kavita failed validation: could not find schema for Namespace\n1.26/lexicon/kavita.yaml - PersistentVolume kavita-pv-config failed validation: could not find schema for PersistentVolume\n1.26/lexicon/kavita.yaml - PersistentVolume kavita-pv-books failed validation: could not find schema for PersistentVolume\n1.26/lexicon/kavita.yaml - PersistentVolumeClaim kavita-pvc-config failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/kavita.yaml - PersistentVolumeClaim kavita-pvc-books failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/kavita.yaml - Deployment kavita failed validation: could not find schema for Deployment\n1.26/lexicon/kavita.yaml - Service kavita-svc failed validation: could not find schema for Service\n1.26/lexicon/audiobookshelf.yaml - Ingress audiobookshelf-ingress failed validation: could not find schema for Ingress\n1.26/lexicon/komga.yaml - Namespace komga failed validation: could not find schema for Namespace\n1.26/lexicon/komga.yaml - PersistentVolume komga-pv-config failed validation: could not find schema for PersistentVolume\n1.26/lexicon/komga.yaml - PersistentVolume komga-pv-books failed validation: could not find schema for PersistentVolume\n1.26/lexicon/komga.yaml - PersistentVolumeClaim komga-pvc-config failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/kavita.yaml - Ingress kavita-ingress failed validation: could not find schema for Ingress\n1.26/lexicon/firefly-iii.yaml - Ingress firefly-iii-ingress failed validation: could not find schema for Ingress\n1.26/lexicon/homebox.yaml - Ingress homebox-ingress failed validation: could not find schema for Ingress\n1.26/lexicon/komga.yaml - PersistentVolumeClaim komga-pvc-books failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/komga.yaml - Deployment komga failed validation: could not find schema for Deployment\n1.26/lexicon/komga.yaml - Service komga-svc failed validation: could not find schema for Service\n1.26/lexicon/komga.yaml - Ingress komga-ingress failed validation: could not find schema for Ingress\n1.26/lexicon/minecraft-server.yaml - Namespace minecraft-server failed validation: could not find schema for Namespace\n1.26/lexicon/minecraft-server.yaml - Service minecraft-server failed validation: could not find schema for Service\n1.26/lexicon/minecraft-server.yaml - PersistentVolume minecraft-server-pv failed validation: could not find schema for PersistentVolume\n1.26/lexicon/minecraft-server.yaml - PersistentVolumeClaim minecraft-server-pv-claim failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/plex-media-server.yaml - Namespace plexserver failed validation: could not find schema for Namespace\n1.26/lexicon/minecraft-server.yaml - Deployment minecraft-server failed validation: could not find schema for Deployment\n1.26/lexicon/plex-media-server.yaml - PersistentVolume plexserver-pv-config failed validation: could not find schema for PersistentVolume\n1.26/lexicon/plex-media-server.yaml - PersistentVolume plexserver-pv-data-depot failed validation: could not find schema for PersistentVolume\n1.26/lexicon/plex-media-server.yaml - PersistentVolume plexserver-pv-data-video failed validation: could not find schema for PersistentVolume\n1.26/lexicon/plex-media-server.yaml - PersistentVolumeClaim plexserver-pvc-config failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/plex-media-server.yaml - PersistentVolumeClaim plexserver-pvc-data-depot failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/plex-media-server.yaml - PersistentVolumeClaim plexserver-pvc-data-video failed validation: could not find schema for PersistentVolumeClaim\n1.26/lexicon/plex-media-server.yaml - Service plex-udp failed validation: could not find schema for Service\n1.26/lexicon/plex-media-server.yaml - Service plex-tcp failed validation: could not find schema for Service\n1.26/lexicon/plex-media-server.yaml - Deployment plexserver failed validation: could not find schema for Deployment\n1.26/metallb/ipaddress-pool-rapture.yaml - IPAddressPool rapture-pool failed validation: could not find schema for IPAddressPool\n1.26/metallb/metallb-native.yaml - Namespace metallb-system failed validation: could not find schema for Namespace\n1.26/metallb/ipaddress-pool-lexicon.yaml - IPAddressPool production failed validation: could not find schema for IPAddressPool\n1.26/metallb/ipaddress-pool-rapture.yaml - L2Advertisement l2-advert failed validation: could not find schema for L2Advertisement\n1.26/metallb/ipaddress-pool-lexicon.yaml - L2Advertisement l2-advert failed validation: could not find schema for L2Advertisement\n1.26/metallb/metallb-native.yaml - CustomResourceDefinition bfdprofiles.metallb.io failed validation: could not find schema for CustomResourceDefinition\n1.26/metallb/metallb-native.yaml - CustomResourceDefinition ipaddresspools.metallb.io failed validation: could not find schema for CustomResourceDefinition\n1.26/metallb/metallb-native.yaml - CustomResourceDefinition bgpadvertisements.metallb.io failed validation: could not find schema for CustomResourceDefinition\n1.26/metallb/metallb-native.yaml - CustomResourceDefinition communities.metallb.io failed validation: could not find schema for CustomResourceDefinition\n1.26/metallb/metallb-native.yaml - CustomResourceDefinition bgppeers.metallb.io failed validation: could not find schema for CustomResourceDefinition\n1.26/metallb/metallb-native.yaml - ServiceAccount controller failed validation: could not find schema for ServiceAccount\n1.26/metallb/metallb-native.yaml - CustomResourceDefinition l2advertisements.metallb.io failed validation: could not find schema for CustomResourceDefinition\n1.26/metallb/metallb-native.yaml - ServiceAccount speaker failed validation: could not find schema for ServiceAccount\n1.26/metallb/metallb-native.yaml - CustomResourceDefinition servicel2statuses.metallb.io failed validation: could not find schema for CustomResourceDefinition\n1.26/metallb/metallb-native.yaml - Role controller failed validation: could not find schema for Role\n1.26/metallb/metallb-native.yaml - Role pod-lister failed validation: could not find schema for Role\n1.26/metallb/metallb-native.yaml - ClusterRole metallb-system:speaker failed validation: could not find schema for ClusterRole\n1.26/metallb/metallb-native.yaml - RoleBinding controller failed validation: could not find schema for RoleBinding\n1.26/metallb/metallb-native.yaml - ClusterRole metallb-system:controller failed validation: could not find schema for ClusterRole\n1.26/metallb/metallb-native.yaml - ClusterRoleBinding metallb-system:controller failed validation: could not find schema for ClusterRoleBinding\n1.26/metallb/metallb-native.yaml - RoleBinding pod-lister failed validation: could not find schema for RoleBinding\n1.26/metallb/metallb-native.yaml - ConfigMap metallb-excludel2 failed validation: could not find schema for ConfigMap\n1.26/metallb/metallb-native.yaml - ClusterRoleBinding metallb-system:speaker failed validation: could not find schema for ClusterRoleBinding\n1.26/metallb/metallb-native.yaml - Secret metallb-webhook-cert failed validation: could not find schema for Secret\n1.26/metallb/metallb-native.yaml - Service metallb-webhook-service failed validation: could not find schema for Service\n1.26/metallb/metallb-native.yaml - Deployment controller failed validation: could not find schema for Deployment\n1.26/rapture/audiobookshelf.yaml - Namespace audiobookshelf failed validation: could not find schema for Namespace\n1.26/rapture/audiobookshelf.yaml - PersistentVolume audiobookshelf-pv-metadata failed validation: could not find schema for PersistentVolume\n1.26/rapture/audiobookshelf.yaml - PersistentVolume audiobookshelf-pv-config failed validation: could not find schema for PersistentVolume\n1.26/rapture/audiobookshelf.yaml - PersistentVolume audiobookshelf-pv-audiobooks failed validation: could not find schema for PersistentVolume\n1.26/rapture/audiobookshelf.yaml - PersistentVolumeClaim audiobookshelf-pvc-metadata failed validation: could not find schema for PersistentVolumeClaim\n1.26/rapture/audiobookshelf.yaml - PersistentVolumeClaim audiobookshelf-pvc-config failed validation: could not find schema for PersistentVolumeClaim\n1.26/rapture/audiobookshelf.yaml - PersistentVolumeClaim audiobookshelf-pvc-audiobooks failed validation: could not find schema for PersistentVolumeClaim\n1.26/rapture/audiobookshelf.yaml - Service audiobookshelf-svc failed validation: could not find schema for Service\n1.26/rapture/photoprism.yaml - Namespace photoprism failed validation: could not find schema for Namespace\n1.26/rapture/audiobookshelf.yaml - Deployment audiobookshelf failed validation: could not find schema for Deployment\n1.26/rapture/photoprism.yaml - Secret photoprism-secrets failed validation: could not find schema for Secret\n1.26/rapture/photoprism.yaml - PersistentVolume photoprism-pv-originals failed validation: could not find schema for PersistentVolume\n1.26/rapture/photoprism.yaml - PersistentVolume photoprism-pv-import failed validation: could not find schema for PersistentVolume\n1.26/rapture/photoprism.yaml - PersistentVolume photoprism-pv-storage failed validation: could not find schema for PersistentVolume\n1.26/rapture/photoprism.yaml - PersistentVolumeClaim photoprism-pvc-originals failed validation: could not find schema for PersistentVolumeClaim\n1.26/rapture/photoprism.yaml - PersistentVolumeClaim photoprism-pvc-import failed validation: could not find schema for PersistentVolumeClaim\n1.26/rapture/photoprism.yaml - PersistentVolumeClaim photoprism-pvc-storage failed validation: could not find schema for PersistentVolumeClaim\n1.26/rapture/photoprism.yaml - Service photoprism failed validation: could not find schema for Service\n1.26/rapture/plex-media-server.yaml - Namespace plexserver failed validation: could not find schema for Namespace\n1.26/rapture/plex-media-server.yaml - PersistentVolume plexserver-pv-config failed validation: could not find schema for PersistentVolume\n1.26/rapture/plex-media-server.yaml - PersistentVolume plexserver-pv-data-audio failed validation: could not find schema for PersistentVolume\n1.26/rapture/plex-media-server.yaml - PersistentVolume plexserver-pv-data-video failed validation: could not find schema for PersistentVolume\n1.26/rapture/plex-media-server.yaml - PersistentVolumeClaim plexserver-pvc-config failed validation: could not find schema for PersistentVolumeClaim\n1.26/rapture/plex-media-server.yaml - PersistentVolumeClaim plexserver-pvc-data-audio failed validation: could not find schema for PersistentVolumeClaim\n1.26/rapture/plex-media-server.yaml - PersistentVolumeClaim plexserver-pvc-data-video failed validation: could not find schema for PersistentVolumeClaim\n1.26/rapture/plex-media-server.yaml - Deployment plexserver failed validation: could not find schema for Deployment\n1.26/rapture/plex-media-server.yaml - Service plex-udp failed validation: could not find schema for Service\n1.26/rapture/plex-media-server.yaml - Service plex-tcp failed validation: could not find schema for Service\n1.26/metallb/metallb-native.yaml - DaemonSet speaker failed validation: could not find schema for DaemonSet\n1.26/rapture/photoprism.yaml - StatefulSet photoprism failed validation: could not find schema for StatefulSet\n1.26/metallb/metallb-native.yaml - ValidatingWebhookConfiguration metallb-webhook-configuration failed validation: could not find schema for ValidatingWebhookConfiguration\nSummary: 133 resources found in 15 files - Valid: 0, Invalid: 0, Errors: 133, Skipped: 0\n</code></pre>"},{"location":"blog/2024/08/24/self-hosted-photo-albums-with-photoprism/","title":"Self-hosted photo albums with PhotoPrism\u00ae","text":"<p>PhotoPrism\u00ae is an AI-Powered Photos App for the Decentralized Web I heard some good comments about. I tried it on my other Kubernetes cluster and here are impressions so far.</p>"},{"location":"blog/2024/08/24/self-hosted-photo-albums-with-photoprism/#deployment","title":"Deployment","text":"<p>The following deployment is based on the PhotoPrism\u00ae Setup Using Docker Compose:</p> Kubernetes deployment: <code>photoprism.yaml</code> photoprism.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: photoprism\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: photoprism-secrets\n  namespace: photoprism\nstringData:\n  PHOTOPRISM_ADMIN_PASSWORD: \"************\"\n  PHOTOPRISM_DATABASE_DRIVER: \"sqlite\"\n  PHOTOPRISM_DATABASE_DSN: /photoprism/storage/db.sqlite\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: photoprism-pv-originals\n  namespace: photoprism\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/photoprism/originals\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: photoprism-pv-import\n  namespace: photoprism\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/photoprism/import\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: photoprism-pv-storage\n  namespace: photoprism\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/photoprism/storage\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: photoprism-pvc-originals\n  namespace: photoprism\nspec:\n  storageClassName: manual\n  volumeName: photoprism-pv-originals\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: photoprism-pvc-import\n  namespace: photoprism\nspec:\n  storageClassName: manual\n  volumeName: photoprism-pv-import\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: photoprism-pvc-storage\n  namespace: photoprism\nspec:\n  storageClassName: manual\n  volumeName: photoprism-pv-storage\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: photoprism\n  namespace: photoprism\nspec:\n  selector:\n    matchLabels:\n      app: photoprism\n  serviceName: photoprism\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: photoprism\n    spec:\n      containers:\n      - name: photoprism\n        image: photoprism/photoprism:latest\n        env:\n        - name: PHOTOPRISM_DEBUG\n          value: \"true\"\n        - name: PHOTOPRISM_DATABASE_DRIVER\n          value: sqlite\n        - name: PHOTOPRISM_HTTP_HOST\n          value: 0.0.0.0\n        - name: PHOTOPRISM_HTTP_PORT\n          value: \"2342\"\n        - name: PHOTOPRISM_GID\n          value: \"1000\"\n        - name: PHOTOPRISM_UID\n          value: \"1000\"\n        - name: PHOTOPRISM_ORIGINALS_LIMIT\n          value: \"20000\"\n        # Load database DSN &amp; admin password from secret\n        envFrom:\n        - secretRef:\n            name: photoprism-secrets\n            optional: false\n        ports:\n        - containerPort: 2342\n          name: http\n        volumeMounts:\n        - mountPath: /photoprism/originals\n          name: photoprism-originals\n        - mountPath: /photoprism/import\n          name: photoprism-import\n        - mountPath: /photoprism/storage\n          name: photoprism-storage\n        readinessProbe:\n          httpGet:\n            path: /api/v1/status\n            port: http\n        securityContext:\n          allowPrivilegeEscalation: false\n          runAsUser: 1000\n          runAsGroup: 1000\n      volumes:\n      - name: photoprism-originals\n        persistentVolumeClaim:\n          claimName: photoprism-pvc-originals\n      - name: photoprism-import\n        persistentVolumeClaim:\n          claimName: photoprism-pvc-import\n      - name: photoprism-storage\n        persistentVolumeClaim:\n          claimName: photoprism-pvc-storage\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: photoprism\n  namespace: photoprism\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  selector:\n    app: photoprism\n  type: LoadBalancer\n</code></pre> <p>In addition to the deployment, I create a <code>.ppignore</code> file at the top level directory of the photo collection, to leave out (for now) some of the largest parts of my photo collection, to save time on indexing, as well as several directories of no interest:</p> /photoprism/originals/.ppignore<pre><code># Temporarily ignore the biggest directories.\nPeople/Family\nPeople/Weddings\nVideos/Hobbies\nVideos/Locations\nVideos/People\n# Permanently ignore a directories of no interest.\ndata\nKrita\nMyTracks\nNew\nphotos\nold-photos-scripts\nRejects\nscripts\nSelections\nTakeout\nwatermark\n# Permanently ignore all raw/ folders.\nraw\n[raw]*\n# Permanently ignore non-JPEG images.\n*.xcf\n*.gif\n*.nef\n*.pp3\n*.raf\n*.raf.xmp\n</code></pre> <p>After starting the deployment with the usual <code>kubectl apply -f photoprism.yaml</code> the service is ready to use at:</p> <pre><code>$ kubectl -n photoprism get all\nNAME               READY   STATUS    RESTARTS        AGE\npod/photoprism-0   1/1     Running   9 (4h14m ago)   12m\n\nNAME                 TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE\nservice/photoprism   LoadBalancer   10.108.135.112   192.168.0.220   80:31202/TCP   103m\n\nNAME                          READY   AGE\nstatefulset.apps/photoprism   1/1     103m\n</code></pre> <p>To make my life a little easier, I add an entry to <code>/etc/hosts</code> so that I can directly visit http://photoprism.rapture.uu.am</p> <pre><code># Kubernetes MetalLB\n192.168.0.220   photoprism.rapture.uu.am\n</code></pre>"},{"location":"blog/2024/08/24/self-hosted-photo-albums-with-photoprism/#performance","title":"Performance","text":"<p>Before using the web application, PhotoPrism\u00ae needs some time to scan and index the images. In this case, it was a good half hour:</p> <p></p> <p>The second CPU-intensive task was a rather lengthy preprocessing of a movie. I tried to watch it simply because I was surprised to find that a few videos were indexed, but then was even more surprised how long it took to even start playing. PhotoPrism\u00ae spent a good 20 minutes running <code>ffmpeg</code>; in particular this command:</p> <pre><code>/usr/bin/ffmpeg \\\n  -i /photoprism/originals/Videos/Fan.Films/Troll.Bridge.2018/Troll.Bridge.2018.avi \\\n  -c:v libx264 \\\n  -map 0:v:0 \\\n  -map 0:a:0? \\\n  -c:a aac \\\n  -vf scale='if(gte(iw,ih), min(4096, iw), -2):if(gte(iw,ih), -2, min(4096, ih))',format=yuv420p \\\n  -max_muxing_queue_size 1024 \\\n  -crf 23 \\\n  -r 30 \\\n  -b:v 50M \\\n  -f mp4 \\\n  -movflags +faststart \\\n  -y /photoprism/storage/sidecar/Videos/Fan.Films/Troll.Bridge.2018/Troll.Bridge.2018.avi.avc\n</code></pre> <p>Eventually this lead to a 1.5G sidecar <code>.avc</code> file, for a video that was originally 5.9G. That was taking up nearly 10% of the 17G under <code>/photoprism/storage</code> and all the while all I cold see was</p> <p></p>"},{"location":"blog/2024/08/24/self-hosted-photo-albums-with-photoprism/#big-problem","title":"Big Problem","text":"<p>But the most surprising, and eventually frustrating of all, was that only a small portion of the overall photo library appears to have been indexed.</p> <p>The library, that is the set of images not ignored by <code>.ppignore</code>, is made of 14,762 65,276 JPEG files in 2,599 directories (not including a large number of RAW files and a few stray videos).</p> <p>The final scan was very incomplete, finding only 436 folders, 35 videos and 14,178 photos. Even after disabling the Quality filter only 14,762 images are found.</p> <p>Going through the checklist in Missing Pictures the only relevant step seems to be checking <code>.ppignore</code>, nothing else seems to apply here and nothing at all explains why  only 22.6% of images are found.</p> <p>Even disabling all the options to Stack photos, and then forcing a full reindex.</p>"},{"location":"blog/2024/08/24/self-hosted-photo-albums-with-photoprism/#more-problems","title":"More Problems","text":"<p>Additional (significant) issues:</p> <ol> <li>Images cannot be found by their file name; despite the file name    being stored in the EXIF <code>DocumentName</code> tag (<code>0x010d</code>).</li> <li>Stacks includes 351 images (stacks) despite all my attempts at    disabling that feature, purging and reindexing.</li> <li>Images missing EXIF <code>DateTimeOriginal</code> (<code>0x9003</code>) get a Taken on    date of January 1st in the current year.</li> <li>Only 2 out of 1,854 images geotagged in Australia are recognized.</li> <li>None of the 25 folder under <code>/photoprism/originals/Travel/Australia</code> show up under Folders.</li> <li>All the folders show under Library &gt; Originals but there is no    easy to even re-index a single folder.</li> </ol> <p>Other (not so minor) issues:</p> <ol> <li>All 3 layouts use square thumbnails.</li> <li>Some images without GPS tags are somehow geolocated in Canada.</li> </ol> <p>Other (really minor) issues:</p> <ol> <li>Monochrome includes a few images with quite a bit of color.</li> <li>Panoramas are essentially any images with an aspect ratio    beyond some threshold (e.g. 21:9), including small screenshots    and cropped photos.</li> <li>Scans includes just one photo, even though there are hundreds of    scanned film photos.</li> </ol>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/","title":"Ubuntu Studio 24.04 on Computer, for a young artist","text":"<p>It has been nearly two years since installing  Ubuntu Studio 22.04 on Computer, for a young artist and so it is time to upgrade it to the next LTS version: Ubuntu Studio 24.04.</p>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#prepare-new-4tb-m2-ssd","title":"Prepare new 4TB M.2 SSD","text":"<p>This time around the installation will go on a new 4TB NVMe SSD to provide plenty of storage capacity and speed for the latest and future creating inclinations.</p> <p>Before installing Ubuntu Studio 24.04 the disk is installed in the M.2 slot and prepared from the running Ubuntu Studio 22.04 system.</p> <p>A new GPT partition table is created, including</p> <ol> <li>A 260 MB EFI partition, with the ESP (boot) flag set.</li> <li>Two 64 GB partitions for the root filesystem, of the upcoming    Ubuntu Studio 24.04 system and the future Ubuntu Studio 26.04.</li> <li>A large partition taking up most of the disk, for user storage.</li> </ol> <pre><code># parted -a optimal /dev/nvme0n1 \n(parted) mktable gpt\n(parted) mkpart EFI 0% 260M\n(parted) mkpart root1 260M 65796M\n(parted) mkpart root2 65796M 131332M\n(parted) mkpart home 131332M 100%\n(parted) set 1 esp on\n(parted) print\nModel: KINGSTON SFYRDK4000G (nvme)\nDisk /dev/nvme0n1: 4001GB\nSector size (logical/physical): 512B/512B\nPartition Table: gpt\nDisk Flags: \n\nNumber  Start   End     Size    File system  Name   Flags\n 1      1049kB  260MB   259MB                EFI    boot, esp\n 2      260MB   65.8GB  65.5GB               root1\n 3      65.8GB  131GB   65.5GB               root2\n 4      131GB   4001GB  3869GB               home\n\n(parted) quit\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#install-ubuntu-studio-2404","title":"Install Ubuntu Studio 24.04","text":"<p>Prepared the USB stick with <code>usb-creator-kde</code> and booted into it, then used the \u201cInstall Ubuntu\u201d launcher on the desktop.</p> <ol> <li>Plug the USB stick and turn the PC on.</li> <li>Press <code>F8</code> to select the boot device and choose the     UEFI: ... option.</li> <li>In the Grub menu, choose to Try or Install Ubuntu.</li> <li>Select language (English) and then Install Ubuntu.</li> <li>Select keyboard layout (can be from a different language).</li> <li>Select the appropriate wired or wireless network.</li> <li>Select Install Ubuntu Studio.</li> <li>Select Type of install: Interactive Installation.</li> <li>Enable the options to Install third-party software for graphics     and Wifi hardware and Download and install support for **     additional media formats**.</li> <li>Select Manual Installation<ul> <li>Use the arrow keys to navigate down to the nvme0n1 disk.</li> <li>Set nvme0n1p1 (259 MB) as EFI System Partition mounted     on <code>/boot/efi</code></li> <li>Set nvme0n1p2 (66 GB) as ext4 mounted on <code>/</code></li> <li>Leave nvme0n1p3 (66 GB) alone (to be used for Ubuntu 26.04)</li> <li>Set nvme0n1p4 (3.87 TB) as Leave formatted as Btrfs     mounted on <code>/home</code></li> <li>Set Device for boot loader installation to nvme0n1</li> </ul> </li> <li>Click on Next to confirm the partition selection.</li> <li>Confirm first non-root user name (<code>ponder</code>) and computer     name (<code>computer</code>).</li> <li>Select time zone (seems to be detected correctly).</li> <li>Review the choices and click on Install to start copying files.</li> <li>Once it's done, select Restart     (remove install media and hit <code>Enter</code>).</li> </ol>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#wayland-is-dead","title":"Wayland is Dead?","text":"<p>The very first thing I tried, out of curiousity, was to log in using Plasma (Wayland). The screen went black shortly after the Plasma splash screen and, shortly after that, the computer rebooted. This doesn't look like it's ready to use just yet.</p>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#multiple-ips-on-lan","title":"Multiple IPs on LAN","text":"<p>Connecting to the local wired network provides a dynamic IP address that may change over time, but it is more convenient to have fixed IP addresses. Moreover, the DHCP range is shared with the wireless network, we want to have an additional wired-only LAN and set both IP addresses on the same NIC.</p> <p>To this effect, edit <code>/etc/netplan/01-network-manager-all.yaml</code> with with the following contenct and apply the changes with <code>netplan apply</code>:</p> /etc/netplan/01-network-manager-all.yaml<pre><code>network:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp8s0:\n      dhcp4: no\n      dhcp6: no\n      # Ser IP address &amp; subnet mask\n      addresses: [ 10.0.0.3/24, 192.168.0.3/24 ]\n      # Set default gateway\n      routes:\n       - to: default\n         via: 192.168.0.1\n      nameservers:\n        # Set DNS name servers\n        addresses: [62.2.24.158, 62.2.17.61]\n</code></pre> <pre><code># netplan apply \n# ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n       valid_lft forever preferred_lft forever\n2: enp8s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 60:45:cb:a0:16:7b brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.2/24 brd 10.0.0.255 scope global enp8s0\n       valid_lft forever preferred_lft forever\n    inet 192.168.0.9/24 brd 192.168.0.255 scope global enp8s0\n       valid_lft forever preferred_lft forever\n    inet 192.168.0.118/24 metric 100 brd 192.168.0.255 scope global secondary dynamic enp8s0\n       valid_lft 86398sec preferred_lft 86398sec\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#ssh-server","title":"SSH Server","text":"<p>Ubuntu Studio doesn't enable the SSH server by default, but we want this to adjust the system remotely:</p> <pre><code># apt install ssh -y\n# sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config\n# systemctl enable --now ssh\n</code></pre> <p>Note</p> <p>Remember to copy over files under <code>/root</code> from previous system/s, in case it contains useful scripts (and/or SSH keys worth keeping under <code>.ssh</code>).</p> <pre><code># mount /dev/sdb1 /mnt/\n# rm /root/.ssh/authorized_keys \n# rmdir /root/.ssh/ \n# cp -a /mnt/root/.ssh/ /root/\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#etchosts","title":"<code>/etc/hosts</code>","text":"<p>Having the old system's root partition mounted (see above), copy over <code>/etc/hosts</code> so that connections to local hosts work as smoothly as in the old system (e.g. for Continuous Monitoring).</p>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#install-essential-packages","title":"Install Essential Packages","text":"<p>Before proceeding further, install a few basic APT packages:</p> <pre><code># apt install gdebi-core wget gkrellm vim curl gkrellm-leds \\\n  gkrellm-xkb gkrellm-cpufreq geeqie playonlinux exfat-fuse \\\n  clementine id3v2 htop vnstat neofetch tigervnc-viewer sox \\\n  scummvm wine gamemode python-is-python3 exiv2 rename scrot \\\n  speedtest-cli xcalib python3-pip netcat-openbsd jstest-gtk \\\n  etherwake python3-selenium lm-sensors sysstat tor unrar \\\n  ttf-mscorefonts-installer winetricks icc-profiles ffmpeg \\\n  iotop-c xdotool redshift-gtk inxi vainfo vdpauinfo mpv \\\n  tigervnc-tools screen -y\n...\nThe following additional packages will be installed:\n  cabextract caca-utils chafa chromium-browser chromium-chromedriver evemu-tools evtest\n  exiftran fonts-wine fuseiso gamemode-daemon geeqie-common gir1.2-ayatanaappindicator3-0.1\n  icoutils joystick jp2a libasound2-plugins libayatana-appindicator3-1 libayatana-ido3-0.4-0\n  libayatana-indicator3-7 libcapi20-3t64 libchafa0t64 libcpufreq0 libdbusmenu-gtk3-4\n  libevemu3t64 libgamemode0 libgamemodeauto0 libgnutls-openssl27t64 libgpod-common libgpod4t64\n  libinih1 liblastfm5-1 liblua5.3-0 libmikmod3 libmspack0t64 libmygpo-qt5-1 libntlm0 libosmesa6\n  libpython3-dev libpython3.12-dev libsdl2-net-2.0-0 libsgutils2-1.46-2 libsixel-bin\n  libsonivox3 libutempter0 libwine libxdo3 libxkbregistry0 libz-mingw-w64 python3-dev\n  python3-exceptiongroup python3-h11 python3-natsort python3-outcome python3-sniffio\n  python3-trio python3-trio-websocket python3-wsproto python3.12-dev redshift scummvm-data\n  toilet toilet-fonts tor-geoipdb torsocks tree vim-runtime w3m w3m-img wakeonlan\n  webp-pixbuf-loader wine64\nSuggested packages:\n  gnome-shell-extension-gamemode xpaint libjpeg-progs libterm-readline-gnu-perl\n  | libterm-readline-perl-perl libxml-dumper-perl sg3-utils fancontrol read-edid i2c-tools\n  libcuda1 winbind python-natsort-doc byobu | screenie | iselect beneath-a-steel-sky drascula\n  flight-of-the-amazon-queen lure-of-the-temptress libsox-fmt-all figlet mixmaster\n  torbrowser-launcher apparmor-utils nyx obfs4proxy ctags vim-doc vim-scripts vnstati brotli\n  cmigemo compface dict dict-wn dictd mailcap w3m-el xsel q4wine wine-binfmt dosbox\n  wine64-preloader\nRecommended packages:\n  libgamemode0:i386 libgamemodeauto0:i386 wine32\nThe following NEW packages will be installed:\n  cabextract caca-utils chafa chromium-browser chromium-chromedriver clementine curl etherwake\n  evemu-tools evtest exfat-fuse exiftran exiv2 fonts-wine fuseiso gamemode gamemode-daemon\n  gdebi-core geeqie geeqie-common gir1.2-ayatanaappindicator3-0.1 gkrellm gkrellm-cpufreq\n  gkrellm-leds gkrellm-xkb htop icc-profiles icoutils id3v2 inxi iotop-c joystick jp2a\n  jstest-gtk libasound2-plugins libayatana-appindicator3-1 libayatana-ido3-0.4-0\n  libayatana-indicator3-7 libcapi20-3t64 libchafa0t64 libcpufreq0 libdbusmenu-gtk3-4\n  libevemu3t64 libgamemode0 libgamemodeauto0 libgnutls-openssl27t64 libgpod-common libgpod4t64\n  libinih1 liblastfm5-1 liblua5.3-0 libmikmod3 libmspack0t64 libmygpo-qt5-1 libntlm0 libosmesa6\n  libpython3-dev libpython3.12-dev libsdl2-net-2.0-0 libsgutils2-1.46-2 libsixel-bin\n  libsonivox3 libutempter0 libwine libxdo3 libxkbregistry0 libz-mingw-w64 lm-sensors mpv\n  neofetch playonlinux python-is-python3 python3-dev python3-exceptiongroup python3-h11\n  python3-natsort python3-outcome python3-pip python3-selenium python3-sniffio python3-trio\n  python3-trio-websocket python3-wsproto python3.12-dev redshift redshift-gtk rename screen\n  scrot scummvm scummvm-data sox speedtest-cli tigervnc-tools tigervnc-viewer toilet\n  toilet-fonts tor tor-geoipdb torsocks tree ttf-mscorefonts-installer unrar vainfo vdpauinfo\n  vim vim-runtime vnstat w3m w3m-img wakeonlan webp-pixbuf-loader wine wine64 winetricks xcalib\n  xdotool\n0 upgraded, 117 newly installed, 0 to remove and 5 not upgraded.\nNeed to get 314 MB of archives.\nAfter this operation, 1,131 MB of additional disk space will be used.\n</code></pre> A few packages are missing here ... <p>A few packages are missing here, compared to those installed with Ubuntu Studio 22.04:</p> <ul> <li><code>netcat</code> is a virtual package now; instead of on its providers     must be selected explicitly:<ul> <li><code>netcat-openbsd</code> contains the OpenBSD rewrite of netcat,     including support for IPv6, proxies, and Unix sockets.</li> <li><code>netcat-traditional</code> is the \"classic\" netcat, written by     Hobbit. It lacks many features found in netcat-openbsd.</li> </ul> </li> <li><code>gkrellm-hdplop</code> and <code>gkrellm-x86info</code> are no longer included.</li> <li><code>ttf-mscorefonts-installer:i386</code> is not available because the     <code>i386</code> architecture has not been enabled (and is not needed).</li> </ul> <p>There is Warning regarding <code>/snap/bin</code> not found in your <code>$PATH</code></p> <pre><code>Preparing to unpack .../000-chromium-browser_2%3a1snap1-0ubuntu2_amd64.deb ...\n=&gt; Installing the chromium snap\n==&gt; Checking connectivity with the snap store\n==&gt; Installing the chromium snap\nWarning: /snap/bin was not found in your $PATH. If you've not restarted your session since you\n         installed snapd, try doing that. Please see https://forum.snapcraft.io/t/9469 for more\n         details.\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#hardware-sensors","title":"Hardware Sensors","text":"<p>Initially there is only a limited amount of hardware sensors:</p> <pre><code># sensors -A\nnvme-pci-0100\nComposite:    +30.9\u00b0C  (low  = -20.1\u00b0C, high = +83.8\u00b0C)\n                       (crit = +88.8\u00b0C)\nERROR: Can't get value of subfeature temp3_min: I/O error\nERROR: Can't get value of subfeature temp3_max: I/O error\nSensor 2:     +47.9\u00b0C  (low  =  +0.0\u00b0C, high =  +0.0\u00b0C)\n\nk10temp-pci-00c3\nTctl:         +44.9\u00b0C  \nTdie:         +24.9\u00b0C  \n</code></pre> <p>HDD temperatures are available by loading the drivetemp kernel module:</p> <pre><code># echo drivetemp &gt; /etc/modules-load.d/drivetemp.conf\n# modprobe drivetemp\n# sensors -A\ntemp1:        +27.0\u00b0C  (low  =  +0.0\u00b0C, high = +70.0\u00b0C)\n                       (crit low =  +0.0\u00b0C, crit = +70.0\u00b0C)\n                       (lowest = +27.0\u00b0C, highest = +40.0\u00b0C)\n\nnvme-pci-0100\nComposite:    +30.9\u00b0C  (low  = -20.1\u00b0C, high = +83.8\u00b0C)\n                       (crit = +88.8\u00b0C)\nERROR: Can't get value of subfeature temp3_min: I/O error\nERROR: Can't get value of subfeature temp3_max: I/O error\nSensor 2:     +47.9\u00b0C  (low  =  +0.0\u00b0C, high =  +0.0\u00b0C)\n\ndrivetemp-scsi-0-0\ntemp1:        +29.0\u00b0C  (low  =  +0.0\u00b0C, high = +70.0\u00b0C)\n                       (crit low =  +0.0\u00b0C, crit = +70.0\u00b0C)\n                       (lowest = +27.0\u00b0C, highest = +29.0\u00b0C)\n\nk10temp-pci-00c3\nTctl:         +47.5\u00b0C  \nTdie:         +27.5\u00b0C  \n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#continuous-monitoring","title":"Continuous Monitoring","text":"<p>Install the multi-thread version of the <code>conmon</code> script as <code>/usr/local/bin/conmon</code> and run it as a service; create <code>/etc/systemd/system/conmon.service</code> as follows:</p> /etc/systemd/system/conmon.service<pre><code>[Unit]\nDescription=Continuous Monitoring\n\n[Service]\nExecStart=/usr/local/bin/conmon\nRestart=on-failure\nStandardOutput=null\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Then enable and start the services in <code>systemd</code>:</p> <pre><code># systemctl enable conmon.service\n# systemctl daemon-reload\n# systemctl start conmon.service\n# systemctl status conmon.service\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#system-configuration","title":"System Configuration","text":""},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#logitech-trackman-marble","title":"Logitech Trackman Marble","text":"<p>To enable 2-axis scrolling with the Logitech Trackman Marble create or edit <code>/usr/share/X11/xorg.conf.d/10-libinput.conf</code> like this:</p> /usr/share/X11/xorg.conf.d/10-libinput.conf<pre><code>Section \"InputClass\"\n    Identifier      \"Marble Mouse\"\n    MatchProduct    \"Logitech USB Trackball\"\n    MatchIsPointer  \"on\"\n    MatchDevicePath \"/dev/input/event*\"\n    Driver          \"libinput\"\n    Option          \"SendCoreEvents\" \"true\"\n\n    #  Physical buttons come from the mouse as:\n    #     Big:   1 3\n    #     Small: 8 9\n    #\n    # This makes right small button (9) into the middle, and puts\n    #  scrolling on the left small button (8).\n    #\n    Option \"Buttons\"            \"9\"\n    Option \"ButtonMapping\"      \"1 8 3 4 5 6 7 9 2\"\n    Option \"ScrollMethod\"       \"button\"\n    Option \"ScrollButton\"       \"8\"\n    Option \"EmulateWheel\"       \"true\"\n    Option \"EmulateWheelButton\" \"8\"\n    Option \"YAxisMapping\"       \"4 5\"\n    Option \"XAxisMapping\"       \"6 7\"\nEndSection\n</code></pre> <p>On a tangentially related note, it is also good to forbid joystick mouse emulation when using certain joystick-like gaming controllers. To do this,  create or edit <code>/usr/share/X11/xorg.conf.d/50-joystick.conf</code></p> /usr/share/X11/xorg.conf.d/50-joystick.conf<pre><code>Section \"InputClass\"\n    Identifier \"joystick catchall\"\n    MatchIsJoystick \"on\"\n    MatchDevicePath \"/dev/input/event*\"\n    Driver \"joystick\"\n  Option \"StartKeysEnabled\" \"False\"       #Disable mouse\n  Option \"StartMouseEnabled\" \"False\"      #support\nEndSection\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#weekly-btrfs-scrub","title":"Weekly btrfs scrub","text":"<p>To keep BTRFS file systems healthy, it is recommended to run a weekly scrub to check everything for consistency. For this, I run the script from crontab every Saturday morning, early enough that it will be done by the time anyone wakes up.</p> <pre><code># wget -O /usr/local/bin/btrfs-scrub-all \\\n  http://marc.merlins.org/linux/scripts/btrfs-scrub\n\n# apt install inn -y\n\n# crontab -l | grep btrfs\n# m h  dom mon dow   command\n50 5 * * 6 /usr/local/bin/btrfs-scrub-all\n\n# /usr/local/bin/btrfs-scrub-all\n&lt;13&gt;Sep 22 17:33:43 root: Quick Metadata and Data Balance of /home (/dev/nvme0n1p4)\nDone, had to relocate 0 out of 615 chunks\nDone, had to relocate 0 out of 615 chunks\nDone, had to relocate 1 out of 615 chunks\n&lt;13&gt;Sep 22 17:34:13 root: Starting scrub of /home\nbtrfs scrub start -Bd /home\nStarting scrub on devid 1\n\n\nScrub device /dev/nvme0n1p4 (id 1) done\nScrub started:    Sun Sep 22 17:34:13 2024\nStatus:           finished\nDuration:         0:03:40\nTotal to scrub:   612.16GiB\nRate:             2.78GiB/s\nError summary:    no errors found\n\nreal    3m40.034s\nuser    0m0.002s\nsys     1m25.821s\n</code></pre> <p>The whole process takes less than 10 minutes with a 2TB NVMe SSD:</p> <p></p>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#create-users","title":"Create Users","text":"<p>As is always the case when installing a new system, even though personal directories are already in <code>/home</code> the users need to be re-created; in the correct order to match the user IDs from the previous system. These can be checked with <code>ls -ln</code>:</p> <pre><code># ls -ln /home\ntotal 0\ndrwx------ 1 1002 1002 5440 Sep 21 20:03 artist\ndrwx------ 1 1001 1001 1722 Nov  2  2019 other\ndrwxr-x--- 1 1000 1000  464 Sep 22 17:43 coder\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#make-sddm-look-good","title":"Make SDDM Look Good","text":"<p>Ubuntu Studio 24.04 uses  Simple Desktop Display Manager (SDDM) (sddm/sddm in GitHub) which is quite good looking out of the box, but I like to customize this for each computer.</p> <p>For most computers my favorite SDDM theme is Breeze-Noir-Dark, which I like to install system-wide:</p> <pre><code># unzip Breeze-Noir-Dark.zip\n# mv Breeze-Noir-Dark /usr/share/sddm/themes/\n</code></pre> <p>Warning</p> <p>Action icons won\u2019t render if the directory name is changed. If needed, change the directory name in the <code>iconSource</code> fields in <code>Main.qml</code> to match final directory name so icons show.</p> <p>Other than installing this theme, all I really change in it is the background image, e.g. assuming it's downloaded as <code>/tmp/background.jpg</code></p> <pre><code># mv /tmp/background.jpg /usr/share/sddm/themes/breeze-noir-dark\n# mv /tmp/ponyo.jpg /usr/share/sddm/themes/breeze-noir-dark\n# cd /usr/share/sddm/themes/breeze-noir-dark\n# vi theme.conf\nbackground=/usr/share/sddm/themes/breeze-noir-dark/background.jpg\n# vi theme.conf.user\nbackground=background.jpg\n</code></pre> <p>Additionally, as this is new in Ubuntu 24.04, the theme has to be selected by adding a <code>[Theme]</code> section in the system config in <code>/usr/lib/sddm/sddm.conf.d/ubuntustudio.conf</code></p> /usr/lib/sddm/sddm.conf.d/ubuntustudio.conf<pre><code>[General]\nInputMethod=\n\n[Theme]\nCurrent=\"Breeze-Noir-Dark\"\nEnableAvatars=True\n</code></pre> <p>This did not set the theme, even though it did Make SDDM Listen to TCP.</p> <p>Reportedly, you have to create the <code>/etc/sddm.conf.d</code> directory to add the Local configuration file that allows setting the theme:</p> <pre><code># mkdir /etc/sddm.conf.d\n# vi /etc/sddm.conf.d/ubuntustudio.conf\n</code></pre> <p>Besides setting the theme, it is also good to limit the range of user ids so that only human users show up:</p> /etc/sddm.conf.d/ubuntustudio.conf<pre><code>[Theme]\nCurrent=Breeze-Noir-Dark\n\n[Users]\nMaximumUid=1004\nMinimumUid=1000\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#make-sddm-listen-to-tcp","title":"Make SDDM Listen to TCP","text":"<p>By default, SDDM launches X.org with <code>-nolisten tcp</code> for security reasons. To override this, set the flag under the <code>[X11]</code> section in <code>/usr/lib/sddm/sddm.conf.d/ubuntustudio.conf</code></p> /usr/lib/sddm/sddm.conf.d/ubuntustudio.conf<pre><code>[General]\nInputMethod=\n\n[Theme]\nCurrent=\"Breeze-Noir-Dark\"\nEnableAvatars=True\n\n[X11]\nServerArguments=-listen tcp\n</code></pre> <p>Then add a short script to authorize connections from <code>localhost</code> to the user (<code>artist</code>) session, e.g. as <code>~/bin/xhost-localhost</code></p> <pre><code>#!/bin/bash\nxhost +localhost\n</code></pre> <p>This allows an SSH session for the user (<code>artist</code>) to send messages to the screen with <code>zenity</code>:</p> <pre><code>DISPLAY=localhost:0 /usr/bin/zenity --warning \\\n  --text='Computer Will Shut Down in 20 Minutes'\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#ubuntu-studio-audio-configuration","title":"Ubuntu Studio Audio Configuration","text":"<p>Rather than something one has to setup, this seems to be something that just happens: the first time logging in with a new user (<code>artist</code>) a pop-up message from the Ubuntu Studio Audio Configuration shows up and only offers an OK button. Upon clicking this button, the system reboots. After logging in again, audio seems to work just fine.</p> <p></p>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#apt-respositories-clean-up","title":"APT respositories clean-up","text":"<pre><code># apt update\nHit:1 http://ch.archive.ubuntu.com/ubuntu noble InRelease\nGet:2 http://ch.archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]                      \nHit:3 http://archive.ubuntu.com/ubuntu noble InRelease                                          \nGet:4 http://archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]                         \nGet:5 https://dl.google.com/linux/chrome/deb stable InRelease [1,825 B]                         \nHit:6 http://ch.archive.ubuntu.com/ubuntu noble-backports InRelease                             \nGet:7 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]\nGet:8 https://dl.google.com/linux/chrome/deb stable/main amd64 Packages [1,083 B]\nGet:9 http://ch.archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [530 kB]\nGet:10 http://ch.archive.ubuntu.com/ubuntu noble-updates/main Translation-en [128 kB]\nGet:11 http://ch.archive.ubuntu.com/ubuntu noble-updates/main amd64 c-n-f Metadata [8,548 B]\nGet:12 http://ch.archive.ubuntu.com/ubuntu noble-updates/restricted amd64 Packages [353 kB]\nGet:13 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [373 kB]\nGet:14 http://ch.archive.ubuntu.com/ubuntu noble-updates/restricted Translation-en [68.1 kB]\nGet:15 http://ch.archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [373 kB]       \nGet:16 http://ch.archive.ubuntu.com/ubuntu noble-updates/universe Translation-en [153 kB]       \nGet:17 http://ch.archive.ubuntu.com/ubuntu noble-updates/universe amd64 c-n-f Metadata [14.6 kB]\nGet:18 http://archive.ubuntu.com/ubuntu noble-updates/universe Translation-en [153 kB]          \nGet:19 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 c-n-f Metadata [14.6 kB] \nGet:20 http://security.ubuntu.com/ubuntu noble-security/universe amd64 Packages [269 kB]       \nGet:21 http://security.ubuntu.com/ubuntu noble-security/universe Translation-en [113 kB]\nGet:22 http://security.ubuntu.com/ubuntu noble-security/universe amd64 c-n-f Metadata [10.1 kB]\nGet:23 http://security.ubuntu.com/ubuntu noble-security/main amd64 Packages [377 kB]\nGet:24 http://security.ubuntu.com/ubuntu noble-security/main Translation-en [81.4 kB]\nGet:25 http://security.ubuntu.com/ubuntu noble-security/main amd64 c-n-f Metadata [4,516 B]\nGet:26 http://security.ubuntu.com/ubuntu noble-security/restricted amd64 Packages [353 kB]\nGet:27 http://security.ubuntu.com/ubuntu noble-security/restricted Translation-en [68.1 kB]\nFetched 3,827 kB in 1s (2,664 kB/s)                                \nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\n27 packages can be upgraded. Run 'apt list --upgradable' to see them.\nW: Target Packages (universe/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Packages (universe/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Translations (universe/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Translations (universe/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11 (universe/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11 (universe/dep11/Components-all.yml) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons-small (universe/dep11/icons-48x48.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons (universe/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons-hidpi (universe/dep11/icons-64x64@2.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons-large (universe/dep11/icons-128x128.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target CNF (universe/cnf/Commands-amd64) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target CNF (universe/cnf/Commands-all) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Packages (multiverse/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Packages (multiverse/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Translations (multiverse/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Translations (multiverse/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11 (multiverse/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11 (multiverse/dep11/Components-all.yml) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons-small (multiverse/dep11/icons-48x48.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons (multiverse/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons-hidpi (multiverse/dep11/icons-64x64@2.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons-large (multiverse/dep11/icons-128x128.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target CNF (multiverse/cnf/Commands-amd64) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target CNF (multiverse/cnf/Commands-all) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Packages (universe/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Packages (universe/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Translations (universe/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Translations (universe/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11 (universe/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11 (universe/dep11/Components-all.yml) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons-small (universe/dep11/icons-48x48.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons (universe/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons-hidpi (universe/dep11/icons-64x64@2.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons-large (universe/dep11/icons-128x128.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target CNF (universe/cnf/Commands-amd64) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target CNF (universe/cnf/Commands-all) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Packages (multiverse/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Packages (multiverse/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Translations (multiverse/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target Translations (multiverse/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11 (multiverse/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11 (multiverse/dep11/Components-all.yml) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons-small (multiverse/dep11/icons-48x48.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons (multiverse/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons-hidpi (multiverse/dep11/icons-64x64@2.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target DEP-11-icons-large (multiverse/dep11/icons-128x128.tar) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target CNF (multiverse/cnf/Commands-amd64) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\nW: Target CNF (multiverse/cnf/Commands-all) is configured multiple times in /etc/apt/sources.list.d/dvd.list:3 and /etc/apt/sources.list.d/ubuntu.sources:2\n</code></pre> <p>This seems to be because the same repository is defined in two different ways:</p> <pre><code>root@computer:~# cat /etc/apt/sources.list.d/dvd.list \ndeb http://archive.ubuntu.com/ubuntu/ noble universe multiverse\ndeb http://archive.ubuntu.com/ubuntu/ noble-updates universe multiverse\ndeb http://security.ubuntu.com/ubuntu/ noble-security universe multiverse\n\nroot@computer:~# cat /etc/apt/sources.list.d/ubuntu.sources\nTypes: deb\nURIs: http://ch.archive.ubuntu.com/ubuntu/\nSuites: noble noble-updates noble-backports\nComponents: main restricted universe multiverse\nSigned-By: /usr/share/keyrings/ubuntu-archive-keyring.gpg\n\nTypes: deb\nURIs: http://security.ubuntu.com/ubuntu/\nSuites: noble-security\nComponents: main restricted universe multiverse\nSigned-By: /usr/share/keyrings/ubuntu-archive-keyring.gpg\n</code></pre> <p>The minimal fix for this was to comment out the last line in <code>dvd.list</code> and let <code>noble-security</code> be defined (only) in <code>ubuntu.sources</code>.</p>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#ubuntu-pro","title":"Ubuntu Pro","text":"<p>When updating the system with <code>apt full-upgrade -y</code> a notice comes up about additional security updates:</p> <pre><code>Get more security updates through Ubuntu Pro with 'esm-apps' enabled:\n  libdcmtk17t64 libcjson1 libavdevice60 ffmpeg libpostproc57 libavcodec60\n  libavutil58 libswscale7 libswresample4 libavformat60 libavfilter9\nLearn more about Ubuntu Pro at https://ubuntu.com/pro\n</code></pre> <p>This being a new system, indeed it's not attached to an Ubuntu Pro account (the old system was):</p> <pre><code># pro security-status\n3208 packages installed:\n     1638 packages from Ubuntu Main/Restricted repository\n     1569 packages from Ubuntu Universe/Multiverse repository\n     1 package from a third party\n\nTo get more information about the packages, run\n    pro security-status --help\nfor a list of available options.\n\nThis machine is receiving security patching for Ubuntu Main/Restricted\nrepository until 2029.\nThis machine is NOT attached to an Ubuntu Pro subscription.\n\nUbuntu Pro with 'esm-infra' enabled provides security updates for\nMain/Restricted packages until 2034.\n\nUbuntu Pro with 'esm-apps' enabled provides security updates for\nUniverse/Multiverse packages until 2034. There are 11 pending security updates.\n\nTry Ubuntu Pro with a free personal subscription on up to 5 machines.\nLearn more at https://ubuntu.com/pro\n</code></pre> <p>After creating an Ubuntu account a token is available to use with <code>pro attach</code>:</p> <pre><code># pro attach ...\nEnabling Ubuntu Pro: ESM Apps\nUbuntu Pro: ESM Apps enabled\nEnabling Ubuntu Pro: ESM Infra\nUbuntu Pro: ESM Infra enabled\nEnabling Livepatch\nLivepatch enabled\nThis machine is now attached to 'Ubuntu Pro - free personal subscription'\n\nSERVICE          ENTITLED  STATUS       DESCRIPTION\nanbox-cloud      yes       disabled     Scalable Android in the cloud\nesm-apps         yes       enabled      Expanded Security Maintenance for Applications\nesm-infra        yes       enabled      Expanded Security Maintenance for Infrastructure\nlandscape        yes       disabled     Management and administration tool for Ubuntu\nlivepatch        yes       warning      Current kernel is not covered by livepatch\nrealtime-kernel* yes       disabled     Ubuntu kernel with PREEMPT_RT patches integrated\n\n * Service has variants\n\nNOTICES\nOperation in progress: pro attach\nThe current kernel (6.8.0-44-lowlatency, x86_64) is not covered by livepatch.\nCovered kernels are listed here: https://ubuntu.com/security/livepatch/docs/kernels\nEither switch to a covered kernel or `sudo pro disable livepatch` to dismiss this warning.\n\nFor a list of all Ubuntu Pro services and variants, run 'pro status --all'\nEnable services with: pro enable &lt;service&gt;\n\n     Account: ponder.stibbons@uu.am\nSubscription: Ubuntu Pro - free personal subscription\n\n# pro status --all\nSERVICE          ENTITLED  STATUS       DESCRIPTION\nanbox-cloud      yes       disabled     Scalable Android in the cloud\ncc-eal           yes       n/a          Common Criteria EAL2 Provisioning Packages\nesm-apps         yes       enabled      Expanded Security Maintenance for Applications\nesm-infra        yes       enabled      Expanded Security Maintenance for Infrastructure\nfips             yes       n/a          NIST-certified FIPS crypto packages\nfips-preview     yes       n/a          Preview of FIPS crypto packages undergoing certification with NIST\nfips-updates     yes       n/a          FIPS compliant crypto packages with stable security updates\nlandscape        yes       disabled     Management and administration tool for Ubuntu\nlivepatch        yes       warning      Current kernel is not covered by livepatch\nrealtime-kernel  yes       disabled     Ubuntu kernel with PREEMPT_RT patches integrated\n\u251c generic        yes       disabled     Generic version of the RT kernel (default)\n\u251c intel-iotg     yes       n/a          RT kernel optimized for Intel IOTG platform\n\u2514 raspi          yes       n/a          24.04 Real-time kernel optimised for Raspberry Pi\nros              yes       n/a          Security Updates for the Robot Operating System\nros-updates      yes       n/a          All Updates for the Robot Operating System\nusg              yes       n/a          Security compliance and audit tools\n\nNOTICES\nThe current kernel (6.8.0-44-lowlatency, x86_64) is not covered by livepatch.\nCovered kernels are listed here: https://ubuntu.com/security/livepatch/docs/kernels\nEither switch to a covered kernel or `sudo pro disable livepatch` to dismiss this warning.\n\nEnable services with: pro enable &lt;service&gt;\n</code></pre> <p>Now the system can be updated again with <code>apt full-upgrade -y</code> to receive those additional security updates:</p> <pre><code># apt full-upgrade -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\nThe following upgrades have been deferred due to phasing:\n  python3-distupgrade ubuntu-release-upgrader-core ubuntu-release-upgrader-qt\nThe following packages will be upgraded:\n  ffmpeg libavcodec60 libavdevice60 libavfilter9 libavformat60 libavutil58 libcjson1\n  libdcmtk17t64 libpostproc57 libswresample4 libswscale7\n11 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.\n11 esm-apps security updates\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#crontab-bedtime","title":"Crontab Bedtime","text":"<p>Everybody has a natural tendency to stay in front of their  computer, or other entertaining devices, for longer than is good for them. This can be very detrimental when it impacts sleep pattners, to avoid this this computer will shut down on a regular schedule:</p> <pre><code># crontab -l | grep -i 'shut.*down'\n# Shut down at 20:30 (Sun-Thu)\n30 20 * * 0-4 /sbin/shutdown -h now\n35 20 * * 0-4 /sbin/shutdown -h now\n40 20 * * 0-4 /sbin/shutdown -h now\n45 20 * * 0-4 /sbin/shutdown -h now\n50 20 * * 0-4 /sbin/shutdown -h now\n55 20 * * 0-4 /sbin/shutdown -h now\n*/5 21,22,23 * * 0-4 /sbin/shutdown -h now\n# Shut down at 21:30 (Fri-Sat)\n30 21 * * 5-6 /sbin/shutdown -h now\n35 21 * * 5-6 /sbin/shutdown -h now\n40 21 * * 5-6 /sbin/shutdown -h now\n45 21 * * 5-6 /sbin/shutdown -h now\n50 21 * * 5-6 /sbin/shutdown -h now\n55 21 * * 5-6 /sbin/shutdown -h now\n0 22 * * 5-6 /sbin/shutdown -h now\n*/5 22,23 * * 5-6 /sbin/shutdown -h now\n</code></pre> <p>To avoid nasty surprises, the <code>artist</code> user has their own <code>crontab</code> to notify them how much time they have left:</p> <pre><code>$ crontab -l\n# m h  dom mon dow   command\n* 12 * * 6,7 /home/artist/Desktop/.bin/restore .edu .fun\n* 18 * * 1,2,3,4,5 /home/artist/Desktop/.bin/restore .edu .fun\n00 20 * * * /home/artist/Desktop/.bin/restore\n\n# Count down to shut down at 20:30 (Sun-Thu) with UI pop-up\n00 20 * * 0-4 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 30\n05 20 * * 0-4 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 25\n10 20 * * 0-4 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 20\n15 20 * * 0-4 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 15\n20 20 * * 0-4 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 10\n25 20 * * 0-4 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 5\n27 20 * * 0-4 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 3\n28 20 * * 0-4 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 2\n29 20 * * 0-4 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 1\n\n# Count down to shut down at 21:00 (interim) with UI pop-up\n30 20 * * * /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 30\n35 20 * * * /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 25\n40 20 * * * /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 20\n45 20 * * * /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 15\n50 20 * * * /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 10\n55 20 * * * /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 5\n57 20 * * * /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 3\n58 20 * * * /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 2\n59 20 * * * /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 1\n\n# Count down to shut down at 22:00 (Fri-Sat) with UI pop-up\n00 21 * * 5,6 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 30\n05 21 * * 5,6 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 25\n10 21 * * 5,6 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 20\n15 21 * * 5,6 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 15\n20 21 * * 5,6 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 10\n25 21 * * 5,6 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 5\n27 21 * * 5,6 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 3\n28 21 * * 5,6 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 2\n29 21 * * 5,6 /home/artist/bin/zenity-warning-shutdown-in-minutes.sh 1\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#install-additional-software","title":"Install Additional Software","text":""},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#google-chrome","title":"Google Chrome","text":"<p>Installing Google Chrome is as simple as downloading the Debian package and installing it:</p> <pre><code># dpkg -i google-chrome-stable_current_amd64.deb\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#steam","title":"Steam","text":"<p>Installing Steam from Snap  couldn't be simplers:</p> <pre><code># snap install steam\nsteam 1.0.0.79 from Canonical\u2713 installed\n</code></pre> <p>Note</p> <p>snapcraft.io/steam is provided by Canonical.</p> <p>When runing the Steam client for the first time, a pop-up shows up advising to install additional 32-bit drivers for best experience</p> <p></p> <pre><code># dpkg --add-architecture i386\n# apt update\n# sudo apt install libnvidia-gl-550:i386 -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  gcc-14-base:i386 libbsd0:i386 libc6:i386 libdrm2:i386 libffi8:i386 libgcc-s1:i386\n  libidn2-0:i386 libmd0:i386 libnvidia-egl-wayland1:i386 libunistring5:i386\n  libwayland-client0:i386 libwayland-server0:i386 libx11-6:i386 libxau6:i386 libxcb1:i386\n  libxdmcp6:i386 libxext6:i386\nSuggested packages:\n  glibc-doc:i386 locales:i386 libnss-nis:i386 libnss-nisplus:i386\nThe following NEW packages will be installed:\n  gcc-14-base:i386 libbsd0:i386 libc6:i386 libdrm2:i386 libffi8:i386 libgcc-s1:i386\n  libidn2-0:i386 libmd0:i386 libnvidia-egl-wayland1:i386 libnvidia-gl-550:i386\n  libunistring5:i386 libwayland-client0:i386 libwayland-server0:i386 libx11-6:i386 libxau6:i386\n  libxcb1:i386 libxdmcp6:i386 libxext6:i386\n0 upgraded, 18 newly installed, 0 to remove and 14 not upgraded.\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#non-snap-alternative","title":"Non-snap alternative","text":"<p>Installing the Steam client direction from store.steampowered.com/about is not as simple, it requires first installing several <code>i386</code> (32-bit) libraries:</p> <pre><code># apt install libgl1-mesa-glx:i386 libc6:amd64 libc6:i386 \\\n  libegl1:amd64 libegl1:i386 libgbm1:amd64 libgbm1:i386 \\\n  libgl1-mesa-dri:amd64 libgl1-mesa-dri:i386 libgl1:amd64 \\\n  libgl1:i386 steam-libs-amd64:amd64 steam-libs-i386:i386 -y\n</code></pre> <p>With those installed, one can download <code>steam_latest.deb</code> from</p> <p>and install it with <code>gdebi</code>:</p> <pre><code># gdebi steam_latest.deb\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#minecraft","title":"Minecraft","text":"<p>To avoid taking chances, copy the Minecraft launcher from the previous system:</p> <pre><code># mount /dev/sdb1 /mnt/\n# cp -a /mnt/opt/minecraft-launcher/ /opt/\n</code></pre> <p>It works perfectly right after installing; no need to login again.</p> <p>In contrast, at least 2 years ago the <code>Minecraft_staging.deb</code> launcher no longer worked, because it only allows trying to log in with Mojang accounts, and those have been deprecated.</p> <p>Today, the page for Alternative Download Options for Minecraft: Java Edition still points to the old <code>Minecraft.deb</code> (<code>md5sum: 9ec53b60fba93b1adf05a3246055e7a4</code>) and an additional Other Linux <code>Minecraft.tar.gz</code></p> <p>Installing the <code>Minecraft.deb</code> (freshly downloaded) produces a new binary with a different MD5 than the old backup:</p> <pre><code>root@computer:~/minecraft# md5sum /usr/bin/minecraft-launcher\neb40c31c9c15449770ea6a61f8f794a3  /usr/bin/minecraft-launcher\n\nlexicon:~/computer-backup$ md5sum \\\n  usr/bin/minecraft-launcher \\\n5d0a29a858de070384fcbe84540fcdc9  usr/bin/minecraft-launcher\n</code></pre> <p>Today, the page for Alternative Download Options for Minecraft: Java Edition also seems to recommend installing via Snap, but the minecraft-launcher.snap links to the AUR (Arc) repo and none of the actual snaps available are verified (or seem very popular):</p> <ul> <li>snapcraft.io/mc-installer from kz6fittycent/mc-installer has only 17 stars.</li> <li>snapcraft.io/launcher-ot-minecraft from petebuffon/launcher-ot-minecraft has only 2 star.</li> </ul>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#blender","title":"Blender","text":"<p>Blender 4.2 LTS is already available even for Ubuntu 24.04 via  snapcraft.io/blender so there is no reason to install it any other way:</p> <pre><code># snap install blender --classic\nblender 4.2.1 from Blender Foundation (blenderfoundation\u2713) installed\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#visual-studio-code","title":"Visual Studio Code","text":"<p>The artist wants to learn how to create websites and wants to use Visual Studio Code to comfortably edit HTML, CSS, JavaScript, etc.</p> <p>Installation is fairly simple, so much a single <code>.deb</code> file can be installed directly, but the apt repository can also be installed manually with the following script:</p> <pre><code># apt-get install apt-transport-https gpg wget -y\n# wget -qO- https://packages.microsoft.com/keys/microsoft.asc \\\n  | gpg --dearmor &gt; packages.microsoft.gpg\n# install -D -o root -g root -m 644 packages.microsoft.gpg \\\n  /etc/apt/keyrings/packages.microsoft.gpg\n# echo \"deb [arch=amd64,arm64,armhf signed-by=/etc/apt/keyrings/packages.microsoft.gpg] https://packages.microsoft.com/repos/code stable main\" \\\n  | tee /etc/apt/sources.list.d/vscode.list &gt; /dev/null\n# rm -f packages.microsoft.gpg\n# apt update\n# apt install code -y\n</code></pre>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#opentoonz","title":"OpenToonz","text":"<p>The artist may want to try using OpenToonz later, but that may be worth its own post given how involved Setting up the development environment for GNU/Linux is.</p>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#user-specific-settings","title":"User-specific Settings","text":""},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#redshift","title":"RedShift","text":"<p>Redshift adjusts the color temperature of your screen according to your surroundings. This may help your eyes hurt less if you are working in front of the screen at night.</p> <p>More importantly, blue light can affect your sleep, because exposure to blue light before bedtime can disrupt sleep patterns as it affects when our bodies create melatonin.</p> <p>This is why <code>redshift-gtk</code> was already installed as part of the Essential Packages. What is left to do for a full customization is to adjust the color temperature values and manual location in <code>~/.config/redshift.conf</code></p> ~/.config/redshift.conf<pre><code>[redshift]\ntemp-day=6000\ntemp-night=4000\ntransition=1\nbrightness-night=0.7\ngamma=0.8\ngamma-night=0.7\nlocation-provider=manual\nadjustment-method=randr\n\n[manual]\nlat=48\nlon=8\n\n[randr]\nscreen=0\n</code></pre> <p>Note</p> <p>It seems no longer necessary to manually add Redshift to the user's desktop session. Previously, it would be necessary to launch Autostart and Add Application\u2026 to add Redshift.</p>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#troubleshooting","title":"Troubleshooting","text":""},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#nvidia-gpu-i2c-timeout-error-e0000000","title":"<code>nvidia-gpu i2c timeout error e0000000</code>","text":"<p>On December 17, 2024 the 1440p display claims to support only a resolution of 1024x764; there is no option to change this. Previously, a similar issue had happened with a very old GPU (Nvidia GT-610) which would be no longer supported by recent NVidia drivers. </p> <p>This PC has an MSI GeForce GTX 1660 SUPER VENTUS XS OC (6GB) which is supported in the latest driver version 550.120. At that time there was a clear message in <code>dmesg</code> but this time there is nearly nothing:</p> <pre><code>root@computer:~# dmesg | grep -i nvidia\n[    6.394310] input: HDA NVidia HDMI/DP,pcm=3 as /devices/pci0000:00/0000:00:03.1/0000:0a:00.1/sound/card0/input8\n[    6.394526] input: HDA NVidia HDMI/DP,pcm=7 as /devices/pci0000:00/0000:00:03.1/0000:0a:00.1/sound/card0/input9\n[    6.394643] input: HDA NVidia HDMI/DP,pcm=8 as /devices/pci0000:00/0000:00:03.1/0000:0a:00.1/sound/card0/input10\n[    6.394733] input: HDA NVidia HDMI/DP,pcm=9 as /devices/pci0000:00/0000:00:03.1/0000:0a:00.1/sound/card0/input11\n[    7.382902] nvidia-gpu 0000:0a:00.3: i2c timeout error e0000000\n</code></pre> <p>The timing of this aligns with a system update to kernel 6.8.0-50:</p> <pre><code>root@computer:~# ls -l /boot/\ntotal 204049\n-rw-r--r-- 1 root root   287453 Nov  9 21:38 config-6.8.0-49-lowlatency\n-rw-r--r-- 1 root root   287407 Nov 21 02:13 config-6.8.0-50-lowlatency\ndrwxr-xr-x 3 root root      512 Jan  1  1970 efi\ndrwxr-xr-x 6 root root     4096 Dec 17 12:47 grub\nlrwxrwxrwx 1 root root       30 Dec 17 12:46 initrd.img -&gt; initrd.img-6.8.0-50-lowlatency\n-rw-r--r-- 1 root root 24212231 Sep 14 22:39 initrd.img-6.8.0-41-lowlatency\n-rw-r--r-- 1 root root 67743692 Dec 10 19:38 initrd.img-6.8.0-49-lowlatency\n-rw-r--r-- 1 root root 67736037 Dec 17 12:47 initrd.img-6.8.0-50-lowlatency\nlrwxrwxrwx 1 root root       30 Dec 17 12:46 initrd.img.old -&gt; initrd.img-6.8.0-49-lowlatency\n-rw-r--r-- 1 root root   142796 Apr  8  2024 memtest86+ia32.bin\n-rw-r--r-- 1 root root   143872 Apr  8  2024 memtest86+ia32.efi\n-rw-r--r-- 1 root root   147744 Apr  8  2024 memtest86+x64.bin\n-rw-r--r-- 1 root root   148992 Apr  8  2024 memtest86+x64.efi\n-rw------- 1 root root  9064172 Nov  9 21:38 System.map-6.8.0-49-lowlatency\n-rw------- 1 root root  9072978 Nov 21 02:13 System.map-6.8.0-50-lowlatency\nlrwxrwxrwx 1 root root       27 Dec 17 12:46 vmlinuz -&gt; vmlinuz-6.8.0-50-lowlatency\n-rw------- 1 root root 14961032 Nov  9 21:40 vmlinuz-6.8.0-49-lowlatency\n-rw------- 1 root root 14969224 Nov 21 02:33 vmlinuz-6.8.0-50-lowlatency\nlrwxrwxrwx 1 root root       27 Dec 17 12:46 vmlinuz.old -&gt; vmlinuz-6.8.0-49-lowlatency\n</code></pre> <p>Scrolling through the entire <code>dmesg</code> ouput there is not much to see about the NVidia driver, only this <code>i2c</code> timeout:</p> <pre><code>root@computer:~# dmesg \n[    0.000000] Linux version 6.8.0-50-lowlatency (buildd@lcy02-amd64-026) (x86_64-linux-gnu-gcc-13 (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0, GNU ld (GNU Binutils for Ubuntu) 2.42) #51.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 21 12:44:17 UTC 2024 (Ubuntu 6.8.0-50.51.1-lowlatency 6.8.12)\n[    0.000000] Command line: BOOT_IMAGE=/boot/vmlinuz-6.8.0-50-lowlatency root=UUID=0c531d25-fb2f-4f74-8382-c2160008b355 ro quiet splash threadirqs vt.handoff=7\n...\n[    7.093902] cfg80211: Loading compiled-in X.509 certificates for regulatory database\n[    7.094199] Loaded X.509 cert 'sforshee: 00b28ddf47aef9cea7'\n[    7.094419] Loaded X.509 cert 'wens: 61c038651aabdcf94bd0ac7ff06c7248db18c600'\n[    7.382902] nvidia-gpu 0000:0a:00.3: i2c timeout error e0000000\n[    7.382910] ucsi_ccg 3-0008: i2c_transfer failed -110\n[    7.382914] ucsi_ccg 3-0008: ucsi_ccg_init failed - -110\n[    7.382919] ucsi_ccg: probe of 3-0008 failed with error -110\n[    7.888419] usb 3-2.2: set resolution quirk: cval-&gt;res = 384\n[    7.888768] usbcore: registered new interface driver snd-usb-audio\n</code></pre> <p>Web search for recent(ish) thread on this topic, concretely \"nvidia-gpu\" \"i2c timeout error e0000000\" \"i2c_transfer failed -110\" \"ucsi_ccg_init failed\" yields a few results with a few common themes:</p> <ul> <li>From <code>/r/openSUSE</code> on Sep 29, 2024: No desktop after updating to kernel 6.11 - snapshot 20240927</li> <li>based on Issues with kernel 6.11.0-1-default and Nvidia drivers on Sep 27, 2024</li> <li>suggests adding kernel parameters:<ul> <li><code>nvidia_drm.modeset=1 nvidia_drm.fbdev=1</code></li> </ul> </li> <li>From <code>/r/linuxmint</code> on May 20, 2023: My computer no longer works after updating my nvidia drivers to 520 open</li> <li><code>i2c</code> timeout starts after updating NVidia drivers</li> <li>suggests reverting to previous kernel helps</li> <li>leaves unclear whether drivers were replaced with <code>nouveau</code></li> <li>From <code>/r/Ubuntu</code> on Aug 10, 2024: Need help with i2c timeout error - Ubuntu 22.04.4 LTM (Jelly Jamfish) - Dual booted version</li> <li>suggests blocklisting the <code>i2c_nvidia_gpu</code> module</li> <li>other steps include troubleshooting an NVME drive and removing NVidia drivers, to then install default drivers with    <code>ubuntu-drivers autoinstall</code> (unclear what this installed)</li> <li>Ubuntu not booting - i2c timeout error? thread in     askubuntu.com yielded no useful answers.</li> <li>From the NVidia developer forum on Jul 29, 2023:    GT1660 Nvidia errors in ubuntu 18.04 - nvidia-gpu i2c timeout error e0000000</li> <li>From <code>r/archlinux</code> on Oct 31, 2021: nvidia-gpu 0000:07:00.3: i2c timeout error e0000000</li> <li>shows this error is nothing new, and points to</li> <li>[Dual system, Ubuntu 20.04]nvidia-gpu: i2c timeout error; ucsi_ccg: i2c_transfer failed -110, ucsi_ccg_init failed<ul> <li>which also suggests blocklisting the <code>i2c_nvidia_gpu</code> module</li> </ul> </li> </ul> <p>So far the most popular workaround seems to be:</p> <pre><code># echo \"blacklist i2c_nvidia_gpu\" &gt; \\\n  /etc/modprobe.d/blacklist_i2c-nvidia-gpu.conf\n# update-initramfs -u\n</code></pre> <p>The module is currently loaded, but not in used; it can be unloaded:</p> <pre><code># lsmod | grep -i nv\ni2c_nvidia_gpu         12288  0\ni2c_ccgx_ucsi          12288  1 i2c_nvidia_gpu\nnvme                   61440  3\nnvme_core             212992  4 nvme\nnvme_auth              28672  1 nvme_core\n\nroot@computer:~# modprobe -r i2c_nvidia_gpu\nroot@computer:~# lsmod | grep -i nv\nnvme                   61440  3\nnvme_core             212992  4 nvme\nnvme_auth              28672  1 nvme_core\n</code></pre> <p>However, a more recent workaround is adding kernel parameters <code>nvidia_drm.modeset=1 nvidia_drm.fbdev=1</code>.</p> <p>This could be related to a recent change in those parameters, since there is already something similar under </p> <pre><code>root@computer:~# ls -l /etc/modprobe.d/nvidia-graphics-drivers-kms.conf \n-rw-r--r-- 1 root root 117 Jul 31 16:08 /etc/modprobe.d/nvidia-graphics-drivers-kms.conf\n\nroot@computer:~# cat /etc/modprobe.d/nvidia-graphics-drivers-kms.conf \n# This file was generated by nvidia-driver-550\n# Set value to 0 to disable modesetting\noptions nvidia-drm modeset=1\n</code></pre> <p>There is also a blocklist for <code>nvidiafb</code> in  <code>/etc/modprobe.d/blacklist-framebuffer.conf</code></p> <p>Before trying any of this, there are a few updates available, including <code>linux-firmware</code>, so first update the system with</p> <code># apt update &amp;&amp; apt full-upgrade -y</code> <pre><code>root@computer:~# apt update &amp;&amp; apt full-upgrade -y\nHit:1 http://ch.archive.ubuntu.com/ubuntu noble InRelease\nHit:2 http://ch.archive.ubuntu.com/ubuntu noble-updates InRelease\nHit:3 https://brave-browser-apt-release.s3.brave.com stable InRelease\nHit:4 http://archive.ubuntu.com/ubuntu noble InRelease\nHit:5 http://ch.archive.ubuntu.com/ubuntu noble-backports InRelease\nHit:6 https://dl.google.com/linux/chrome/deb stable InRelease\nHit:7 http://security.ubuntu.com/ubuntu noble-security InRelease\nHit:8 https://packages.microsoft.com/repos/code stable InRelease                                                  \nHit:9 http://archive.ubuntu.com/ubuntu noble-updates InRelease                              \nHit:10 https://esm.ubuntu.com/apps/ubuntu noble-apps-security InRelease\nHit:11 https://esm.ubuntu.com/apps/ubuntu noble-apps-updates InRelease\nHit:12 https://esm.ubuntu.com/infra/ubuntu noble-infra-security InRelease\nHit:13 https://esm.ubuntu.com/infra/ubuntu noble-infra-updates InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\n39 packages can be upgraded. Run 'apt list --upgradable' to see them.\nN: Skipping acquire of configured file 'main/binary-i386/Packages' as repository 'https://brave-browser-apt-release.s3.brave.com stable InRelease' doesn't support architecture 'i386'\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\nThe following packages were automatically installed and are no longer required:\n  libnvidia-cfg1-550 libnvidia-decode-550 libnvidia-egl-wayland1 libnvidia-encode-550\n  libnvidia-extra-550 libnvidia-fbc1-550 libnvidia-gl-550 nvidia-compute-utils-550\n  nvidia-kernel-source-550 nvidia-prime nvidia-settings nvidia-utils-550\n  screen-resolution-extra xserver-xorg-video-nvidia-550\nUse 'apt autoremove' to remove them.\nThe following upgrades have been deferred due to phasing:\n  cloud-init python3-distupgrade ubuntu-release-upgrader-core ubuntu-release-upgrader-qt\nThe following packages will be upgraded:\n  apport apport-core-dump-handler apport-kde brave-browser code fwupd\n  gir1.2-packagekitglib-1.0 gir1.2-udisks-2.0 google-chrome-stable libegl-mesa0\n  libegl1-mesa-dev libfwupd2 libgbm1 libgl1-mesa-dri libglapi-mesa libglx-mesa0 libnm0\n  libosmesa6 libpackagekit-glib2-18 libudisks2-0 libxatracker2 linux-firmware linux-libc-dev\n  linux-tools-common lp-solve mesa-va-drivers mesa-vdpau-drivers mesa-vulkan-drivers\n  network-manager packagekit packagekit-tools python3-apport python3-problem-report thermald\n  udisks2\n35 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n2 standard LTS security updates\nNeed to get 0 B/881 MB of archives.\nAfter this operation, 6,394 kB of additional disk space will be used.\nExtracting templates from packages: 100%\nPreconfiguring packages ...\n(Reading database ... 469435 files and directories currently installed.)\nPreparing to unpack .../00-python3-problem-report_2.28.1-0ubuntu3.3_all.deb ...\nUnpacking python3-problem-report (2.28.1-0ubuntu3.3) over (2.28.1-0ubuntu3.1) ...\nPreparing to unpack .../01-python3-apport_2.28.1-0ubuntu3.3_all.deb ...\nUnpacking python3-apport (2.28.1-0ubuntu3.3) over (2.28.1-0ubuntu3.1) ...\nPreparing to unpack .../02-apport-core-dump-handler_2.28.1-0ubuntu3.3_all.deb ...\nUnpacking apport-core-dump-handler (2.28.1-0ubuntu3.3) over (2.28.1-0ubuntu3.1) ...\nPreparing to unpack .../03-apport_2.28.1-0ubuntu3.3_all.deb ...\nUnpacking apport (2.28.1-0ubuntu3.3) over (2.28.1-0ubuntu3.1) ...\nPreparing to unpack .../04-libosmesa6_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libosmesa6:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.2) ...\nPreparing to unpack .../05-libgl1-mesa-dri_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libgl1-mesa-dri:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.2) ...\nPreparing to unpack .../06-libglx-mesa0_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libglx-mesa0:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.2) ...\nPreparing to unpack .../07-libegl-mesa0_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libegl-mesa0:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.2) ...\nPreparing to unpack .../08-libglapi-mesa_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libglapi-mesa:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.2) ...\nPreparing to unpack .../09-libgbm1_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libgbm1:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.2) ...\nPreparing to unpack .../10-brave-browser_1.73.101_amd64.deb ...\nUnpacking brave-browser (1.73.101) over (1.73.97) ...\nPreparing to unpack .../11-google-chrome-stable_131.0.6778.139-1_amd64.deb ...\nUnpacking google-chrome-stable (131.0.6778.139-1) over (131.0.6778.108-1) ...\nPreparing to unpack .../12-apport-kde_2.28.1-0ubuntu3.3_all.deb ...\nUnpacking apport-kde (2.28.1-0ubuntu3.3) over (2.28.1-0ubuntu3.1) ...\nPreparing to unpack .../13-code_1.96.0-1733888194_amd64.deb ...\nUnpacking code (1.96.0-1733888194) over (1.95.3-1731513102) ...\nWarning in file \"/usr/share/applications/displaycal-vrml-to-x3d-converter.desktop\": usage of MIME type \"x-world/x-vrml\" is discouraged (the use of \"x-world\" as media type is strongly discouraged in favor of a subtype of the \"application\" media type)\nWarning in file \"/usr/share/applications/displaycal-vrml-to-x3d-converter.desktop\": usage of MIME type \"x-world/x-vrml\" is discouraged (the use of \"x-world\" as media type is strongly discouraged in favor of a subtype of the \"application\" media type)\nPreparing to unpack .../14-libfwupd2_1.9.27-0ubuntu1~24.04.1_amd64.deb ...\nUnpacking libfwupd2:amd64 (1.9.27-0ubuntu1~24.04.1) over (1.9.24-1~24.04.1) ...\nPreparing to unpack .../15-fwupd_1.9.27-0ubuntu1~24.04.1_amd64.deb ...\nUnpacking fwupd (1.9.27-0ubuntu1~24.04.1) over (1.9.24-1~24.04.1) ...\nPreparing to unpack .../16-libpackagekit-glib2-18_1.2.8-2ubuntu1_amd64.deb ...\nUnpacking libpackagekit-glib2-18:amd64 (1.2.8-2ubuntu1) over (1.2.8-2build3) ...\nPreparing to unpack .../17-gir1.2-packagekitglib-1.0_1.2.8-2ubuntu1_amd64.deb ...\nUnpacking gir1.2-packagekitglib-1.0 (1.2.8-2ubuntu1) over (1.2.8-2build3) ...\nPreparing to unpack .../18-udisks2_2.10.1-6ubuntu1_amd64.deb ...\nUnpacking udisks2 (2.10.1-6ubuntu1) over (2.10.1-6build1) ...\nPreparing to unpack .../19-libudisks2-0_2.10.1-6ubuntu1_amd64.deb ...\nUnpacking libudisks2-0:amd64 (2.10.1-6ubuntu1) over (2.10.1-6build1) ...\nPreparing to unpack .../20-gir1.2-udisks-2.0_2.10.1-6ubuntu1_amd64.deb ...\nUnpacking gir1.2-udisks-2.0:amd64 (2.10.1-6ubuntu1) over (2.10.1-6build1) ...\nPreparing to unpack .../21-libegl1-mesa-dev_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libegl1-mesa-dev:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.2) ...\nPreparing to unpack .../22-network-manager_1.46.0-1ubuntu2.2_amd64.deb ...\nUnpacking network-manager (1.46.0-1ubuntu2.2) over (1.46.0-1ubuntu2) ...\nPreparing to unpack .../23-libnm0_1.46.0-1ubuntu2.2_amd64.deb ...\nUnpacking libnm0:amd64 (1.46.0-1ubuntu2.2) over (1.46.0-1ubuntu2) ...\nPreparing to unpack .../24-libxatracker2_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libxatracker2:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.2) ...\nPreparing to unpack .../25-linux-firmware_20240318.git3b128b60-0ubuntu2.6_amd64.deb ...\nUnpacking linux-firmware (20240318.git3b128b60-0ubuntu2.6) over (20240318.git3b128b60-0ubuntu2.5) ...\nPreparing to unpack .../26-linux-libc-dev_6.8.0-51.52_amd64.deb ...\nUnpacking linux-libc-dev:amd64 (6.8.0-51.52) over (6.8.0-50.51) ...\nPreparing to unpack .../27-linux-tools-common_6.8.0-51.52_all.deb ...\nUnpacking linux-tools-common (6.8.0-51.52) over (6.8.0-50.51) ...\nPreparing to unpack .../28-lp-solve_5.5.2.5-2ubuntu0.1_amd64.deb ...\nUnpacking lp-solve (5.5.2.5-2ubuntu0.1) over (5.5.2.5-2build4) ...\nPreparing to unpack .../29-mesa-va-drivers_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking mesa-va-drivers:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.2) ...\nPreparing to unpack .../30-mesa-vdpau-drivers_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking mesa-vdpau-drivers:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.2) ...\nPreparing to unpack .../31-mesa-vulkan-drivers_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking mesa-vulkan-drivers:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.2) ...\nPreparing to unpack .../32-packagekit-tools_1.2.8-2ubuntu1_amd64.deb ...\nUnpacking packagekit-tools (1.2.8-2ubuntu1) over (1.2.8-2build3) ...\nPreparing to unpack .../33-packagekit_1.2.8-2ubuntu1_amd64.deb ...\nUnpacking packagekit (1.2.8-2ubuntu1) over (1.2.8-2build3) ...\nPreparing to unpack .../34-thermald_2.5.6-2ubuntu0.24.04.1_amd64.deb ...\nUnpacking thermald (2.5.6-2ubuntu0.24.04.1) over (2.5.6-2build2) ...\nSetting up mesa-vulkan-drivers:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up mesa-vdpau-drivers:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up libgbm1:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up linux-firmware (20240318.git3b128b60-0ubuntu2.6) ...\nSetting up python3-problem-report (2.28.1-0ubuntu3.3) ...\nSetting up libfwupd2:amd64 (1.9.27-0ubuntu1~24.04.1) ...\nSetting up lp-solve (5.5.2.5-2ubuntu0.1) ...\nSetting up linux-libc-dev:amd64 (6.8.0-51.52) ...\nSetting up libpackagekit-glib2-18:amd64 (1.2.8-2ubuntu1) ...\nSetting up libxatracker2:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up python3-apport (2.28.1-0ubuntu3.3) ...\nSetting up gir1.2-packagekitglib-1.0 (1.2.8-2ubuntu1) ...\nSetting up libglapi-mesa:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up libnm0:amd64 (1.46.0-1ubuntu2.2) ...\nSetting up brave-browser (1.73.101) ...\nSetting up packagekit (1.2.8-2ubuntu1) ...\nSetting up mesa-va-drivers:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up linux-tools-common (6.8.0-51.52) ...\nSetting up thermald (2.5.6-2ubuntu0.24.04.1) ...\nSetting up fwupd (1.9.27-0ubuntu1~24.04.1) ...\nfwupd-offline-update.service is a disabled or a static unit not running, not starting it.\nfwupd-refresh.service is a disabled or a static unit not running, not starting it.\nfwupd.service is a disabled or a static unit not running, not starting it.\nSetting up libudisks2-0:amd64 (2.10.1-6ubuntu1) ...\nSetting up libosmesa6:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up libegl1-mesa-dev:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up code (1.96.0-1733888194) ...\nWarning in file \"/usr/share/applications/displaycal-vrml-to-x3d-converter.desktop\": usage of MIME type \"x-world/x-vrml\" is discouraged (the use of \"x-world\" as media type is strongly discouraged in favor of a subtype of the \"application\" media type)\nWarning in file \"/usr/share/applications/displaycal-vrml-to-x3d-converter.desktop\": usage of MIME type \"x-world/x-vrml\" is discouraged (the use of \"x-world\" as media type is strongly discouraged in favor of a subtype of the \"application\" media type)\nSetting up packagekit-tools (1.2.8-2ubuntu1) ...\nSetting up udisks2 (2.10.1-6ubuntu1) ...\nSetting up google-chrome-stable (131.0.6778.139-1) ...\nSetting up gir1.2-udisks-2.0:amd64 (2.10.1-6ubuntu1) ...\nSetting up libgl1-mesa-dri:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up network-manager (1.46.0-1ubuntu2.2) ...\nSetting up libegl-mesa0:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up libglx-mesa0:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up apport-core-dump-handler (2.28.1-0ubuntu3.3) ...\nSetting up apport (2.28.1-0ubuntu3.3) ...\napport-autoreport.service is a disabled or a static unit not running, not starting it.\nSetting up apport-kde (2.28.1-0ubuntu3.3) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for dbus (1.14.10-4ubuntu4.1) ...\nProcessing triggers for shared-mime-info (2.4-4) ...\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.3) ...\nProcessing triggers for initramfs-tools (0.142ubuntu25.4) ...\nupdate-initramfs: Generating /boot/initrd.img-6.8.0-50-lowlatency\n</code></pre> <p>Reboot after <code>/boot/initrd.img-6.8.0-50-lowlatency</code> is regenerated...</p> <p>The problem is not gone, so lets try the most popular workaround...</p> <pre><code>root@computer:~# echo \"blacklist i2c_nvidia_gpu\" &gt; \\\n  /etc/modprobe.d/blacklist_i2c-nvidia-gpu.conf\nroot@computer:~# cat /etc/modprobe.d/blacklist_i2c-nvidia-gpu.conf\nblacklist i2c_nvidia_gpu\nroot@computer:~# update-initramfs -u\nupdate-initramfs: Generating /boot/initrd.img-6.8.0-50-lowlatency\n</code></pre> <p>The problem is not gone, the error is gone from <code>dmesg</code> but this is only because none of the NVidia driver modules are loaded at all.</p> <p>Lets undo the more popular workaround and try the more recent workaround... nope, that doesn't work!</p> <p>What does work is booting the previous kernel (6.8.0-49), so the workaround may be to rollback, or possibly to Install Linux Kernel 6.9. In the meantime, selecting the old kernel manually should work.</p>"},{"location":"blog/2024/09/14/ubuntu-studio-2404-on-computer-for-a-young-artist/#2nd-round","title":"2nd round","text":"<p>Found an even more recent thread (December 2024) in https://forums.linuxmint.com/viewtopic.php?t=435952 with a slightly more complete workaround:</p> <pre><code># echo \"blacklist nouveau\" \\\n  &gt;&gt; /etc/modprobe.d/blacklist-nouveua.conf\n\n# echo \"options nvidia NVreg_PreserveVideoMemoryAllocations=1\" \\\n  &gt;&gt; /etc/modprobe.d/nvidia.conf\n\n# echo \"options nvidia-drm modeset=1 fbdev=1\" \\\n  &gt;&gt; /etc/modprobe.d/nvidia.conf\n\n# update-initramfs -c -k $(uname -r)\n</code></pre> <p>This adds blacklisting the <code>nouveau</code> driver, but this is not enough; at least not combined with <code>fbdev=1</code> alone:</p> <pre><code># dmesg | egrep -i -C4 'nouveau|vesa|afb|nvidia|modeset'\n</code></pre> <pre><code>[    6.009721] systemd-journald[685]: Rotating system journal.\n[    6.234214] ccp 0000:0b:00.2: enabling device (0000 -&gt; 0002)\n[    6.235094] piix4_smbus 0000:00:14.0: SMBus Host Controller at 0xb00, revision 0\n[    6.235099] piix4_smbus 0000:00:14.0: Using register 0x02 for SMBus port selection\n[    6.243520] nvidiafb: Device ID: 10de21c4 \n[    6.243528] nvidiafb: unknown NV_ARCH\n[    6.244322] piix4_smbus 0000:00:14.0: Auxiliary SMBus Host Controller at 0xb20\n[    6.247032] mc: Linux media interface: v0.10\n[    6.251040] ccp 0000:0b:00.2: ccp enabled\n[    6.251147] ccp 0000:0b:00.2: psp enabled\n--\n[    6.447418] snd_hda_codec_realtek hdaudioC2D0:    inputs:\n[    6.447422] snd_hda_codec_realtek hdaudioC2D0:      Front Mic=0x19\n[    6.447426] snd_hda_codec_realtek hdaudioC2D0:      Rear Mic=0x18\n[    6.447430] snd_hda_codec_realtek hdaudioC2D0:      Line=0x1a\n[    6.448014] input: HDA NVidia HDMI/DP,pcm=3 as /devices/pci0000:00/0000:00:03.1/0000:0a:00.1/sound/card0/input8\n[    6.448126] input: HDA NVidia HDMI/DP,pcm=7 as /devices/pci0000:00/0000:00:03.1/0000:0a:00.1/sound/card0/input9\n[    6.448241] input: HDA NVidia HDMI/DP,pcm=8 as /devices/pci0000:00/0000:00:03.1/0000:0a:00.1/sound/card0/input10\n[    6.448365] input: HDA NVidia HDMI/DP,pcm=9 as /devices/pci0000:00/0000:00:03.1/0000:0a:00.1/sound/card0/input11\n[    6.462474] input: HD-Audio Generic Front Mic as /devices/pci0000:00/0000:00:08.1/0000:0c:00.3/sound/card2/input12\n[    6.462592] input: HD-Audio Generic Rear Mic as /devices/pci0000:00/0000:00:08.1/0000:0c:00.3/sound/card2/input13\n[    6.462665] input: HD-Audio Generic Line as /devices/pci0000:00/0000:00:08.1/0000:0c:00.3/sound/card2/input14\n[    6.462727] input: HD-Audio Generic Line Out Front as /devices/pci0000:00/0000:00:08.1/0000:0c:00.3/sound/card2/input15\n--\n[    7.085598] RPC: Registered tcp NFSv4.1 backchannel transport module.\n[    7.164422] cfg80211: Loading compiled-in X.509 certificates for regulatory database\n[    7.164640] Loaded X.509 cert 'sforshee: 00b28ddf47aef9cea7'\n[    7.164788] Loaded X.509 cert 'wens: 61c038651aabdcf94bd0ac7ff06c7248db18c600'\n[    7.428902] nvidia-gpu 0000:0a:00.3: i2c timeout error e0000000\n[    7.428912] ucsi_ccg 1-0008: i2c_transfer failed -110\n[    7.428918] ucsi_ccg 1-0008: ucsi_ccg_init failed - -110\n[    7.428923] ucsi_ccg: probe of 1-0008 failed with error -110\n[    7.974293] usb 3-2.2: set resolution quirk: cval-&gt;res = 384\n</code></pre> <p>The output from <code>inxi -Gx</code> still shows the <code>nouveau</code> is the one in use:</p> <pre><code># inxi -Gx\nGraphics:\n  Device-1: NVIDIA TU116 [GeForce GTX 1660 SUPER] vendor: Micro-Star MSI driver: N/A arch: Turing\n    bus-ID: 0a:00.0\n  Device-2: Logitech Webcam C270 driver: snd-usb-audio,uvcvideo type: USB bus-ID: 3-2.2:4\n  Display: server: X.org v: 1.21.1.11 with: Xwayland v: 23.2.6 driver: X:\n    loaded: modesetting,nouveau unloaded: fbdev,vesa gpu: N/A tty: 110x59\n  API: EGL v: 1.5 drivers: swrast platforms: active: surfaceless,device\n    inactive: gbm,wayland,x11\n  API: OpenGL v: 4.5 vendor: mesa v: 24.2.8-1ubuntu1~24.04.1 note: console (EGL sourced)\n    renderer: llvmpipe (LLVM 19.1.1 256 bits)\n  API: Vulkan v: 1.3.275 drivers: N/A surfaces: N/A devices: 1\n\n# lspci -vnn | grep -Ei \"vga|3d|display|kernel\"\n\n0a:00.0 VGA compatible controller [0300]: NVIDIA Corporation TU116 [GeForce GTX 1660 SUPER] [10de:21c4] (rev a1) (prog-if 00 [VGA controller])\n        Kernel modules: nvidiafb, nouveau\n</code></pre> <p>Todo</p> <p>Try again with <code>options nvidia NVreg_PreserveVideoMemoryAllocations=1</code> in <code>/etc/modprobe.d/nvidia.conf</code>.</p>"},{"location":"blog/2024/09/22/kubernetes-cluster-dns-issues/","title":"Kubernetes cluster DNS issues","text":"<p>Always follow Kubernetes setup step carefully!</p> <p>Forwarding IPv4 and letting iptables see bridged traffic is not optional; even if it looks like it.</p> <p>Skipping that crucial step that takes a few minutes eventually led to wasting over 3 hours troubleshooting a issue that, apparently, nobody has ever solved on the Internet before. Naturally, because nobody should ever need to.</p>"},{"location":"blog/2024/09/22/kubernetes-cluster-dns-issues/#the-problems","title":"The Problem(s)","text":"<p>When I tried to push to Github from Visual Studio Code, after upgrading the Kubernetes on Rapture (which is the article I was trying to publish), it failed with:</p> <p>Git: fatal: unable to access 'https://github.com/stibbons1990/hex.git/': Could not resolve host: github.com</p> <p>Showing command output only confirmed the error came from <code>git</code>:</p> <pre><code>$ git push origin main:main\nfatal: unable to access 'https://github.com/stibbons1990/hex.git/': Could not resolve host: github.com\n</code></pre> <p>At this moment several other issues immediately came to mind, as I have been having DNS-related issues other pods and in the other cluster:</p> <ol> <li>Plex Media Server in Rapture first appeared unavailable, then    would show itself as not authorized, eventually when trying to    unclaim and reclaim the it the very last step failed silently.    In hindsight, this looks like it tried to send a request to the    Plex network and it fails due to the name not resolving.</li> <li>Monitoring suddently stopped working after a reboot the    previous day, but the failure was only between Grafana and    InfluxDB and only when using Grafana reached for InfluxDB on    its internal service name (<code>influxdb-svc</code>) and port     http://influxdb-svc:18086, even    though the service was reachable on its external HTTPS address     (which actually relies on the same internal service name).</li> <li>Audiobookshelf is still now unable to resolve the address of    <code>api.audnex.us</code> even (several days) after the issue    (#3385)    was resolved. This fails in Rapture too, when deploying there.</li> <li>Minecraft server is no longer running and stuck in     <code>CrashLoopBackOff</code> because it cannot resolve <code>api.papermc.io</code>.</li> </ol> <p>The Minecraft server issue was discovered while troubleshooting the others, and the logs offer a little more details:</p> <pre><code>$ kubectl -n minecraft-server logs minecraft-server-76f44bd597-kpw58 -f\n[init] Running as uid=1003 gid=1003 with /data as 'drwxrwxr-x 1 1003 1003 886 Sep 20 04:10 /data'\n[init] Resolving type given PAPER\n[mc-image-helper] 10:53:56.316 ERROR : 'install-paper' command failed. Version is 1.39.11\nreactor.core.Exceptions$ReactiveException: io.netty.resolver.dns.DnsResolveContext$SearchDomainUnknownHostException: Failed to resolve 'api.papermc.io' [A(1)] and search domain query for configured domains failed as well: [minecraft-server.svc.cluster.local, svc.cluster.local, cluster.local]\n...\nCaused by: io.netty.resolver.dns.DnsNameResolverTimeoutException: [9444: /10.96.0.10:53] DefaultDnsQuestion(api.papermc.io.minecraft-server.svc.cluster.local. IN A) query '9444' via UDP timed out after 5000 milliseconds (no stack trace available)\n</code></pre>"},{"location":"blog/2024/09/22/kubernetes-cluster-dns-issues/#kube-dns-is-healthy","title":"Kube-DNS is healthy","text":"<p>The first thing to check would be whether Kube DNS (CoreDNS) is running and healthy (answers queries correctly). This apperas to be the case in both clusters:</p>"},{"location":"blog/2024/09/22/kubernetes-cluster-dns-issues/#rapture","title":"Rapture","text":"<p>The service is running on ClusterIP 10.96.0.10 and it answers correctly (104.21.44.87 is a known-good address of api.audnex.us):</p> <pre><code>$ kubectl get all -A | grep -i dns\nkube-system            pod/coredns-5d78c9869d-9xbh4                    1/1     Running   0               160m\nkube-system            pod/coredns-5d78c9869d-w8954                    1/1     Running   0               160m\nkube-system            service/kube-dns                             ClusterIP      10.96.0.10       &lt;none&gt;          53/UDP,53/TCP,9153/TCP                                                                          132d\nkube-system            deployment.apps/coredns                     2/2     2            2           132d\nkube-system            replicaset.apps/coredns-5d78c9869d                    2         2         2       160m\nkube-system            replicaset.apps/coredns-787d4945fb                    0         0         0       132d\n\n$ nslookup k8s.io 10.96.0.10\nServer:         10.96.0.10\nAddress:        10.96.0.10#53\n\nNon-authoritative answer:\nName:   k8s.io\nAddress: 34.107.204.206\nName:   k8s.io\nAddress: 2600:1901:0:26f3::\n\n$ nslookup k8s.io \nServer:         127.0.0.53\nAddress:        127.0.0.53#53\n\nNon-authoritative answer:\nName:   k8s.io\nAddress: 34.107.204.206\nName:   k8s.io\nAddress: 2600:1901:0:26f3::\n\n$ nslookup api.audnex.us 10.96.0.10\nServer:         10.96.0.10\nAddress:        10.96.0.10#53\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 104.21.44.87\nName:   api.audnex.us\nAddress: 172.67.198.55\nName:   api.audnex.us\nAddress: 2606:4700:3030::ac43:c637\nName:   api.audnex.us\nAddress: 2606:4700:3037::6815:2c57\n\n$ nslookup api.audnex.us\nServer:         127.0.0.53\nAddress:        127.0.0.53#53\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 104.21.44.87\nName:   api.audnex.us\nAddress: 172.67.198.55\nName:   api.audnex.us\nAddress: 2606:4700:3030::ac43:c637\nName:   api.audnex.us\nAddress: 2606:4700:3037::6815:2c57\n\n$ nslookup api.papermc.io 10.96.0.10\nServer:         10.96.0.10\nAddress:        10.96.0.10#53\n\nNon-authoritative answer:\nName:   api.papermc.io\nAddress: 104.26.12.138\nName:   api.papermc.io\nAddress: 172.67.72.198\nName:   api.papermc.io\nAddress: 104.26.13.138\nName:   api.papermc.io\nAddress: 2606:4700:20::681a:c8a\nName:   api.papermc.io\nAddress: 2606:4700:20::ac43:48c6\nName:   api.papermc.io\nAddress: 2606:4700:20::681a:d8a\n\n$ nslookup api.papermc.io\nServer:         127.0.0.53\nAddress:        127.0.0.53#53\n\nNon-authoritative answer:\nName:   api.papermc.io\nAddress: 104.26.13.138\nName:   api.papermc.io\nAddress: 172.67.72.198\nName:   api.papermc.io\nAddress: 104.26.12.138\nName:   api.papermc.io\nAddress: 2606:4700:20::681a:d8a\nName:   api.papermc.io\nAddress: 2606:4700:20::681a:c8a\nName:   api.papermc.io\nAddress: 2606:4700:20::ac43:48c6\n</code></pre>"},{"location":"blog/2024/09/22/kubernetes-cluster-dns-issues/#rapture_1","title":"Rapture","text":"<p>The service is running on ClusterIP 10.96.0.10 and it answers correctly (104.21.44.87 is a known-good address of <code>api.audnex.us</code>)</p> <pre><code>$ kubectl get all -A | grep -i dns\nkube-system              pod/coredns-787d4945fb-67z8g                                 1/1     Running            54 (28h ago)      549d\nkube-system              pod/coredns-787d4945fb-gsx6h                                 1/1     Running            54 (28h ago)      549d\nkube-system              service/kube-dns                                                ClusterIP      10.96.0.10       &lt;none&gt;          53/UDP,53/TCP,9153/TCP                                                                          549d\nkube-system              deployment.apps/coredns                                 2/2     2            2           549d\nkube-system              replicaset.apps/coredns-787d4945fb                                 2         2         2       549d\n\n$ nslookup k8s.io 10.96.0.10\nServer:         10.96.0.10\nAddress:        10.96.0.10#53\n\nNon-authoritative answer:\nName:   k8s.io\nAddress: 34.107.204.206\nName:   k8s.io\nAddress: 2600:1901:0:26f3::\n\n$ nslookup k8s.io \nServer:         127.0.0.53\nAddress:        127.0.0.53#53\n\nNon-authoritative answer:\nName:   k8s.io\nAddress: 34.107.204.206\nName:   k8s.io\nAddress: 2600:1901:0:26f3::\n\n$ nslookup api.audnex.us 10.96.0.10\nServer:         10.96.0.10\nAddress:        10.96.0.10#53\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 172.67.198.55\nName:   api.audnex.us\nAddress: 104.21.44.87\nName:   api.audnex.us\nAddress: 2606:4700:3037::6815:2c57\nName:   api.audnex.us\nAddress: 2606:4700:3030::ac43:c637\n\n$ nslookup api.audnex.us \nServer:         127.0.0.53\nAddress:        127.0.0.53#53\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 104.21.44.87\nName:   api.audnex.us\nAddress: 172.67.198.55\nName:   api.audnex.us\nAddress: 2606:4700:3037::6815:2c57\nName:   api.audnex.us\nAddress: 2606:4700:3030::ac43:c637\n\n$ nslookup api.papermc.io 10.96.0.10\nServer:         10.96.0.10\nAddress:        10.96.0.10#53\n\nNon-authoritative answer:\nName:   api.papermc.io\nAddress: 104.26.12.138\nName:   api.papermc.io\nAddress: 172.67.72.198\nName:   api.papermc.io\nAddress: 104.26.13.138\nName:   api.papermc.io\nAddress: 2606:4700:20::681a:c8a\nName:   api.papermc.io\nAddress: 2606:4700:20::ac43:48c6\nName:   api.papermc.io\nAddress: 2606:4700:20::681a:d8a\n\n$ nslookup api.papermc.io\nServer:         127.0.0.53\nAddress:        127.0.0.53#53\n\nNon-authoritative answer:\nName:   api.papermc.io\nAddress: 104.26.13.138\nName:   api.papermc.io\nAddress: 172.67.72.198\nName:   api.papermc.io\nAddress: 104.26.12.138\nName:   api.papermc.io\nAddress: 2606:4700:20::681a:d8a\nName:   api.papermc.io\nAddress: 2606:4700:20::681a:c8a\nName:   api.papermc.io\nAddress: 2606:4700:20::ac43:48c6\n</code></pre>"},{"location":"blog/2024/09/22/kubernetes-cluster-dns-issues/#pods-are-not-reaching-kube-dns","title":"Pods are not reaching Kube-DNS","text":"<p>The error from the Minecraft server suggests the issue is with the pods not being able to reach the DNS server; at least over UDP:</p> <pre><code>Caused by: io.netty.resolver.dns.DnsNameResolverTimeoutException: [9444: /10.96.0.10:53] DefaultDnsQuestion(api.papermc.io.minecraft-server.svc.cluster.local. IN A) query '9444' via UDP timed out after 5000 milliseconds (no stack trace available)\n</code></pre> <p>Worried about both clusters having the same ClusterIP for Kube DNS (10.96.0.10); I tried stopping the whole customer in Rapture to see if that helps Lexicon:</p> <pre><code># systemctl stop kubelet\n# systemctl stop containerd.service\n# systemctl stop docker.service\n# systemctl stop docker.socket\n</code></pre> <p>Once everything is stopped, Rapture Kube DNS is no longer available and Lexicon Kube DNS still works, but audiobookshelf is still not able to resolve <code>api.audnex.us</code>:</p> <pre><code>root@rapture:~# nslookup api.audnex.us 10.96.0.10\n;; communications error to 10.96.0.10#53: connection refused\n;; communications error to 10.96.0.10#53: connection refused\n;; communications error to 10.96.0.10#53: connection refused\n;; no servers could be reached\n\nroot@lexicon:~# nslookup api.audnex.us 10.96.0.10\nServer:         10.96.0.10\nAddress:        10.96.0.10#53\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 172.67.198.55\nName:   api.audnex.us\nAddress: 104.21.44.87\nName:   api.audnex.us\nAddress: 2606:4700:3030::ac43:c637\nName:   api.audnex.us\nAddress: 2606:4700:3037::6815:2c57\n</code></pre> <p>Trying to reach the DNS server from the pod always fails:</p> <pre><code>$ kubectl -n audiobookshelf exec audiobookshelf-987b955fb-64gxm -- cat /etc/resolv.conf \nsearch audiobookshelf.svc.cluster.local svc.cluster.local cluster.local\nnameserver 10.96.0.10\noptions ndots:5\n\n$ kubectl -n audiobookshelf exec audiobookshelf-987b955fb-64gxm -- ping api.audnex.us\nping: bad address 'api.audnex.us'\ncommand terminated with exit code 1\n\n$ kubectl -n audiobookshelf exec audiobookshelf-987b955fb-64gxm -- nslookup api.audnex.us\n;; connection timed out; no servers could be reached\n\n$ kubectl -n audiobookshelf exec audiobookshelf-987b955fb-64gxm -- nslookup api.audnex.us 10.96.0.10\n;; connection timed out; no servers could be reached\n</code></pre> <p>However, the pods are able to reach Kube DNS on its endpoints:</p> <pre><code>$ kubectl get ep kube-dns --namespace=kube-system\nNAME       ENDPOINTS                                                     AGE\nkube-dns   10.244.0.178:53,10.244.0.183:53,10.244.0.178:53 + 3 more...   549d\n\n$ kubectl -n audiobookshelf exec audiobookshelf-987b955fb-64gxm -- nslookup api.audnex.us 10.244.0.178\nServer:         10.244.0.178\nAddress:        10.244.0.178:53\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 104.21.44.87\nName:   api.audnex.us\nAddress: 172.67.198.55\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 2606:4700:3037::6815:2c57\nName:   api.audnex.us\nAddress: 2606:4700:3030::ac43:c637\n\n$ kubectl -n audiobookshelf exec audiobookshelf-987b955fb-64gxm -- nslookup api.audnex.us 10.244.0.183\nServer:         10.244.0.183\nAddress:        10.244.0.183:53\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 172.67.198.55\nName:   api.audnex.us\nAddress: 104.21.44.87\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 2606:4700:3037::6815:2c57\nName:   api.audnex.us\nAddress: 2606:4700:3030::ac43:c637\n</code></pre>"},{"location":"blog/2024/09/22/kubernetes-cluster-dns-issues/#the-solution","title":"The Solution","text":"<p>At this point the problem is pretty clear: CoreDNS works just fine, it's just unreachable by its ClusterIP. On to searching for threads discussing similiar issues.</p> <p>After about 3 hours of unsuccessfully trying other approaches, the solution was found quite indirectly from a single check mentioned in the opening of kubernetes issue #57096: kube-dns (10.96.0.10) unavailable on minion nodes:</p> <p>I ensure to set <code>net.ipv4.ip_forward=1</code> and  <code>net.bridge.bridge-nf-call-iptables=1</code> via <code>sysctl</code></p> <p>That caught my attention because it was not mentioned by any of the other threads I had been reading in the previous 3 hours. Indeed, going back to Forwarding IPv4 and letting iptables see bridged traffic it turned out that somehow bridged traffic was no longer allowed:</p> <pre><code># sysctl -a | egrep 'net.ipv4.ip_forward|net.bridge.bridge-nf-call-ip'\nnet.ipv4.ip_forward = 1\nnet.ipv4.ip_forward_update_priority = 1\nnet.ipv4.ip_forward_use_pmtu = 0\n</code></pre> <p>Comparing with what was there when checking prerequisites in May, <code>net.bridge.*</code> variables are missing:</p> <pre><code>root@rapture:~# sysctl -a | egrep 'net.ipv4.ip_forward|net.bridge.bridge-nf-call-ip'\nnet.ipv4.ip_forward = 1\nnet.ipv4.ip_forward_update_priority = 1\nnet.ipv4.ip_forward_use_pmtu = 0\n\nroot@rapture:~# lsmod | egrep 'overlay|bridge|br_netfilter'\nbridge                311296  0\nstp                    16384  1 bridge\nllc                    16384  2 bridge,stp\noverlay               151552  30\n</code></pre> <p>These are missing now:</p> <pre><code>net.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\n</code></pre> <p>And they are missing in Lexicon too. Finally, a likely culprit!</p> <p>So I went back and added these explicitly, with the commands from Forwarding IPv4 and letting iptables see bridged traffic</p> <pre><code>root@rapture:~# cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\noverlay\nbr_netfilter\n\nroot@rapture:~# sudo modprobe overlay\nroot@rapture:~# sudo modprobe br_netfilter\n\nroot@rapture:~# ls -l /etc/sysctl.d\ntotal 40\n-rw-r--r-- 1 root root   77 Feb 25  2022 10-console-messages.conf\n-rw-r--r-- 1 root root  490 Feb 25  2022 10-ipv6-privacy.conf\n-rw-r--r-- 1 root root 1229 Feb 25  2022 10-kernel-hardening.conf\n-rw-r--r-- 1 root root 1184 Feb 25  2022 10-magic-sysrq.conf\n-rw-r--r-- 1 root root  158 Feb 25  2022 10-network-security.conf\n-rw-r--r-- 1 root root 1292 Feb 25  2022 10-ptrace.conf\n-rw-r--r-- 1 root root  506 Feb 25  2022 10-zeropage.conf\n-rw-rw-r-- 1 root root  146 Aug  1 15:14 30-brave.conf\n-rw-r--r-- 1 root root  597 Mar 19  2022 50-ubuntustudio.conf\nlrwxrwxrwx 1 root root   14 Nov 21  2023 99-sysctl.conf -&gt; ../sysctl.conf\n-rw-r--r-- 1 root root  798 Feb 25  2022 README.sysctl\n\nroot@rapture:~# grep bridge  /etc/sysctl.d/*.conf\nroot@rapture:~# grep forward /etc/sysctl.d/*.conf\n/etc/sysctl.d/99-sysctl.conf:# Uncomment the next line to enable packet forwarding for IPv4\n/etc/sysctl.d/99-sysctl.conf:#net.ipv4.ip_forward=1\n/etc/sysctl.d/99-sysctl.conf:# Uncomment the next line to enable packet forwarding for IPv6\n/etc/sysctl.d/99-sysctl.conf:#net.ipv6.conf.all.forwarding=1\n\nroot@rapture:~# cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\n\nroot@rapture:~# sudo sysctl --system\n* Applying /etc/sysctl.d/10-console-messages.conf ...\nkernel.printk = 4 4 1 7\n* Applying /etc/sysctl.d/10-ipv6-privacy.conf ...\nnet.ipv6.conf.all.use_tempaddr = 2\nnet.ipv6.conf.default.use_tempaddr = 2\n* Applying /etc/sysctl.d/10-kernel-hardening.conf ...\nkernel.kptr_restrict = 1\n* Applying /etc/sysctl.d/10-magic-sysrq.conf ...\nkernel.sysrq = 176\n* Applying /etc/sysctl.d/10-network-security.conf ...\nnet.ipv4.conf.default.rp_filter = 2\nnet.ipv4.conf.all.rp_filter = 2\n* Applying /etc/sysctl.d/10-ptrace.conf ...\nkernel.yama.ptrace_scope = 1\n* Applying /etc/sysctl.d/10-zeropage.conf ...\nvm.mmap_min_addr = 65536\n* Applying /etc/sysctl.d/30-brave.conf ...\n* Applying /usr/lib/sysctl.d/50-bubblewrap.conf ...\nkernel.unprivileged_userns_clone = 1\n* Applying /usr/lib/sysctl.d/50-default.conf ...\nkernel.core_uses_pid = 1\nnet.ipv4.conf.default.rp_filter = 2\nnet.ipv4.conf.default.accept_source_route = 0\nsysctl: setting key \"net.ipv4.conf.all.accept_source_route\": Invalid argument\nnet.ipv4.conf.default.promote_secondaries = 1\nsysctl: setting key \"net.ipv4.conf.all.promote_secondaries\": Invalid argument\nnet.ipv4.ping_group_range = 0 2147483647\nnet.core.default_qdisc = fq_codel\nfs.protected_hardlinks = 1\nfs.protected_symlinks = 1\nfs.protected_regular = 1\nfs.protected_fifos = 1\n* Applying /usr/lib/sysctl.d/50-pid-max.conf ...\nkernel.pid_max = 4194304\n* Applying /etc/sysctl.d/50-ubuntustudio.conf ...\nvm.swappiness = 10\n* Applying /usr/lib/sysctl.d/99-protect-links.conf ...\nfs.protected_fifos = 1\nfs.protected_hardlinks = 1\nfs.protected_regular = 2\nfs.protected_symlinks = 1\n* Applying /etc/sysctl.d/99-sysctl.conf ...\nkernel.dmesg_restrict = 0\n* Applying /etc/sysctl.d/k8s.conf ...\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward = 1\n* Applying /etc/sysctl.conf ...\nkernel.dmesg_restrict = 0\n\nroot@rapture:~# lsmod | egrep 'overlay|bridge|br_netfilter'\nbr_netfilter           32768  0\nbridge                311296  1 br_netfilter\nstp                    16384  1 bridge\nllc                    16384  2 bridge,stp\noverlay               151552  30\n\nroot@rapture:~# sysctl -a | egrep 'net.ipv4.ip_forward|net.bridge.bridge-nf-call-ip'\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nnet.ipv4.ip_forward_update_priority = 1\nnet.ipv4.ip_forward_use_pmtu = 0\n</code></pre> <p>This made all the difference; pods can now reach the CoreDNS service on its ClusterIP:</p> <pre><code>$ kubectl -n ingress-nginx exec ingress-nginx-controller-7c7754d4b6-w4w5g -- nslookup api.audnex.us 10.96.0.10\nServer:         10.96.0.10\nAddress:        10.96.0.10:53\n\nName:   api.audnex.us\nAddress: 2606:4700:3037::6815:2c57\nName:   api.audnex.us\nAddress: 2606:4700:3030::ac43:c637\n\nName:   api.audnex.us\nAddress: 172.67.198.55\nName:   api.audnex.us\nAddress: 104.21.44.87\n</code></pre> <p>To validate this fix, started Audiobookshelf in Rapture and tried to match audiobooks. Not only it worked, it also was able to find books by title and author (which had stopped working days ago). The test with <code>nslookup</code> was also successful in this new pod. also when not specifying which DNS server to query:</p> <pre><code>$ kubectl -n audiobookshelf exec pod/audiobookshelf-59c5f7c9f5-m5ztg -- nslookup api.audnex.us 10.96.0.10\nServer:         10.96.0.10\nAddress:        10.96.0.10:53\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 2606:4700:3030::ac43:c637\nName:   api.audnex.us\nAddress: 2606:4700:3037::6815:2c57\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 104.21.44.87\nName:   api.audnex.us\nAddress: 172.67.198.55\n\n$ kubectl -n audiobookshelf exec pod/audiobookshelf-59c5f7c9f5-m5ztg -- nslookup api.audnex.us \nServer:         10.96.0.10\nAddress:        10.96.0.10:53\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 104.21.44.87\nName:   api.audnex.us\nAddress: 172.67.198.55\n\nNon-authoritative answer:\nName:   api.audnex.us\nAddress: 2606:4700:3037::6815:2c57\nName:   api.audnex.us\nAddress: 2606:4700:3030::ac43:c637\n\n$ kubectl -n audiobookshelf exec pod/audiobookshelf-59c5f7c9f5-m5ztg -- cat /etc/resolv.conf \nsearch audiobookshelf.svc.cluster.local svc.cluster.local cluster.local\nnameserver 10.96.0.10\noptions ndots:5\n</code></pre> <p>After applying the same fix to Lexicon, all the above issues are finally solved:</p> <ol> <li>Plex Media Server in Rapture is successfully claimed and is now    available to stream media.</li> <li>Grafana can reach InfluxDB on its internal service name    (<code>influxdb-svc</code>) and port     http://influxdb-svc:18086.</li> <li>Audiobookshelf is again able to find book matches by title and    author, just as in Rapture.</li> <li>Minecraft is running again; it started up pretty much as soon    as the DNS service was reachable again.</li> </ol>"},{"location":"blog/2024/09/22/kubernetes-cluster-dns-issues/#not-over-yet","title":"Not Over Yet","text":"<p>Just a couple days later I noticed in the Kubernetes dashboard that the <code>coredns</code> deployment was broken because both pods were crash-looping:</p> <pre><code># kubectl get all -A | grep -i dns\nkube-system            pod/coredns-5d78c9869d-9xbh4                    0/1     CrashLoopBackOff   81 (2m14s ago)   2d12h\nkube-system            pod/coredns-5d78c9869d-w8954                    0/1     CrashLoopBackOff   81 (2m30s ago)   2d12h\nkube-system            service/kube-dns                             ClusterIP      10.96.0.10       &lt;none&gt;          53/UDP,53/TCP,9153/TCP                                                                          135d\nkube-system            deployment.apps/coredns                     0/2     2            0           135d\nkube-system            replicaset.apps/coredns-5d78c9869d                    2         2         0       2d12h\nkube-system            replicaset.apps/coredns-787d4945fb                    0         0         0       135d\n\n# get ep kube-dns --namespace=kube-system\nNAME       ENDPOINTS   AGE\nkube-dns               135d\n\n# kubectl -n kube-system logs coredns-5d78c9869d-9xbh4 -f\n.:53\n[INFO] plugin/reload: Running configuration SHA512 = c0af6acba93e75312d34dc3f6c44bf8573acff497d229202a4a49405ad5d8266c556ca6f83ba0c9e74088593095f714ba5b916d197aa693d6120af8451160b80\nCoreDNS-1.10.1\nlinux/amd64, go1.20, 055b2c3\n[FATAL] plugin/loop: Loop (127.0.0.1:42071 -&gt; :53) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 5799046874759025118.6581212788663693097.\"\n\n# kubectl -n kube-system logs coredns-5d78c9869d-w8954 -f\n[INFO] plugin/ready: Still waiting on: \"kubernetes\"\n.:53\n[INFO] plugin/reload: Running configuration SHA512 = c0af6acba93e75312d34dc3f6c44bf8573acff497d229202a4a49405ad5d8266c556ca6f83ba0c9e74088593095f714ba5b916d197aa693d6120af8451160b80\nCoreDNS-1.10.1\nlinux/amd64, go1.20, 055b2c3\n[FATAL] plugin/loop: Loop (127.0.0.1:42908 -&gt; :53) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 426107543664332041.3956332948985369701.\"\n\n# kubectl get ep kube-dns --namespace=kube-system\nNAME       ENDPOINTS   AGE\nkube-dns               135d\n\n# nslookup k8s.io 10.96.0.10\n;; communications error to 10.96.0.10#53: connection refused\n;; communications error to 10.96.0.10#53: connection refused\n;; communications error to 10.96.0.10#53: connection refused\n;; no servers could be reached\n</code></pre> <p>The Troubleshooting page explains this is because a DNs loop has been detected and recommend making sure that <code>kubelet</code> configuration points to the real <code>resolve.conf</code>... which it already does:</p> <pre><code># grep -iA2 resolv /var/lib/kubelet/config.yaml\nresolvConf: /run/systemd/resolve/resolv.conf\nrotateCertificates: true\nruntimeRequestTimeout: 0s\n\n# cat /run/systemd/resolve/resolv.conf\nnameserver 62.2.24.158\nnameserver 62.2.17.61\nsearch .\n</code></pre> <p>At least the bridging of traffic seems to be fine:</p> <pre><code># sysctl -a | egrep 'net.ipv4.ip_forward|net.bridge.bridge-nf-call-ip'\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nnet.ipv4.ip_forward_update_priority = 1\nnet.ipv4.ip_forward_use_pmtu = 0\n\n# cat /etc/resolv.conf\nnameserver 127.0.0.53\noptions edns0 trust-ad\nsearch .\n\n# iptables -L\n\nChain KUBE-SERVICES (2 references)\ntarget     prot opt source               destination         \nREJECT     tcp  --  anywhere             10.96.0.10           /* kube-system/kube-dns:metrics has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             10.96.0.10           /* kube-system/kube-dns:dns has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.96.0.10           /* kube-system/kube-dns:dns-tcp has no endpoints */ reject-with icmp-port-unreachable\n</code></pre> <p>Troubleshooting Loops In Kubernetes Clusters seems very generic so I turn to searching for more answer and find a few clues... but not enough.</p> <p>Maybe I should just disable local dns cache on ubuntu?</p> <pre><code># netstat -tulpn | grep '\\&lt;53\\&gt;'\ntcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      2280/systemd-resolv \nudp        0      0 127.0.0.53:53           0.0.0.0:*                           2280/systemd-resolv\n</code></pre> <p>Comparing <code>/etc/resolv.conf</code> between the two servers, it seems odd that the one in Rapture has <code>search .</code>; that doesn't seem right.</p> <p>This can be overriden via Netplan:</p> <pre><code># tail -3 /etc/resolv.conf \nnameserver 127.0.0.53\noptions edns0 trust-ad\nsearch .\n\n# vi /etc/netplan/01-network-manager-all.yaml \n</code></pre> <p>Add <code>search</code> under <code>nameservers</code> and apply the change:</p> <pre><code>      nameservers:\n      # Set DNS name servers\n        search: [v.cable.com]\n</code></pre> <pre><code># netplan apply\n\n# tail -3 /etc/resolv.conf \nnameserver 127.0.0.53\noptions edns0 trust-ad\nsearch v.cable.com\n</code></pre> <p>Note: <code>netplan apply</code> showed a couple of warnings, one due to Ubuntu bug #2041727 (solved with <code>apt install openvswitch-switch</code>) and another one due to having the same default route on 2 interfaces (it was needed in only one of them).</p> <p>All the above still did not help; both <code>coredns</code> pods keep crash-looping with the same error about a lop being detected for zone \".\"; even after forcing them to restart with</p> <pre><code># kubectl scale --replicas=0 deployment.apps/coredns -n kube-system\n# sleep 10\n# kubectl scale --replicas=2 deployment.apps/coredns -n kube-system\n</code></pre> <p>Flushing the local DNS cache also did not help:</p> <pre><code>root@rapture:~# resolvectl statistics\nDNSSEC supported by current servers: no\n\nTransactions              \nCurrent Transactions: 0\n  Total Transactions: 4486\n\nCache                     \n  Current Cache Size: 56\n          Cache Hits: 61\n        Cache Misses: 339\n\nDNSSEC Verdicts           \n              Secure: 0\n            Insecure: 0\n               Bogus: 0\n       Indeterminate: 0\nroot@rapture:~# resolvectl flush-caches\nroot@rapture:~# resolvectl statistics\nDNSSEC supported by current servers: no\n\nTransactions              \nCurrent Transactions: 0\n  Total Transactions: 4490\n\nCache                     \n  Current Cache Size: 0\n          Cache Hits: 63\n        Cache Misses: 341\n\nDNSSEC Verdicts           \n              Secure: 0\n            Insecure: 0\n               Bogus: 0\n       Indeterminate: 0\n</code></pre> <p>The next thing to try is entirely Disabling Local DNS Caching.</p> <pre><code># systemctl list-units --type=service | grep -E 'systemd-resolved|dnsmasq' \n  systemd-resolved.service\n             loaded active running Network Name Resolution\n\n# vi /etc/systemd/resolved.conf\n</code></pre> <pre><code>[Resolve]\nDomains=v.cablecom.net\nCache=no\n</code></pre> <pre><code># systemctl restart systemd-resolved\nroot@rapture:~# dig tecadmin.net\n\n; &lt;&lt;&gt;&gt; DiG 9.18.28-0ubuntu0.22.04.1-Ubuntu &lt;&lt;&gt;&gt; tecadmin.net\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 14739\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 65494\n;; QUESTION SECTION:\n;tecadmin.net.                  IN      A\n\n;; ANSWER SECTION:\ntecadmin.net.           150     IN      A       104.21.25.106\ntecadmin.net.           150     IN      A       172.67.134.5\n\n;; Query time: 39 msec\n;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)\n;; WHEN: Wed Sep 25 20:37:28 CEST 2024\n;; MSG SIZE  rcvd: 73\n\n# kubectl get pods -n kube-system | grep -i dns\ncoredns-5d78c9869d-gv9q5          0/1     CrashLoopBackOff   4 (14s ago)   107s\ncoredns-5d78c9869d-zv5rt          0/1     CrashLoopBackOff   4 (8s ago)    107s\n\n# kubectl -n kube-system logs coredns-5d78c9869d-zv5rt\n.:53\n[INFO] plugin/reload: Running configuration SHA512 = c0af6acba93e75312d34dc3f6c44bf8573acff497d229202a4a49405ad5d8266c556ca6f83ba0c9e74088593095f714ba5b916d197aa693d6120af8451160b80\nCoreDNS-1.10.1\nlinux/amd64, go1.20, 055b2c3\n[FATAL] plugin/loop: Loop (127.0.0.1:48053 -&gt; :53) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 2605680300388702341.222935474922528127.\"\n</code></pre> <p>This does change the TTL but is still making DNS queries go to the local DNS service on <code>127.0.0.53</code>; so this still does not help.</p> <p>To get the local DNS service on <code>127.0.0.53</code> out of the way the configuration in <code>/etc/systemd/resolved.conf</code> must disable the DNS sbut listener:</p> <pre><code>[Resolve]\nDNSStubListener=no\n</code></pre> <p>This gets DNS requests answered directly by the non-local DNS servers, but even this still does not help removing the loop!</p> <pre><code># systemctl restart systemd-resolved\n\n# nslookup k8s.io 127.0.0.53\n;; communications error to 127.0.0.1#53: connection refused\n;; communications error to 127.0.0.1#53: connection refused\n;; communications error to 127.0.0.1#53: connection refused\n;; no servers could be reached\n\n# dig tecadmin.net\n\n; &lt;&lt;&gt;&gt; DiG 9.18.28-0ubuntu0.22.04.1-Ubuntu &lt;&lt;&gt;&gt; tecadmin.net\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 46975\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;tecadmin.net.                  IN      A\n\n;; ANSWER SECTION:\ntecadmin.net.           300     IN      A       104.21.25.106\ntecadmin.net.           300     IN      A       172.67.134.5\n\n;; Query time: 19 msec\n;; SERVER: 62.2.24.158#53(62.2.24.158) (UDP)\n;; WHEN: Wed Sep 25 20:48:57 CEST 2024\n;; MSG SIZE  rcvd: 73\n\n# kubectl get pods -n kube-system | grep -i dns\ncoredns-5d78c9869d-489wl          0/1     CrashLoopBackOff   4 (61s ago)    2m49s\ncoredns-5d78c9869d-pwn9x          0/1     CrashLoopBackOff   4 (68s ago)    2m49s\n\n# kubectl -n kube-system logs coredns-5d78c9869d-489wl\n.:53\n[INFO] plugin/reload: Running configuration SHA512 = c0af6acba93e75312d34dc3f6c44bf8573acff497d229202a4a49405ad5d8266c556ca6f83ba0c9e74088593095f714ba5b916d197aa693d6120af8451160b80\nCoreDNS-1.10.1\nlinux/amd64, go1.20, 055b2c3\n[FATAL] plugin/loop: Loop (127.0.0.1:43216 -&gt; :53) detected for zone \".\", see https://coredns.io/plugins/loop#troubleshooting. Query: \"HINFO 7289571247349402084.8563501253473529468.\"\n</code></pre> <p>The next thing I can think of trying is the quick and dirty fix Troubleshooting Loops In Kubernetes Clusters mentions as the last resort; there is just no DNS listening on 127.0.0.53 anyway so may as well forward it to a real one.</p> <pre><code># kubectl get -n kube-system cm/coredns -o yaml\napiVersion: v1\ndata:\n  Corefile: |\n    .:53 {\n        log\n        errors\n        health {\n           lameduck 5s\n        }\n        ready\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n           pods insecure\n           fallthrough in-addr.arpa ip6.arpa\n           ttl 30\n        }\n        prometheus :9153\n        forward . /etc/resolv.conf {\n           max_concurrent 1000\n        }\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2024-05-12T10:52:00Z\"\n  name: coredns\n  namespace: kube-system\n  resourceVersion: \"5838688\"\n  uid: d29d50a6-7f42-4046-b157-f93876807af1\n\n# kubectl -n kube-system edit configmaps coredns -o yaml\napiVersion: v1\ndata:\n  Corefile: |\n    .:53 {\n        log\n        errors\n        health {\n           lameduck 5s\n        }\n        ready\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n           pods insecure\n           fallthrough in-addr.arpa ip6.arpa\n           ttl 30\n        }\n        prometheus :9153\n        forward . 62.2.24.158 {\n           max_concurrent 1000\n        }\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2024-05-12T10:52:00Z\"\n  name: coredns\n  namespace: kube-system\n  resourceVersion: \"5959049\"\n  uid: d29d50a6-7f42-4046-b157-f93876807af1\n\n# systemctl restart kubelet\n\n# kubectl get pods -n kube-system | grep dns\ncoredns-55cb58b774-ph8qz          1/1     Running   33 (5m39s ago)   23h\ncoredns-55cb58b774-rd6wg          1/1     Running   33 (5m30s ago)   23h\n</code></pre> <p>Finally!!! Now we wait and see if this really fixed it...</p>"},{"location":"blog/2024/09/22/kubernetes-cluster-dns-issues/#what-did-not-work","title":"What Did Not Work","text":""},{"location":"blog/2024/09/22/kubernetes-cluster-dns-issues/#workaround-around-ip-tables-rules","title":"Workaround around IP tables rules","text":"<p>The answer to coredns do not resolve service name correctly kubeadm issue #1056: Fresh deploy with CoreDNS not resolving any dns lookup where it is noted that firewall rules are blocking access to the DNS service ClusterIP:</p> <pre><code>root@rapture:~# iptables -L | grep 10.96.0.10\nREJECT     udp  --  anywhere             10.96.0.10           /* kube-system/kube-dns:dns has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.96.0.10           /* kube-system/kube-dns:dns-tcp has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.96.0.10           /* kube-system/kube-dns:metrics has no endpoints */ reject-with icmp-port-unreachable\n</code></pre> <p>That would explain why the DNS is unreachable on its Cluster IP, but why are those rules there?</p> <p>Prodian0013 points out  that these rules are added by <code>kube-proxy</code> when it sees that <code>kube-dns</code> has no endpoints... but it does!</p> <pre><code>$ kubectl describe ep kube-dns -n kube-system\nName:         kube-dns\nNamespace:    kube-system\nLabels:       k8s-app=kube-dns\n              kubernetes.io/cluster-service=true\n              kubernetes.io/name=CoreDNS\nAnnotations:  &lt;none&gt;\nSubsets:\n  Addresses:          10.244.0.62,10.244.0.63\n  NotReadyAddresses:  &lt;none&gt;\n  Ports:\n    Name     Port  Protocol\n    ----     ----  --------\n    dns-tcp  53    TCP\n    dns      53    UDP\n    metrics  9153  TCP\n\nEvents:  &lt;none&gt;\n</code></pre> <p>So those rules should not be there. In fact, when I went back to check again, they were gone.</p> <p>Before:</p> <code># iptables -L</code> <pre><code># iptables -L\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \nKUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */\nKUBE-NODEPORTS  all  --  anywhere             anywhere             /* kubernetes health check service ports */\nKUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes externally-visible service portals */\nKUBE-FIREWALL  all  --  anywhere             anywhere            \n\nChain FORWARD (policy DROP)\ntarget     prot opt source               destination         \nKUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */\nKUBE-FORWARD  all  --  anywhere             anywhere             /* kubernetes forwarding rules */\nKUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */\nKUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes externally-visible service portals */\nDOCKER-USER  all  --  anywhere             anywhere            \nDOCKER-ISOLATION-STAGE-1  all  --  anywhere             anywhere            \nACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED\nDOCKER     all  --  anywhere             anywhere            \nACCEPT     all  --  anywhere             anywhere            \nACCEPT     all  --  anywhere             anywhere            \nFLANNEL-FWD  all  --  anywhere             anywhere             /* flanneld forward */\n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination         \nKUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */\nKUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */\nKUBE-FIREWALL  all  --  anywhere             anywhere            \n\nChain DOCKER (1 references)\ntarget     prot opt source               destination         \n\nChain DOCKER-ISOLATION-STAGE-1 (1 references)\ntarget     prot opt source               destination         \nDOCKER-ISOLATION-STAGE-2  all  --  anywhere             anywhere            \nRETURN     all  --  anywhere             anywhere            \n\nChain DOCKER-ISOLATION-STAGE-2 (1 references)\ntarget     prot opt source               destination         \nDROP       all  --  anywhere             anywhere            \nRETURN     all  --  anywhere             anywhere            \n\nChain DOCKER-USER (1 references)\ntarget     prot opt source               destination         \nRETURN     all  --  anywhere             anywhere            \n\nChain FLANNEL-FWD (1 references)\ntarget     prot opt source               destination         \nACCEPT     all  --  rapture/16           anywhere             /* flanneld forward */\nACCEPT     all  --  anywhere             rapture/16           /* flanneld forward */\n\nChain KUBE-EXTERNAL-SERVICES (2 references)\ntarget     prot opt source               destination         \nREJECT     tcp  --  anywhere             nginx.rapture.uu.am  /* ingress-nginx/ingress-nginx-controller:https has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             anywhere             /* ingress-nginx/ingress-nginx-controller:https has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             plex.rapture.uu.am   /* plexserver/plex-udp:discovery-udp has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             anywhere             /* plexserver/plex-udp:discovery-udp has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             plex.rapture.uu.am   /* plexserver/plex-udp:gdm-32412 has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             anywhere             /* plexserver/plex-udp:gdm-32412 has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             plex.rapture.uu.am   /* plexserver/plex-udp:gdm-32413 has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             anywhere             /* plexserver/plex-udp:gdm-32413 has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             plex.rapture.uu.am   /* plexserver/plex-tcp:pms-web has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             anywhere             /* plexserver/plex-tcp:pms-web has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             plex.rapture.uu.am   /* plexserver/plex-udp:dlna-udp has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             anywhere             /* plexserver/plex-udp:dlna-udp has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             plex.rapture.uu.am   /* plexserver/plex-tcp:plex-roku has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             anywhere             /* plexserver/plex-tcp:plex-roku has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             photoprism.rapture.uu.am  /* photoprism/photoprism:http has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             anywhere             /* photoprism/photoprism:http has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             plex.rapture.uu.am   /* plexserver/plex-tcp:plex-companion has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             anywhere             /* plexserver/plex-tcp:plex-companion has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             plex.rapture.uu.am   /* plexserver/plex-tcp:dlna-tcp has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             anywhere             /* plexserver/plex-tcp:dlna-tcp has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             nginx.rapture.uu.am  /* ingress-nginx/ingress-nginx-controller:http has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             anywhere             /* ingress-nginx/ingress-nginx-controller:http has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             plex.rapture.uu.am   /* plexserver/plex-udp:gdm-32410 has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             anywhere             /* plexserver/plex-udp:gdm-32410 has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             plex.rapture.uu.am   /* plexserver/plex-udp:gdm-32414 has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             anywhere             /* plexserver/plex-udp:gdm-32414 has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             k8s.rapture.uu.am    /* kubernetes-dashboard/kubernetes-dashboard has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             anywhere             /* kubernetes-dashboard/kubernetes-dashboard has no endpoints */ ADDRTYPE match dst-type LOCAL reject-with icmp-port-unreachable\n\nChain KUBE-FIREWALL (2 references)\ntarget     prot opt source               destination         \nDROP       all  -- !localhost/8          localhost/8          /* block incoming localnet connections */ ! ctstate RELATED,ESTABLISHED,DNAT\n\nChain KUBE-FORWARD (1 references)\ntarget     prot opt source               destination         \nDROP       all  --  anywhere             anywhere             ctstate INVALID\nACCEPT     all  --  anywhere             anywhere             /* kubernetes forwarding rules */\nACCEPT     all  --  anywhere             anywhere             /* kubernetes forwarding conntrack rule */ ctstate RELATED,ESTABLISHED\n\nChain KUBE-KUBELET-CANARY (0 references)\ntarget     prot opt source               destination         \n\nChain KUBE-NODEPORTS (1 references)\ntarget     prot opt source               destination         \nACCEPT     tcp  --  anywhere             anywhere             /* ingress-nginx/ingress-nginx-controller:https health check node port */\nACCEPT     tcp  --  anywhere             anywhere             /* ingress-nginx/ingress-nginx-controller:http health check node port */\n\nChain KUBE-PROXY-CANARY (0 references)\ntarget     prot opt source               destination         \n\nChain KUBE-PROXY-FIREWALL (3 references)\ntarget     prot opt source               destination         \n\nChain KUBE-SERVICES (2 references)\ntarget     prot opt source               destination         \nREJECT     tcp  --  anywhere             10.97.59.176         /* ingress-nginx/ingress-nginx-controller:https has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.98.208.0          /* ingress-nginx/ingress-nginx-controller-admission:https-webhook has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             10.96.0.10           /* kube-system/kube-dns:dns has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             10.100.30.101        /* plexserver/plex-udp:discovery-udp has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             10.100.30.101        /* plexserver/plex-udp:gdm-32412 has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             10.100.30.101        /* plexserver/plex-udp:gdm-32413 has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.97.83.71          /* plexserver/plex-tcp:pms-web has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             10.100.30.101        /* plexserver/plex-udp:dlna-udp has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.97.83.71          /* plexserver/plex-tcp:plex-roku has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.96.0.10           /* kube-system/kube-dns:dns-tcp has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.109.211.96        /* metallb-system/metallb-webhook-service has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.108.135.112       /* photoprism/photoprism:http has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.97.83.71          /* plexserver/plex-tcp:plex-companion has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.97.83.71          /* plexserver/plex-tcp:dlna-tcp has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.97.59.176         /* ingress-nginx/ingress-nginx-controller:http has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.96.0.10           /* kube-system/kube-dns:metrics has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.107.131.104       /* kubernetes-dashboard/dashboard-metrics-scraper has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             10.100.30.101        /* plexserver/plex-udp:gdm-32410 has no endpoints */ reject-with icmp-port-unreachable\nREJECT     udp  --  anywhere             10.100.30.101        /* plexserver/plex-udp:gdm-32414 has no endpoints */ reject-with icmp-port-unreachable\nREJECT     tcp  --  anywhere             10.107.235.155       /* kubernetes-dashboard/kubernetes-dashboard has no endpoints */ reject-with icmp-port-unreachable\n</code></pre> <p>After:</p> <code># iptables -L</code> <pre><code># iptables -L\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \nKUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */\nKUBE-NODEPORTS  all  --  anywhere             anywhere             /* kubernetes health check service ports */\nKUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes externally-visible service portals */\nKUBE-FIREWALL  all  --  anywhere             anywhere            \n\nChain FORWARD (policy DROP)\ntarget     prot opt source               destination         \nKUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */\nKUBE-FORWARD  all  --  anywhere             anywhere             /* kubernetes forwarding rules */\nKUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */\nKUBE-EXTERNAL-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes externally-visible service portals */\nFLANNEL-FWD  all  --  anywhere             anywhere             /* flanneld forward */\n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination         \nKUBE-PROXY-FIREWALL  all  --  anywhere             anywhere             ctstate NEW /* kubernetes load balancer firewall */\nKUBE-SERVICES  all  --  anywhere             anywhere             ctstate NEW /* kubernetes service portals */\nKUBE-FIREWALL  all  --  anywhere             anywhere            \n\nChain DOCKER (0 references)\ntarget     prot opt source               destination         \n\nChain DOCKER-ISOLATION-STAGE-1 (0 references)\ntarget     prot opt source               destination         \n\nChain DOCKER-ISOLATION-STAGE-2 (0 references)\ntarget     prot opt source               destination         \n\nChain DOCKER-USER (0 references)\ntarget     prot opt source               destination         \n\nChain FLANNEL-FWD (1 references)\ntarget     prot opt source               destination         \nACCEPT     all  --  rapture/16           anywhere             /* flanneld forward */\nACCEPT     all  --  anywhere             rapture/16           /* flanneld forward */\n\nChain KUBE-EXTERNAL-SERVICES (2 references)\ntarget     prot opt source               destination         \n\nChain KUBE-FIREWALL (2 references)\ntarget     prot opt source               destination         \nDROP       all  -- !localhost/8          localhost/8          /* block incoming localnet connections */ ! ctstate RELATED,ESTABLISHED,DNAT\n\nChain KUBE-FORWARD (1 references)\ntarget     prot opt source               destination         \nDROP       all  --  anywhere             anywhere             ctstate INVALID\nACCEPT     all  --  anywhere             anywhere             /* kubernetes forwarding rules */\nACCEPT     all  --  anywhere             anywhere             /* kubernetes forwarding conntrack rule */ ctstate RELATED,ESTABLISHED\n\nChain KUBE-KUBELET-CANARY (0 references)\ntarget     prot opt source               destination         \n\nChain KUBE-NODEPORTS (1 references)\ntarget     prot opt source               destination         \nACCEPT     tcp  --  anywhere             anywhere             /* ingress-nginx/ingress-nginx-controller:http health check node port */\nACCEPT     tcp  --  anywhere             anywhere             /* ingress-nginx/ingress-nginx-controller:https health check node port */\n\nChain KUBE-PROXY-CANARY (0 references)\ntarget     prot opt source               destination         \n\nChain KUBE-PROXY-FIREWALL (3 references)\ntarget     prot opt source               destination         \n\nChain KUBE-SERVICES (2 references)\ntarget     prot opt source               destination         \n</code></pre> <p>Even then, I tried the commands in  that comment but they didn't help.</p> <p>Note: had to make a symlink for <code>crictl</code> commands to work well:</p> <pre><code># ln -s /var/run/containerd/containerd.sock /var/run/dockershim.sock\n# ls -l /var/run/containerd/containerd.sock /var/run/dockershim.sock\nsrw-rw---- 1 root root  0 Sep 22 13:34 /var/run/containerd/containerd.sock\nlrwxrwxrwx 1 root root 35 Sep 22 13:53 /var/run/dockershim.sock -&gt; /var/run/containerd/containerd.sock\n</code></pre>"},{"location":"blog/2024/09/22/kubernetes-cluster-dns-issues/#debugging-dns-resolution","title":"Debugging DNS Resolution","text":"<p>Kubernetes guide for Debugging DNS Resolution, which was linked from Kubernetes : kube-dns service not accessible via ClusterIP, did not help either. Naturally, that documentation would assume the installation didn't miss a crucial step.</p>"},{"location":"blog/2024/09/22/kubernetes-cluster-dns-issues/#override-clusterdns-in-kubelet-configuration","title":"Override <code>clusterDNS</code> in Kubelet configuration","text":"<p>pkeuter's comment in kubeadm issue #1056: Fresh deploy with CoreDNS not resolving any dns lookup suggesting to override the <code>clusterDNS</code> variable in <code>/var/lib/kubelet/config.yaml</code> also did not help.</p> <p>This was also reported to be a working solution for Pods cannot resolve kubernetes DNS.</p> <p>However, this variable must be set to the ClusterIP, since that is the one that won't change over time. This may be an easy workaround in other situations, but in my case fixing the missed prerequisite seems to have been the correct solution.</p>"},{"location":"blog/2024/09/22/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404/","title":"Upgrading single-node Kubernetes cluster on Ubuntu Studio 24.04","text":"<p>Last month I took a look at checking deployments before upgrading kubeadm clusters and found results mostly reassuring.</p> <p>As a practice run to upgrade more complex setups, lets upgrade the cluster running on the desktop PC, which is only running a Plex Media Server (which recently become unresponsive) and the PhotoPrism\u00ae photo album (which never worked well enough to be critical to me).</p> <p>The cluster is running version 1.26 (which is quite old):</p> <pre><code># kubeadm version\nkubeadm version: &amp;version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.15\", GitCommit:\"1649f592f1909b97aa3c2a0a8f968a3fd05a7b8b\", GitTreeState:\"clean\", BuildDate:\"2024-03-14T01:03:33Z\", GoVersion:\"go1.21.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n</code></pre> <p>Kubernetes clusters can only be upgraded one minor version at a time (skipping minor version is not supported), so we start with upgrading a Kubernetes cluster created with kubeadm from version 1.26.x to version 1.27.x.</p> <p>Although the workloads are not critical, lets pretend they are and use <code>kubectl drain</code> to remove the node from service to ensure all pods are shut down gracefully:</p> <pre><code># kubectl get nodes\nNAME      STATUS   ROLES           AGE    VERSION\nrapture   Ready    control-plane   132d   v1.26.15\n\n# kubectl drain --ignore-daemonsets rapture\ncordoned\nerror: unable to drain node \"rapture\" due to error:cannot delete Pods with local storage (use --delete-emptydir-data to override): kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-s77ss, kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-9dxc2, continuing command...\nThere are pending nodes to be drained:\n rapture\ncannot delete Pods with local storage (use --delete-emptydir-data to override): kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-s77ss, kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-9dxc2\n</code></pre> <p>Turns out this clustomers needs the <code>--delete-emptydir-data</code> flag because there are pods using emptyDir and local data that will be deleted when the node is drained.</p> <code># kubectl drain --ignore-daemonsets rapture --delete-emptydir-data</code> <pre><code># kubectl drain --ignore-daemonsets rapture --delete-emptydir-data\nnode/rapture already cordoned\nWarning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-bbhn8, kube-system/kube-proxy-tszr7, metallb-system/speaker-58krr\nevicting pod ingress-nginx/ingress-nginx-admission-create-nfm7b\nevicting pod ingress-nginx/ingress-nginx-controller-7c7754d4b6-jshnd\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-5ncn8\nevicting pod kube-system/coredns-787d4945fb-k8m6b\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-bhzkt\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-78t4g\nevicting pod ingress-nginx/ingress-nginx-admission-patch-8x6r7\nevicting pod ingress-nginx/ingress-nginx-controller-7c7754d4b6-qrj7s\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-4lwwm\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-5pgkv\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-b2n6c\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-4sswx\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-6b5wz\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-6v4ds\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-67p2t\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-7r4xz\nevicting pod ingress-nginx/ingress-nginx-controller-7c7754d4b6-4nnmr\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-nhw62\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-gzvjt\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-dbpwt\nevicting pod ingress-nginx/ingress-nginx-controller-7c7754d4b6-xkvg8\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-qtljx\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-5dj66\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-hzzsl\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-9ndhr\nevicting pod metallb-system/controller-759b6c5bb8-6sp2m\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-nthjr\nevicting pod ingress-nginx/ingress-nginx-controller-7c7754d4b6-mvs2g\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-bkppp\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-krkdp\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-9vz7w\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-s77ss\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-l5pm9\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-bctwj\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-brzmr\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-mktgp\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-t64ks\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-p6l6x\nevicting pod default/command-demo\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-twnjf\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-cbfb4\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-dmlzv\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-9dxc2\nevicting pod metallb-system/controller-759b6c5bb8-zfxwn\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-v22hn\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-ntftm\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-zdv25\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-7g5s8\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-9kwrh\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-w5pzx\nevicting pod photoprism/photoprism-0\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-nv9pk\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-n6vpl\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-hxwmf\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-qgwjz\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-djsjh\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-9qg2h\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-dqrn8\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-r5cng\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-42q9b\nevicting pod metallb-system/controller-759b6c5bb8-57nb6\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-b22xz\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-rjrjb\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-f2kfh\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-952v2\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-b6nwl\nevicting pod ingress-nginx/ingress-nginx-controller-7c7754d4b6-dfc2k\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-bs26f\nevicting pod metallb-system/controller-759b6c5bb8-9zbn4\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-pc2f7\nevicting pod ingress-nginx/ingress-nginx-controller-7c7754d4b6-4tgjq\nevicting pod metallb-system/controller-759b6c5bb8-b2pch\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-c82nc\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-qbbmd\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-bjw5s\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-bkt7v\nevicting pod metallb-system/controller-759b6c5bb8-bl2tn\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-qz758\nevicting pod metallb-system/controller-759b6c5bb8-cbsq4\nevicting pod ingress-nginx/ingress-nginx-controller-7c7754d4b6-x5trq\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-bm6bw\nevicting pod metallb-system/controller-759b6c5bb8-cvkmj\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-ss2sq\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-b8lzj\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-s5sr7\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-jfjsx\nevicting pod metallb-system/controller-759b6c5bb8-f6zjm\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-fl49q\nevicting pod ingress-nginx/ingress-nginx-controller-7c7754d4b6-8h9rw\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-s9tcr\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-cqrhs\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-pxb5m\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-pzgln\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-pls88\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-t27j4\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-pzmbn\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-prw6v\nevicting pod metallb-system/controller-759b6c5bb8-g9g2x\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-jn4bt\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-8gzvj\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-zrrb8\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-krrtr\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-hkxzh\nevicting pod metallb-system/controller-759b6c5bb8-ht7tz\nevicting pod metallb-system/controller-759b6c5bb8-nr92c\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-n2gn8\nevicting pod metallb-system/controller-759b6c5bb8-ktq7h\nevicting pod metallb-system/controller-759b6c5bb8-xx5bb\nevicting pod metallb-system/controller-759b6c5bb8-hx4dl\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-lbzrz\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-n8cxk\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-m8mb8\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-vd88s\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-7bjzs\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-45qcw\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-kbhtg\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-fx98h\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-hjwcl\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-ztfwd\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-v8lkd\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-kt46k\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-mbkmb\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-fscmh\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-6n2zd\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-wbrk4\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-x462f\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-26hxx\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-zs228\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-9htbw\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-hxnnz\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-74flg\nevicting pod metallb-system/controller-759b6c5bb8-jvpbg\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-vkfxn\nevicting pod metallb-system/controller-759b6c5bb8-2gpk2\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-dmws2\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-xr74p\nevicting pod metallb-system/controller-759b6c5bb8-hxhzc\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-wlzfq\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-wgtll\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-qpp5w\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-hmjdg\nevicting pod ingress-nginx/ingress-nginx-controller-7c7754d4b6-mgkdz\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-v66mn\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-nj2l7\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-7z57g\nevicting pod kube-system/coredns-787d4945fb-sqfcw\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-b94tb\nevicting pod plexserver/plexserver-7cb77ddc8-d6bqg\npod/ingress-nginx-controller-7c7754d4b6-x5trq evicted\npod/dashboard-metrics-scraper-7bc864c59-bhzkt evicted\npod/kubernetes-dashboard-6c7ccbcf87-qz758 evicted\npod/controller-759b6c5bb8-zfxwn evicted\npod/kubernetes-dashboard-6c7ccbcf87-v22hn evicted\npod/kubernetes-dashboard-6c7ccbcf87-hzzsl evicted\npod/kubernetes-dashboard-6c7ccbcf87-l5pm9 evicted\npod/controller-759b6c5bb8-cbsq4 evicted\npod/dashboard-metrics-scraper-7bc864c59-67p2t evicted\npod/kubernetes-dashboard-6c7ccbcf87-bm6bw evicted\npod/kubernetes-dashboard-6c7ccbcf87-nthjr evicted\npod/kubernetes-dashboard-6c7ccbcf87-bctwj evicted\nI0922 09:37:13.980459 1271042 request.go:690] Waited for 1.000499779s due to client-side throttling, not priority and fairness, request: POST:https://10.0.0.2:6443/api/v1/namespaces/metallb-system/pods/controller-759b6c5bb8-cvkmj/eviction\npod/controller-759b6c5bb8-cvkmj evicted\npod/kubernetes-dashboard-6c7ccbcf87-9dxc2 evicted\npod/dashboard-metrics-scraper-7bc864c59-78t4g evicted\npod/dashboard-metrics-scraper-7bc864c59-ntftm evicted\npod/dashboard-metrics-scraper-7bc864c59-6b5wz evicted\npod/dashboard-metrics-scraper-7bc864c59-ss2sq evicted\npod/kubernetes-dashboard-6c7ccbcf87-brzmr evicted\npod/ingress-nginx-controller-7c7754d4b6-xkvg8 evicted\npod/kubernetes-dashboard-6c7ccbcf87-zdv25 evicted\npod/kubernetes-dashboard-6c7ccbcf87-b8lzj evicted\npod/ingress-nginx-admission-patch-8x6r7 evicted\npod/dashboard-metrics-scraper-7bc864c59-qtljx evicted\npod/kubernetes-dashboard-6c7ccbcf87-s5sr7 evicted\npod/dashboard-metrics-scraper-7bc864c59-mktgp evicted\npod/kubernetes-dashboard-6c7ccbcf87-nhw62 evicted\npod/kubernetes-dashboard-6c7ccbcf87-jfjsx evicted\npod/ingress-nginx-controller-7c7754d4b6-qrj7s evicted\npod/kubernetes-dashboard-6c7ccbcf87-gzvjt evicted\npod/dashboard-metrics-scraper-7bc864c59-t64ks evicted\npod/kubernetes-dashboard-6c7ccbcf87-dbpwt evicted\npod/dashboard-metrics-scraper-7bc864c59-5dj66 evicted\npod/dashboard-metrics-scraper-7bc864c59-4lwwm evicted\npod/ingress-nginx-admission-create-nfm7b evicted\npod/controller-759b6c5bb8-f6zjm evicted\npod/kubernetes-dashboard-6c7ccbcf87-cbfb4 evicted\npod/dashboard-metrics-scraper-7bc864c59-p6l6x evicted\npod/dashboard-metrics-scraper-7bc864c59-5pgkv evicted\npod/coredns-787d4945fb-k8m6b evicted\npod/dashboard-metrics-scraper-7bc864c59-fl49q evicted\npod/kubernetes-dashboard-6c7ccbcf87-djsjh evicted\npod/ingress-nginx-controller-7c7754d4b6-8h9rw evicted\npod/kubernetes-dashboard-6c7ccbcf87-9qg2h evicted\npod/command-demo evicted\npod/kubernetes-dashboard-6c7ccbcf87-s9tcr evicted\npod/dashboard-metrics-scraper-7bc864c59-dmlzv evicted\npod/dashboard-metrics-scraper-7bc864c59-hxwmf evicted\npod/kubernetes-dashboard-6c7ccbcf87-twnjf evicted\npod/kubernetes-dashboard-6c7ccbcf87-7r4xz evicted\npod/dashboard-metrics-scraper-7bc864c59-qgwjz evicted\npod/kubernetes-dashboard-6c7ccbcf87-w5pzx evicted\npod/kubernetes-dashboard-6c7ccbcf87-9kwrh evicted\npod/dashboard-metrics-scraper-7bc864c59-nv9pk evicted\npod/photoprism-0 evicted\npod/dashboard-metrics-scraper-7bc864c59-4sswx evicted\npod/dashboard-metrics-scraper-7bc864c59-bkppp evicted\npod/kubernetes-dashboard-6c7ccbcf87-7g5s8 evicted\nI0922 09:37:24.142398 1271042 request.go:690] Waited for 1.349459923s due to client-side throttling, not priority and fairness, request: GET:https://10.0.0.2:6443/api/v1/namespaces/kubernetes-dashboard/pods/dashboard-metrics-scraper-7bc864c59-6v4ds\npod/dashboard-metrics-scraper-7bc864c59-6v4ds evicted\npod/dashboard-metrics-scraper-7bc864c59-42q9b evicted\npod/kubernetes-dashboard-6c7ccbcf87-9ndhr evicted\npod/dashboard-metrics-scraper-7bc864c59-dqrn8 evicted\npod/dashboard-metrics-scraper-7bc864c59-9vz7w evicted\npod/ingress-nginx-controller-7c7754d4b6-mvs2g evicted\npod/kubernetes-dashboard-6c7ccbcf87-b22xz evicted\npod/kubernetes-dashboard-6c7ccbcf87-pc2f7 evicted\npod/dashboard-metrics-scraper-7bc864c59-5ncn8 evicted\npod/kubernetes-dashboard-6c7ccbcf87-qbbmd evicted\npod/kubernetes-dashboard-6c7ccbcf87-bkt7v evicted\npod/dashboard-metrics-scraper-7bc864c59-rjrjb evicted\npod/controller-759b6c5bb8-bl2tn evicted\npod/kubernetes-dashboard-6c7ccbcf87-krkdp evicted\npod/ingress-nginx-controller-7c7754d4b6-4tgjq evicted\npod/dashboard-metrics-scraper-7bc864c59-c82nc evicted\npod/dashboard-metrics-scraper-7bc864c59-f2kfh evicted\npod/kubernetes-dashboard-6c7ccbcf87-bjw5s evicted\npod/ingress-nginx-controller-7c7754d4b6-dfc2k evicted\npod/controller-759b6c5bb8-57nb6 evicted\npod/controller-759b6c5bb8-b2pch evicted\npod/controller-759b6c5bb8-9zbn4 evicted\npod/kubernetes-dashboard-6c7ccbcf87-b6nwl evicted\npod/dashboard-metrics-scraper-7bc864c59-s77ss evicted\npod/dashboard-metrics-scraper-7bc864c59-bs26f evicted\npod/dashboard-metrics-scraper-7bc864c59-952v2 evicted\npod/dashboard-metrics-scraper-7bc864c59-n6vpl evicted\npod/ingress-nginx-controller-7c7754d4b6-4nnmr evicted\npod/dashboard-metrics-scraper-7bc864c59-cqrhs evicted\npod/dashboard-metrics-scraper-7bc864c59-r5cng evicted\npod/dashboard-metrics-scraper-7bc864c59-pxb5m evicted\npod/dashboard-metrics-scraper-7bc864c59-pzgln evicted\npod/dashboard-metrics-scraper-7bc864c59-pls88 evicted\npod/kubernetes-dashboard-6c7ccbcf87-t27j4 evicted\npod/dashboard-metrics-scraper-7bc864c59-pzmbn evicted\npod/dashboard-metrics-scraper-7bc864c59-prw6v evicted\npod/controller-759b6c5bb8-g9g2x evicted\npod/dashboard-metrics-scraper-7bc864c59-b2n6c evicted\npod/dashboard-metrics-scraper-7bc864c59-jn4bt evicted\npod/kubernetes-dashboard-6c7ccbcf87-8gzvj evicted\npod/dashboard-metrics-scraper-7bc864c59-zrrb8 evicted\npod/dashboard-metrics-scraper-7bc864c59-krrtr evicted\npod/dashboard-metrics-scraper-7bc864c59-hkxzh evicted\npod/controller-759b6c5bb8-ht7tz evicted\npod/controller-759b6c5bb8-nr92c evicted\npod/kubernetes-dashboard-6c7ccbcf87-n2gn8 evicted\npod/controller-759b6c5bb8-ktq7h evicted\npod/controller-759b6c5bb8-xx5bb evicted\nI0922 09:37:34.180242 1271042 request.go:690] Waited for 21.199712674s due to client-side throttling, not priority and fairness, request: POST:https://10.0.0.2:6443/api/v1/namespaces/kubernetes-dashboard/pods/dashboard-metrics-scraper-7bc864c59-kbhtg/eviction\npod/controller-759b6c5bb8-6sp2m evicted\npod/controller-759b6c5bb8-hx4dl evicted\npod/kubernetes-dashboard-6c7ccbcf87-lbzrz evicted\npod/kubernetes-dashboard-6c7ccbcf87-n8cxk evicted\npod/kubernetes-dashboard-6c7ccbcf87-m8mb8 evicted\npod/dashboard-metrics-scraper-7bc864c59-vd88s evicted\npod/dashboard-metrics-scraper-7bc864c59-7bjzs evicted\npod/kubernetes-dashboard-6c7ccbcf87-45qcw evicted\npod/dashboard-metrics-scraper-7bc864c59-kbhtg evicted\npod/kubernetes-dashboard-6c7ccbcf87-fx98h evicted\npod/dashboard-metrics-scraper-7bc864c59-hjwcl evicted\npod/dashboard-metrics-scraper-7bc864c59-ztfwd evicted\npod/dashboard-metrics-scraper-7bc864c59-v8lkd evicted\npod/dashboard-metrics-scraper-7bc864c59-kt46k evicted\npod/dashboard-metrics-scraper-7bc864c59-mbkmb evicted\npod/kubernetes-dashboard-6c7ccbcf87-fscmh evicted\npod/kubernetes-dashboard-6c7ccbcf87-6n2zd evicted\npod/dashboard-metrics-scraper-7bc864c59-wbrk4 evicted\npod/kubernetes-dashboard-6c7ccbcf87-x462f evicted\npod/kubernetes-dashboard-6c7ccbcf87-26hxx evicted\npod/kubernetes-dashboard-6c7ccbcf87-zs228 evicted\npod/kubernetes-dashboard-6c7ccbcf87-9htbw evicted\npod/dashboard-metrics-scraper-7bc864c59-hxnnz evicted\npod/kubernetes-dashboard-6c7ccbcf87-74flg evicted\npod/controller-759b6c5bb8-jvpbg evicted\npod/kubernetes-dashboard-6c7ccbcf87-vkfxn evicted\npod/controller-759b6c5bb8-2gpk2 evicted\npod/dashboard-metrics-scraper-7bc864c59-dmws2 evicted\npod/kubernetes-dashboard-6c7ccbcf87-xr74p evicted\npod/controller-759b6c5bb8-hxhzc evicted\npod/ingress-nginx-controller-7c7754d4b6-jshnd evicted\npod/kubernetes-dashboard-6c7ccbcf87-wlzfq evicted\npod/dashboard-metrics-scraper-7bc864c59-wgtll evicted\npod/dashboard-metrics-scraper-7bc864c59-qpp5w evicted\npod/dashboard-metrics-scraper-7bc864c59-hmjdg evicted\npod/ingress-nginx-controller-7c7754d4b6-mgkdz evicted\npod/dashboard-metrics-scraper-7bc864c59-v66mn evicted\npod/dashboard-metrics-scraper-7bc864c59-nj2l7 evicted\npod/dashboard-metrics-scraper-7bc864c59-7z57g evicted\npod/kubernetes-dashboard-6c7ccbcf87-b94tb evicted\npod/plexserver-7cb77ddc8-d6bqg evicted\npod/coredns-787d4945fb-sqfcw evicted\nnode/rapture drained\n</code></pre> <p>Note</p> <p>Local storage defined as <code>PersistentVolume</code> is not deleted:</p> <pre><code># du -sh /home/k8s/*\n920M    /home/k8s/audiobookshelf\n17G     /home/k8s/photoprism\n7.6G    /home/k8s/plexmediaserver\n</code></pre> <p>At this point the Kubernetes cluster is running everything but those pods that are not part of Kubernetes:</p> <pre><code># kubectl get deployments -A\nNAMESPACE              NAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ningress-nginx          ingress-nginx-controller    0/1     1            0           132d\nkube-system            coredns                     0/2     2            0           132d\nkubernetes-dashboard   dashboard-metrics-scraper   0/1     1            0           132d\nkubernetes-dashboard   kubernetes-dashboard        0/1     1            0           132d\nmetallb-system         controller                  0/1     1            0           132d\nplexserver             plexserver                  0/1     1            0           132d\n\n# kubectl get pods -A\nNAMESPACE              NAME                                        READY   STATUS    RESTARTS        AGE\ningress-nginx          ingress-nginx-controller-7c7754d4b6-w4w5g   0/1     Pending   0               9m15s\nkube-flannel           kube-flannel-ds-bbhn8                       1/1     Running   120 (27h ago)   132d\nkube-system            coredns-787d4945fb-8xkqt                    0/1     Pending   0               9m3s\nkube-system            coredns-787d4945fb-kjrlx                    0/1     Pending   0               9m30s\nkube-system            etcd-rapture                                1/1     Running   89 (27h ago)    132d\nkube-system            kube-apiserver-rapture                      1/1     Running   89 (27h ago)    132d\nkube-system            kube-controller-manager-rapture             1/1     Running   89 (27h ago)    132d\nkube-system            kube-proxy-tszr7                            1/1     Running   89 (27h ago)    132d\nkube-system            kube-scheduler-rapture                      1/1     Running   89 (27h ago)    132d\nkubernetes-dashboard   dashboard-metrics-scraper-7bc864c59-4lw4r   0/1     Pending   0               9m16s\nkubernetes-dashboard   kubernetes-dashboard-6c7ccbcf87-mg8j6       0/1     Pending   0               9m30s\nmetallb-system         controller-759b6c5bb8-d4lwr                 0/1     Pending   0               9m25s\nmetallb-system         speaker-58krr                               1/1     Running   50 (27h ago)    76d\nphotoprism             photoprism-0                                0/1     Pending   0               9m21s\nplexserver             plexserver-7cb77ddc8-rn2p6                  0/1     Pending   0               9m3s\n</code></pre> <p>At this point it would be safe to stop all the services; but this is not necessary (and may get in the way of upgrading):</p> <pre><code># systemctl stop kubelet\n# systemctl stop containerd.service\n# systemctl stop docker.service\n# systemctl stop docker.socket\n</code></pre> <p>Instead, the next step is to determine which version to upgrade to by updating the minor version in the repository configuration and then finding the latest patch version:</p> <pre><code># vi /etc/apt/sources.list.d/kubernetes.list\ndeb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.27/deb/ /\n\n# apt update\n# apt-cache madison kubeadm\n   kubeadm | 1.27.16-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.15-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.14-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.13-2.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.12-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.11-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.10-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.9-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.8-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.7-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.6-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.5-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.4-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.3-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.0-2.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n</code></pre> <p>The latest patch version is 1.27.16 and that is the one to upgrade control plane nodes to:</p> <pre><code># apt-mark unhold kubeadm &amp;&amp; \\\n  apt-get update &amp;&amp; apt-get install -y kubeadm='1.27.16-*' &amp;&amp; \\\n  apt-mark hold kubeadm\nkubeadm was already not on hold.\n...\nSelected version '1.27.16-1.1' (isv:kubernetes:core:stable:v1.27:pkgs.k8s.io [amd64]) for 'kubeadm'\nThe following packages will be upgraded:\n  kubeadm\n1 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\nNeed to get 10.0 MB of archives.\nAfter this operation, 848 kB of additional disk space will be used.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.27/deb  kubeadm 1.27.16-1.1 [10.0 MB]\nFetched 10.0 MB in 1s (15.8 MB/s)\n(Reading database ... 645263 files and directories currently installed.)\nPreparing to unpack .../kubeadm_1.27.16-1.1_amd64.deb ...\nUnpacking kubeadm (1.27.16-1.1) over (1.26.15-1.1) ...\nSetting up kubeadm (1.27.16-1.1) ...\nkubeadm set on hold.\n\n# kubeadm version\nkubeadm version: &amp;version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.16\", GitCommit:\"cbb86e0d7f4a049666fac0551e8b02ef3d6c3d9a\", GitTreeState:\"clean\", BuildDate:\"2024-07-17T01:52:04Z\", GoVersion:\"go1.22.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\n# kubeadm upgrade plan\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks.\n[upgrade] Running cluster health checks\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: v1.26.15\n[upgrade/versions] kubeadm version: v1.27.16\nI0922 10:02:36.788943 1866090 version.go:256] remote version is much newer: v1.31.1; falling back to: stable-1.27\n[upgrade/versions] Target version: v1.27.16\n[upgrade/versions] Latest version in the v1.26 series: v1.26.15\n\nComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\nCOMPONENT   CURRENT        TARGET\nkubelet     1 x v1.26.15   v1.27.16\n\nUpgrade to the latest stable version:\n\nCOMPONENT                 CURRENT    TARGET\nkube-apiserver            v1.26.15   v1.27.16\nkube-controller-manager   v1.26.15   v1.27.16\nkube-scheduler            v1.26.15   v1.27.16\nkube-proxy                v1.26.15   v1.27.16\nCoreDNS                   v1.9.3     v1.10.1\netcd                      3.5.10-0   3.5.12-0\n\nYou can now apply the upgrade by executing the following command:\n\n        kubeadm upgrade apply v1.27.16\n\n_____________________________________________________________________\n\n\nThe table below shows the current state of component configs as understood by this version of kubeadm.\nConfigs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or\nresetting to kubeadm defaults before a successful upgrade can be performed. The version to manually\nupgrade to is denoted in the \"PREFERRED VERSION\" column.\n\nAPI GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED\nkubeproxy.config.k8s.io   v1alpha1          v1alpha1            no\nkubelet.config.k8s.io     v1beta1           v1beta1             no\n_____________________________________________________________________\n\n\n# kubeadm upgrade apply v1.27.16\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks.\n[upgrade] Running cluster health checks\n[upgrade/version] You have chosen to change the cluster version to \"v1.27.16\"\n[upgrade/versions] Cluster version: v1.26.15\n[upgrade/versions] kubeadm version: v1.27.16\n[upgrade] Are you sure you want to proceed? [y/N]: y\n[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster\n[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection\n[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'\nW0922 10:04:12.442139 1888818 checks.go:835] detected that the sandbox image \"registry.k8s.io/pause:3.6\" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.\n[upgrade/apply] Upgrading your Static Pod-hosted control plane to version \"v1.27.16\" (timeout: 5m0s)...\n[upgrade/etcd] Upgrading to TLS for etcd\n[upgrade/staticpods] Preparing for \"etcd\" upgrade\n[upgrade/staticpods] Renewing etcd-server certificate\n[upgrade/staticpods] Renewing etcd-peer certificate\n[upgrade/staticpods] Renewing etcd-healthcheck-client certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/etcd.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-09-22-10-04-17/etcd.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=etcd\n[upgrade/staticpods] Component \"etcd\" upgraded successfully!\n[upgrade/etcd] Waiting for etcd to become available\n[upgrade/staticpods] Writing new Static Pod manifests to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests446600773\"\n[upgrade/staticpods] Preparing for \"kube-apiserver\" upgrade\n[upgrade/staticpods] Renewing apiserver certificate\n[upgrade/staticpods] Renewing apiserver-kubelet-client certificate\n[upgrade/staticpods] Renewing front-proxy-client certificate\n[upgrade/staticpods] Renewing apiserver-etcd-client certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-09-22-10-04-17/kube-apiserver.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=kube-apiserver\n[upgrade/staticpods] Component \"kube-apiserver\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-controller-manager\" upgrade\n[upgrade/staticpods] Renewing controller-manager.conf certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-09-22-10-04-17/kube-controller-manager.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=kube-controller-manager\n[upgrade/staticpods] Component \"kube-controller-manager\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-scheduler\" upgrade\n[upgrade/staticpods] Renewing scheduler.conf certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-09-22-10-04-17/kube-scheduler.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=kube-scheduler\n[upgrade/staticpods] Component \"kube-scheduler\" upgraded successfully!\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config4276155515/config.yaml\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\n[upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.27.16\". Enjoy!\n\n[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.\n\n# kubeadm version\nkubeadm version: &amp;version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.16\", GitCommit:\"cbb86e0d7f4a049666fac0551e8b02ef3d6c3d9a\", GitTreeState:\"clean\", BuildDate:\"2024-07-17T01:52:04Z\", GoVersion:\"go1.22.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n</code></pre> <p>Now that the control plane is updated, proceed to upgrade kubelet and kubectl</p> <pre><code># apt-mark unhold kubelet kubectl &amp;&amp; \\\n  apt-get update &amp;&amp; apt-get install -y kubelet='1.27.16-*' kubectl='1.27.16-*' &amp;&amp; \\\n  apt-mark hold kubelet kubectl\nkubelet was already not on hold.\nkubectl was already not on hold.\n...\nPreparing to unpack .../kubectl_1.27.16-1.1_amd64.deb ...\nUnpacking kubectl (1.27.16-1.1) over (1.26.15-1.1) ...\nPreparing to unpack .../kubelet_1.27.16-1.1_amd64.deb ...\nUnpacking kubelet (1.27.16-1.1) over (1.26.15-1.1) ...\ndpkg: warning: unable to delete old directory '/etc/sysconfig': Directory not empty\nSetting up kubectl (1.27.16-1.1) ...\nSetting up kubelet (1.27.16-1.1) ...\nkubelet set on hold.\nkubectl set on hold.\n\n# systemctl daemon-reload\n# systemctl restart kubelet\n\n# kubectl  version --output=yaml\nclientVersion:\n  buildDate: \"2024-07-17T01:53:56Z\"\n  compiler: gc\n  gitCommit: cbb86e0d7f4a049666fac0551e8b02ef3d6c3d9a\n  gitTreeState: clean\n  gitVersion: v1.27.16\n  goVersion: go1.22.5\n  major: \"1\"\n  minor: \"27\"\n  platform: linux/amd64\nkustomizeVersion: v5.0.1\nserverVersion:\n  buildDate: \"2024-07-17T01:44:26Z\"\n  compiler: gc\n  gitCommit: cbb86e0d7f4a049666fac0551e8b02ef3d6c3d9a\n  gitTreeState: clean\n  gitVersion: v1.27.16\n  goVersion: go1.22.5\n  major: \"1\"\n  minor: \"27\"\n  platform: linux/amd64\n</code></pre> <p>Finally, bring the node back online by marking it schedulable:</p> <pre><code># kubectl uncordon rapture\nnode/rapture uncordoned\n\n# kubectl get pods -A\nNAMESPACE              NAME                                        READY   STATUS        RESTARTS        AGE\ningress-nginx          ingress-nginx-controller-7c7754d4b6-w4w5g   0/1     Running       0               70m\nkube-flannel           kube-flannel-ds-bbhn8                       1/1     Running       120 (28h ago)   132d\nkube-system            coredns-5d78c9869d-9xbh4                    1/1     Running       0               41m\nkube-system            coredns-5d78c9869d-w8954                    1/1     Running       0               41m\nkube-system            coredns-787d4945fb-8xkqt                    1/1     Terminating   0               70m\nkube-system            etcd-rapture                                1/1     Running       2 (72s ago)     73s\nkube-system            kube-apiserver-rapture                      1/1     Running       2 (72s ago)     73s\nkube-system            kube-controller-manager-rapture             1/1     Running       2 (72s ago)     73s\nkube-system            kube-proxy-nzmwz                            1/1     Running       0               40m\nkube-system            kube-scheduler-rapture                      1/1     Running       1 (72s ago)     73s\nkubernetes-dashboard   dashboard-metrics-scraper-7bc864c59-4lw4r   1/1     Running       0               70m\nkubernetes-dashboard   kubernetes-dashboard-6c7ccbcf87-mg8j6       1/1     Running       0               70m\nmetallb-system         controller-759b6c5bb8-d4lwr                 0/1     Running       0               70m\nmetallb-system         speaker-58krr                               1/1     Running       50 (28h ago)    76d\nphotoprism             photoprism-0                                0/1     Running       0               70m\nplexserver             plexserver-7cb77ddc8-rn2p6                  1/1     Running       0               70m\n</code></pre> <p>After a few minutes, the Plex Media Server and the PhotoPrism\u00ae services are up and running again.</p> <p>In the end the process seems to have gone pretty smoothly. With this out of the way, and given that deployments passed checks for deprecations, the next step is to repeat this process for every minor version up to the latest.</p>"},{"location":"blog/2024/10/26/self-hosted-music-streaming-with-navidrome/","title":"Self-hosted music streaming with Navidrome","text":"<p>Navidrome is a self-hosted, open source music server and streamer. It gives you freedom to listen to your music collection from any browser or mobile device I heard about in the Linux Matters podcast.</p> <p>I tried it on my little Kubernetes cluster and here are impressions so far.</p> <p>The following deployment is based on the Navidrome Installing with Docker:</p> Kubernetes deployment: <code>navidrome.yaml</code> navidrome.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: navidrome\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: navidrome-pv-data\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/navidrome/data\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: navidrome-pv-depot-music\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 100Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/depot/audio/Music\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: navidrome-pvc-data\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  volumeName: navidrome-pv-data\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: navidrome-pvc-depot-music\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  volumeName: navidrome-pv-depot-music\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: navidrome\n  name: navidrome\n  namespace: navidrome\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: navidrome\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: navidrome\n    spec:\n      containers:\n        - image: deluan/navidrome:latest\n          imagePullPolicy: Always\n          name: navidrome\n          env:\n          - name: ND_BASEURL\n            value: \"https://music.ssl.uu.am/\"\n          - name: ND_LOGLEVEL\n            value: \"info\"\n          - name: ND_SCANSCHEDULE\n            value: \"1h\"\n          - name: ND_SESSIONTIMEOUT\n            value: \"24h\"\n          ports:\n          - containerPort: 4533\n          resources: {}\n          stdin: true\n          tty: true\n          volumeMounts:\n          - mountPath: /data\n            name: navidrome-data\n          - mountPath: /music\n            name: navidrome-music\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 1008\n            runAsGroup: 1008\n      restartPolicy: Always\n      volumes:\n      - name: navidrome-data\n        persistentVolumeClaim:\n          claimName: navidrome-pvc-data\n      - name: navidrome-music\n        persistentVolumeClaim:\n          claimName: navidrome-pvc-depot-music\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: navidrome-svc\n  namespace: navidrome\nspec:\n  type: NodePort\n  ports:\n  - port: 4533\n    nodePort: 30533\n    targetPort: 4533\n  selector:\n    app: navidrome\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: navidrome-ingress\n  namespace: navidrome\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: navidrome-svc\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: music.ssl.uu.am\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: navidrome-svc\n                port:\n                  number: 4533\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - music.ssl.uu.am\n</code></pre> <p>To use a configuration file, create a <code>navidrome.toml</code> config file in the <code>/data</code> folder and set the option <code>ND_CONFIGFILE=/data/navidrome.toml</code>. So far this has not seemed to be necessary.</p> <p>Before running this deployment, create the required directory and local user, get its user id (<code>1008</code>) and plug it above:</p> <pre><code># useradd navidrome\n# mkdir /home/k8s/navidrome\n# chown navidrome.navidrome /home/k8s/navidrome\n# ls -lan /home/k8s/navidrome/\ntotal 0\ndrwxr-xr-x 1 1008 1008   0 Oct 26 08:53 .\ndrwxr-xr-x 1    0    0 372 Oct 26 08:53 ..\n</code></pre> <p>Once everything is ready, apply the deployment and give it a few minutes to start the pods and get the SSL certificated ready:</p> <pre><code>$ kubectl apply -f navidrome.yaml\nnamespace/navidrome created\npersistentvolume/navidrome-pv-data created\npersistentvolume/navidrome-pv-depot-music created\npersistentvolumeclaim/navidrome-pvc-data created\npersistentvolumeclaim/navidrome-pvc-depot-music created\ndeployment.apps/navidrome created\nservice/navidrome-svc created\ningress.networking.k8s.io/navidrome-ingress created\n</code></pre> <p>After a couple of minutes the server's web interface is at https://music.ssl.uu.am/ and one can start by creating an admin user, then (optionally) more users (some of which can be admins too). In the meantime, the server will detect and scan music files and, as they are found, make them available for playback. The web player works doesn't look like much but works very nicely:</p> <p></p> <p>This web player alone already satisfy my first need, which is to listen to music while working, i.e. from a computer. For the use case of listening to music while on the go (i.e. from a phone), there are a few Subsonic-compatible Android apps available directly from Google Play:</p> GoSONIC Symfonium Ultrasonic <p>The jury is still out, and not in a hurry to come back, as to which of these players will win me over. Symfonium seems to be the only one that require payment, which likely won't help.</p> <p>News</p> <p>After a couple of months, I find myself always going back to GoSONIC. The main reason for this choice is just how easy, fast and reliable it is to start playing the most recently played playlist. This is essentially what I use every music player for, so this matters most for me.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/","title":"Ubuntu Studio 24.04 on Rapture, Gaming PC (and more)","text":"<p>The time has came to update my main PC, which I use for gaming, coding, media production and just about everything, to  Ubuntu Studio 24.04.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#considering-timing","title":"Considering Timing","text":"<p>Ubuntu Studio 24.04 LTS Released on April 25th but, as they themselves put it since it\u2019s just out, you may experience some issues, so you might want to wait a bit before upgrading.</p> <p>There doesn't seem to be anything particular scarey in release notes:</p> <ul> <li>Ubuntu Studio 24.04 LTS Release Notes</li> <li>Kubuntu 24.04 Release Notes</li> <li>Noble Numbat Release Notes</li> </ul> <p>And my plan is not to upgrade in place; I like to keep the previous version around, just in case I need a stable system to fall back to.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#preparation","title":"Preparation","text":"<p>In preparation to upgrade my main PC to Ubuntu Studio 24.04, I installed a new Kingston FURY Renegade 4000 GB M.2 SSD and prepared partitions as follows:</p> <ol> <li>260 MB EFI System for the EFI boot.</li> <li>75 GB Linux filesystem for the root (ext4).</li> <li>75 GB Linux filesystem for the alternative root (ext4).</li> <li>3.5T GB Linux filesystem for <code>/home</code> (btrfs).</li> </ol> <p>Note</p> <p>75 GB has proven to be a reasonable size for the root partition for the amount of software that tends to be installed in my PC, including 20 GB in <code>/usr</code> and 13 GB in <code>/snap</code>.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#partitions","title":"Partitions","text":"<p>First, create a GPT partition table (<code>label</code>), then create the partitions with <code>optimal</code> alignment</p> <pre><code># parted /dev/nvme1n1 --script -- mklabel gpt\n# parted -a optimal /dev/nvme1n1 mkpart primary fat32 0% 260MiB\n# parted -a optimal /dev/nvme1n1 mkpart primary ext4 260MiB 75GiB\n# parted -a optimal /dev/nvme1n1 mkpart primary ext4 75GiB 150GiB\n# parted -a optimal /dev/nvme1n1 mkpart primary btrfs 150GiB 100%\n\n# parted /dev/nvme1n1 print\nModel: KINGSTON SFYRD4000G (nvme)\nDisk /dev/nvme1n1: 4001GB\nSector size (logical/physical): 512B/512B\nPartition Table: gpt\nDisk Flags: \n\nNumber  Start   End     Size    File system  Name     Flags\n 1      1049kB  273MB   272MB                primary  msftdata\n 2      273MB   80.5GB  80.3GB               primary\n 3      80.5GB  161GB   80.5GB               primary\n 4      161GB   4001GB  3840GB               primary\n\n# fdisk -l /dev/nvme1n1 \nDisk /dev/nvme1n1: 3.64 TiB, 4000787030016 bytes, 7814037168 sectors\nDisk model: KINGSTON SFYRD4000G                     \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: gpt\nDisk identifier: 3C935B5D-3EFC-4683-9483-BC110B2AEB17\n\nDevice             Start        End    Sectors  Size Type\n/dev/nvme1n1p1      2048     532479     530432  259M Microsoft basic data\n/dev/nvme1n1p2    532480  157286399  156753920 74.7G Linux filesystem\n/dev/nvme1n1p3 157286400  314572799  157286400   75G Linux filesystem\n/dev/nvme1n1p4 314572800 7814035455 7499462656  3.5T Linux filesystem\n</code></pre> <p>Note</p> <p>In retrospect, it seems to be necessary to also apply the <code>boot</code> flag to the EFI partition; otherwise the Ubuntu installer will not offer the possibility of installing the boot loader in this disk:</p> <pre><code># parted /dev/nvme1n1 toggle 1 boot\n# parted /dev/nvme1n1 print\nModel: KINGSTON SFYRD4000G (nvme)\nDisk /dev/nvme1n1: 4001GB\nSector size (logical/physical): 512B/512B\nPartition Table: gpt\nDisk Flags: \n\nNumber  Start   End     Size    File system  Name     Flags\n1      1049kB  273MB   272MB                primary  boot, esp\n2      273MB   80.5GB  80.3GB               primary\n3      80.5GB  161GB   80.5GB               primary\n4      161GB   4001GB  3840GB               primary\n</code></pre> <p>Partitions can be created during the installation of the system, there is no need to create them before hand because the previous M.2 SSD is not going anywhere any time soon.</p> <p>In the future, the previous (2 TB) M.2 SSD may be replaced with another 4 TB SSD, e.g.</p> <ul> <li>Kingston FURY Renegade with Heatsink ($320+)</li> <li>Samsung 990 Pro with Heatsink ($300+).</li> </ul> <p>However, that replacement will likely not happen under 2026, once Ubuntu Studio 26.04 is installed and there is no longer a point to keep the old Ubuntu 22.04 around.</p> <p>In the meantime, the new SSD can be converted into being the new <code>/home</code> and, while the old one is still around, this could be a good time to try bcachefs on the new one. However, that requires either building a kernel or waiting for Linux 6.7 or later. Ubuntu 24.04 LTS Will Aim To Ship With The Linux 6.8 Kernel, so the easier approach would be to wait for the installation of Ubuntu 24.04 to move <code>/home</code> to the new SSD.</p> <p>Listen to Linux Matters #23: An Exodus of Bitcoin were Martin has excitedly installed it on everything! In particular, there seems to be options to configure large file systems across multiple devices with redundancy (details to be confirmed) and tiered storage to keep hot data on the faster storage (NVME) and rebalance unused data back to slower storage (SATA).</p> <p>An update on bcachefs</p> <p>The future of bcachefs in the kernel is uncertain, and lots of things aren't looking good. For more details, see Bcachefs Changes Rejected Reportedly Due To CoC, Kernel Future \"Uncertain\".</p> <p>For the time being, there is no plan to use bcachefs here.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#new-btrfs-home","title":"New BtrFS Home","text":"<p>Create a new <code>btrfs</code> file system to test the new SSD, while we're waiting for the upcoming Ubuntu 24.04 release:</p> <pre><code># mkfs.btrfs /dev/nvme1n1p4\nbtrfs-progs v5.16.2\nSee http://btrfs.wiki.kernel.org for more information.\n\nPerforming full device TRIM /dev/nvme1n1p4 (3.49TiB) ...\nNOTE: several default settings have changed in version 5.15, please make sure\n      this does not affect your deployments:\n      - DUP for metadata (-m dup)\n      - enabled no-holes (-O no-holes)\n      - enabled free-space-tree (-R free-space-tree)\n\nLabel:              (null)\nUUID:               8edfc3ba-4981-4423-8730-7e229bfa63f3\nNode size:          16384\nSector size:        4096\nFilesystem size:    3.49TiB\nBlock group profiles:\n  Data:             single            8.00MiB\n  Metadata:         DUP               1.00GiB\n  System:           DUP               8.00MiB\nSSD detected:       yes\nZoned device:       no\nIncompat features:  extref, skinny-metadata, no-holes\nRuntime features:   free-space-tree\nChecksum:           crc32c\nNumber of devices:  1\nDevices:\n   ID        SIZE  PATH\n    1     3.49TiB  /dev/nvme1n1p4\n\n# mkdir /home/new-m2\n# tail -1 /etc/fstab \nUUID=8edfc3ba-4981-4423-8730-7e229bfa63f3 /home/new-m2      btrfs   defaults        0       0\n</code></pre> <p>Suddently there is a lot of space available!</p> <pre><code>$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme0n1p5  1.7T  1.2T  517G  70% /home\n/dev/sdc        3.7T  2.7T  1.1T  72% /home/ssd\n/dev/sda        5.5T  5.3T  254G  96% /home/raid\n/dev/sdb        3.7T  3.0T  667G  83% /home/new-ssd\n/dev/nvme1n1p4  3.5T  3.7M  3.5T   1% /home/new-m2\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#transfer-speed-test","title":"Transfer Speed Test","text":"<p>First, transfer 1.4 GB of media (family photos and videos) from the SATA SSD. Theoretical maximum transfer speed is around 550 MB/s, but in practice the transfer speed fluctuates between 500 and 540 MB/s and the transfer took 48 minutes. There was little difference between using <code>rync -uva</code> vs <code>cp -av</code>.</p> <pre><code># time cp -a /home/ssd/Fotos /home/new-m2/\n\nreal    48m39.874s\nuser    0m3.933s\nsys     13m10.019s\n</code></pre> <p>Then, transfer 1.1 GB of personal files from the old NVME SSD. Theoretical maximum transfer speed is around 3000 MB/s, in practice the transfer speed fluctuates between 800 and 2500 MB/s and the transfer took 25 minutes, so just about twice as fast as the previous one.</p> <pre><code># time rsync -a /home/coder /home/new-m2/\n\nreal    24m54.444s\nuser    4m31.149s\nsys     17m54.582s\n</code></pre> <p>With these 2 transfers, the new M.2 SSD is nearly at 70%:</p> <pre><code>$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme0n1p5  1.7T  1.2T  517G  70% /home\n/dev/sdc        3.7T  2.7T  1.1T  72% /home/ssd\n/dev/sda        5.5T  5.3T  254G  96% /home/raid\n/dev/sdb        3.7T  3.0T  667G  83% /home/new-ssd\n/dev/nvme1n1p4  3.5T  2.4T  1.2T  69% /home/new-m2\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#installation","title":"Installation","text":"<p>With the above partitions prepared well in advance, to Install Ubuntu Studio 24.04 the process should be as simple, easy and smooth as it was with other systems.</p> <p>Alas, it wasn't. Even after setting up all the partition correctly for the new install, the installer would not allow selecting the correct device for to install the boot loader in it: <code>nvme1n1</code> is grayed out!</p> <p>This problem is one I had seen recently, but didn't write down what the solution was.</p> <p>While search (in vain) for others facing the same issue, I took advantage of being in a live USB system and cloned the current Ubuntu Studio 22.04 root partition (<code>nvme0n1p6</code>) onto what would have been the new Ubuntu Studio 24.04 root partition (<code>nvme1n1p2</code>), in case this may come in handy later.</p> <p>Important</p> <p>After cloning a root file system, the <code>/etc/fstab</code> file in the new clone must be updated with the correct UUID of that partition.</p> <p>If the (new) EFI partition has not been formatted yet; format it as FAT32:</p> <pre><code># mkfs.fat -F32 /dev/nvme1n1p1\nmkfs.fat 4.2 (2021-01-31)\n\n# lsblk -f\nnvme1n1\n\u2502\n\u251c\u2500nvme1n1p1\n\u2502    vfat   FAT32       73CC-6E86\n...\n\u251c\u2500nvme1n1p2\n\u2502    ext4   1.0         409501ea-d63d-49b2-bd45-3b876404dc53\n\nnvme0n1\n\u2502\n...\n\u2514\u2500nvme0n1p6\n     ext4   1.0         de317ca5-96dd-49a7-b72b-4bd050a8d15c   20.8G    67% /var/snap/firefox/common/host-hunspell\n</code></pre> <p>The mount the new root and edit <code>/etc/fstab</code> in it:</p> <pre><code># mount /dev/nvme1n1p2 /media/cdrom/\n# vi /media/cdrom/etc/fstab\n...\n# &lt;file system&gt;             &lt;mount point&gt;  &lt;type&gt;  &lt;options&gt;  &lt;dump&gt;  &lt;pass&gt;\nUUID=73CC-6E86                            /boot/efi      vfat    umask=0077 0 2\nUUID=409501ea-d63d-49b2-bd45-3b876404dc53 /              ext4    defaults,discard 0 1\n...\n</code></pre> <p>One potential problem is that no partition in <code>nvme1n1</code>  has tbe <code>boot</code> flag. It appears all bootable partitions that are working have flags <code>boot, esp</code> so the solution may be simply to add those flags. At the very least, this seems like it is necessary, if not sufficient:</p> <pre><code># parted /dev/nvme1n1\nGNU Parted 3.4\nUsing /dev/nvme1n1\nWelcome to GNU Parted! Type 'help' to view a list of commands.\n(parted) print                                                            \nModel: KINGSTON SFYRD4000G (nvme)\nDisk /dev/nvme1n1: 4001GB\nSector size (logical/physical): 512B/512B\nPartition Table: gpt\nDisk Flags: \n\nNumber  Start   End     Size    File system  Name     Flags\n 1      1049kB  273MB   272MB                primary  msftdata\n 2      273MB   80.5GB  80.3GB  ext4         primary\n 4      161GB   4001GB  3840GB  btrfs        primary\n\n(parted) toggle 1 boot\n(parted) print\nModel: KINGSTON SFYRD4000G (nvme)\nDisk /dev/nvme1n1: 4001GB\nSector size (logical/physical): 512B/512B\nPartition Table: gpt\nDisk Flags: \n\nNumber  Start   End     Size    File system  Name     Flags\n 1      1049kB  273MB   272MB                primary  boot, esp\n 2      273MB   80.5GB  80.3GB  ext4         primary\n 4      161GB   4001GB  3840GB  btrfs        primary\n</code></pre> <p>Note</p> <p>The <code>boot</code> and <code>esp</code> flags are the same; if you toggle both, you end up with the initial state.</p> <p>At this point, decided to follow  askubuntu.com/a/1463655 to fully cloned the current system onto the new 4TB NVME and try to boot from it.</p> <pre><code># mount /dev/nvme1n1p2 /media/cdrom/\n# mount /dev/nvme1n1p1 /media/cdrom/boot/efi/\n\n# grub-install \\\n  --target x86_64-efi \\\n  --efi-directory /media/cdrom/boot/efi \\\n  --boot-directory /media/cdrom/boot\nInstalling for x86_64-efi platform.\nInstallation finished. No error reported.\n\n# grub-mkconfig -o /media/cdrom/boot/grub/grub.cfg\nSourcing file `/etc/default/grub'\nSourcing file `/etc/default/grub.d/init-select.cfg'\nGenerating grub configuration file ...\nFound linux image: /boot/vmlinuz-5.15.0-124-lowlatency\nFound initrd image: /boot/initrd.img-5.15.0-124-lowlatency\nFound linux image: /boot/vmlinuz-5.15.0-124-lowlatency\nFound initrd image: /boot/initrd.img-5.15.0-124-lowlatency\nFound linux image: /boot/vmlinuz-5.15.0-122-lowlatency\nFound initrd image: /boot/initrd.img-5.15.0-122-lowlatency\nFound linux image: /boot/vmlinuz-5.15.0-60-lowlatency\nFound initrd image: /boot/initrd.img-5.15.0-60-lowlatency\nMemtest86+ needs a 16-bit boot, that is not available on EFI, exiting\nWarning: os-prober will be executed to detect other bootable partitions.\nIts output will be used to detect bootable binaries on them and create new boot entries.\nFound Ubuntu 22.04.1 LTS (22.04) on /dev/nvme0n1p2\nFound Ubuntu 22.04.5 LTS (22.04) on /dev/nvme1n1p2\nAdding boot menu entry for UEFI Firmware Settings ...\ndone\n</code></pre> <p>At this point there should be a boot loader, ready to boot, on thew new NVME. If anything, it looks like it would boot the old old 22.04.1 <code>nvme0n1p2</code> instead of the old (current) 22.04.5 <code>nvme0n1p6</code>.</p> <p>Indeed upon reboot, the UEFI boot menu now shows both NVME disks and selecting the 4TB disk the new bootloader shows those entries. After this, the installation process was, finally, smooth and successful, installing the new boot loader in <code>nvme1n1</code>.</p> <p>This new new bootloader in <code>nvme1n1</code> now shows all 4 systems available to boot - Ubuntu 22.04.1 in <code>nvme0n1p2</code> (have not used in some time) - Ubuntu 22.04.5 in <code>nvme0n1p6</code> (current daily driver) - Ubuntu 22.04.5 in <code>nvme1n1p2</code> (future backup daily driver) - Ubuntu 24.04.1 in <code>nvme1n1p3</code> (future daily driver)</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#adjusting-all-etcfstab-files","title":"Adjusting all <code>/etc/fstab</code> files","text":"<p>To make sure all those root partitions are usable, their <code>/etc/fstab</code> files need to be adjusted to point to the correct <code>/boot/efi</code> partitions (the one in the same disk) and the new one will has a new UUID after the last installation:</p> <pre><code># lsblk -f\nNAME FSTYPE FSVER LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINTS\n...\nnvme0n1\n\u2502\n\u251c\u2500nvme0n1p1\n\u2502                  \n\u251c\u2500nvme0n1p2\n\u2502    ext4   1.0         833c6403-a771-46b2-bde8-704f2ab7e88b\n\u251c\u2500nvme0n1p3\n\u2502    ext4   1.0         343f75fe-ec96-49fa-a4f8-0d32c69c1424\n\u251c\u2500nvme0n1p4\n\u2502    vfat   FAT32 NO_LABEL\n\u2502                       C38B-C318                             293.3M     2% /boot/efi\n\u251c\u2500nvme0n1p5\n\u2502    btrfs              18238846-d411-4dcb-af87-a2d19a17fef3  654.9G    62% /home\n\u2514\u2500nvme0n1p6\n     ext4   1.0         de317ca5-96dd-49a7-b72b-4bd050a8d15c\n\nnvme1n1\n\u2502\n\u251c\u2500nvme1n1p1\n\u2502    vfat   FAT16       4485-0F5E\n\u251c\u2500nvme1n1p2\n\u2502    ext4   1.0         409501ea-d63d-49b2-bd45-3b876404dc53   18.7G    69%\n\u251c\u2500nvme1n1p3\n\u2502    ext4   1.0         1d30a16e-b4f6-4459-9b19-8c9093b0d047                \n\u2514\u2500nvme1n1p4\n     btrfs              8edfc3ba-4981-4423-8730-7e229bfa63f3      1T    71% /home/new-m2\n</code></pre> <p>Following the above order, make sure that each <code>/etc/fstab</code> file points to the correct partition/s:</p> <p>Ubuntu 22.04.1 in <code>nvme0n1p2</code> (not used in some time) is not even using an EFI partition at all; this one dates back to a time when this disk was used in legacy BIOS mode:</p> <pre><code># df -h | grep nvme0n1p2\n/dev/nvme0n1p2   50G   27G   21G  57% /jellyfish\n\n# grep -E '/ |/bo'  /jellyfish/etc/fstab \nUUID=833c6403-a771-46b2-bde8-704f2ab7e88b /              ext4    defaults   0 1\n</code></pre> <p>Ubuntu 22.04.5 in <code>nvme0n1p6</code> (current daily driver) has not changed, as expected:</p> <pre><code># grep -E '/ |/bo'  /etc/fstab \nUUID=C38B-C318                            /boot/efi      vfat    umask=0077 0 2\nUUID=de317ca5-96dd-49a7-b72b-4bd050a8d15c /              ext4    defaults,discard 0 1\n</code></pre> <p>Ubuntu 22.04.5 in <code>nvme1n1p2</code> (the future backup daily driver) is a clone of the one in <code>nvme0n1p6</code> but it is also on the  newer NVME disk, so both partitions must be updated:</p> <ul> <li><code>/boot/efi</code> must point to <code>nvme1n1p1</code></li> <li><code>/</code> must point to <code>nvme1n1p2</code></li> </ul> <pre><code># mount /dev/nvme1n1p2 /media/cdrom/\n# grep -E '/ |/bo'  /media/cdrom/etc/fstab \nUUID=4485-0F5E                            /boot/efi      vfat    umask=0077 0 2\nUUID=409501ea-d63d-49b2-bd45-3b876404dc53 /              ext4    defaults,discard 0 1\n# umount /dev/nvme1n1p2\n</code></pre> <p>This being an updated copy of the current <code>/etc/fstab</code> file, it already has all the partitions currently in use.</p> <p>Ubuntu 24.04.1 in <code>nvme1n1p3</code> (the future daily driver) should be already good to go; since it was just installed:</p> <pre><code># mount /dev/nvme1n1p3 /noble\n# grep -E '/ |/bo'  /noble/etc/fstab \n# / was on /dev/nvme1n1p3 during curtin installation\n/dev/disk/by-uuid/1d30a16e-b4f6-4459-9b19-8c9093b0d047 / ext4 defaults 0 1\n# /boot/efi was on /dev/nvme1n1p1 during curtin installation\n/dev/disk/by-uuid/4485-0F5E /boot/efi vfat defaults 0 1\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#new-mount-points-for-all-partitions","title":"New mount points for all partitions","text":"<p>The <code>/etc/fstab</code> in Ubuntu 24.04.1 (<code>nvme1n1p3</code>) is missing all the other partitions currently in use, and includes a swap file that is unnecessary in a system with 32 GB of RAM:</p> /etc/fstab<pre><code># /etc/fstab: static file system information.\n#\n# Use 'blkid' to print the universally unique identifier for a\n# device; this may be used with UUID= as a more robust way to name devices\n# that works even if disks are added and removed. See fstab(5).\n#\n# &lt;file system&gt; &lt;mount point&gt;   &lt;type&gt;  &lt;options&gt;       &lt;dump&gt;  &lt;pass&gt;\n# / was on /dev/nvme1n1p3 during curtin installation\n/dev/disk/by-uuid/1d30a16e-b4f6-4459-9b19-8c9093b0d047 / ext4 defaults 0 1\n# /boot/efi was on /dev/nvme1n1p1 during curtin installation\n/dev/disk/by-uuid/4485-0F5E /boot/efi vfat defaults 0 1\n# /home was on /dev/nvme1n1p4 during curtin installation\n/dev/disk/by-uuid/8edfc3ba-4981-4423-8730-7e229bfa63f3 /home btrfs defaults 0 1\n/swap.img       none    swap    sw      0       0\n</code></pre> <p>Because this PC has been collecting hard drives over the years, there are many additional partitions in use:</p> <pre><code># df -h | head -1; df -h | grep -E 'nvme|sd.'\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme0n1p6   74G   49G   21G  71% /\n/dev/nvme0n1p4  300M  6.1M  294M   3% /boot/efi\n/dev/nvme0n1p5  1.7T  1.1T  655G  62% /home\n/dev/sdb        3.7T  2.1T  1.6T  57% /home/new-ssd\n/dev/sdc        3.7T  2.9T  833G  78% /home/ssd\n/dev/nvme1n1p4  3.5T  2.5T  1.1T  71% /home/new-m2\n/dev/sda        5.5T  5.1T  370G  94% /home/raid\n/dev/nvme1n1p3   74G   22G   49G  31% /noble\n</code></pre> <p>For just about the same reason, there are also several symlinks strategically pointing from where thigns used to be to where they are now:</p> <pre><code># ls -l /home/\ntotal 80\nlrwxrwxrwx 1 root   root      17 Sep 24  2022 depot -&gt; /home/raid/depot/\nlrwxrwxrwx 1 root   root      16 May 12 19:37 k8s -&gt; /home/new-m2/k8s\nlrwxrwxrwx 1 root   root      16 May 12 10:02 lib -&gt; /home/new-m2/lib\n\n# ls -l /home/raid/depot/[av]*\nlrwxrwxrwx 1 coder coder 16 Aug 20  2023 /home/raid/depot/audio -&gt; /home/raid/audio\nlrwxrwxrwx 1 coder coder 16 Aug 20  2023 /home/raid/depot/video -&gt; /home/raid/video\n\n# ls -l /home/new-ssd/video\nlrwxrwxrwx 1 root root 16 Aug 20  2023 /home/new-ssd/video -&gt; /home/raid/video\n</code></pre> <p>In the future daily driver, the old 2TB NVME should not be used, so it can be replaced later by a newer 4TB disk.</p> <p>To that effect, all data in <code>/dev/nvme0n1p5</code> must be copied over to <code>/dev/nvme1n1p4</code> and in fact most of it is already there. There are only a few users' home directories and empty directories (mount points):</p> <pre><code># du -sh /home/*\n952G    /home/coder\n18G     /home/ernest\n134G    /home/manuel\n1.5G    /home/minecraft\n44K     /home/sam\n\n# du -sh /home/new-m2/*\n952G    /home/new-m2/coder\n18G     /home/new-m2/ernest\n1.5T    /home/new-m2/Fotos\n26G     /home/new-m2/k8s\n35G     /home/new-m2/lib\n134G    /home/new-m2/manuel\n1.5G    /home/new-m2/minecraft\n44K     /home/new-m2/sam\n\n# cd /home/new-m2/\nroot@rapture:/home/new-m2# mkdir new-ssd raid ssd\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#boot-cloned-ubuntu-studio-2204-without-old-nvme","title":"Boot cloned Ubuntu Studio 22.04 without old NVME","text":"<p>As an intermediate step to booting the new Ubuntu Studio 24.04 later, boot the newly cloned Ubuntu Studio 22.04 in <code>nvme1n1p3</code> without mounting the old NVME.</p> <p>With the above preparetions done in the new NVME, this should be as simple as mounting the new NVME on <code>/home</code> and simply not mounting the old NVME:</p> <pre><code># mount /dev/nvme1n1p2 /media/cdrom/\n</code></pre> /etc/fstab<pre><code># Previous-new (June 2022) 2TB NVME SSD (/home)\n#UUID=18238846-d411-4dcb-af87-a2d19a17fef3 /home          btrfs   defaults,noatime,autodefrag,discard,compress=lzo 0 0\n\n# New-new 4TB M.2 SSD (newer /home; previously /home/new-m2)\nUUID=8edfc3ba-4981-4423-8730-7e229bfa63f3 /home      btrfs   defaults        0       0\n</code></pre> <pre><code># umount /media/cdrom/\n</code></pre> <p>Reboot into the newly cloned Ubuntu Studio 22.04 and check that everything works as usual. This would be the first green light to removing the old 2TB NVME disk from the system.</p> <p>Despite booting from the new bootloader in the 4TB NVME the Ubuntu Studio 22.04.5 on /dev/nvme1n1p2 option,  somehow the system boots the old old root:</p> <pre><code>$ df-h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme0n1p6   74G   49G   22G  70% /\n/dev/nvme0n1p2   50G   27G   21G  57% /jellyfish\n/dev/nvme0n1p4  300M  6.1M  294M   3% /boot/efi\n/dev/nvme0n1p5  1.7T  1.1T  655G  62% /home\n/dev/sdc        3.7T  2.9T  833G  78% /home/ssd\n/dev/sdb        3.7T  2.1T  1.6T  57% /home/new-ssd\n/dev/nvme1n1p4  3.5T  2.7T  887G  76% /home/new-m2\n/dev/sda        5.5T  5.1T  370G  94% /home/raid\n</code></pre> <p>The bootloader entry specifies the root file system as <code>409501ea-d63d-49b2-bd45-3b876404dc53</code> but it boots on <code>de317ca5-96dd-49a7-b72b-4bd050a8d15c</code>; despite the correct UUID in the new <code>/etc/fstab</code>.</p> <p>The problem is in precisely this entry in the boot loader, as is the only one that boots the kernel with the wrong <code>root=</code> parameter. This can be confirmed in the bootloader by pressing <code>e</code> after selecting the entry to boot the newly cloned Ubuntu 22.04:</p> <pre><code>setparams 'Ubuntu 22.04.5 LTS (22.04) (on /dev/nvme1n1p2)'\n\n        insmod part_gpt\n        insmod ext2\n        search --no-floppy --fs-uuid --set=root 409501ea-d63d-49b2-bd45-3b876404dc53\n        linux /boot/vmlinuz-5.15.0-124-lowlatency root=UUID=de317ca5-96dd-49a7-b72b-4bd050a8d15c ro threadirqs noquiet nosplash\n        initrd /boot/initrd.img-5.15.0-124-lowlatency\n</code></pre> <p>There is why the kernel is mounting the root from the old NMVE: the UUID in the <code>search</code> line is the desired root, the newly cloned 22.04 in the 4TB NVME, but the <code>linux</code> line is pointing to the old root in the 2TB NVME. Editing this value and then pressing <code>Ctrl+x</code> finally boots into the newly cloned 22.04 and the old 2TB NVMe is not mounted or used at all:</p> <pre><code>$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme1n1p2   74G   51G   19G  74% /\n/dev/nvme1n1p1  259M  6.2M  253M   3% /boot/efi\n/dev/nvme1n1p4  3.5T  2.7T  887G  76% /home\n/dev/sdc        3.7T  2.9T  833G  78% /home/ssd\n/dev/sdb        3.7T  2.1T  1.6T  57% /home/new-ssd\n/dev/sda        5.5T  5.1T  370G  94% /home/raid\n</code></pre> <p>To make this change permanent, the answer (from 2019) in askubuntu.com/a/1140397 seems to suggest that simply running <code>sudo update-grub</code> would pick up the correct root. This would make sense, but perhaps would be better to do it from the new 24.04 system. Perhaps the old root UUID has been left somewhere in the new root UUID, which would be updated by now.</p> <p>So the next move is to boot the new 24.04 system, but before doing that its <code>/etc/fstab</code> needs to be updated to add the additional partitions in the old SATA disks.</p> <p>First, lets add the 24.04 root as a read-only mount:</p> <pre><code># vi /etc/fstab\n...\n# NEW Ubuntu Studio 24.04 root (nvme1n1p3)\nUUID=1d30a16e-b4f6-4459-9b19-8c9093b0d047 /noble      ext4   defaults,ro        0       0\n</code></pre> <p>Then update its <code>/etc/fstab</code> after remounting as read-write:</p> <pre><code># mount /noble/ -o remount,rw\n\n# cat /etc/fstab \\\n  | grep --color=no -C2 sd. \\\n  &gt;&gt; /noble/etc/fstab \n</code></pre> /noble/etc/fstab<pre><code># /etc/fstab: static file system information.\n#\n# Use 'blkid' to print the universally unique identifier for a\n# device; this may be used with UUID= as a more robust way to name devices\n# that works even if disks are added and removed. See fstab(5).\n#\n# &lt;file system&gt; &lt;mount point&gt;   &lt;type&gt;  &lt;options&gt;       &lt;dump&gt;  &lt;pass&gt;\n# / was on /dev/nvme1n1p3 during curtin installation\n/dev/disk/by-uuid/1d30a16e-b4f6-4459-9b19-8c9093b0d047 / ext4 defaults 0 1\n# /boot/efi was on /dev/nvme1n1p1 during curtin installation\n/dev/disk/by-uuid/4485-0F5E /boot/efi vfat defaults 0 1\n# /home was on /dev/nvme1n1p4 during curtin installation\n/dev/disk/by-uuid/8edfc3ba-4981-4423-8730-7e229bfa63f3 /home btrfs defaults 0 1\n/swap.img       none    swap    sw      0       0\n\n# Previous (June 2021) 4TB SSD (previous-previous /home)\n# /home/ssd is now on /dev/sde with a new UUID\nUUID=5cf65a95-4ae5-41ed-9a14-7d7fbeee1951 /home/ssd       btrfs   defaults        0       2\n\n# /home/raid was on /dev/sdb during installation\nUUID=a4ee872d-b985-445f-94a2-15232e93dcd5 /home/raid      btrfs   defaults        0       0\n\n# New (Jan 2023) 4TB SSD (Crucial MX)\n# Always write slowly! e.g.\n# rsync -turva --bwlimit=500000 /home/depot/audio/* /home/ssd/audio/\nUUID=6b809fc0-0b85-4041-ac25-47ec4682f5f5 /home/new-ssd      btrfs   defaults        0       0\n</code></pre> <p>Finally, comment out the line for the swap file (<code>/swap.img</code>), because that won't be necessary in a system with 32GB of RAM, and add a line to mount the newly clone 22.04 as read-only:</p> <pre><code># mkdir /noble/jammy\n# vi /noble/etc/fstab\n...\n# Ubuntu Studio 22.04 root (ro)\nUUID=409501ea-d63d-49b2-bd45-3b876404dc53 /jammy      ext4   defaults,ro        0       0\n</code></pre> <p>With all these adjustments, after booting into the new Ubuntu Studio 24.04 this should be what is mounted:</p> <pre><code># df -h | head -1; df -h | grep -E 'nvme|sd.'\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme1n1p3   74G   22G   49G  31% /\n/dev/nvme1n1p1  300M  6.1M  294M   3% /boot/efi\n/dev/nvme1n1p4  3.5T  2.5T  1.1T  71% /home/\n/dev/sdb        3.7T  2.1T  1.6T  57% /home/new-ssd\n/dev/sdc        3.7T  2.9T  833G  78% /home/ssd\n/dev/sda        5.5T  5.1T  370G  94% /home/raid\n/dev/nvme1n1p2   74G   51G   19G  74% /jammy\n</code></pre> <p>And then it will be a better time to run <code>sudo update-grub</code> to hopefully pick up the correct root for the new 22.04.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#first-boot-into-ubuntu-studio-2404","title":"First boot into Ubuntu Studio 24.04","text":"<p>The first time booting into the new system, right after login for the first time an additional reboot is required for the Ubuntu Studio Audio Configuration.</p> <p>After rebooting again, <code>df</code> shows partitions are mounted like this:</p> <pre><code># df -h\nFilesystem      Size  Used Avail Use% Mounted on\ntmpfs           3.2G  2.6M  3.2G   1% /run\n/dev/nvme1n1p3   74G   22G   48G  32% /\ntmpfs            16G     0   16G   0% /dev/shm\ntmpfs           5.0M   24K  5.0M   1% /run/lock\nefivarfs        128K   51K   73K  42% /sys/firmware/efi/efivars\n/dev/nvme1n1p2   74G   51G   19G  74% /jammy\n/dev/nvme1n1p1  259M  6.2M  253M   3% /boot/efi\n/dev/nvme1n1p4  3.5T  2.7T  887G  76% /home\n/dev/sdb        3.7T  2.1T  1.6T  57% /home/new-ssd\n/dev/sdc        3.7T  2.9T  833G  78% /home/ssd\n/dev/sda        5.5T  5.1T  370G  94% /home/raid\ntmpfs           3.2G  136K  3.2G   1% /run/user/1000\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#fix-boot-for-cloned-ubuntu-studio-2204","title":"Fix boot for cloned Ubuntu Studio 22.04","text":"<p>Contrary to initial expectaions, running <code>sudo update-grub</code> on the new 24.04 system did not help fix the root UUID for <code>nvme1n1p2</code>; it actually unfixed the one that was correct!</p> <p>Instead, it is necessary to edit <code>/boot/grub/grub.cfg</code> as suggested in https://superuser.com/a/485763 and replace <code>nvme0n1p6</code> UUID with that of <code>nvme1n1p2</code> for all entries under the name <code>Ubuntu 22.04.5 LTS (22.04) (on /dev/nvme1n1p2)</code> and then simply reboot. This does lead to the new old system (22.04 in the new NVME) to boot correctly and <code>df -h</code> shows the desired partitions mounted:</p> <pre><code>$ df-h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme1n1p2   74G   51G   19G  73% /\n/dev/nvme1n1p1  259M  6.2M  253M   3% /boot/efi\n/dev/nvme1n1p3   74G   24G   47G  34% /noble\n/dev/nvme1n1p4  3.5T  2.7T  886G  76% /home\n/dev/sdc        3.7T  2.9T  833G  78% /home/ssd\n/dev/sdb        3.7T  2.1T  1.6T  57% /home/new-ssd\n/dev/sda        5.5T  5.1T  370G  94% /home/raid\n</code></pre> <p>Warning</p> <p>Running <code>sudo update-grub</code> will unfix the root UUID for <code>nvme1n1p2</code> again; and this will happen each time a new kernel is installed.</p> <p>Once the new old 22.04 system is reliably bootable, it can be left alone as a fallback system, and continue setting up the new one.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#multiple-ips-on-lan","title":"Multiple IPs on LAN","text":"<p>Connecting to the local wired network provides a dynamic IP address that may change over time, but it is more convenient to have fixed IP addresses. Moreover, the DHCP range is shared with the wireless network, we want to have an additional wired-only LAN and set both IP addresses on the same NIC.</p> <p>To this effect, copy <code>/etc/netplan/01-network-manager-all.yaml</code> from the old system, change the network interface name if different (run <code>ip a</code> to check) and apply the changes with <code>netplan apply</code>:</p> <pre><code># ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n       valid_lft forever preferred_lft forever\n2: enp5s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 04:42:1a:97:4e:47 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.0.2/24 brd 192.168.0.255 scope global dynamic noprefixroute enp5s0\n       valid_lft 78508sec preferred_lft 78508sec\n    inet6 fe80::642:1aff:fe97:4e47/64 scope link \n       valid_lft forever preferred_lft forever\n</code></pre> <p>Network interface is <code>enp5s0</code>; </p> /etc/netplan/01-network-manager-all.yaml<pre><code># Dual static IP on LAN, nothing else.\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp4s0:\n      dhcp4: no\n      dhcp6: no\n      # Ser IP address &amp; subnet mask\n      addresses: [ 10.0.0.2/24, 192.168.0.2/24 ]\n      nameservers:\n      # Set DNS name servers\n        search: [v.cablecom.net]\n        addresses: [62.2.24.158, 62.2.17.61]\n    enp5s0:\n      dhcp4: no\n      dhcp6: no\n      # Ser IP address &amp; subnet mask\n      addresses: [ 10.0.0.2/24, 192.168.0.2/24 ]\n      # Set default gateway\n      routes:\n        - to: default\n          via: 192.168.0.1\n      nameservers:\n      # Set DNS name servers\n        search: [v.cablecom.net]\n        addresses: [62.2.24.158, 62.2.17.61]\n</code></pre> <pre><code># chmod 400 /etc/netplan/01-network-manager-all.yaml\n# netplan apply\n\n# ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n       valid_lft forever preferred_lft forever\n2: enp5s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 04:42:1a:97:4e:47 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.2/24 brd 10.0.0.255 scope global enp5s0\n       valid_lft forever preferred_lft forever\n    inet 192.168.0.2/24 metric 100 brd 192.168.0.255 scope global dynamic enp5s0\n       valid_lft 86382sec preferred_lft 86382sec\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#ssh-server","title":"SSH Server","text":"<p>Ubuntu Studio doesn't enable the SSH server by default, but we want this to adjust the system remotely:</p> <pre><code># apt install ssh -y\n# sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config\n# systemctl enable --now ssh\n</code></pre> <p>Note</p> <p>Remember to copy over files under <code>/root</code> from previous system/s, in case it contains useful scripts (and/or SSH keys worth keeping under <code>.ssh</code>).</p> <pre><code># rm /root/.ssh/authorized_keys \n# rmdir /root/.ssh/ \n# cp -a /mnt/root/.ssh/ /root/\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#etchosts","title":"<code>/etc/hosts</code>","text":"<p>Having the old system's root partition mounted (see above), copy over <code>/etc/hosts</code> so that connections to local hosts work as smoothly as in the old system (e.g. for Continuous Monitoring).</p> <p>Better yet, append the old <code>/etc/hosts</code> to the new one, then edit the new one to remove redundant lines. There are a few interesting lines in the new one:</p> /etc/hosts<pre><code># Standard host addresses\n127.0.0.1 localhost\n127.0.1.1 rapture\n\n# The following lines are desirable for IPv6 capable hosts\n::1     ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#install-essential-packages","title":"Install Essential Packages","text":"<p>Start by installing a few essential packages:</p> <pre><code># apt install gdebi-core wget gkrellm vim curl gkrellm-leds \\\n  gkrellm-xkb gkrellm-cpufreq geeqie playonlinux exfat-fuse \\\n  clementine id3v2 htop vnstat neofetch tigervnc-viewer sox \\\n  scummvm wine gamemode python-is-python3 exiv2 rename scrot \\\n  speedtest-cli xcalib python3-pip netcat-openbsd jstest-gtk \\\n  etherwake python3-selenium lm-sensors sysstat tor unrar \\\n  ttf-mscorefonts-installer winetricks icc-profiles ffmpeg \\\n  iotop-c xdotool redshift-qt inxi vainfo vdpauinfo mpv xsane \\\n  tigervnc-tools screen lutris libxxf86vm-dev displaycal \\\n  python3-absl python3-unidecode -y\n</code></pre> <p>After installing these, Redshift is immediately available.</p> <p>Even before installing any packages KDE Connect is already running, which comes in very handy if it was already setup.</p> <p>At this point a reboot should not be necessary.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#install-additional-software","title":"Install Additional Software","text":""},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#brave-browser","title":"Brave browser","text":"<p>Install from the Release Channel:</p> <pre><code># curl -fsSLo \\\n  /usr/share/keyrings/brave-browser-archive-keyring.gpg \\\n  https://brave-browser-apt-release.s3.brave.com/brave-browser-archive-keyring.gpg\n\n# echo \"deb [signed-by=/usr/share/keyrings/brave-browser-archive-keyring.gpg] https://brave-browser-apt-release.s3.brave.com/ stable main\" \\\n| tee /etc/apt/sources.list.d/brave-browser-release.list\n\n# apt update &amp;&amp; apt install brave-browser -y\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#google-chrome","title":"Google Chrome","text":"<p>Installing Google Chrome is as simple as downloading the Debian package and installing it:</p> <pre><code># dpkg -i google-chrome-stable_current_amd64.deb\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#activitywatch","title":"ActivityWatch","text":"<p>As soon as Chrome is started, the ActivityWatch extension complains that it can't connect to the server, because it only runs locally.</p> <p>Install the latest version of ActivityWatch and then run <code>/opt/activitywatch/aw-qt</code> manually once. This should already be in the Autostart settings, if it was setup previously, so it only needs to be run manually this once. </p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#steam","title":"Steam","text":""},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#snap-package-removed","title":"Snap package (removed)","text":"<p>Installing Steam from Snap  couldn't be simpler:</p> <pre><code># snap install steam\nsteam 1.0.0.81 from Canonical\u2713 installed\n</code></pre> <p>This snapcraft.io/steam package is provided by Canonical.</p> <p>When runing the Steam client for the first time, a pop-up shows up advising to install additional 32-bit drivers for best experience</p> <code># apt install libnvidia-gl-550:i386 -y</code> <pre><code># dpkg --add-architecture i386\n# apt update\n# apt install libnvidia-gl-550:i386 -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  gcc-14-base:i386 libbsd0:i386 libc6:i386 libdrm2:i386 libffi8:i386 libgcc-s1:i386 libidn2-0:i386 libmd0:i386\n  libnvidia-egl-wayland1:i386 libunistring5:i386 libwayland-client0:i386 libwayland-server0:i386 libx11-6:i386\n  libxau6:i386 libxcb1:i386 libxdmcp6:i386 libxext6:i386\nSuggested packages:\n  glibc-doc:i386 locales:i386 libnss-nis:i386 libnss-nisplus:i386\nThe following NEW packages will be installed:\n  gcc-14-base:i386 libbsd0:i386 libc6:i386 libdrm2:i386 libffi8:i386 libgcc-s1:i386 libidn2-0:i386 libmd0:i386\n  libnvidia-egl-wayland1:i386 libnvidia-gl-550:i386 libunistring5:i386 libwayland-client0:i386 libwayland-server0:i386\n  libx11-6:i386 libxau6:i386 libxcb1:i386 libxdmcp6:i386 libxext6:i386\n0 upgraded, 18 newly installed, 0 to remove and 6 not upgraded.\n</code></pre> <p>It should also be possible to install the official Steam client, with the non-snap alternative. This doesn't seems necessary anymore.</p> <p>News</p> <p>After repeating this process in Raven and finding out how badly it messes with installed games, I came back to Rapture to Remove Steam Snap, Install Steam Again and also install the latest Glorious Eggroll, if only because it was so easy in Raven.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#remove-steam-snap","title":"Remove Steam Snap","text":"<p>Removing steam snap can take a long time because, as explained here, by default on <code>snap remove</code>, <code>snapd</code> creates a snapshot of the data and keeps it around for 31 days. This can take a long time for Steam. To avoid this, follow these steps to disable the snapshot before removing Steam:</p> <pre><code># snap changes\nID   Status  Spawn               Ready               Summary\n25   Done    today at 13:42 CET  today at 13:42 CET  Auto-refresh snap \"chromium\"\n\n# snap set core snapshots.automatic.retention=no\n\n# snap remove steam\nsteam removed\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#install-steam-again","title":"Install Steam Again","text":"<p>Now, instead of repeating the steps used to install Steam in Ubuntu Studio 22.04, this time I installed the longer list of packages found to be required by Steam in Ubuntu Studio 22.04:</p> <pre><code># apt install libglx-mesa0:i386 libc6:amd64 libc6:i386 \\\n  libegl1:amd64 libegl1:i386 libgbm1:amd64 libgbm1:i386 \\\n  libgl1-mesa-dri:amd64 libgl1-mesa-dri:i386 libgl1:amd64 \\\n  libgl1:i386 steam-libs-amd64:amd64 steam-libs-i386:i386 -y\n</code></pre> <code># apt install libnvidia-gl-550:i386 -y</code> <pre><code># apt install libglx-mesa0:i386 libc6:amd64 libc6:i386 \\\n  libegl1:amd64 libegl1:i386 libgbm1:amd64 libgbm1:i386 \\\n  libgl1-mesa-dri:amd64 libgl1-mesa-dri:i386 libgl1:amd64 \\\n  libgl1:i386 steam-libs-amd64:amd64 steam-libs-i386:i386 -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nNote, selecting 'steam-libs' instead of 'steam-libs-amd64'\nlibc6 is already the newest version (2.39-0ubuntu8.3).\nlibc6 set to manually installed.\nlibc6:i386 is already the newest version (2.39-0ubuntu8.3).\nlibc6:i386 set to manually installed.\nlibegl1 is already the newest version (1.7.0-1build1).\nlibegl1 set to manually installed.\nlibgbm1 is already the newest version (24.0.9-0ubuntu0.3).\nlibgbm1 set to manually installed.\nlibgl1-mesa-dri is already the newest version (24.0.9-0ubuntu0.3).\nlibgl1-mesa-dri set to manually installed.\nlibgl1 is already the newest version (1.7.0-1build1).\nlibgl1 set to manually installed.\nThe following additional packages will be installed:\n  i965-va-driver:i386 intel-media-va-driver:i386 libaom3:i386\n  libapparmor1:i386 libasound2-plugins:i386 libasound2t64:i386\n  libasyncns0:i386 libatk-bridge2.0-0t64:i386 libatk1.0-0t64:i386\n  libatspi2.0-0t64:i386 libavahi-client3:i386\n  libavahi-common-data:i386 libavahi-common3:i386 libavcodec60:i386\n  libavutil58:i386 libblkid1:i386 libbrotli1:i386 libbz2-1.0:i386\n  libcairo-gobject2:i386 libcairo2:i386 libcap2:i386\n  libcodec2-1.2:i386 libcolord2:i386 libcom-err2:i386 libcrypt1:i386\n  libcups2t64:i386 libdatrie1:i386 libdav1d7:i386 libdb5.3t64:i386\n  libdbus-1-3:i386 libdecor-0-0:i386 libdecor-0-plugin-1-gtk:i386\n  libdeflate0:i386 libdrm-intel1:i386 libdrm-nouveau2:i386\n  libdrm-radeon1:i386 libegl-mesa0:i386 libepoxy0:i386\n  libflac12t64:i386 libfontconfig1:i386 libfreetype6:i386\n  libfribidi0:i386 libgcrypt20:i386 libgdk-pixbuf-2.0-0:i386\n  libglapi-mesa:i386 libglib2.0-0t64:i386 libglvnd0:i386 libglx0:i386\n  libgmp10:i386 libgnutls30t64:i386 libgomp1:i386 libgpg-error0:i386\n  libgraphite2-3:i386 libgsm1:i386 libgssapi-krb5-2:i386\n  libgtk-3-0t64:i386 libharfbuzz0b:i386 libhogweed6t64:i386\n  libigdgmm12:i386 libjack-jackd2-0:i386 libjbig0:i386\n  libjpeg-turbo8:i386 libjpeg8:i386 libk5crypto3:i386\n  libkeyutils1:i386 libkrb5-3:i386 libkrb5support0:i386\n  liblcms2-2:i386 liblz4-1:i386 libmount1:i386 libmp3lame0:i386\n  libmpg123-0t64:i386 libnettle8t64:i386 libnm0:i386 libnuma1:i386\n  libogg0:i386 libopenjp2-7:i386 libopus0:i386 libp11-kit0:i386\n  libpango-1.0-0:i386 libpangocairo-1.0-0:i386 libpangoft2-1.0-0:i386\n  libpciaccess0:i386 libpcre2-8-0:i386 libpixman-1-0:i386\n  libpng16-16t64:i386 libpulse0:i386 librsvg2-2:i386\n  librsvg2-common:i386 libsamplerate0:i386 libsdl2-2.0-0:i386\n  libselinux1:i386 libsensors5:i386 libsharpyuv0:i386 libshine3:i386\n  libsnappy1v5:i386 libsndfile1:i386 libsoxr0:i386 libspeex1:i386\n  libspeexdsp1:i386 libssl3t64:i386 libsvtav1enc1d1:i386\n  libswresample4:i386 libsystemd0:i386 libtasn1-6:i386 libthai0:i386\n  libtheora0:i386 libtiff6:i386 libtwolame0:i386 libudev1:i386\n  libusb-1.0-0:i386 libva-drm2:i386 libva-glx2 libva-glx2:i386\n  libva-x11-2:i386 libva2:i386 libvdpau1:i386 libvorbis0a:i386\n  libvorbisenc2:i386 libvpx9:i386 libwayland-cursor0:i386\n  libwayland-egl1:i386 libwebp7:i386 libwebpmux3:i386 libx264-164:i386\n  libx265-199:i386 libxcb-dri2-0:i386 libxcb-glx0:i386\n  libxcb-render0:i386 libxcomposite1:i386 libxcursor1:i386\n  libxdamage1:i386 libxfixes3:i386 libxi6:i386 libxinerama1:i386\n  libxkbcommon0:i386 libxrandr2:i386 libxrender1:i386 libxss1:i386\n  libxvidcore4:i386 libxxf86vm1:i386 libzvbi0t64:i386\n  mesa-va-drivers:i386 mesa-vdpau-drivers:i386 ocl-icd-libopencl1:i386\n  steam-devices steam-libs:i386 va-driver-all:i386\n  vdpau-driver-all:i386 xterm\nSuggested packages:\n  i965-va-driver-shaders:i386 libcuda1:i386 libnvcuvid1:i386\n  libnvidia-encode1:i386 rng-tools:i386 low-memory-monitor:i386\n  krb5-doc:i386 krb5-user:i386 gvfs:i386 jackd2:i386 opus-tools:i386\n  pulseaudio:i386 lm-sensors:i386 speex:i386 opencl-icd:i386\n  steam-installer libudev0 nvidia-driver-libs nvidia-vulkan-icd\n  gtk2-engines-pixbuf:i386 libgtk2.0-0:i386 libudev0:i386\n  nvidia-driver-libs:i386 nvidia-vulkan-icd:i386 pipewire:i386\n  libvdpau-va-gl1:i386 xfonts-cyrillic\nRecommended packages:\n  libgl1-amber-dri:i386 luit | x11-utils\nThe following NEW packages will be installed:\n  i965-va-driver:i386 intel-media-va-driver:i386 libaom3:i386\n  libapparmor1:i386 libasound2-plugins:i386 libasound2t64:i386\n  libasyncns0:i386 libatk-bridge2.0-0t64:i386 libatk1.0-0t64:i386\n  libatspi2.0-0t64:i386 libavahi-client3:i386\n  libavahi-common-data:i386 libavahi-common3:i386 libavcodec60:i386\n  libavutil58:i386 libblkid1:i386 libbrotli1:i386 libbz2-1.0:i386\n  libcairo-gobject2:i386 libcairo2:i386 libcap2:i386\n  libcodec2-1.2:i386 libcolord2:i386 libcom-err2:i386 libcrypt1:i386\n  libcups2t64:i386 libdatrie1:i386 libdav1d7:i386 libdb5.3t64:i386\n  libdbus-1-3:i386 libdecor-0-0:i386 libdecor-0-plugin-1-gtk:i386\n  libdeflate0:i386 libdrm-intel1:i386 libdrm-nouveau2:i386\n  libdrm-radeon1:i386 libegl-mesa0:i386 libegl1:i386 libepoxy0:i386\n  libflac12t64:i386 libfontconfig1:i386 libfreetype6:i386\n  libfribidi0:i386 libgbm1:i386 libgcrypt20:i386\n  libgdk-pixbuf-2.0-0:i386 libgl1:i386 libgl1-mesa-dri:i386\n  libglapi-mesa:i386 libglib2.0-0t64:i386 libglvnd0:i386\n  libglx-mesa0:i386 libglx0:i386 libgmp10:i386 libgnutls30t64:i386\n  libgomp1:i386 libgpg-error0:i386 libgraphite2-3:i386 libgsm1:i386\n  libgssapi-krb5-2:i386 libgtk-3-0t64:i386 libharfbuzz0b:i386\n  libhogweed6t64:i386 libigdgmm12:i386 libjack-jackd2-0:i386\n  libjbig0:i386 libjpeg-turbo8:i386 libjpeg8:i386 libk5crypto3:i386\n  libkeyutils1:i386 libkrb5-3:i386 libkrb5support0:i386\n  liblcms2-2:i386 liblz4-1:i386 libmount1:i386 libmp3lame0:i386\n  libmpg123-0t64:i386 libnettle8t64:i386 libnm0:i386 libnuma1:i386\n  libogg0:i386 libopenjp2-7:i386 libopus0:i386 libp11-kit0:i386\n  libpango-1.0-0:i386 libpangocairo-1.0-0:i386 libpangoft2-1.0-0:i386\n  libpciaccess0:i386 libpcre2-8-0:i386 libpixman-1-0:i386\n  libpng16-16t64:i386 libpulse0:i386 librsvg2-2:i386\n  librsvg2-common:i386 libsamplerate0:i386 libsdl2-2.0-0:i386\n  libselinux1:i386 libsensors5:i386 libsharpyuv0:i386 libshine3:i386\n  libsnappy1v5:i386 libsndfile1:i386 libsoxr0:i386 libspeex1:i386\n  libspeexdsp1:i386 libssl3t64:i386 libsvtav1enc1d1:i386\n  libswresample4:i386 libsystemd0:i386 libtasn1-6:i386 libthai0:i386\n  libtheora0:i386 libtiff6:i386 libtwolame0:i386 libudev1:i386\n  libusb-1.0-0:i386 libva-drm2:i386 libva-glx2 libva-glx2:i386\n  libva-x11-2:i386 libva2:i386 libvdpau1:i386 libvorbis0a:i386\n  libvorbisenc2:i386 libvpx9:i386 libwayland-cursor0:i386\n  libwayland-egl1:i386 libwebp7:i386 libwebpmux3:i386 libx264-164:i386\n  libx265-199:i386 libxcb-dri2-0:i386 libxcb-glx0:i386\n  libxcb-render0:i386 libxcomposite1:i386 libxcursor1:i386\n  libxdamage1:i386 libxfixes3:i386 libxi6:i386 libxinerama1:i386\n  libxkbcommon0:i386 libxrandr2:i386 libxrender1:i386 libxss1:i386\n  libxvidcore4:i386 libxxf86vm1:i386 libzvbi0t64:i386\n  mesa-va-drivers:i386 mesa-vdpau-drivers:i386 ocl-icd-libopencl1:i386\n  steam-devices steam-libs steam-libs:i386 steam-libs-i386:i386\n  va-driver-all:i386 vdpau-driver-all:i386 xterm\n0 upgraded, 157 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 72.1 MB of archives.\nAfter this operation, 228 MB of additional disk space will be used.\nFetched 72.1 MB in 4s (19.5 MB/s)          \nExtracting templates from packages: 100%\nSelecting previously unselected package libblkid1:i386.\n(Reading database ... 487873 files and directories currently installed.)\nPreparing to unpack .../000-libblkid1_2.39.3-9ubuntu6.1_i386.deb ...\nUnpacking libblkid1:i386 (2.39.3-9ubuntu6.1) ...\nSelecting previously unselected package libbz2-1.0:i386.\nPreparing to unpack .../001-libbz2-1.0_1.0.8-5.1build0.1_i386.deb ...\nUnpacking libbz2-1.0:i386 (1.0.8-5.1build0.1) ...\nSelecting previously unselected package libcap2:i386.\nPreparing to unpack .../002-libcap2_1%3a2.66-5ubuntu2_i386.deb ...\nUnpacking libcap2:i386 (1:2.66-5ubuntu2) ...\nSelecting previously unselected package libcom-err2:i386.\nPreparing to unpack .../003-libcom-err2_1.47.0-2.4~exp1ubuntu4.1_i386.deb ...\nUnpacking libcom-err2:i386 (1.47.0-2.4~exp1ubuntu4.1) ...\nSelecting previously unselected package libcrypt1:i386.\nPreparing to unpack .../004-libcrypt1_1%3a4.4.36-4build1_i386.deb ...\nUnpacking libcrypt1:i386 (1:4.4.36-4build1) ...\nSelecting previously unselected package libgpg-error0:i386.\nPreparing to unpack .../005-libgpg-error0_1.47-3build2_i386.deb ...\nUnpacking libgpg-error0:i386 (1.47-3build2) ...\nSelecting previously unselected package libgcrypt20:i386.\nPreparing to unpack .../006-libgcrypt20_1.10.3-2build1_i386.deb ...\nUnpacking libgcrypt20:i386 (1.10.3-2build1) ...\nSelecting previously unselected package libgmp10:i386.\nPreparing to unpack .../007-libgmp10_2%3a6.3.0+dfsg-2ubuntu6_i386.deb ...\nUnpacking libgmp10:i386 (2:6.3.0+dfsg-2ubuntu6) ...\nSelecting previously unselected package liblz4-1:i386.\nPreparing to unpack .../008-liblz4-1_1.9.4-1build1.1_i386.deb ...\nUnpacking liblz4-1:i386 (1.9.4-1build1.1) ...\nSelecting previously unselected package libpcre2-8-0:i386.\nPreparing to unpack .../009-libpcre2-8-0_10.42-4ubuntu2_i386.deb ...\nUnpacking libpcre2-8-0:i386 (10.42-4ubuntu2) ...\nSelecting previously unselected package libselinux1:i386.\nPreparing to unpack .../010-libselinux1_3.5-2ubuntu2_i386.deb ...\nUnpacking libselinux1:i386 (3.5-2ubuntu2) ...\nSelecting previously unselected package libmount1:i386.\nPreparing to unpack .../011-libmount1_2.39.3-9ubuntu6.1_i386.deb ...\nUnpacking libmount1:i386 (2.39.3-9ubuntu6.1) ...\nSelecting previously unselected package libssl3t64:i386.\nPreparing to unpack .../012-libssl3t64_3.0.13-0ubuntu3.4_i386.deb ...\nUnpacking libssl3t64:i386 (3.0.13-0ubuntu3.4) ...\nSelecting previously unselected package libsystemd0:i386.\nPreparing to unpack .../013-libsystemd0_255.4-1ubuntu8.4_i386.deb ...\nUnpacking libsystemd0:i386 (255.4-1ubuntu8.4) ...\nSelecting previously unselected package libudev1:i386.\nPreparing to unpack .../014-libudev1_255.4-1ubuntu8.4_i386.deb ...\nUnpacking libudev1:i386 (255.4-1ubuntu8.4) ...\nSelecting previously unselected package libapparmor1:i386.\nPreparing to unpack .../015-libapparmor1_4.0.1really4.0.1-0ubuntu0.24.04.3_i386.deb ...\nUnpacking libapparmor1:i386 (4.0.1really4.0.1-0ubuntu0.24.04.3) ...\nSelecting previously unselected package libdb5.3t64:i386.\nPreparing to unpack .../016-libdb5.3t64_5.3.28+dfsg2-7_i386.deb ...\nUnpacking libdb5.3t64:i386 (5.3.28+dfsg2-7) ...\nSelecting previously unselected package libdbus-1-3:i386.\nPreparing to unpack .../017-libdbus-1-3_1.14.10-4ubuntu4.1_i386.deb ...\nUnpacking libdbus-1-3:i386 (1.14.10-4ubuntu4.1) ...\nSelecting previously unselected package libfribidi0:i386.\nPreparing to unpack .../018-libfribidi0_1.0.13-3build1_i386.deb ...\nUnpacking libfribidi0:i386 (1.0.13-3build1) ...\nSelecting previously unselected package libglib2.0-0t64:i386.\nPreparing to unpack .../019-libglib2.0-0t64_2.80.0-6ubuntu3.2_i386.deb ...\nUnpacking libglib2.0-0t64:i386 (2.80.0-6ubuntu3.2) ...\nSelecting previously unselected package libnettle8t64:i386.\nPreparing to unpack .../020-libnettle8t64_3.9.1-2.2build1.1_i386.deb ...\nUnpacking libnettle8t64:i386 (3.9.1-2.2build1.1) ...\nSelecting previously unselected package libhogweed6t64:i386.\nPreparing to unpack .../021-libhogweed6t64_3.9.1-2.2build1.1_i386.deb ...\nUnpacking libhogweed6t64:i386 (3.9.1-2.2build1.1) ...\nSelecting previously unselected package libp11-kit0:i386.\nPreparing to unpack .../022-libp11-kit0_0.25.3-4ubuntu2.1_i386.deb ...\nUnpacking libp11-kit0:i386 (0.25.3-4ubuntu2.1) ...\nSelecting previously unselected package libtasn1-6:i386.\nPreparing to unpack .../023-libtasn1-6_4.19.0-3build1_i386.deb ...\nUnpacking libtasn1-6:i386 (4.19.0-3build1) ...\nSelecting previously unselected package libgnutls30t64:i386.\nPreparing to unpack .../024-libgnutls30t64_3.8.3-1.1ubuntu3.2_i386.deb ...\nUnpacking libgnutls30t64:i386 (3.8.3-1.1ubuntu3.2) ...\nSelecting previously unselected package libkrb5support0:i386.\nPreparing to unpack .../025-libkrb5support0_1.20.1-6ubuntu2.2_i386.deb ...\nUnpacking libkrb5support0:i386 (1.20.1-6ubuntu2.2) ...\nSelecting previously unselected package libk5crypto3:i386.\nPreparing to unpack .../026-libk5crypto3_1.20.1-6ubuntu2.2_i386.deb ...\nUnpacking libk5crypto3:i386 (1.20.1-6ubuntu2.2) ...\nSelecting previously unselected package libkeyutils1:i386.\nPreparing to unpack .../027-libkeyutils1_1.6.3-3build1_i386.deb ...\nUnpacking libkeyutils1:i386 (1.6.3-3build1) ...\nSelecting previously unselected package libkrb5-3:i386.\nPreparing to unpack .../028-libkrb5-3_1.20.1-6ubuntu2.2_i386.deb ...\nUnpacking libkrb5-3:i386 (1.20.1-6ubuntu2.2) ...\nSelecting previously unselected package libgssapi-krb5-2:i386.\nPreparing to unpack .../029-libgssapi-krb5-2_1.20.1-6ubuntu2.2_i386.deb ...\nUnpacking libgssapi-krb5-2:i386 (1.20.1-6ubuntu2.2) ...\nSelecting previously unselected package libnuma1:i386.\nPreparing to unpack .../030-libnuma1_2.0.18-1build1_i386.deb ...\nUnpacking libnuma1:i386 (2.0.18-1build1) ...\nSelecting previously unselected package libpng16-16t64:i386.\nPreparing to unpack .../031-libpng16-16t64_1.6.43-5build1_i386.deb ...\nUnpacking libpng16-16t64:i386 (1.6.43-5build1) ...\nSelecting previously unselected package libsensors5:i386.\nPreparing to unpack .../032-libsensors5_1%3a3.6.0-9build1_i386.deb ...\nUnpacking libsensors5:i386 (1:3.6.0-9build1) ...\nSelecting previously unselected package libusb-1.0-0:i386.\nPreparing to unpack .../033-libusb-1.0-0_2%3a1.0.27-1_i386.deb ...\nUnpacking libusb-1.0-0:i386 (2:1.0.27-1) ...\nSelecting previously unselected package libxkbcommon0:i386.\nPreparing to unpack .../034-libxkbcommon0_1.6.0-1build1_i386.deb ...\nUnpacking libxkbcommon0:i386 (1.6.0-1build1) ...\nSelecting previously unselected package libva2:i386.\nPreparing to unpack .../035-libva2_2.20.0-2build1_i386.deb ...\nUnpacking libva2:i386 (2.20.0-2build1) ...\nSelecting previously unselected package libigdgmm12:i386.\nPreparing to unpack .../036-libigdgmm12_22.3.17+ds1-1_i386.deb ...\nUnpacking libigdgmm12:i386 (22.3.17+ds1-1) ...\nSelecting previously unselected package intel-media-va-driver:i386.\nPreparing to unpack .../037-intel-media-va-driver_24.1.0+dfsg1-1_i386.deb ...\nUnpacking intel-media-va-driver:i386 (24.1.0+dfsg1-1) ...\nSelecting previously unselected package libaom3:i386.\nPreparing to unpack .../038-libaom3_3.8.2-2ubuntu0.1_i386.deb ...\nUnpacking libaom3:i386 (3.8.2-2ubuntu0.1) ...\nSelecting previously unselected package libasound2t64:i386.\nPreparing to unpack .../039-libasound2t64_1.2.11-1build2_i386.deb ...\nUnpacking libasound2t64:i386 (1.2.11-1build2) ...\nSelecting previously unselected package libva-drm2:i386.\nPreparing to unpack .../040-libva-drm2_2.20.0-2build1_i386.deb ...\nUnpacking libva-drm2:i386 (2.20.0-2build1) ...\nSelecting previously unselected package libxfixes3:i386.\nPreparing to unpack .../041-libxfixes3_1%3a6.0.0-2build1_i386.deb ...\nUnpacking libxfixes3:i386 (1:6.0.0-2build1) ...\nSelecting previously unselected package libva-x11-2:i386.\nPreparing to unpack .../042-libva-x11-2_2.20.0-2build1_i386.deb ...\nUnpacking libva-x11-2:i386 (2.20.0-2build1) ...\nSelecting previously unselected package libvdpau1:i386.\nPreparing to unpack .../043-libvdpau1_1.5-2build1_i386.deb ...\nUnpacking libvdpau1:i386 (1.5-2build1) ...\nSelecting previously unselected package ocl-icd-libopencl1:i386.\nPreparing to unpack .../044-ocl-icd-libopencl1_2.3.2-1build1_i386.deb ...\nUnpacking ocl-icd-libopencl1:i386 (2.3.2-1build1) ...\nSelecting previously unselected package libavutil58:i386.\nPreparing to unpack .../045-libavutil58_7%3a6.1.1-3ubuntu5+esm2_i386.deb ...\nUnpacking libavutil58:i386 (7:6.1.1-3ubuntu5+esm2) ...\nSelecting previously unselected package libbrotli1:i386.\nPreparing to unpack .../046-libbrotli1_1.1.0-2build2_i386.deb ...\nUnpacking libbrotli1:i386 (1.1.0-2build2) ...\nSelecting previously unselected package libfreetype6:i386.\nPreparing to unpack .../047-libfreetype6_2.13.2+dfsg-1build3_i386.deb ...\nUnpacking libfreetype6:i386 (2.13.2+dfsg-1build3) ...\nSelecting previously unselected package libfontconfig1:i386.\nPreparing to unpack .../048-libfontconfig1_2.15.0-1.1ubuntu2_i386.deb ...\nUnpacking libfontconfig1:i386 (2.15.0-1.1ubuntu2) ...\nSelecting previously unselected package libpixman-1-0:i386.\nPreparing to unpack .../049-libpixman-1-0_0.42.2-1build1_i386.deb ...\nUnpacking libpixman-1-0:i386 (0.42.2-1build1) ...\nSelecting previously unselected package libxcb-render0:i386.\nPreparing to unpack .../050-libxcb-render0_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb-render0:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libxrender1:i386.\nPreparing to unpack .../051-libxrender1_1%3a0.9.10-1.1build1_i386.deb ...\nUnpacking libxrender1:i386 (1:0.9.10-1.1build1) ...\nSelecting previously unselected package libcairo2:i386.\nPreparing to unpack .../052-libcairo2_1.18.0-3build1_i386.deb ...\nUnpacking libcairo2:i386 (1.18.0-3build1) ...\nSelecting previously unselected package libcodec2-1.2:i386.\nPreparing to unpack .../053-libcodec2-1.2_1.2.0-2build1_i386.deb ...\nUnpacking libcodec2-1.2:i386 (1.2.0-2build1) ...\nSelecting previously unselected package libdav1d7:i386.\nPreparing to unpack .../054-libdav1d7_1.4.1-1build1_i386.deb ...\nUnpacking libdav1d7:i386 (1.4.1-1build1) ...\nSelecting previously unselected package libgsm1:i386.\nPreparing to unpack .../055-libgsm1_1.0.22-1build1_i386.deb ...\nUnpacking libgsm1:i386 (1.0.22-1build1) ...\nSelecting previously unselected package libmp3lame0:i386.\nPreparing to unpack .../056-libmp3lame0_3.100-6build1_i386.deb ...\nUnpacking libmp3lame0:i386 (3.100-6build1) ...\nSelecting previously unselected package libopenjp2-7:i386.\nPreparing to unpack .../057-libopenjp2-7_2.5.0-2ubuntu0.2_i386.deb ...\nUnpacking libopenjp2-7:i386 (2.5.0-2ubuntu0.2) ...\nSelecting previously unselected package libopus0:i386.\nPreparing to unpack .../058-libopus0_1.4-1build1_i386.deb ...\nUnpacking libopus0:i386 (1.4-1build1) ...\nSelecting previously unselected package libcairo-gobject2:i386.\nPreparing to unpack .../059-libcairo-gobject2_1.18.0-3build1_i386.deb ...\nUnpacking libcairo-gobject2:i386 (1.18.0-3build1) ...\nSelecting previously unselected package libjpeg-turbo8:i386.\nPreparing to unpack .../060-libjpeg-turbo8_2.1.5-2ubuntu2_i386.deb ...\nUnpacking libjpeg-turbo8:i386 (2.1.5-2ubuntu2) ...\nSelecting previously unselected package libjpeg8:i386.\nPreparing to unpack .../061-libjpeg8_8c-2ubuntu11_i386.deb ...\nUnpacking libjpeg8:i386 (8c-2ubuntu11) ...\nSelecting previously unselected package libdeflate0:i386.\nPreparing to unpack .../062-libdeflate0_1.19-1build1.1_i386.deb ...\nUnpacking libdeflate0:i386 (1.19-1build1.1) ...\nSelecting previously unselected package libjbig0:i386.\nPreparing to unpack .../063-libjbig0_2.1-6.1ubuntu2_i386.deb ...\nUnpacking libjbig0:i386 (2.1-6.1ubuntu2) ...\nSelecting previously unselected package libsharpyuv0:i386.\nPreparing to unpack .../064-libsharpyuv0_1.3.2-0.4build3_i386.deb ...\nUnpacking libsharpyuv0:i386 (1.3.2-0.4build3) ...\nSelecting previously unselected package libwebp7:i386.\nPreparing to unpack .../065-libwebp7_1.3.2-0.4build3_i386.deb ...\nUnpacking libwebp7:i386 (1.3.2-0.4build3) ...\nSelecting previously unselected package libtiff6:i386.\nPreparing to unpack .../066-libtiff6_4.5.1+git230720-4ubuntu2.2_i386.deb ...\nUnpacking libtiff6:i386 (4.5.1+git230720-4ubuntu2.2) ...\nSelecting previously unselected package libgdk-pixbuf-2.0-0:i386.\nPreparing to unpack .../067-libgdk-pixbuf-2.0-0_2.42.10+dfsg-3ubuntu3.1_i386.deb ...\nUnpacking libgdk-pixbuf-2.0-0:i386 (2.42.10+dfsg-3ubuntu3.1) ...\nSelecting previously unselected package libgraphite2-3:i386.\nPreparing to unpack .../068-libgraphite2-3_1.3.14-2build1_i386.deb ...\nUnpacking libgraphite2-3:i386 (1.3.14-2build1) ...\nSelecting previously unselected package libharfbuzz0b:i386.\nPreparing to unpack .../069-libharfbuzz0b_8.3.0-2build2_i386.deb ...\nUnpacking libharfbuzz0b:i386 (8.3.0-2build2) ...\nSelecting previously unselected package libdatrie1:i386.\nPreparing to unpack .../070-libdatrie1_0.2.13-3build1_i386.deb ...\nUnpacking libdatrie1:i386 (0.2.13-3build1) ...\nSelecting previously unselected package libthai0:i386.\nPreparing to unpack .../071-libthai0_0.1.29-2build1_i386.deb ...\nUnpacking libthai0:i386 (0.1.29-2build1) ...\nSelecting previously unselected package libpango-1.0-0:i386.\nPreparing to unpack .../072-libpango-1.0-0_1.52.1+ds-1build1_i386.deb ...\nUnpacking libpango-1.0-0:i386 (1.52.1+ds-1build1) ...\nSelecting previously unselected package libpangoft2-1.0-0:i386.\nPreparing to unpack .../073-libpangoft2-1.0-0_1.52.1+ds-1build1_i386.deb ...\nUnpacking libpangoft2-1.0-0:i386 (1.52.1+ds-1build1) ...\nSelecting previously unselected package libpangocairo-1.0-0:i386.\nPreparing to unpack .../074-libpangocairo-1.0-0_1.52.1+ds-1build1_i386.deb ...\nUnpacking libpangocairo-1.0-0:i386 (1.52.1+ds-1build1) ...\nSelecting previously unselected package librsvg2-2:i386.\nPreparing to unpack .../075-librsvg2-2_2.58.0+dfsg-1build1_i386.deb ...\nUnpacking librsvg2-2:i386 (2.58.0+dfsg-1build1) ...\nSelecting previously unselected package libshine3:i386.\nPreparing to unpack .../076-libshine3_3.1.1-2build1_i386.deb ...\nUnpacking libshine3:i386 (3.1.1-2build1) ...\nSelecting previously unselected package libsnappy1v5:i386.\nPreparing to unpack .../077-libsnappy1v5_1.1.10-1build1_i386.deb ...\nUnpacking libsnappy1v5:i386 (1.1.10-1build1) ...\nSelecting previously unselected package libspeex1:i386.\nPreparing to unpack .../078-libspeex1_1.2.1-2ubuntu2.24.04.1_i386.deb ...\nUnpacking libspeex1:i386 (1.2.1-2ubuntu2.24.04.1) ...\nSelecting previously unselected package libsvtav1enc1d1:i386.\nPreparing to unpack .../079-libsvtav1enc1d1_1.7.0+dfsg-2build1_i386.deb ...\nUnpacking libsvtav1enc1d1:i386 (1.7.0+dfsg-2build1) ...\nSelecting previously unselected package libgomp1:i386.\nPreparing to unpack .../080-libgomp1_14.2.0-4ubuntu2~24.04_i386.deb ...\nUnpacking libgomp1:i386 (14.2.0-4ubuntu2~24.04) ...\nSelecting previously unselected package libsoxr0:i386.\nPreparing to unpack .../081-libsoxr0_0.1.3-4build3_i386.deb ...\nUnpacking libsoxr0:i386 (0.1.3-4build3) ...\nSelecting previously unselected package libswresample4:i386.\nPreparing to unpack .../082-libswresample4_7%3a6.1.1-3ubuntu5+esm2_i386.deb ...\nUnpacking libswresample4:i386 (7:6.1.1-3ubuntu5+esm2) ...\nSelecting previously unselected package libogg0:i386.\nPreparing to unpack .../083-libogg0_1.3.5-3build1_i386.deb ...\nUnpacking libogg0:i386 (1.3.5-3build1) ...\nSelecting previously unselected package libtheora0:i386.\nPreparing to unpack .../084-libtheora0_1.1.1+dfsg.1-16.1build3_i386.deb ...\nUnpacking libtheora0:i386 (1.1.1+dfsg.1-16.1build3) ...\nSelecting previously unselected package libtwolame0:i386.\nPreparing to unpack .../085-libtwolame0_0.4.0-2build3_i386.deb ...\nUnpacking libtwolame0:i386 (0.4.0-2build3) ...\nSelecting previously unselected package libvorbis0a:i386.\nPreparing to unpack .../086-libvorbis0a_1.3.7-1build3_i386.deb ...\nUnpacking libvorbis0a:i386 (1.3.7-1build3) ...\nSelecting previously unselected package libvorbisenc2:i386.\nPreparing to unpack .../087-libvorbisenc2_1.3.7-1build3_i386.deb ...\nUnpacking libvorbisenc2:i386 (1.3.7-1build3) ...\nSelecting previously unselected package libvpx9:i386.\nPreparing to unpack .../088-libvpx9_1.14.0-1ubuntu2.1_i386.deb ...\nUnpacking libvpx9:i386 (1.14.0-1ubuntu2.1) ...\nSelecting previously unselected package libwebpmux3:i386.\nPreparing to unpack .../089-libwebpmux3_1.3.2-0.4build3_i386.deb ...\nUnpacking libwebpmux3:i386 (1.3.2-0.4build3) ...\nSelecting previously unselected package libx264-164:i386.\nPreparing to unpack .../090-libx264-164_2%3a0.164.3108+git31e19f9-1_i386.deb ...\nUnpacking libx264-164:i386 (2:0.164.3108+git31e19f9-1) ...\nSelecting previously unselected package libx265-199:i386.\nPreparing to unpack .../091-libx265-199_3.5-2build1_i386.deb ...\nUnpacking libx265-199:i386 (3.5-2build1) ...\nSelecting previously unselected package libxvidcore4:i386.\nPreparing to unpack .../092-libxvidcore4_2%3a1.3.7-1build1_i386.deb ...\nUnpacking libxvidcore4:i386 (2:1.3.7-1build1) ...\nSelecting previously unselected package libzvbi0t64:i386.\nPreparing to unpack .../093-libzvbi0t64_0.2.42-2_i386.deb ...\nUnpacking libzvbi0t64:i386 (0.2.42-2) ...\nSelecting previously unselected package libavcodec60:i386.\nPreparing to unpack .../094-libavcodec60_7%3a6.1.1-3ubuntu5+esm2_i386.deb ...\nUnpacking libavcodec60:i386 (7:6.1.1-3ubuntu5+esm2) ...\nSelecting previously unselected package libsamplerate0:i386.\nPreparing to unpack .../095-libsamplerate0_0.2.2-4build1_i386.deb ...\nUnpacking libsamplerate0:i386 (0.2.2-4build1) ...\nSelecting previously unselected package libjack-jackd2-0:i386.\nPreparing to unpack .../096-libjack-jackd2-0_1.9.21~dfsg-3ubuntu3_i386.deb ...\nUnpacking libjack-jackd2-0:i386 (1.9.21~dfsg-3ubuntu3) ...\nSelecting previously unselected package libasyncns0:i386.\nPreparing to unpack .../097-libasyncns0_0.8-6build4_i386.deb ...\nUnpacking libasyncns0:i386 (0.8-6build4) ...\nSelecting previously unselected package libflac12t64:i386.\nPreparing to unpack .../098-libflac12t64_1.4.3+ds-2.1ubuntu2_i386.deb ...\nUnpacking libflac12t64:i386 (1.4.3+ds-2.1ubuntu2) ...\nSelecting previously unselected package libmpg123-0t64:i386.\nPreparing to unpack .../099-libmpg123-0t64_1.32.5-1ubuntu1.1_i386.deb ...\nUnpacking libmpg123-0t64:i386 (1.32.5-1ubuntu1.1) ...\nSelecting previously unselected package libsndfile1:i386.\nPreparing to unpack .../100-libsndfile1_1.2.2-1ubuntu5_i386.deb ...\nUnpacking libsndfile1:i386 (1.2.2-1ubuntu5) ...\nSelecting previously unselected package libpulse0:i386.\nPreparing to unpack .../101-libpulse0_1%3a16.1+dfsg1-2ubuntu10_i386.deb ...\nUnpacking libpulse0:i386 (1:16.1+dfsg1-2ubuntu10) ...\nSelecting previously unselected package libspeexdsp1:i386.\nPreparing to unpack .../102-libspeexdsp1_1.2.1-1ubuntu3_i386.deb ...\nUnpacking libspeexdsp1:i386 (1.2.1-1ubuntu3) ...\nSelecting previously unselected package libasound2-plugins:i386.\nPreparing to unpack .../103-libasound2-plugins_1.2.7.1-1ubuntu5_i386.deb ...\nUnpacking libasound2-plugins:i386 (1.2.7.1-1ubuntu5) ...\nSelecting previously unselected package libatk1.0-0t64:i386.\nPreparing to unpack .../104-libatk1.0-0t64_2.52.0-1build1_i386.deb ...\nUnpacking libatk1.0-0t64:i386 (2.52.0-1build1) ...\nSelecting previously unselected package libxi6:i386.\nPreparing to unpack .../105-libxi6_2%3a1.8.1-1build1_i386.deb ...\nUnpacking libxi6:i386 (2:1.8.1-1build1) ...\nSelecting previously unselected package libatspi2.0-0t64:i386.\nPreparing to unpack .../106-libatspi2.0-0t64_2.52.0-1build1_i386.deb ...\nUnpacking libatspi2.0-0t64:i386 (2.52.0-1build1) ...\nSelecting previously unselected package libatk-bridge2.0-0t64:i386.\nPreparing to unpack .../107-libatk-bridge2.0-0t64_2.52.0-1build1_i386.deb ...\nUnpacking libatk-bridge2.0-0t64:i386 (2.52.0-1build1) ...\nSelecting previously unselected package libavahi-common-data:i386.\nPreparing to unpack .../108-libavahi-common-data_0.8-13ubuntu6_i386.deb ...\nUnpacking libavahi-common-data:i386 (0.8-13ubuntu6) ...\nSelecting previously unselected package libavahi-common3:i386.\nPreparing to unpack .../109-libavahi-common3_0.8-13ubuntu6_i386.deb ...\nUnpacking libavahi-common3:i386 (0.8-13ubuntu6) ...\nSelecting previously unselected package libavahi-client3:i386.\nPreparing to unpack .../110-libavahi-client3_0.8-13ubuntu6_i386.deb ...\nUnpacking libavahi-client3:i386 (0.8-13ubuntu6) ...\nSelecting previously unselected package liblcms2-2:i386.\nPreparing to unpack .../111-liblcms2-2_2.14-2build1_i386.deb ...\nUnpacking liblcms2-2:i386 (2.14-2build1) ...\nSelecting previously unselected package libcolord2:i386.\nPreparing to unpack .../112-libcolord2_1.4.7-1build2_i386.deb ...\nUnpacking libcolord2:i386 (1.4.7-1build2) ...\nSelecting previously unselected package libcups2t64:i386.\nPreparing to unpack .../113-libcups2t64_2.4.7-1.2ubuntu7.3_i386.deb ...\nUnpacking libcups2t64:i386 (2.4.7-1.2ubuntu7.3) ...\nSelecting previously unselected package libdecor-0-0:i386.\nPreparing to unpack .../114-libdecor-0-0_0.2.2-1build2_i386.deb ...\nUnpacking libdecor-0-0:i386 (0.2.2-1build2) ...\nSelecting previously unselected package libepoxy0:i386.\nPreparing to unpack .../115-libepoxy0_1.5.10-1build1_i386.deb ...\nUnpacking libepoxy0:i386 (1.5.10-1build1) ...\nSelecting previously unselected package libwayland-cursor0:i386.\nPreparing to unpack .../116-libwayland-cursor0_1.22.0-2.1build1_i386.deb ...\nUnpacking libwayland-cursor0:i386 (1.22.0-2.1build1) ...\nSelecting previously unselected package libwayland-egl1:i386.\nPreparing to unpack .../117-libwayland-egl1_1.22.0-2.1build1_i386.deb ...\nUnpacking libwayland-egl1:i386 (1.22.0-2.1build1) ...\nSelecting previously unselected package libxcomposite1:i386.\nPreparing to unpack .../118-libxcomposite1_1%3a0.4.5-1build3_i386.deb ...\nUnpacking libxcomposite1:i386 (1:0.4.5-1build3) ...\nSelecting previously unselected package libxcursor1:i386.\nPreparing to unpack .../119-libxcursor1_1%3a1.2.1-1build1_i386.deb ...\nUnpacking libxcursor1:i386 (1:1.2.1-1build1) ...\nSelecting previously unselected package libxdamage1:i386.\nPreparing to unpack .../120-libxdamage1_1%3a1.1.6-1build1_i386.deb ...\nUnpacking libxdamage1:i386 (1:1.1.6-1build1) ...\nSelecting previously unselected package libxinerama1:i386.\nPreparing to unpack .../121-libxinerama1_2%3a1.1.4-3build1_i386.deb ...\nUnpacking libxinerama1:i386 (2:1.1.4-3build1) ...\nSelecting previously unselected package libxrandr2:i386.\nPreparing to unpack .../122-libxrandr2_2%3a1.5.2-2build1_i386.deb ...\nUnpacking libxrandr2:i386 (2:1.5.2-2build1) ...\nSelecting previously unselected package libgtk-3-0t64:i386.\nPreparing to unpack .../123-libgtk-3-0t64_3.24.41-4ubuntu1.2_i386.deb ...\nUnpacking libgtk-3-0t64:i386 (3.24.41-4ubuntu1.2) ...\nSelecting previously unselected package libdecor-0-plugin-1-gtk:i386.\nPreparing to unpack .../124-libdecor-0-plugin-1-gtk_0.2.2-1build2_i386.deb ...\nUnpacking libdecor-0-plugin-1-gtk:i386 (0.2.2-1build2) ...\nSelecting previously unselected package libpciaccess0:i386.\nPreparing to unpack .../125-libpciaccess0_0.17-3build1_i386.deb ...\nUnpacking libpciaccess0:i386 (0.17-3build1) ...\nSelecting previously unselected package libdrm-intel1:i386.\nPreparing to unpack .../126-libdrm-intel1_2.4.120-2build1_i386.deb ...\nUnpacking libdrm-intel1:i386 (2.4.120-2build1) ...\nSelecting previously unselected package libdrm-nouveau2:i386.\nPreparing to unpack .../127-libdrm-nouveau2_2.4.120-2build1_i386.deb ...\nUnpacking libdrm-nouveau2:i386 (2.4.120-2build1) ...\nSelecting previously unselected package libdrm-radeon1:i386.\nPreparing to unpack .../128-libdrm-radeon1_2.4.120-2build1_i386.deb ...\nUnpacking libdrm-radeon1:i386 (2.4.120-2build1) ...\nSelecting previously unselected package libgbm1:i386.\nPreparing to unpack .../129-libgbm1_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking libgbm1:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package libglapi-mesa:i386.\nPreparing to unpack .../130-libglapi-mesa_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking libglapi-mesa:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package libxcb-dri2-0:i386.\nPreparing to unpack .../131-libxcb-dri2-0_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb-dri2-0:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libegl-mesa0:i386.\nPreparing to unpack .../132-libegl-mesa0_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking libegl-mesa0:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package libgl1-mesa-dri:i386.\nPreparing to unpack .../133-libgl1-mesa-dri_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking libgl1-mesa-dri:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package libxcb-glx0:i386.\nPreparing to unpack .../134-libxcb-glx0_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb-glx0:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libxxf86vm1:i386.\nPreparing to unpack .../135-libxxf86vm1_1%3a1.1.4-1build4_i386.deb ...\nUnpacking libxxf86vm1:i386 (1:1.1.4-1build4) ...\nSelecting previously unselected package libglx-mesa0:i386.\nPreparing to unpack .../136-libglx-mesa0_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking libglx-mesa0:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package libnm0:i386.\nPreparing to unpack .../137-libnm0_1.46.0-1ubuntu2.2_i386.deb ...\nUnpacking libnm0:i386 (1.46.0-1ubuntu2.2) ...\nSelecting previously unselected package librsvg2-common:i386.\nPreparing to unpack .../138-librsvg2-common_2.58.0+dfsg-1build1_i386.deb ...\nUnpacking librsvg2-common:i386 (2.58.0+dfsg-1build1) ...\nSelecting previously unselected package libxss1:i386.\nPreparing to unpack .../139-libxss1_1%3a1.2.3-1build3_i386.deb ...\nUnpacking libxss1:i386 (1:1.2.3-1build3) ...\nSelecting previously unselected package libsdl2-2.0-0:i386.\nPreparing to unpack .../140-libsdl2-2.0-0_2.30.0+dfsg-1build3_i386.deb ...\nUnpacking libsdl2-2.0-0:i386 (2.30.0+dfsg-1build3) ...\nSelecting previously unselected package libva-glx2:amd64.\nPreparing to unpack .../141-libva-glx2_2.20.0-2build1_amd64.deb ...\nUnpacking libva-glx2:amd64 (2.20.0-2build1) ...\nSelecting previously unselected package libglvnd0:i386.\nPreparing to unpack .../142-libglvnd0_1.7.0-1build1_i386.deb ...\nUnpacking libglvnd0:i386 (1.7.0-1build1) ...\nSelecting previously unselected package libglx0:i386.\nPreparing to unpack .../143-libglx0_1.7.0-1build1_i386.deb ...\nUnpacking libglx0:i386 (1.7.0-1build1) ...\nSelecting previously unselected package libgl1:i386.\nPreparing to unpack .../144-libgl1_1.7.0-1build1_i386.deb ...\nUnpacking libgl1:i386 (1.7.0-1build1) ...\nSelecting previously unselected package libva-glx2:i386.\nPreparing to unpack .../145-libva-glx2_2.20.0-2build1_i386.deb ...\nUnpacking libva-glx2:i386 (2.20.0-2build1) ...\nSelecting previously unselected package mesa-va-drivers:i386.\nPreparing to unpack .../146-mesa-va-drivers_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking mesa-va-drivers:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package mesa-vdpau-drivers:i386.\nPreparing to unpack .../147-mesa-vdpau-drivers_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking mesa-vdpau-drivers:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package steam-libs:amd64.\nPreparing to unpack .../148-steam-libs_1%3a1.0.0.79~ds-2_amd64.deb ...\nUnpacking steam-libs:amd64 (1:1.0.0.79~ds-2) ...\nSelecting previously unselected package steam-libs:i386.\nPreparing to unpack .../149-steam-libs_1%3a1.0.0.79~ds-2_i386.deb ...\nUnpacking steam-libs:i386 (1:1.0.0.79~ds-2) ...\nSelecting previously unselected package steam-libs-i386:i386.\nPreparing to unpack .../150-steam-libs-i386_1%3a1.0.0.79~ds-2_i386.deb ...\nUnpacking steam-libs-i386:i386 (1:1.0.0.79~ds-2) ...\nSelecting previously unselected package i965-va-driver:i386.\nPreparing to unpack .../151-i965-va-driver_2.4.1+dfsg1-1build2_i386.deb ...\nUnpacking i965-va-driver:i386 (2.4.1+dfsg1-1build2) ...\nSelecting previously unselected package va-driver-all:i386.\nPreparing to unpack .../152-va-driver-all_2.20.0-2build1_i386.deb ...\nUnpacking va-driver-all:i386 (2.20.0-2build1) ...\nSelecting previously unselected package vdpau-driver-all:i386.\nPreparing to unpack .../153-vdpau-driver-all_1.5-2build1_i386.deb ...\nUnpacking vdpau-driver-all:i386 (1.5-2build1) ...\nSelecting previously unselected package xterm.\nPreparing to unpack .../154-xterm_390-1ubuntu3_amd64.deb ...\nUnpacking xterm (390-1ubuntu3) ...\nSelecting previously unselected package libegl1:i386.\nPreparing to unpack .../155-libegl1_1.7.0-1build1_i386.deb ...\nUnpacking libegl1:i386 (1.7.0-1build1) ...\nSelecting previously unselected package steam-devices.\nPreparing to unpack .../156-steam-devices_1%3a1.0.0.79~ds-2_all.deb ...\nUnpacking steam-devices (1:1.0.0.79~ds-2) ...\nSetting up libgraphite2-3:i386 (1.3.14-2build1) ...\nSetting up liblcms2-2:i386 (2.14-2build1) ...\nSetting up libpixman-1-0:i386 (0.42.2-1build1) ...\nSetting up libsharpyuv0:i386 (1.3.2-0.4build3) ...\nSetting up libaom3:i386 (3.8.2-2ubuntu0.1) ...\nSetting up libpciaccess0:i386 (0.17-3build1) ...\nSetting up libkeyutils1:i386 (1.6.3-3build1) ...\nSetting up libapparmor1:i386 (4.0.1really4.0.1-0ubuntu0.24.04.3) ...\nSetting up libdrm-nouveau2:i386 (2.4.120-2build1) ...\nSetting up libxdamage1:i386 (1:1.1.6-1build1) ...\nSetting up libogg0:i386 (1.3.5-3build1) ...\nSetting up libspeex1:i386 (1.2.1-2ubuntu2.24.04.1) ...\nSetting up libshine3:i386 (3.1.1-2build1) ...\nSetting up libxi6:i386 (2:1.8.1-1build1) ...\nSetting up libx264-164:i386 (2:0.164.3108+git31e19f9-1) ...\nSetting up libtwolame0:i386 (0.4.0-2build3) ...\nSetting up libgpg-error0:i386 (1.47-3build2) ...\nSetting up libxrender1:i386 (1:0.9.10-1.1build1) ...\nSetting up libdatrie1:i386 (0.2.13-3build1) ...\nSetting up libgbm1:i386 (24.0.9-0ubuntu0.3) ...\nSetting up libgsm1:i386 (1.0.22-1build1) ...\nSetting up libxcb-render0:i386 (1.15-1ubuntu2) ...\nSetting up libssl3t64:i386 (3.0.13-0ubuntu3.4) ...\nSetting up libdrm-radeon1:i386 (2.4.120-2build1) ...\nSetting up liblz4-1:i386 (1.9.4-1build1.1) ...\nSetting up libglvnd0:i386 (1.7.0-1build1) ...\nSetting up libcodec2-1.2:i386 (1.2.0-2build1) ...\nSetting up steam-devices (1:1.0.0.79~ds-2) ...\nSetting up libxcb-glx0:i386 (1.15-1ubuntu2) ...\nSetting up libbrotli1:i386 (1.1.0-2build2) ...\nSetting up libdrm-intel1:i386 (2.4.120-2build1) ...\nSetting up steam-libs:amd64 (1:1.0.0.79~ds-2) ...\nSetting up libdeflate0:i386 (1.19-1build1.1) ...\nSetting up libgcrypt20:i386 (1.10.3-2build1) ...\nSetting up libcrypt1:i386 (1:4.4.36-4build1) ...\nSetting up libsvtav1enc1d1:i386 (1.7.0+dfsg-2build1) ...\nSetting up libcom-err2:i386 (1.47.0-2.4~exp1ubuntu4.1) ...\nSetting up libigdgmm12:i386 (22.3.17+ds1-1) ...\nSetting up libmpg123-0t64:i386 (1.32.5-1ubuntu1.1) ...\nSetting up libgomp1:i386 (14.2.0-4ubuntu2~24.04) ...\nSetting up libxvidcore4:i386 (2:1.3.7-1build1) ...\nSetting up libjbig0:i386 (2.1-6.1ubuntu2) ...\nSetting up libcap2:i386 (1:2.66-5ubuntu2) ...\nSetting up libxxf86vm1:i386 (1:1.1.4-1build4) ...\nSetting up libsnappy1v5:i386 (1.1.10-1build1) ...\nSetting up libkrb5support0:i386 (1.20.1-6ubuntu2.2) ...\nSetting up libthai0:i386 (0.1.29-2build1) ...\nSetting up libnettle8t64:i386 (3.9.1-2.2build1.1) ...\nSetting up libasound2t64:i386 (1.2.11-1build2) ...\nSetting up libva2:i386 (2.20.0-2build1) ...\nSetting up libepoxy0:i386 (1.5.10-1build1) ...\nSetting up libxfixes3:i386 (1:6.0.0-2build1) ...\nSetting up libgmp10:i386 (2:6.3.0+dfsg-2ubuntu6) ...\nSetting up libavahi-common-data:i386 (0.8-13ubuntu6) ...\nSetting up libfribidi0:i386 (1.0.13-3build1) ...\nSetting up libopus0:i386 (1.4-1build1) ...\nSetting up libp11-kit0:i386 (0.25.3-4ubuntu2.1) ...\nSetting up libxinerama1:i386 (2:1.1.4-3build1) ...\nSetting up intel-media-va-driver:i386 (24.1.0+dfsg1-1) ...\nSetting up libpng16-16t64:i386 (1.6.43-5build1) ...\nSetting up libvorbis0a:i386 (1.3.7-1build3) ...\nSetting up libxrandr2:i386 (2:1.5.2-2build1) ...\nSetting up libsensors5:i386 (1:3.6.0-9build1) ...\nSetting up libpcre2-8-0:i386 (10.42-4ubuntu2) ...\nSetting up libk5crypto3:i386 (1.20.1-6ubuntu2.2) ...\nSetting up libjpeg-turbo8:i386 (2.1.5-2ubuntu2) ...\nSetting up libglapi-mesa:i386 (24.0.9-0ubuntu0.3) ...\nSetting up libwebp7:i386 (1.3.2-0.4build3) ...\nSetting up libdb5.3t64:i386 (5.3.28+dfsg2-7) ...\nSetting up libudev1:i386 (255.4-1ubuntu8.4) ...\nSetting up libxcb-dri2-0:i386 (1.15-1ubuntu2) ...\nSetting up libnuma1:i386 (2.0.18-1build1) ...\nSetting up libvpx9:i386 (1.14.0-1ubuntu2.1) ...\nSetting up libdav1d7:i386 (1.4.1-1build1) ...\nSetting up libhogweed6t64:i386 (3.9.1-2.2build1.1) ...\nSetting up libva-drm2:i386 (2.20.0-2build1) ...\nSetting up ocl-icd-libopencl1:i386 (2.3.2-1build1) ...\nSetting up libasyncns0:i386 (0.8-6build4) ...\nSetting up libvdpau1:i386 (1.5-2build1) ...\nSetting up libwayland-cursor0:i386 (1.22.0-2.1build1) ...\nSetting up libspeexdsp1:i386 (1.2.1-1ubuntu3) ...\nSetting up libtasn1-6:i386 (4.19.0-3build1) ...\nSetting up libdecor-0-0:i386 (0.2.2-1build2) ...\nSetting up libopenjp2-7:i386 (2.5.0-2ubuntu0.2) ...\nSetting up libkrb5-3:i386 (1.20.1-6ubuntu2.2) ...\nSetting up libflac12t64:i386 (1.4.3+ds-2.1ubuntu2) ...\nSetting up libwayland-egl1:i386 (1.22.0-2.1build1) ...\nSetting up libxss1:i386 (1:1.2.3-1build3) ...\nSetting up libusb-1.0-0:i386 (2:1.0.27-1) ...\nSetting up xterm (390-1ubuntu3) ...\nSetting up mesa-va-drivers:i386 (24.0.9-0ubuntu0.3) ...\nSetting up libbz2-1.0:i386 (1.0.8-5.1build0.1) ...\nSetting up libsamplerate0:i386 (0.2.2-4build1) ...\nSetting up libva-x11-2:i386 (2.20.0-2build1) ...\nSetting up libwebpmux3:i386 (1.3.2-0.4build3) ...\nSetting up libva-glx2:amd64 (2.20.0-2build1) ...\nSetting up libxcomposite1:i386 (1:0.4.5-1build3) ...\nSetting up libblkid1:i386 (2.39.3-9ubuntu6.1) ...\nSetting up libmp3lame0:i386 (3.100-6build1) ...\nSetting up i965-va-driver:i386 (2.4.1+dfsg1-1build2) ...\nSetting up libvorbisenc2:i386 (1.3.7-1build3) ...\nSetting up libxkbcommon0:i386 (1.6.0-1build1) ...\nSetting up libjpeg8:i386 (8c-2ubuntu11) ...\nSetting up libgnutls30t64:i386 (3.8.3-1.1ubuntu3.2) ...\nSetting up mesa-vdpau-drivers:i386 (24.0.9-0ubuntu0.3) ...\nSetting up libzvbi0t64:i386 (0.2.42-2) ...\nSetting up libsoxr0:i386 (0.1.3-4build3) ...\nSetting up libxcursor1:i386 (1:1.2.1-1build1) ...\nSetting up libgl1-mesa-dri:i386 (24.0.9-0ubuntu0.3) ...\nSetting up libavahi-common3:i386 (0.8-13ubuntu6) ...\nSetting up libavutil58:i386 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up libsystemd0:i386 (255.4-1ubuntu8.4) ...\nSetting up libselinux1:i386 (3.5-2ubuntu2) ...\nSetting up libegl-mesa0:i386 (24.0.9-0ubuntu0.3) ...\nSetting up libfreetype6:i386 (2.13.2+dfsg-1build3) ...\nSetting up libswresample4:i386 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up va-driver-all:i386 (2.20.0-2build1) ...\nSetting up libdbus-1-3:i386 (1.14.10-4ubuntu4.1) ...\nSetting up libgssapi-krb5-2:i386 (1.20.1-6ubuntu2.2) ...\nSetting up libx265-199:i386 (3.5-2build1) ...\nSetting up libjack-jackd2-0:i386 (1.9.21~dfsg-3ubuntu3) ...\nSetting up vdpau-driver-all:i386 (1.5-2build1) ...\nSetting up libmount1:i386 (2.39.3-9ubuntu6.1) ...\nSetting up libtiff6:i386 (4.5.1+git230720-4ubuntu2.2) ...\nSetting up libegl1:i386 (1.7.0-1build1) ...\nSetting up libfontconfig1:i386 (2.15.0-1.1ubuntu2) ...\nSetting up libsndfile1:i386 (1.2.2-1ubuntu5) ...\nSetting up libavahi-client3:i386 (0.8-13ubuntu6) ...\nSetting up libglx-mesa0:i386 (24.0.9-0ubuntu0.3) ...\nSetting up libglx0:i386 (1.7.0-1build1) ...\nSetting up libpulse0:i386 (1:16.1+dfsg1-2ubuntu10) ...\nSetting up libcairo2:i386 (1.18.0-3build1) ...\nSetting up libglib2.0-0t64:i386 (2.80.0-6ubuntu3.2) ...\nSetting up libgl1:i386 (1.7.0-1build1) ...\nSetting up libtheora0:i386 (1.1.1+dfsg.1-16.1build3) ...\nSetting up libsdl2-2.0-0:i386 (2.30.0+dfsg-1build3) ...\nSetting up libva-glx2:i386 (2.20.0-2build1) ...\nSetting up libcups2t64:i386 (2.4.7-1.2ubuntu7.3) ...\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.3) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for libglib2.0-0t64:amd64 (2.80.0-6ubuntu3.2) ...\nSetting up libatspi2.0-0t64:i386 (2.52.0-1build1) ...\nSetting up libnm0:i386 (1.46.0-1ubuntu2.2) ...\nSetting up libharfbuzz0b:i386 (8.3.0-2build2) ...\nSetting up libgdk-pixbuf-2.0-0:i386 (2.42.10+dfsg-3ubuntu3.1) ...\nSetting up libcairo-gobject2:i386 (1.18.0-3build1) ...\nSetting up libatk1.0-0t64:i386 (2.52.0-1build1) ...\nSetting up libpango-1.0-0:i386 (1.52.1+ds-1build1) ...\nSetting up steam-libs:i386 (1:1.0.0.79~ds-2) ...\nSetting up libcolord2:i386 (1.4.7-1build2) ...\nSetting up steam-libs-i386:i386 (1:1.0.0.79~ds-2) ...\nSetting up libpangoft2-1.0-0:i386 (1.52.1+ds-1build1) ...\nSetting up libpangocairo-1.0-0:i386 (1.52.1+ds-1build1) ...\nSetting up libatk-bridge2.0-0t64:i386 (2.52.0-1build1) ...\nSetting up librsvg2-2:i386 (2.58.0+dfsg-1build1) ...\nSetting up libgtk-3-0t64:i386 (3.24.41-4ubuntu1.2) ...\nSetting up librsvg2-common:i386 (2.58.0+dfsg-1build1) ...\nSetting up libavcodec60:i386 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up libdecor-0-plugin-1-gtk:i386 (0.2.2-1build2) ...\nSetting up libasound2-plugins:i386 (1.2.7.1-1ubuntu5) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.3) ...\nProcessing triggers for libgdk-pixbuf-2.0-0:i386 (2.42.10+dfsg-3ubuntu3.1) ...\n</code></pre> <p>With those installed, one can download <code>steam_latest.deb</code> from store.steampowered.com/about and install it with <code>gdebi</code>:</p> <pre><code># gdebi steam_latest.deb\n/usr/bin/gdebi:113: SyntaxWarning: invalid escape sequence '\\S'\n  c = findall(\"[[(](\\S+)/\\S+[])]\", msg)[0].lower()\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nReading state information... Done\n\nLauncher for the Steam software distribution service\n Steam is a software distribution service with an online store, automated\n installation, automatic updates, achievements, SteamCloud synchronized\n savegame and screenshot functionality, and many social features.\nDo you want to install the software package? [y/N]:y\n/usr/bin/gdebi:113: FutureWarning: Possible nested set at position 1\n  c = findall(\"[[(](\\S+)/\\S+[])]\", msg)[0].lower()\nSelecting previously unselected package steam-launcher.\ndpkg: considering removing steam-devices in favour of steam-launcher ...\ndpkg: yes, will remove steam-devices in favour of steam-launcher\n(Reading database ... 488374 files and directories currently installed.)\nPreparing to unpack steam_latest.deb ...\nUnpacking steam-launcher (1:1.0.0.81) ...\nRemoving steam-devices (1:1.0.0.79~ds-2), to allow configuration of steam-launcher (1:1.0.0.81) ...\nSetting up steam-launcher (1:1.0.0.81) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\n</code></pre> <p>Despite all the above, upon running <code>/usr/bin/steam</code> a new terminal window still opens and requests installing the missing packages:</p> <pre><code>The packages cache seems to be out of date\n\nPress return to update the list of available packages: \n...................\nPackage steam-libs-amd64:amd64 needs to be installed\n\nSteam needs to install these additional packages:\nlibc6:amd64 libc6:i386 libegl1:amd64 libegl1:i386 libgbm1:amd64 libgbm1:i386 libgl1-mesa-dri:amd64 libgl1-mesa-dri:i386 libgl1:amd64 libgl1:i386 steam-libs-amd64:amd64\n\nPress return to proceed with the installation: \n</code></pre> <p>Apparently Steam requires installing all those packages for supporting AMD GPUs as well, which makes sense and I had not noticed until now.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#glorious-eggroll","title":"Glorious Eggroll","text":"<p>Many games depend on gloriouseggroll/proton-ge-custom to run and this needs to be installed separately.</p> <p>This was already installed in Rapture but, just because it was so easy to install the latest one in Raven, installed the same in Rapture as well:</p> <pre><code>$ ~/bin/install-latest-protonge.sh\nCreating temporary working directory...\nFetching tarball URL...\nDownloading tarball: GE-Proton9-22.tar.gz...\nFetching checksum URL...\nDownloading checksum: GE-Proton9-22.sha512sum...\nVerifying tarball GE-Proton9-22.tar.gz with checksum GE-Proton9-22.sha512sum...\nGE-Proton9-22.tar.gz: OK\nCreating Steam directory if it does not exist...\nExtracting GE-Proton9-22.tar.gz to Steam directory...\nAll done :)\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#snap-package-reinstalled","title":"Snap package (reinstalled)","text":"<p>Turns out some games actually work better with the snap version of Steam, or at least it was easy to get them to run this time around. In particular, The Elder Scrolls\u00ae Online worked out of the box with the snap version of Steam, while it wouldn't install or launch otherwise, so Steam from Snap was installed again:</p> <pre><code># snap install steam\nsteam 1.0.0.81 from Canonical\u2713 installed\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#minecraft-java-edition","title":"Minecraft Java Edition","text":"<p>To avoid taking chances, copy the Minecraft launcher from the previous system:</p> <pre><code># cp -a /jammy/opt/minecraft-launcher/ /opt/\n</code></pre> <p>It works perfectly right after installing; no need to login again.</p> <p>In contrast, trying to re-download Minecraft Java Edition seems to lead nowhere good.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#minecraft-bedrock-edition","title":"Minecraft Bedrock Edition","text":"<p>There is an unofficial Minecraft Bedrock Launcher, including smiple steps to install it on Debian / Ubuntu. This has not seemed necessary so far, since the family enjoys playing the Java edition more.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#blender","title":"Blender","text":"<p>Blender 4.2 LTS is already available even for Ubuntu 24.04 via  snapcraft.io/blender so there is no reason to install it any other way:</p> <pre><code># snap install blender --classic\nblender 4.2.3 from Blender Foundation (blenderfoundation\u2713) installed\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#continuous-monitoring","title":"Continuous Monitoring","text":"<p>Install the multi-thread version of the <code>conmon</code> script as <code>/usr/local/bin/conmon</code> and run it as a service; create <code>/etc/systemd/system/conmon.service</code> as follows:</p> /etc/systemd/system/conmon.service<pre><code>[Unit]\nDescription=Continuous Monitoring\n\n[Service]\nExecStart=/usr/local/bin/conmon\nRestart=on-failure\nStandardOutput=null\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Then enable and start the services in <code>systemd</code>:</p> <pre><code># systemctl enable conmon.service\n# systemctl daemon-reload\n# systemctl start conmon.service\n# systemctl status conmon.service\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#hardware-sensors","title":"Hardware Sensors","text":"<p>Initially there is only a limited amount of hardware sensors:</p> <pre><code># sensors -A\nnvme-pci-0400\nComposite:    +36.9\u00b0C  (low  = -20.1\u00b0C, high = +83.8\u00b0C)\n                       (crit = +88.8\u00b0C)\nSensor 2:     +72.8\u00b0C  \n\nk10temp-pci-00c3\nTctl:         +42.5\u00b0C  \nTccd2:        +36.0\u00b0C  \n\nnvme-pci-0100\nComposite:    +42.9\u00b0C  (low  = -273.1\u00b0C, high = +84.8\u00b0C)\n                       (crit = +84.8\u00b0C)\nSensor 1:     +42.9\u00b0C  (low  = -273.1\u00b0C, high = +65261.8\u00b0C)\nSensor 2:     +42.9\u00b0C  (low  = -273.1\u00b0C, high = +65261.8\u00b0C) \n</code></pre> <p>HDD temperatures are available by loading the drivetemp kernel module:</p> <pre><code># echo drivetemp &gt; /etc/modules-load.d/drivetemp.conf\n# modprobe drivetemp\n</code></pre> <code>sensors -A</code> <pre><code># sensors -A\ndrivetemp-scsi-9-0\ntemp1:        +40.0\u00b0C  (low  =  +0.0\u00b0C, high = +55.0\u00b0C)\n                      (crit low = -40.0\u00b0C, crit = +70.0\u00b0C)\n                      (lowest = +24.0\u00b0C, highest = +40.0\u00b0C)\n\ndrivetemp-scsi-5-0\ntemp1:        +29.0\u00b0C  (low  =  +0.0\u00b0C, high = +70.0\u00b0C)\n                      (crit low =  +0.0\u00b0C, crit = +70.0\u00b0C)\n                      (lowest = +25.0\u00b0C, highest = +34.0\u00b0C)\n\ndrivetemp-scsi-2-0\ntemp1:        +40.0\u00b0C  (low  =  +0.0\u00b0C, high = +60.0\u00b0C)\n                      (crit low = -40.0\u00b0C, crit = +70.0\u00b0C)\n                      (lowest = +24.0\u00b0C, highest = +40.0\u00b0C)\n\nnvme-pci-0400\nComposite:    +36.9\u00b0C  (low  = -20.1\u00b0C, high = +83.8\u00b0C)\n                      (crit = +88.8\u00b0C)\nSensor 2:     +72.8\u00b0C  \n\nk10temp-pci-00c3\nTctl:         +42.5\u00b0C  \nTccd2:        +36.0\u00b0C  \n\ndrivetemp-scsi-8-0\ntemp1:        +36.0\u00b0C  (low  =  +0.0\u00b0C, high = +60.0\u00b0C)\n                      (crit low = -41.0\u00b0C, crit = +85.0\u00b0C)\n                      (lowest = +22.0\u00b0C, highest = +36.0\u00b0C)\n\ndrivetemp-scsi-4-0\ntemp1:        +29.0\u00b0C  (low  =  +0.0\u00b0C, high = +100.0\u00b0C)\n                      (crit low =  +0.0\u00b0C, crit = +100.0\u00b0C)\n                      (lowest = +25.0\u00b0C, highest = +29.0\u00b0C)\n\nnvme-pci-0100\nComposite:    +42.9\u00b0C  (low  = -273.1\u00b0C, high = +84.8\u00b0C)\n                      (crit = +84.8\u00b0C)\nSensor 1:     +42.9\u00b0C  (low  = -273.1\u00b0C, high = +65261.8\u00b0C)\nSensor 2:     +42.9\u00b0C  (low  = -273.1\u00b0C, high = +65261.8\u00b0C)\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#motherboard-sensors-nct6798d","title":"Motherboard Sensors (NCT6798D)","text":"<p>The ASUS TUF X-570-PRO WIFI II motherboard initially shows only the South bridge sensors (<code>k10temp</code>); to gain access to the full range of sensors load the additional driver <code>nct6775</code>:</p> <pre><code># echo nct6775 &gt; /etc/modules-load.d/nct6775.conf\n# modprobe nct6775\n</code></pre> <p>When loading this driver, <code>dmesg</code> should show a single line:</p> <pre><code>nct6775: Found NCT6798D or compatible chip at 0x2e:0x290\n</code></pre> <p>Previously (in Ubuntu Studio 22.04 in late 2022) this driver would encounter a conflict and sensors would not be available:</p> <pre><code>nct6775: Found NCT6798D or compatible chip at 0x2e:0x290\nACPI Warning: SystemIO range 0x0000000000000295-0x0000000000000296 conflicts with OpRegion 0x0000000000000290-0x0000000000000299 (\\AMW0.SHWM) (20210730/utaddress-204)\nACPI: OSL: Resource conflict; ACPI support missing from driver?\n</code></pre> <p>As for late 2024, the driver encounters no conflict, and the output from <code>sensors -A</code> shows the <code>nct6798-isa-0290</code> with many additional sensors:</p> <code>nct6798-isa-0290</code> <pre><code>nct6798-isa-0290\nin0:                        1.39 V  (min =  +0.00 V, max =  +1.74 V)\nin1:                        1.01 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nin2:                        3.41 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nin3:                        3.34 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nin4:                        1.02 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nin5:                      864.00 mV (min =  +0.00 V, max =  +0.00 V)\nin6:                        1.01 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nin7:                        3.41 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nin8:                        3.30 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nin9:                        1.82 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nin10:                     464.00 mV (min =  +0.00 V, max =  +0.00 V)  ALARM\nin11:                     960.00 mV (min =  +0.00 V, max =  +0.00 V)  ALARM\nin12:                       1.04 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nin13:                     1000.00 mV (min =  +0.00 V, max =  +0.00 V)  ALARM\nin14:                       1.01 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nfan1:                     1337 RPM  (min =    0 RPM)\nfan2:                     1021 RPM  (min =    0 RPM)\nfan3:                      963 RPM  (min =    0 RPM)\nfan4:                      964 RPM  (min =    0 RPM)\nfan5:                        0 RPM  (min =    0 RPM)\nfan6:                        0 RPM  (min =    0 RPM)\nfan7:                        0 RPM  (min =    0 RPM)\nSYSTIN:                    +33.0\u00b0C  (high = +80.0\u00b0C, hyst = +75.0\u00b0C)\n                                    (crit = +125.0\u00b0C)  sensor = thermistor\nCPUTIN:                    +38.0\u00b0C  (high = +80.0\u00b0C, hyst = +75.0\u00b0C)\n                                    (crit = +125.0\u00b0C)  sensor = thermistor\nAUXTIN0:                   +26.0\u00b0C  (high = +80.0\u00b0C, hyst = +75.0\u00b0C)\n                                    (crit = +125.0\u00b0C)  sensor = thermistor\nAUXTIN1:                   +67.0\u00b0C  (high = +80.0\u00b0C, hyst = +75.0\u00b0C)\n                                    (crit = +125.0\u00b0C)  sensor = thermistor\nAUXTIN2:                   +27.0\u00b0C  (high = +80.0\u00b0C, hyst = +75.0\u00b0C)\n                                    (crit = +100.0\u00b0C)  sensor = thermistor\nAUXTIN3:                   +25.0\u00b0C  (high = +80.0\u00b0C, hyst = +75.0\u00b0C)\n                                    (crit = +100.0\u00b0C)  sensor = thermistor\nAUXTIN4:                   +33.0\u00b0C  (high = +80.0\u00b0C, hyst = +75.0\u00b0C)\n                                    (crit = +100.0\u00b0C)\nPECI Agent 0 Calibration:  +58.0\u00b0C  (high = +80.0\u00b0C, hyst = +75.0\u00b0C)\nPCH_CHIP_CPU_MAX_TEMP:      +0.0\u00b0C  \nPCH_CHIP_TEMP:              +0.0\u00b0C  \nPCH_CPU_TEMP:               +0.0\u00b0C  \nPCH_MCH_TEMP:               +0.0\u00b0C  \nTSI0_TEMP:                 +69.6\u00b0C  \nTSI1_TEMP:                 +61.2\u00b0C  \nintrusion0:               ALARM\nintrusion1:               ALARM\nbeep_enable:              disabled\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#itchio","title":"Itch.io","text":"<p>There is a binary in <code>.itch/itch</code> but it doesn\u2019t work, it seem to have launched the app but the app itself is nowhere to be seen:</p> <code>$ .itch/itch</code> <pre><code>$ .itch/itch\n2024/11/07 23:27:26 itch-setup will log to /tmp/itch-setup-log.txt\n2024/11/07 23:27:26 =========================================\n2024/11/07 23:27:26 itch-setup \"v1.26.0, built on Apr 22 2021 @ 03:48:12, ref 48f97b3e7b0b065a2478811b8d0ebcae414845fd\" starting up at \"2024-11-07 23:27:26.862907105 +0100 CET m=+0.002170433\" with arguments:\n2024/11/07 23:27:26 \"/home/coder/.itch/itch-setup\"\n2024/11/07 23:27:26 \"--prefer-launch\"\n2024/11/07 23:27:26 \"--appname\"\n2024/11/07 23:27:26 \"itch\"\n2024/11/07 23:27:26 \"--\"\n2024/11/07 23:27:26 =========================================\n2024/11/07 23:27:26 App name specified on command-line: itch\n2024/11/07 23:27:26 Locale:  en-US\n2024/11/07 23:27:26 Initializing installer GUI...\n2024/11/07 23:27:26 Using GTK UI\n\n(process:1496821): Gtk-WARNING **: 23:27:26.864: Locale not supported by C library.\n        Using the fallback 'C' locale.\n2024/11/07 23:27:26 Initializing (itch) multiverse @ (/home/coder/.itch)\n2024/11/07 23:27:26 (/home/coder/.itch)(current = \"26.1.9\", ready = \"\")\n2024/11/07 23:27:26 Launch preferred, attempting...\n2024/11/07 23:27:26 Launching (26.1.9) from (/home/coder/.itch/app-26.1.9)\n2024/11/07 23:27:26 Kernel should support SUID sandboxing, leaving it enabled\n2024/11/07 23:27:26 App launched, getting out of the way\n</code></pre> <p>The solution, albeit possibly only a temporary one, is to disable sandboxing (source) by adding the <code>--no-sandbox</code> in <code>.itch/itch</code>:</p> .itch/itch<pre><code>#!/bin/sh\n/home/coder/.itch/itch-setup \\\n  --prefer-launch --appname itch \\\n  -- --no-sandbox \"$@\"\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#arduino-ide","title":"Arduino IDE","text":"<p>The Arduino IDE in Ubuntu 22.04 (in <code>/jammy/opt/arduino</code>) will be out of date, so it pays to install the latest/nightly version:</p> <pre><code># wget https://downloads.arduino.cc/arduino-ide/nightly/arduino-ide_nightly-latest_Linux_64bit.zip\n# unzip arduino-ide_nightly-latest_Linux_64bit.zip\n# mv arduino-ide_nightly-20241106_Linux_64bit/ /opt/arduino/\n# chmod 4755 /opt/arduino/chrome-sandbox\n</code></pre> <p>Upon launching the Arduino IDE, a notification card offers updating installed libraries, which comes in vary handy to update them all.</p> Without the <code>chmod 4755</code> command, the IDE refuses to run. <pre><code>$ /opt/arduino/arduino-ide\n[1917080:1107/234610.122185:FATAL:setuid_sandbox_host.cc(158)] The SUID sandbox helper binary was found, but is not configured correctly. Rather than run without sandboxing I'm aborting now. You need to make sure that /opt/arduino/chrome-sandbox is owned by root and has mode 4755.\nTrace/breakpoint trap (core dumped)\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#fmnt-autofirma","title":"FMNT Autofirma","text":"<p>Requesting personal certificates from the FMNT the requires configuracion-previa:</p> <p>Download the AutoFirma 1.8 para Debian Linux and the Configurador FNMT-RCM para GNU/Linux 64 bits (DEB) packages and <code>unzip</code> the AutoFirma package and install both, but first install dependencies (<code>libnss</code> and Java Runtime Environment):</p> <code># apt install default-jre libnss3-tools -y</code> <pre><code># apt install default-jre libnss3-tools -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nlibnss3-tools is already the newest version (2:3.98-1build1).\nThe following additional packages will be installed:\n  ca-certificates-java default-jre-headless java-common libatk-wrapper-java\n  libatk-wrapper-java-jni openjdk-21-jre openjdk-21-jre-headless\nSuggested packages:\n  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n  | fonts-wqy-zenhei fonts-indic\nThe following NEW packages will be installed:\n  ca-certificates-java default-jre default-jre-headless java-common\n  libatk-wrapper-java libatk-wrapper-java-jni openjdk-21-jre\n  openjdk-21-jre-headless\n0 upgraded, 8 newly installed, 0 to remove and 14 not upgraded.\nNeed to get 46.9 MB of archives.\nAfter this operation, 203 MB of additional disk space will be used.\nGet:1 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 ca-certificates-java all 20240118 [11.6 kB]\nGet:2 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 java-common all 0.75+exp1 [6,798 B]\nGet:3 http://ch.archive.ubuntu.com/ubuntu noble-updates/main amd64 openjdk-21-jre-headless amd64 21.0.4+7-1ubuntu2~24.04 [46.6 MB]\nGet:4 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 default-jre-headless amd64 2:1.21-75+exp1 [3,094 B]\nGet:5 http://ch.archive.ubuntu.com/ubuntu noble-updates/main amd64 openjdk-21-jre amd64 21.0.4+7-1ubuntu2~24.04 [227 kB]\nGet:6 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 default-jre amd64 2:1.21-75+exp1 [922 B]\nGet:7 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 libatk-wrapper-java all 0.40.0-3build2 [54.3 kB]\nGet:8 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 libatk-wrapper-java-jni amd64 0.40.0-3build2 [46.4 kB]\nFetched 46.9 MB in 1s (38.9 MB/s)                  \nSelecting previously unselected package ca-certificates-java.\n(Reading database ... 430136 files and directories currently installed.)\nPreparing to unpack .../0-ca-certificates-java_20240118_all.deb ...\nUnpacking ca-certificates-java (20240118) ...\nSelecting previously unselected package java-common.\nPreparing to unpack .../1-java-common_0.75+exp1_all.deb ...\nUnpacking java-common (0.75+exp1) ...\nSelecting previously unselected package openjdk-21-jre-headless:amd64.\nPreparing to unpack .../2-openjdk-21-jre-headless_21.0.4+7-1ubuntu2~24.04_amd64.deb ...\nUnpacking openjdk-21-jre-headless:amd64 (21.0.4+7-1ubuntu2~24.04) ...\nSelecting previously unselected package default-jre-headless.\nPreparing to unpack .../3-default-jre-headless_2%3a1.21-75+exp1_amd64.deb ...\nUnpacking default-jre-headless (2:1.21-75+exp1) ...\nSelecting previously unselected package openjdk-21-jre:amd64.\nPreparing to unpack .../4-openjdk-21-jre_21.0.4+7-1ubuntu2~24.04_amd64.deb ...\nUnpacking openjdk-21-jre:amd64 (21.0.4+7-1ubuntu2~24.04) ...\nSelecting previously unselected package default-jre.\nPreparing to unpack .../5-default-jre_2%3a1.21-75+exp1_amd64.deb ...\nUnpacking default-jre (2:1.21-75+exp1) ...\nSelecting previously unselected package libatk-wrapper-java.\nPreparing to unpack .../6-libatk-wrapper-java_0.40.0-3build2_all.deb ...\nUnpacking libatk-wrapper-java (0.40.0-3build2) ...\nSelecting previously unselected package libatk-wrapper-java-jni:amd64.\nPreparing to unpack .../7-libatk-wrapper-java-jni_0.40.0-3build2_amd64.deb ...\nUnpacking libatk-wrapper-java-jni:amd64 (0.40.0-3build2) ...\nSetting up java-common (0.75+exp1) ...\nSetting up libatk-wrapper-java (0.40.0-3build2) ...\nSetting up ca-certificates-java (20240118) ...\nNo JRE found. Skipping Java certificates setup.\nSetting up openjdk-21-jre-headless:amd64 (21.0.4+7-1ubuntu2~24.04) ...\nupdate-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/jpackage to provide /usr/bin/jpackage (jpackage) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\nupdate-alternatives: using /usr/lib/jvm/java-21-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\nSetting up libatk-wrapper-java-jni:amd64 (0.40.0-3build2) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for ca-certificates-java (20240118) ...\nAdding debian:ACCVRAIZ1.pem\nAdding debian:AC_RAIZ_FNMT-RCM.pem\nAdding debian:AC_RAIZ_FNMT-RCM_SERVIDORES_SEGUROS.pem\nAdding debian:ANF_Secure_Server_Root_CA.pem\nAdding debian:Actalis_Authentication_Root_CA.pem\nAdding debian:AffirmTrust_Commercial.pem\nAdding debian:AffirmTrust_Networking.pem\nAdding debian:AffirmTrust_Premium.pem\nAdding debian:AffirmTrust_Premium_ECC.pem\nAdding debian:Amazon_Root_CA_1.pem\nAdding debian:Amazon_Root_CA_2.pem\nAdding debian:Amazon_Root_CA_3.pem\nAdding debian:Amazon_Root_CA_4.pem\nAdding debian:Atos_TrustedRoot_2011.pem\nAdding debian:Atos_TrustedRoot_Root_CA_ECC_TLS_2021.pem\nAdding debian:Atos_TrustedRoot_Root_CA_RSA_TLS_2021.pem\nAdding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\nAdding debian:BJCA_Global_Root_CA1.pem\nAdding debian:BJCA_Global_Root_CA2.pem\nAdding debian:Baltimore_CyberTrust_Root.pem\nAdding debian:Buypass_Class_2_Root_CA.pem\nAdding debian:Buypass_Class_3_Root_CA.pem\nAdding debian:CA_Disig_Root_R2.pem\nAdding debian:CFCA_EV_ROOT.pem\nAdding debian:COMODO_Certification_Authority.pem\nAdding debian:COMODO_ECC_Certification_Authority.pem\nAdding debian:COMODO_RSA_Certification_Authority.pem\nAdding debian:Certainly_Root_E1.pem\nAdding debian:Certainly_Root_R1.pem\nAdding debian:Certigna.pem\nAdding debian:Certigna_Root_CA.pem\nAdding debian:Certum_EC-384_CA.pem\nAdding debian:Certum_Trusted_Network_CA.pem\nAdding debian:Certum_Trusted_Network_CA_2.pem\nAdding debian:Certum_Trusted_Root_CA.pem\nAdding debian:CommScope_Public_Trust_ECC_Root-01.pem\nAdding debian:CommScope_Public_Trust_ECC_Root-02.pem\nAdding debian:CommScope_Public_Trust_RSA_Root-01.pem\nAdding debian:CommScope_Public_Trust_RSA_Root-02.pem\nAdding debian:Comodo_AAA_Services_root.pem\nAdding debian:D-TRUST_BR_Root_CA_1_2020.pem\nAdding debian:D-TRUST_EV_Root_CA_1_2020.pem\nAdding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\nAdding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\nAdding debian:DigiCert_Assured_ID_Root_CA.pem\nAdding debian:DigiCert_Assured_ID_Root_G2.pem\nAdding debian:DigiCert_Assured_ID_Root_G3.pem\nAdding debian:DigiCert_Global_Root_CA.pem\nAdding debian:DigiCert_Global_Root_G2.pem\nAdding debian:DigiCert_Global_Root_G3.pem\nAdding debian:DigiCert_High_Assurance_EV_Root_CA.pem\nAdding debian:DigiCert_TLS_ECC_P384_Root_G5.pem\nAdding debian:DigiCert_TLS_RSA4096_Root_G5.pem\nAdding debian:DigiCert_Trusted_Root_G4.pem\nAdding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\nAdding debian:Entrust_Root_Certification_Authority.pem\nAdding debian:Entrust_Root_Certification_Authority_-_EC1.pem\nAdding debian:Entrust_Root_Certification_Authority_-_G2.pem\nAdding debian:Entrust_Root_Certification_Authority_-_G4.pem\nAdding debian:GDCA_TrustAUTH_R5_ROOT.pem\nAdding debian:GLOBALTRUST_2020.pem\nAdding debian:GTS_Root_R1.pem\nAdding debian:GTS_Root_R2.pem\nAdding debian:GTS_Root_R3.pem\nAdding debian:GTS_Root_R4.pem\nAdding debian:GlobalSign_ECC_Root_CA_-_R4.pem\nAdding debian:GlobalSign_ECC_Root_CA_-_R5.pem\nAdding debian:GlobalSign_Root_CA.pem\nAdding debian:GlobalSign_Root_CA_-_R3.pem\nAdding debian:GlobalSign_Root_CA_-_R6.pem\nAdding debian:GlobalSign_Root_E46.pem\nAdding debian:GlobalSign_Root_R46.pem\nAdding debian:Go_Daddy_Class_2_CA.pem\nAdding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\nAdding debian:HARICA_TLS_ECC_Root_CA_2021.pem\nAdding debian:HARICA_TLS_RSA_Root_CA_2021.pem\nAdding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\nAdding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\nAdding debian:HiPKI_Root_CA_-_G1.pem\nAdding debian:Hongkong_Post_Root_CA_3.pem\nAdding debian:ISRG_Root_X1.pem\nAdding debian:ISRG_Root_X2.pem\nAdding debian:IdenTrust_Commercial_Root_CA_1.pem\nAdding debian:IdenTrust_Public_Sector_Root_CA_1.pem\nAdding debian:Izenpe.com.pem\nAdding debian:Microsec_e-Szigno_Root_CA_2009.pem\nAdding debian:Microsoft_ECC_Root_Certificate_Authority_2017.pem\nAdding debian:Microsoft_RSA_Root_Certificate_Authority_2017.pem\nAdding debian:NAVER_Global_Root_Certification_Authority.pem\nWarning: there was a problem reading the certificate file /etc/ssl/certs/NetLock_Arany_=Class_Gold=_F?tan?s?tv?ny.pem. Message:\n  /etc/ssl/certs/NetLock_Arany_=Class_Gold=_F?tan?s?tv?ny.pem (No such file or directory)\nAdding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\nAdding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\nAdding debian:QuoVadis_Root_CA_1_G3.pem\nAdding debian:QuoVadis_Root_CA_2.pem\nAdding debian:QuoVadis_Root_CA_2_G3.pem\nAdding debian:QuoVadis_Root_CA_3.pem\nAdding debian:QuoVadis_Root_CA_3_G3.pem\nAdding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\nAdding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\nAdding debian:SSL.com_Root_Certification_Authority_ECC.pem\nAdding debian:SSL.com_Root_Certification_Authority_RSA.pem\nAdding debian:SSL.com_TLS_ECC_Root_CA_2022.pem\nAdding debian:SSL.com_TLS_RSA_Root_CA_2022.pem\nAdding debian:SZAFIR_ROOT_CA2.pem\nAdding debian:Sectigo_Public_Server_Authentication_Root_E46.pem\nAdding debian:Sectigo_Public_Server_Authentication_Root_R46.pem\nAdding debian:SecureSign_RootCA11.pem\nAdding debian:SecureTrust_CA.pem\nAdding debian:Secure_Global_CA.pem\nAdding debian:Security_Communication_ECC_RootCA1.pem\nAdding debian:Security_Communication_RootCA2.pem\nAdding debian:Security_Communication_RootCA3.pem\nAdding debian:Security_Communication_Root_CA.pem\nAdding debian:Starfield_Class_2_CA.pem\nAdding debian:Starfield_Root_Certificate_Authority_-_G2.pem\nAdding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\nAdding debian:SwissSign_Gold_CA_-_G2.pem\nAdding debian:SwissSign_Silver_CA_-_G2.pem\nAdding debian:T-TeleSec_GlobalRoot_Class_2.pem\nAdding debian:T-TeleSec_GlobalRoot_Class_3.pem\nAdding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\nAdding debian:TWCA_Global_Root_CA.pem\nAdding debian:TWCA_Root_Certification_Authority.pem\nAdding debian:TeliaSonera_Root_CA_v1.pem\nAdding debian:Telia_Root_CA_v2.pem\nAdding debian:TrustAsia_Global_Root_CA_G3.pem\nAdding debian:TrustAsia_Global_Root_CA_G4.pem\nAdding debian:Trustwave_Global_Certification_Authority.pem\nAdding debian:Trustwave_Global_ECC_P256_Certification_Authority.pem\nAdding debian:Trustwave_Global_ECC_P384_Certification_Authority.pem\nAdding debian:TunTrust_Root_CA.pem\nAdding debian:UCA_Extended_Validation_Root.pem\nAdding debian:UCA_Global_G2_Root.pem\nAdding debian:USERTrust_ECC_Certification_Authority.pem\nAdding debian:USERTrust_RSA_Certification_Authority.pem\nAdding debian:XRamp_Global_CA_Root.pem\nAdding debian:certSIGN_ROOT_CA.pem\nAdding debian:certSIGN_Root_CA_G2.pem\nAdding debian:e-Szigno_Root_CA_2017.pem\nAdding debian:ePKI_Root_Certification_Authority.pem\nAdding debian:emSign_ECC_Root_CA_-_C3.pem\nAdding debian:emSign_ECC_Root_CA_-_G3.pem\nAdding debian:emSign_Root_CA_-_C1.pem\nAdding debian:emSign_Root_CA_-_G1.pem\nAdding debian:vTrus_ECC_Root_CA.pem\nAdding debian:vTrus_Root_CA.pem\ndone.\nSetting up openjdk-21-jre:amd64 (21.0.4+7-1ubuntu2~24.04) ...\nSetting up default-jre-headless (2:1.21-75+exp1) ...\nSetting up default-jre (2:1.21-75+exp1) ...\n</code></pre> <p>The last action (<code>Adding debian:AutoFirma_ROOT.pem</code>) is critical for the installation of <code>AutoFirma</code>:</p> <code># gdebi ./AutoFirma_1_8_3.deb configuradorfnmt_4.0.6_amd64.deb</code> <pre><code># gdebi ./AutoFirma_1_8_3.deb configuradorfnmt_4.0.6_amd64.deb\n/usr/bin/gdebi:113: SyntaxWarning: invalid escape sequence '\\S'\n  c = findall(\"[[(](\\S+)/\\S+[])]\", msg)[0].lower()\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nReading state information... Done\n\nAutoFirma - Cliente @firma\nDo you want to install the software package? [y/N]:y\n/usr/bin/gdebi:113: FutureWarning: Possible nested set at position 1\n  c = findall(\"[[(](\\S+)/\\S+[])]\", msg)[0].lower()\n(Reading database ... 430504 files and directories currently installed.)\nPreparing to unpack ./AutoFirma_1_8_3.deb ...\nUpdating certificates in /etc/ssl/certs...\n0 added, 0 removed; done.\nRunning hooks in /etc/ca-certificates/update.d...\ndone.\nSe ha borrado el certificado CA en el almacenamiento del sistema\nUnpacking autofirma (1.8.3) over (1.8.3) ...\nDesinstalaci\u00f3n completada con exito\nSetting up autofirma (1.8.3) ...\nNov 08, 2024 12:17:11 AM es.gob.afirma.standalone.configurator.AutoFirmaConfigurator &lt;init&gt;\nINFO: Se configurara la aplicacion en modo nativo\nNov 08, 2024 12:17:11 AM es.gob.afirma.standalone.configurator.ConsoleManager getConsole\nINFO: Se utilizara la consola de tipo I/O\nNov 08, 2024 12:17:11 AM es.gob.afirma.standalone.configurator.ConfiguratorLinux configure\nINFO: Identificando directorio de aplicaci\u00f3n...\nNov 08, 2024 12:17:11 AM es.gob.afirma.standalone.configurator.ConfiguratorLinux configure\nINFO: Directorio de aplicaci\u00f3n: /usr/lib/AutoFirma\nNov 08, 2024 12:17:11 AM es.gob.afirma.standalone.configurator.ConfiguratorLinux configure\nINFO: Generando certificado para la comunicaci\u00f3n con el navegador web...\nNov 08, 2024 12:17:12 AM es.gob.afirma.standalone.configurator.ConfiguratorLinux configure\nINFO: Se guarda el almac\u00e9n de claves en el directorio de instalaci\u00f3n de la aplicaci\u00f3n\nNov 08, 2024 12:17:12 AM es.gob.afirma.standalone.configurator.ConfiguratorLinux configure\nINFO: Se va a instalar el certificado en el almacen de Mozilla Firefox\nNov 08, 2024 12:17:12 AM es.gob.afirma.standalone.configurator.ConfiguratorFirefoxLinux createScriptsToSystemKeyStore\nINFO: Comprobamos que se encuentre certutil en el sistema\nNov 08, 2024 12:17:12 AM es.gob.afirma.standalone.configurator.ConfiguratorLinux configure\nINFO: Fin de la configuraci\u00f3n\nGeneracion de certificados\nInstalacion del certificado CA en el almacenamiento de Firefox y Chrome\nUpdating certificates in /etc/ssl/certs...\nrehash: warning: skipping ca-certificates.crt,it does not contain exactly one certificate or CRL\n1 added, 0 removed; done.\nRunning hooks in /etc/ca-certificates/update.d...\ndone.\nInstalacion del certificado CA en el almacenamiento del sistema\nProcessing triggers for ca-certificates-java (20240118) ...\nAdding debian:AutoFirma_ROOT.pem\ndone.\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\n</code></pre> Without a JRE installed, the installation fails. <p>Without a JRE installed, the installation of <code>AutoFirma</code> will fails when <code>AutoFirma_ROOT.pem</code> cannot be found:</p> <pre><code># unzip AutoFirma_Linux_Debian.zip\n# gdebi ./AutoFirma_1_8_3.deb configuradorfnmt_4.0.6_amd64.deb \n/usr/bin/gdebi:113: SyntaxWarning: invalid escape sequence '\\S'\n  c = findall(\"[[(](\\S+)/\\S+[])]\", msg)[0].lower()\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nReading state information... Done\n\nAutoFirma - Cliente @firma\nDo you want to install the software package? [y/N]:y\n/usr/bin/gdebi:113: FutureWarning: Possible nested set at position 1\n  c = findall(\"[[(](\\S+)/\\S+[])]\", msg)[0].lower()\nSelecting previously unselected package autofirma.\n(Reading database ... 430119 files and directories currently installed.)\nPreparing to unpack ./AutoFirma_1_8_3.deb ...\nUnpacking autofirma (1.8.3) ...\nSetting up autofirma (1.8.3) ...\n/var/lib/dpkg/info/autofirma.postinst: 3: java: not found\nGeneracion de certificados\nInstalacion del certificado CA en el almacenamiento de Firefox y Chrome\nCould not open file or uri for loading certificate from /usr/lib/AutoFirma/AutoFirma_ROOT.cer\n40474AFDD37D0000:error:16000069:STORE routines:ossl_store_get0_loader_int:unregistered scheme:../crypto/store/store_register.c:237:scheme=file\n40474AFDD37D0000:error:80000002:system library:file_open:No such file or directory:../providers/implementations/storemgmt/file_store.c:267:calling stat(/usr/lib/AutoFirma/AutoFirma_ROOT.cer)\nUnable to load certificate\nmv: cannot stat '/usr/lib/AutoFirma/AutoFirma_ROOT.pem': No such file or directory\ncp: cannot stat '/usr/lib/AutoFirma/AutoFirma_ROOT.crt': No such file or directory\ncp: cannot stat '/usr/lib/AutoFirma/AutoFirma_ROOT.crt': No such file or directory\nUpdating certificates in /etc/ssl/certs...\n0 added, 0 removed; done.\nRunning hooks in /etc/ca-certificates/update.d...\ndone.\nInstalacion del certificado CA en el almacenamiento del sistema\nrm: cannot remove '/usr/lib/AutoFirma/script.sh': No such file or directory\nrm: cannot remove '/usr/lib/AutoFirma/AutoFirma_ROOT.crt': No such file or directory\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#displaycal","title":"DisplayCal","text":"<p>DisplayCAL is no longer maintained, it was dropped from Ubuntu 20.04 because it would not work with Python 3, but was still possible to build with python2.7 packages. Later, that was no longer possible in Ubuntu 22.04, so a new port to Python 3 was started: the DisplayCAL Python 3 Project.</p> <p>Back in late 2022, the best method around was in (French) DisplayCAL en Python 3 and required only a few basic packages.</p> <p>As or late 2024, the new project has its own Installation Instructions (Linux) but in Ubuntu Studio 24.04 none of this is necessary; <code>apt install displaycal</code> will do.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#prusa-slicer","title":"Prusa Slicer","text":"<p>To install PrusaSlicer first need to Set Up Flathub in Ubuntu:</p> <code># apt install flatpak -y</code> <pre><code># apt install flatpak -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  libmalcontent-0-0 libostree-1-1\nSuggested packages:\n  malcontent-gui\nThe following NEW packages will be installed:\n  flatpak libmalcontent-0-0 libostree-1-1\n0 upgraded, 3 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 1,745 kB of archives.\nAfter this operation, 6,553 kB of additional disk space will be used.\nGet:1 http://ch.archive.ubuntu.com/ubuntu noble-updates/main amd64 libmalcontent-0-0 amd64 0.11.1-1ubuntu1 [22.3 kB]\nGet:2 http://archive.ubuntu.com/ubuntu noble/universe amd64 libostree-1-1 amd64 2024.5-1build2 [373 kB]\nGet:3 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 flatpak amd64 1.14.6-1ubuntu0.1 [1,350 kB]\nFetched 1,745 kB in 1s (3,120 kB/s)\nSelecting previously unselected package libmalcontent-0-0:amd64.\n(Reading database ... 489602 files and directories currently installed.)\nPreparing to unpack .../libmalcontent-0-0_0.11.1-1ubuntu1_amd64.deb ...\nUnpacking libmalcontent-0-0:amd64 (0.11.1-1ubuntu1) ...\nSelecting previously unselected package libostree-1-1:amd64.\nPreparing to unpack .../libostree-1-1_2024.5-1build2_amd64.deb ...\nUnpacking libostree-1-1:amd64 (2024.5-1build2) ...\nSelecting previously unselected package flatpak.\nPreparing to unpack .../flatpak_1.14.6-1ubuntu0.1_amd64.deb ...\nUnpacking flatpak (1.14.6-1ubuntu0.1) ...\nSetting up libostree-1-1:amd64 (2024.5-1build2) ...\nSetting up libmalcontent-0-0:amd64 (0.11.1-1ubuntu1) ...\nSetting up flatpak (1.14.6-1ubuntu0.1) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for dbus (1.14.10-4ubuntu4.1) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.3) ...\n</code></pre> <p>For the time being, the only repository will be <code>flathub.org</code>:</p> <pre><code># flatpak remote-add --if-not-exists flathub \\\n  https://dl.flathub.org/repo/flathub.flatpakrepo\n</code></pre> <p>Once the repository is added, install the application:</p> <code># flatpak install flathub com.prusa3d.PrusaSlicer</code> <pre><code># flatpak install flathub com.prusa3d.PrusaSlicer\nLooking for matches\u2026\nRequired runtime for com.prusa3d.PrusaSlicer/x86_64/stable (runtime/org.gnome.Platform/x86_64/47) found in remote flathub\nDo you want to install it? [Y/n]: \n\n(flatpak install:1943373): dconf-WARNING **: 13:36:55.591: unable to open file '/etc/dconf/db/site': Failed to open file \u201c/etc/dconf/db/site\u201d: open() failed: No such file or directory; expect degraded performance\n\ncom.prusa3d.PrusaSlicer permissions:\n    ipc                  network              x11                    devices\n    file access [1]      dbus access [2]      bus ownership [3]      system dbus access [4]\n\n    [1] /media, /run/media, home, xdg-run/gvfs\n    [2] com.prusa3d.prusaslicer.InstanceCheck.*, org.freedesktop.DBus.Introspectable.*\n    [3] com.prusa3d.prusaslicer.*\n    [4] org.freedesktop.UDisks2\n\n\n        ID                                         Branch     Op Remote  Download\n1. [\u2713] com.prusa3d.PrusaSlicer.Locale             stable     i  flathub 137.4\u00a0kB / 11.6\u00a0MB\n2. [\u2713] org.freedesktop.Platform.GL.default        24.08      i  flathub 156.1\u00a0MB / 156.3\u00a0MB\n3. [\u2713] org.freedesktop.Platform.GL.default        24.08extra i  flathub  25.1\u00a0MB / 156.3\u00a0MB\n4. [\u2713] org.freedesktop.Platform.GL.nvidia-550-120 1.4        i  flathub 308.2\u00a0MB / 308.2\u00a0MB\n5. [\u2713] org.freedesktop.Platform.openh264          2.4.1      i  flathub 920.7\u00a0kB / 976.5\u00a0kB\n6. [\u2713] org.gnome.Platform.Locale                  47         i  flathub  18.6\u00a0kB / 386.5\u00a0MB\n7. [\u2713] org.gnome.Platform                         47         i  flathub 326.6\u00a0MB / 384.1\u00a0MB\n8. [\u2713] com.prusa3d.PrusaSlicer                    stable     i  flathub  84.8\u00a0MB / 91.7\u00a0MB\n\nInstalling 8/8\u2026 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 100%  7.7\u00a0MB/s  00:00\n        ID                                         Branch     Op Remote  Download\n1. [\u2713] com.prusa3d.PrusaSlicer.Locale             stable     i  flathub 137.4\u00a0kB / 11.6\u00a0MB\n2. [\u2713] org.freedesktop.Platform.GL.default        24.08      i  flathub 156.1\u00a0MB / 156.3\u00a0MB\n3. [\u2713] org.freedesktop.Platform.GL.default        24.08extra i  flathub  25.1\u00a0MB / 156.3\u00a0MB\n4. [\u2713] org.freedesktop.Platform.GL.nvidia-550-120 1.4        i  flathub 308.2\u00a0MB / 308.2\u00a0MB\n5. [\u2713] org.freedesktop.Platform.openh264          2.4.1      i  flathub 920.7\u00a0kB / 976.5\u00a0kB\n6. [\u2713] org.gnome.Platform.Locale                  47         i  flathub  18.6\u00a0kB / 386.5\u00a0MB\n7. [\u2713] org.gnome.Platform                         47         i  flathub 326.6\u00a0MB / 384.1\u00a0MB\n8. [\u2713] com.prusa3d.PrusaSlicer                    stable     i  flathub  84.8\u00a0MB / 91.7\u00a0MB\n\nInstallation complete.\n</code></pre> <p>Then to run the application:</p> <pre><code>$ flatpak run com.prusa3d.PrusaSlicer\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#freecad-10","title":"FreeCAD 1.0","text":"<p>FreeCAD recent had a major 1.0 release that makes it a lot more palatable for many more people, so I gave it a try.</p> <p>Download the stable version, which is an AppImage file and can be updated easily with AppImageUpdate:</p> <pre><code>$ mkdir $HOME/Applications\n$ cd $HOME/Applications\n\n$ wget -q https://github.com/FreeCAD/FreeCAD/releases/download/1.0.0/FreeCAD_1.0.0-conda-Linux-x86_64-py311.AppImage\n\n$ wget -q https://github.com/AppImageCommunity/AppImageUpdate/releases/download/2.0.0-alpha-1-20241225/AppImageUpdate-x86_64.AppImage\n\n$ ./AppImageUpdate-x86_64.AppImage FreeCAD_1.0.0-conda-Linux-x86_64-py311.AppImage\nAppImageUpdate version 1-alpha (commit 362e637), build 223 built on 2024-12-25 15:11:36 UTC\nFetching latest release information from GitHub API\nUpdating from GitHub Releases via ZSync\nFetching latest release information from GitHub API\nzsync2: Using CA bundle found on system: /etc/ssl/certs/ca-certificates.crt\nzsync2: /home/ponder/Downloads/FreeCAD_1.0.0-conda-Linux-x86_64-py311.AppImage found, using as seed file\nzsync2: Target file: /home/ponder/Downloads/FreeCAD_1.0.0-conda-Linux-x86_64-py311.AppImage\nzsync2: Reading seed file: /home/ponder/Downloads/FreeCAD_1.0.0-conda-Linux-x86_64-py311.AppImage\nzsync2: Usable data from seed files: 97.175505%\nzsync2: Renaming temp file\nzsync2: Fetching remaining blocks\nzsync2: Downloading from https://objects.githubusercontent.com/github-production-release-asset-2e65be/93114989/a70d4be4-ab21-4577-9ddc-51f71f55709f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=releaseassetproduction%2F20250116%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20250116T194102Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=6cd137d7eea382edbbbd3cee03d5e401cc5995d9c912b2eb734cda90217999f8&amp;X-Amz-SignedHeaders=host&amp;response-content-disposition=attachment%3B%20filename%3DFreeCAD_1.0.0-conda-Linux-x86_64-py311.AppImage&amp;response-content-type=application%2Foctet-stream\nzsync2: optimized ranges, old requests count 254, new requests count 72\n\nzsync2: Verifying downloaded file\nzsync2: checksum matches OK\nzsync2: used 660779008 local, fetched 32539376\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#tweak-ui-font-size","title":"Tweak UI Font Size","text":"<p>FreeCAD does not have an easy way to increase UI font size globally, the CSS tweak here does not work, the recommended solution, while waiting for HiDPI support, is to scale the entire UI up on high resolution screens:</p> <pre><code>/usr/bin/env \\\n  QT_AUTO_SCREEN_SCALE_FACTOR=0 \\\n  QT_SCALE_FACTOR=1.5 \\\n  /home/ponder/Applications/FreeCAD_1.0.0-conda-Linux-x86_64-py311.AppImage\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#system-configuration","title":"System Configuration","text":"<p>The above having covered installing software, there are still system configurations that need to be tweaked.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#apt-respositories-clean-up","title":"APT respositories clean-up","text":"<p>Ubuntu Studio 24.04 seems to consistently need a little APT respositories clean-up; just comment out the last line in <code>/etc/apt/sources.list.d/dvd.list</code> to let <code>noble-security</code> be defined (only) in <code>ubuntu.sources</code>.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#ubuntu-pro","title":"Ubuntu Pro","text":"<p>When updating the system with <code>apt full-upgrade -y</code> a notice comes up about additional security updates:</p> <pre><code>Get more security updates through Ubuntu Pro with 'esm-apps' enabled:\n  libdcmtk17t64 libcjson1 libavdevice60 ffmpeg libpostproc57 libavcodec60\n  libavutil58 libswscale7 libswresample4 libavformat60 libavfilter9\nLearn more about Ubuntu Pro at https://ubuntu.com/pro\n</code></pre> <p>This being a new system, indeed it's not attached to an Ubuntu Pro account (the old system was):</p> <pre><code># pro security-status\n3213 packages installed:\n     1642 packages from Ubuntu Main/Restricted repository\n     1569 packages from Ubuntu Universe/Multiverse repository\n     1 package from a third party\n     1 package no longer available for download\n\nTo get more information about the packages, run\n    pro security-status --help\nfor a list of available options.\n\nThis machine is receiving security patching for Ubuntu Main/Restricted\nrepository until 2029.\nThis machine is NOT attached to an Ubuntu Pro subscription.\n\nUbuntu Pro with 'esm-infra' enabled provides security updates for\nMain/Restricted packages until 2034.\n\nUbuntu Pro with 'esm-apps' enabled provides security updates for\nUniverse/Multiverse packages until 2034. There are 11 pending security updates.\n\nTry Ubuntu Pro with a free personal subscription on up to 5 machines.\nLearn more at https://ubuntu.com/pro\n</code></pre> <p>After creating an Ubuntu account a token is available to use with <code>pro attach</code>:</p> <pre><code># pro attach ...\nEnabling Ubuntu Pro: ESM Apps\nUbuntu Pro: ESM Apps enabled\nEnabling Ubuntu Pro: ESM Infra\nUbuntu Pro: ESM Infra enabled\nEnabling Livepatch\nLivepatch enabled\nThis machine is now attached to 'Ubuntu Pro - free personal subscription'\n\nSERVICE          ENTITLED  STATUS       DESCRIPTION\nanbox-cloud      yes       disabled     Scalable Android in the cloud\nesm-apps         yes       enabled      Expanded Security Maintenance for Applications\nesm-infra        yes       enabled      Expanded Security Maintenance for Infrastructure\nlandscape        yes       disabled     Management and administration tool for Ubuntu\nlivepatch        yes       warning      Current kernel is not covered by livepatch\nrealtime-kernel* yes       disabled     Ubuntu kernel with PREEMPT_RT patches integrated\n\n * Service has variants\n\nNOTICES\nOperation in progress: pro attach\nThe current kernel (6.8.0-47-lowlatency, x86_64) is not covered by livepatch.\nCovered kernels are listed here: https://ubuntu.com/security/livepatch/docs/kernels\nEither switch to a covered kernel or `sudo pro disable livepatch` to dismiss this warning.\n\nFor a list of all Ubuntu Pro services and variants, run 'pro status --all'\nEnable services with: pro enable &lt;service&gt;\n\n     Account: ponder.stibbons@uu.am\nSubscription: Ubuntu Pro - free personal subscription\n\n# pro status --all\nSERVICE          ENTITLED  STATUS       DESCRIPTION\nanbox-cloud      yes       disabled     Scalable Android in the cloud\ncc-eal           yes       n/a          Common Criteria EAL2 Provisioning Packages\nesm-apps         yes       enabled      Expanded Security Maintenance for Applications\nesm-infra        yes       enabled      Expanded Security Maintenance for Infrastructure\nfips             yes       n/a          NIST-certified FIPS crypto packages\nfips-preview     yes       n/a          Preview of FIPS crypto packages undergoing certification with NIST\nfips-updates     yes       n/a          FIPS compliant crypto packages with stable security updates\nlandscape        yes       disabled     Management and administration tool for Ubuntu\nlivepatch        yes       warning      Current kernel is not covered by livepatch\nrealtime-kernel  yes       disabled     Ubuntu kernel with PREEMPT_RT patches integrated\n\u251c generic        yes       disabled     Generic version of the RT kernel (default)\n\u251c intel-iotg     yes       n/a          RT kernel optimized for Intel IOTG platform\n\u2514 raspi          yes       n/a          24.04 Real-time kernel optimised for Raspberry Pi\nros              yes       n/a          Security Updates for the Robot Operating System\nros-updates      yes       n/a          All Updates for the Robot Operating System\nusg              yes       n/a          Security compliance and audit tools\n\nNOTICES\nThe current kernel (6.8.0-47-lowlatency, x86_64) is not covered by livepatch.\nCovered kernels are listed here: https://ubuntu.com/security/livepatch/docs/kernels\nEither switch to a covered kernel or `sudo pro disable livepatch` to dismiss this warning.\n\nEnable services with: pro enable &lt;service&gt;\n</code></pre> <p>Now the system can be updated again with <code>apt full-upgrade -y</code> to receive those additional security updates:</p> <pre><code># apt full-upgrade -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\nThe following upgrades have been deferred due to phasing:\n  python3-distupgrade ubuntu-release-upgrader-core\n  ubuntu-release-upgrader-qt\nThe following packages will be upgraded:\n  ffmpeg libavcodec60 libavdevice60 libavfilter9 libavformat60 libavutil58\n  libcjson1 libdcmtk17t64 libpostproc57 libswresample4 libswscale7\n11 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.\n11 esm-apps security updates\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#fix-failed-locale-settings","title":"Fix failed locale settings","text":"<p>Every time <code>dpkg</code> runs, <code>perl</code> reports failed locale settings:</p> <pre><code>perl: warning: Setting locale failed.\nperl: warning: Please check that your locale settings:\n        LANGUAGE = \"en_US:en\",\n        LC_ALL = (unset),\n        LC_TIME = \"en_CH.UTF-8\",\n        LC_MONETARY = \"de_CH.UTF-8\",\n        LC_ADDRESS = \"C.UTF-8\",\n        LC_TELEPHONE = \"C.UTF-8\",\n        LC_NAME = \"C.UTF-8\",\n        LC_MEASUREMENT = \"de_CH.UTF-8\",\n        LC_IDENTIFICATION = \"C.UTF-8\",\n        LC_NUMERIC = \"de_CH.UTF-8\",\n        LC_PAPER = \"C.UTF-8\",\n        LANG = \"en_US.UTF-8\"\n    are supported and installed on your system.\nperl: warning: Falling back to a fallback locale (\"en_US.UTF-8\").\nlocale: Cannot set LC_ALL to default locale: No such file or directory\n</code></pre> <p>To fix this, set <code>LC_ALL</code> globally:</p> <pre><code># echo 'LC_ALL=\"en_US.UTF-8\"' &gt;&gt; /etc/environment\n</code></pre> <p>Re/generate the desired locales, e.g. at least <code>en_US.UTF-8</code>.</p> <pre><code># dpkg-reconfigure locales\nGenerating locales (this might take a while)...\n  en_US.UTF-8... done\n  ...\nGeneration complete.\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#make-dmesg-non-privileged","title":"Make <code>dmesg</code> non-privileged","text":"<p>Since Ubuntu 22.04, <code>dmesg</code> has become a privileged operation by default:</p> <pre><code>$ dmesg\ndmesg: read kernel buffer failed: Operation not permitted\n</code></pre> <p>This is controlled by </p> <pre><code># sysctl kernel.dmesg_restrict\nkernel.dmesg_restrict = 1\n</code></pre> <p>To revert this default, and make it permanent (source):</p> <pre><code># echo 'kernel.dmesg_restrict=0' | tee -a /etc/sysctl.d/99-sysctl.conf\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#make-sddm-look-good","title":"Make SDDM Look Good","text":"<p>Ubuntu Studio 24.04 uses  Simple Desktop Display Manager (SDDM) (sddm/sddm in GitHub) which is quite good looking out of the box, but I like to customize this for each computer.</p> <p>For most computers my favorite SDDM theme is Breeze-Noir-Dark, which I like to install system-wide.</p> <pre><code># unzip -d /usr/share/sddm/themes Breeze-Noir-Dark.zip\n</code></pre> <p>Action icons won\u2019t render if the directory name is changed.</p> <p>If needed, change the directory name in the <code>iconSource</code> fields in <code>Main.qml</code> to match final directory name so icons show. This is not the only thing that breaks when changing the directory name.</p> <p>Other than installing this theme, all I really change in it is the background image to use  The Rapture [3440x1440].</p> <p></p> <pre><code># mv welcome-to-rapture-opportunity-awaits-3440x1440.jpg \\\n  /usr/share/sddm/themes/Breeze-Noir-Dark/\n# cd /usr/share/sddm/themes/Breeze-Noir-Dark/\n\n# vi theme.conf\n[General]\ntype=image\ncolor=#132e43\nbackground=/usr/share/sddm/themes/Breeze-Noir-Dark/welcome-to-rapture-opportunity-awaits-3440x1440.jpg\n\n# vi theme.conf.user\n[General]\ntype=image\nbackground=welcome-to-rapture-opportunity-awaits-3440x1440.jpg\n</code></pre> <p>Additionally, as this is new in Ubuntu 24.04, the theme has to be selected by adding a <code>[Theme]</code> section in the system config in <code>/usr/lib/sddm/sddm.conf.d/ubuntustudio.conf</code></p> /usr/lib/sddm/sddm.conf.d/ubuntustudio.conf<pre><code>[General]\nInputMethod=\n\n[Theme]\nCurrent=\"Breeze-Noir-Dark\"\nEnableAvatars=True\n</code></pre> <p>Reportedly, you have to create the <code>/etc/sddm.conf.d</code> directory to add the Local configuration file that allows setting the theme:</p> <pre><code># mkdir /etc/sddm.conf.d\n# vi /etc/sddm.conf.d/ubuntustudio.conf\n</code></pre> <p>Besides setting the theme, it is also good to limit the range of user ids so that only human users show up:</p> /etc/sddm.conf.d/ubuntustudio.conf<pre><code>[Theme]\nCurrent=Breeze-Noir-Dark\n\n[Users]\nMaximumUid=1003\nMinimumUid=1000\n</code></pre> <p>It seems no longer necessary to manually add Redshift to one's desktop session. Previously, it would be necessary to launch Autostart and Add Application\u2026 to add Redshift.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#waiting-for-initial-location-to-become-available","title":"Waiting for initial location to become available...","text":"<p>However, redshift in Ubuntu 24.04 seems to be susceptible to crashing whenever toggleing it on/off or adjusting the color temperature. Despite the setting above to force specific coordinates manually, it tries to fetch location from an online service that is not available: a pop-up error tells:</p> <p>Redshift has terminated unexpectedly with exit code 0: Waiting for initial location to become available...</p> <p>The same is seen when running <code>redshift-qt</code> in a shell, then trying to adjust the color temperature:</p> <pre><code>$ redshift-qt\n\"Solar elevations: day above 3.0, night below -6.0\"\n\"Temperatures: 4500K at day, 3500K at night\"\n\"Brightness: 1.00:0.70\"\n\"Gamma (Daytime): 0.700, 0.700, 0.700\"\n\"Gamma (Night): 0.700, 0.700, 0.700\"\n\"Location: 48.00 N, 8.00 E\"\n\"Color temperature: 6500K\"\n\"Brightness: 1.00\"\n\"Status: Enabled\"\n\"Period: Night\"\n\"Color temperature: 3500K\"\n\"Brightness: 0.70\"\n\n\"Status: Disabled\"\n\"Period: None\"\n\"Color temperature: 6500K\"\n\"Brightness: 1.00\"\n\"Waiting for initial location to become available...\"\n</code></pre> <p>In the end, it was again necessary to manually add Redshift to the desktop session. Previously, it would be necessary to launch Autostart and Add Application\u2026 to add <code>redshift-qt</code>.</p> <p>This may be related to the Update app failing with Cannot Refresh Cache Whilst Offline, apparently becuase  Cockpit just 'needs' Network Manager, but there is a Solution, involving the creation of a fake network interface.</p> <pre><code># ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n       valid_lft forever preferred_lft forever\n2: enp5s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 04:42:1a:97:4e:47 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.2/24 brd 10.0.0.255 scope global enp5s0\n       valid_lft forever preferred_lft forever\n    inet 192.168.0.2/24 metric 100 brd 192.168.0.255 scope global dynamic enp5s0\n       valid_lft 85940sec preferred_lft 85940sec\n    inet6 fe80::642:1aff:fe97:4e47/64 scope link \n       valid_lft forever preferred_lft forever\n\n# nmcli con add type dummy con-name fake ifname fake0 ip4 1.2.3.4/24 gw4 1.2.3.1\nConnection 'fake' (f7fe724b-5aa8-4988-b21b-aa9dc96dae1a) successfully added.\n\n# ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n       valid_lft forever preferred_lft forever\n2: enp5s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 04:42:1a:97:4e:47 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.2/24 brd 10.0.0.255 scope global enp5s0\n       valid_lft forever preferred_lft forever\n    inet 192.168.0.2/24 metric 100 brd 192.168.0.255 scope global dynamic enp5s0\n       valid_lft 85937sec preferred_lft 85937sec\n    inet6 fe80::642:1aff:fe97:4e47/64 scope link \n       valid_lft forever preferred_lft forever\n3: fake0: &lt;BROADCAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/ether ce:56:b8:58:f8:32 brd ff:ff:ff:ff:ff:ff\n    inet 1.2.3.4/24 brd 1.2.3.255 scope global noprefixroute fake0\n       valid_lft forever preferred_lft forever\n</code></pre> <p>This doesn't seem to affect anything else's connectivity, but at least now the Updates application no longer fails.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#weekly-btrfs-scrub","title":"Weekly btrfs scrub","text":"<p>To keep BTRFS file systems healthy, it is recommended to run a weekly scrub to check everything for consistency. For this, I run the script from crontab every Saturday night.</p> <pre><code># wget -O /usr/local/bin/btrfs-scrub-all \\\n  http://marc.merlins.org/linux/scripts/btrfs-scrub\n\n# apt install inn -y\n\n# crontab -e\n...\n# m h  dom mon dow   command\n50 23 * * 6 /usr/local/bin/btrfs-scrub-all\n</code></pre> <p>Marc MERLIN keeps the script updated, so each systme may benefit from a few modifications, e.g. 1. Remove tests for laptop battery status, when running on a PC. 2. Set the <code>BTRFS_SCRUB_SKIP</code> to filter out partitions to skip.</p> /usr/local/bin/btrfs-scrub-all<pre><code>#! /bin/bash\n\n# By Marc MERLIN &lt;marc_soft@merlins.org&gt; 2014/03/20\n# License: Apache-2.0\n# http://marc.merlins.org/perso/btrfs/post_2014-03-19_Btrfs-Tips_-Btrfs-Scrub-and-Btrfs-Filesystem-Repair.html\n\nwhich btrfs &gt;/dev/null || exit 0\nexport PATH=/usr/local/bin:/sbin:$PATH\n\nFILTER='(^Dumping|balancing, usage)'\nBTRFS_SCRUB_SKIP=\"sda\"\nsource /etc/btrfs_config 2&gt;/dev/null\ntest -n \"$DEVS\" || DEVS=$(grep '\\&lt;btrfs\\&gt;' /proc/mounts | awk '{ print $1 }' | sort -u | grep -v $BTRFS_SCRUB_SKIP)\nfor btrfs in $DEVS\ndo\n    tail -n 0 -f /var/log/syslog | grep \"BTRFS\" | grep -Ev '(disk space caching is enabled|unlinked .* orphans|turning on discard|device label .* devid .* transid|enabling SSD mode|BTRFS: has skinny extents|BTRFS: device label|BTRFS info )' &amp;\n    mountpoint=\"$(grep \"$btrfs\" /proc/mounts | awk '{ print $2 }' | sort | head -1)\"\n    logger -s \"Quick Metadata and Data Balance of $mountpoint ($btrfs)\" &gt;&amp;2\n    # Even in 4.3 kernels, you can still get in places where balance\n    # won't work (no place left, until you run a -m0 one first)\n    # I'm told that proactively rebalancing metadata may not be a good idea.\n    #btrfs balance start -musage=20 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    # but a null rebalance should help corner cases:\n    sleep 10\n    btrfs balance start -musage=0 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    # After metadata, let's do data:\n    sleep 10\n    btrfs balance start -dusage=0 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    sleep 10\n    btrfs balance start -dusage=20 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    # And now we do scrub. Note that scrub can fail with \"no space left\n    # on device\" if you're very out of balance.\n    logger -s \"Starting scrub of $mountpoint\" &gt;&amp;2\n    echo btrfs scrub start -Bd $mountpoint\n    # -r is read only, but won't fix a redundant array.\n    #ionice -c 3 nice -10 btrfs scrub start -Bdr $mountpoint\n    time ionice -c 3 nice -10 btrfs scrub start -Bd $mountpoint\n    pkill -f 'tail -n 0 -f /var/log/syslog'\n    logger \"Ended scrub of $mountpoint\" &gt;&amp;2\ndone\n</code></pre> <p>Note</p> <p>Setting <code>BTRFS_SCRUB_SKIP=\"sda\"</code> prefents Btrfs balancing from running every week on the larger 6TB RAID 1 array of HDD, because that takes too long.</p> <p>The whole process takes about 15 minutes for the 4TB NVMe SSD, then something between 1 and 2 hours for each of the 4TB SATA SSDs:</p> <code># /usr/local/bin/btrfs-scrub-all</code> <pre><code># /usr/local/bin/btrfs-scrub-all\n&lt;13&gt;Nov 11 23:54:17 root: Quick Metadata and Data Balance of /home (/dev/nvme1n1p4)\nDone, had to relocate 0 out of 2695 chunks\nDone, had to relocate 0 out of 2695 chunks\nDone, had to relocate 1 out of 2695 chunks\n&lt;13&gt;Nov 11 23:54:48 root: Starting scrub of /home\nbtrfs scrub start -Bd /home\nStarting scrub on devid 1\n\nScrub device /dev/nvme1n1p4 (id 1) done\nScrub started:    Mon Nov 11 23:54:48 2024\nStatus:           finished\nDuration:         0:14:39\nTotal to scrub:   2.63TiB\nRate:             3.06GiB/s\nError summary:    no errors found\n\nreal    14m39.641s\nuser    0m0.001s\nsys     3m40.063s\n&lt;13&gt;Nov 12 00:09:27 root: Quick Metadata and Data Balance of /home/new-ssd (/dev/sdb)\nDone, had to relocate 0 out of 2086 chunks\nDone, had to relocate 0 out of 2086 chunks\nDone, had to relocate 42 out of 2086 chunks\n&lt;13&gt;Nov 12 00:10:22 root: Starting scrub of /home/new-ssd\nbtrfs scrub start -Bd /home/new-ssd\nStarting scrub on devid 1\n\nScrub device /dev/sdb (id 1) done\nScrub started:    Tue Nov 12 00:10:22 2024\nStatus:           finished\nDuration:         1:20:26\nTotal to scrub:   1.98TiB\nRate:             431.00MiB/s\nError summary:    no errors found\n\nreal    80m25.930s\nuser    0m0.004s\nsys     2m45.709s\n&lt;13&gt;Nov 12 01:30:48 root: Quick Metadata and Data Balance of /home/ssd (/dev/sdc)\nDone, had to relocate 0 out of 2937 chunks\nDone, had to relocate 0 out of 2937 chunks\nDone, had to relocate 0 out of 2937 chunks\n&lt;13&gt;Nov 12 01:31:18 root: Starting scrub of /home/ssd\nbtrfs scrub start -Bd /home/ssd\nStarting scrub on devid 1\n\nScrub device /dev/sdc (id 1) done\nScrub started:    Tue Nov 12 01:31:18 2024\nStatus:           finished\nDuration:         1:49:18\nTotal to scrub:   2.82TiB\nRate:             451.50MiB/s\nError summary:    no errors found\n\nreal    109m17.722s\nuser    0m0.002s\nsys     4m34.310s\n</code></pre> <p></p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#stop-apparmor-spew-in-the-logs","title":"Stop apparmor spew in the logs","text":"<p>Although also somewhat visible in Ubuntu Studio 22.04, the log spam from <code>audit</code> seems to flood <code>dmesg</code> output a lot more in 24.04:</p> <pre><code>audit: type=1107 audit(1731359486.714:247): pid=2329 uid=101 auid=4294967295 ses=4294967295 subj=unconfined msg='apparmor=\"DENIED\" operation=\"dbus_signal\"  bus=\"system\" path=\"/com/redhat/PrinterSpooler\" interface=\"com.redhat.PrinterSpooler\" member=\"QueueChanged\" mask=\"send\" name=\"org.freedesktop.DBus\" pid=3909 label=\"snap.cups.cupsd\" peer_pid=110058 peer_label=\"plasmashell\"\n                exe=\"/usr/bin/dbus-daemon\" sauid=101 hostname=? addr=? terminal=?'\n...\n                exe=\"/usr/bin/dbus-daemon\" sauid=101 hostname=? addr=? terminal=?'\naudit: type=1107 audit(1731360145.704:378): pid=2329 uid=101 auid=4294967295 ses=4294967295 subj=unconfined msg='apparmor=\"DENIED\" operation=\"dbus_signal\"  bus=\"system\" path=\"/com/redhat/PrinterSpooler\" interface=\"com.redhat.PrinterSpooler\" member=\"QueueChanged\" mask=\"send\" name=\"org.freedesktop.DBus\" pid=3909 label=\"snap.cups.cupsd\" peer_pid=110058 peer_label=\"plasmashell\"\n                exe=\"/usr/bin/dbus-daemon\" sauid=101 hostname=? addr=? terminal=?'\naudit: type=1400 audit(1731360145.807:379): apparmor=\"DENIED\" operation=\"capable\" class=\"cap\" profile=\"/usr/lib/snapd/snap-confine\" pid=344504 comm=\"snap-confine\" capability=12  capname=\"net_admin\"\naudit: type=1400 audit(1731360145.807:380): apparmor=\"DENIED\" operation=\"capable\" class=\"cap\" profile=\"/usr/lib/snapd/snap-confine\" pid=344504 comm=\"snap-confine\" capability=38  capname=\"perfmon\"\naudit: type=1400 audit(1731360145.998:381): apparmor=\"DENIED\" operation=\"capable\" class=\"cap\" profile=\"/usr/lib/snapd/snap-confine\" pid=344893 comm=\"snap-confine\" capability=12  capname=\"net_admin\"\naudit: type=1400 audit(1731360145.998:382): apparmor=\"DENIED\" operation=\"capable\" class=\"cap\" profile=\"/usr/lib/snapd/snap-confine\" pid=344893 comm=\"snap-confine\" capability=38  capname=\"perfmon\"\naudit: type=1107 audit(1731360150.704:383): pid=2329 uid=101 auid=4294967295 ses=4294967295 subj=unconfined msg='apparmor=\"DENIED\" operation=\"dbus_signal\"  bus=\"system\" path=\"/com/redhat/PrinterSpooler\" interface=\"com.redhat.PrinterSpooler\" member=\"QueueChanged\" mask=\"send\" name=\"org.freedesktop.DBus\" pid=3909 label=\"snap.cups.cupsd\" peer_pid=110058 peer_label=\"plasmashell\"\n                exe=\"/usr/bin/dbus-daemon\" sauid=101 hostname=? addr=? terminal=?'\n...\naudit: type=1107 audit(1731360733.595:484): pid=2329 uid=101 auid=4294967295 ses=4294967295 subj=unconfined msg='apparmor=\"DENIED\" operation=\"dbus_signal\"  bus=\"system\" path=\"/com/redhat/PrinterSpooler\" interface=\"com.redhat.PrinterSpooler\" member=\"QueueChanged\" mask=\"send\" name=\"org.freedesktop.DBus\" pid=3909 label=\"snap.cups.cupsd\" peer_pid=110058 peer_label=\"plasmashell\"\n                exe=\"/usr/bin/dbus-daemon\" sauid=101 hostname=? addr=? terminal=?'\n</code></pre> <p>No worries, it is rather easy to stop apparmor spew in dmesg, just install <code>auditd</code> and (most of) the messages will go to <code>/var/log/kern.log</code> instead.</p> <pre><code># apt install auditd -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  libauparse0t64\nSuggested packages:\n  audispd-plugins\nThe following NEW packages will be installed:\n  auditd libauparse0t64\n0 upgraded, 2 newly installed, 0 to remove and 5 not upgraded.\nNeed to get 274 kB of archives.\nAfter this operation, 893 kB of additional disk space will be used.\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#bluetooth-controller-and-devices","title":"Bluetooth controller and devices","text":"<p>Since the death of Wi-Fi and Bluetooth chipset in this PC motherboard, bluetooth devices have been out of the question but also not needed.</p> <p>However, in yet another attempt to get some of these devices to work, a cheap USB 4.0 ASUS USB-BT400 was added. Upon plugging it, the following shows up in <code>dmesg</code>:</p> <pre><code>usb 3-2: new full-speed USB device number 4 using xhci_hcd\nusb 3-2: New USB device found, idVendor=0b05, idProduct=17cb, bcdDevice= 1.12\nusb 3-2: New USB device strings: Mfr=1, Product=2, SerialNumber=3\nusb 3-2: Product: BCM20702A0\nusb 3-2: Manufacturer: Broadcom Corp\nusb 3-2: SerialNumber: 5CF370AB7D8C\nBluetooth: hci0: BCM: chip id 63\nBluetooth: hci0: BCM: features 0x07\nBluetooth: hci0: BCM20702A\nBluetooth: hci0: BCM20702A1 (001.002.014) build 0000\nBluetooth: hci0: BCM: firmware Patch file not found, tried:\nBluetooth: hci0: BCM: 'brcm/BCM20702A1-0b05-17cb.hcd'\nBluetooth: hci0: BCM: 'brcm/BCM-0b05-17cb.hcd'\nBluetooth: MGMT ver 1.22\n</code></pre> <p>Despite the messages about firmware patch files not being found, the controller seems to work just fine.</p> <p>However, the firmware can (and should) be updated from the winterheart/broadcom-bt-firmware repository. Following the Detection and manual installation method, use <code>BCM20702A1-0b05-17cb.hcd</code> (and not others):</p> <pre><code># cd /lib/firmware/brcm\n# ls -l | grep -i bcm\nlrwxrwxrwx 1 root root     21 Sep 13 10:49 BCM-0a5c-6410.hcd.zst -&gt; BCM-0bb4-0306.hcd.zst\n-rw-r--r-- 1 root root  36998 Sep 13 10:49 BCM-0bb4-0306.hcd.zst\n-rw-r--r-- 1 root root 171803 Sep 13 10:49 bcm4329-fullmac-4.bin.zst\n-rw-r--r-- 1 root root  27425 Sep 13 10:49 bcm43xx-0.fw.zst\n-rw-r--r-- 1 root root    115 Sep 13 10:49 bcm43xx_hdr-0.fw.zst\n\n# wget https://github.com/winterheart/broadcom-bt-firmware/blob/master/brcm/BCM20702A1-0b05-17cb.hcd\n</code></pre> <p>Unplug the controller, reboot the PC and plug the controller again. Then, the following shows up in <code>dmesg</code>:</p> <pre><code>usb 3-2: new full-speed USB device number 3 using xhci_hcd\nusb 3-2: New USB device found, idVendor=0b05, idProduct=17cb, bcdDevice= 1.12\nusb 3-2: New USB device strings: Mfr=1, Product=2, SerialNumber=3\nusb 3-2: Product: BCM20702A0\nusb 3-2: Manufacturer: Broadcom Corp\nusb 3-2: SerialNumber: 5CF370AB7D8C\nBluetooth: Core ver 2.22\nNET: Registered PF_BLUETOOTH protocol family\nBluetooth: HCI device and connection manager initialized\nBluetooth: HCI socket layer initialized\nBluetooth: L2CAP socket layer initialized\nBluetooth: SCO socket layer initialized\nusbcore: registered new interface driver btusb\nBluetooth: BNEP (Ethernet Emulation) ver 1.3\nBluetooth: BNEP filters: protocol multicast\nBluetooth: BNEP socket layer initialized\nBluetooth: hci0: BCM: chip id 63\nBluetooth: hci0: BCM: features 0x07\nBluetooth: hci0: BCM20702A\nBluetooth: hci0: BCM20702A1 (001.002.014) build 0000\nBluetooth: hci0: BCM20702A1 'brcm/BCM20702A1-0b05-17cb.hcd' Patch\nBluetooth: hci0: command 0x0a0a tx timeout\nBluetooth: hci0: BCM: Patch command 0a0a failed (-110)\nBluetooth: hci0: BCM: Patch failed (-110)\nBluetooth: hci0: command 0x1001 tx timeout\nBluetooth: hci0: BCM: Reading local version info failed (-110)\n</code></pre> <p>In the face of these timeouts, and considering the firmware file has not been updated in the last 8 years, it seems most sensible to remove it:</p> <pre><code># rm /lib/firmware/brcm/BCM20702A1-0b05-17cb.hcd\n</code></pre> <p>Unplug the controller, reboot the PC and plug the controller again. Then, the following shows up in <code>dmesg</code>:</p> <pre><code>usb 3-2: new full-speed USB device number 3 using xhci_hcd\nusb 3-2: New USB device found, idVendor=0b05, idProduct=17cb, bcdDevice= 1.12\nusb 3-2: New USB device strings: Mfr=1, Product=2, SerialNumber=3\nusb 3-2: Product: BCM20702A0\nusb 3-2: Manufacturer: Broadcom Corp\nusb 3-2: SerialNumber: 5CF370AB7D8C\nBluetooth: Core ver 2.22\nNET: Registered PF_BLUETOOTH protocol family\nBluetooth: HCI device and connection manager initialized\nBluetooth: HCI socket layer initialized\nBluetooth: L2CAP socket layer initialized\nBluetooth: SCO socket layer initialized\nusbcore: registered new interface driver btusb\nBluetooth: BNEP (Ethernet Emulation) ver 1.3\nBluetooth: BNEP filters: protocol multicast\nBluetooth: BNEP socket layer initialized\nBluetooth: hci0: BCM: chip id 63\nBluetooth: hci0: BCM: features 0x07\nBluetooth: hci0: BCM20702A\nBluetooth: hci0: BCM20702A1 (001.002.014) build 0000\nBluetooth: hci0: BCM: firmware Patch file not found, tried:\nBluetooth: hci0: BCM: 'brcm/BCM20702A1-0b05-17cb.hcd'\nBluetooth: hci0: BCM: 'brcm/BCM-0b05-17cb.hcd'\nBluetooth: MGMT ver 1.22\nNET: Registered PF_ALG protocol family\nBluetooth: RFCOMM TTY layer initialized\nBluetooth: RFCOMM socket layer initialized\nBluetooth: RFCOMM ver 1.11\n</code></pre> <p>Warning</p> <p>The above method is recommened in other forums, but there are reports of this method leading to bluetooth system stopped working, upon which the (only) recommended workaround is reinstall the system.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#playstation-dual-shock-4-controller","title":"PlayStation Dual Shock 4 controller","text":"<p>The Dual Shock 4 controller works perfectly (with all features) over any USB 2.0 cable, using over Bluetooth is not always really necessary.</p> <p>The controller is easy enough to pair (hold Share and press PS button) but after that the system always fails to connect to it. During this (failed) attempt, the following shows up in <code>dmesg</code>:</p> <pre><code>Bluetooth: HIDP (Human Interface Emulation) ver 1.2\nBluetooth: HIDP socket layer initialized\nplaystation 0005:054C:09CC.0019: unknown main item tag 0x0\nplaystation 0005:054C:09CC.0019: hidraw22: BLUETOOTH HID v1.00 Gamepad [Wireless Controller] on 5c:f3:70:ab:7d:8c\ninput: Wireless Controller as /devices/pci0000:00/0000:00:01.2/0000:02:00.0/0000:03:08.0/0000:06:00.3/usb3/3-2/3-2:1.0/bluetooth/hci0/hci0:12/0005:054C:09CC.0019/input/input46\ninput: Wireless Controller Motion Sensors as /devices/pci0000:00/0000:00:01.2/0000:02:00.0/0000:03:08.0/0000:06:00.3/usb3/3-2/3-2:1.0/bluetooth/hci0/hci0:12/0005:054C:09CC.0019/input/input47\ninput: Wireless Controller Touchpad as /devices/pci0000:00/0000:00:01.2/0000:02:00.0/0000:03:08.0/0000:06:00.3/usb3/3-2/3-2:1.0/bluetooth/hci0/hci0:12/0005:054C:09CC.0019/input/input48\nplaystation 0005:054C:09CC.0019: Registered DualShock4 controller hw_version=0x0000b400 fw_version=0x0000a007\n</code></pre> <p>Following the instructions to use <code>blueoothctl</code> in wiki.gentoo.org/wiki/Sony_DualShock#DualShock_4 also led to multiple failed attempts to connect, failing with <code>Failed to connect: org.bluez.Error.Failed br-connection-create-socket</code></p> <pre><code>[bluetooth]# agent on\nAgent is already registered\ndefault-agentdefault-agent\n[bluetooth]# Default agent request successful\n[bluetooth]# power on\n[bluetooth]# Changing power on succeeded\ndiscoverable onscoverable on\n[bluetooth]# hci0 new_settings: powered connectable bondable ssp br/edr le secure-conn \n[bluetooth]# hci0 new_settings: powered connectable discoverable bondable ssp br/edr le secure-conn \n[bluetooth]# Changing discoverable on succeeded\n[bluetooth]# [CHG] Controller 5C:F3:70:AB:7D:8C Discoverable: yes\n[bluetooth]# pairable on\n[bluetooth]# scan on\n[bluetooth]# SetDiscoveryFilter success\n[bluetooth]# Discovery started\n...\nDevice DC:0C:2D:DE:61:D1 Wireless Controller\n...\n[bluetooth]# [NEW] Device DC:0C:2D:DE:61:D1 Wireless Controller\npair DC:0C:2D:DE:61:D1C:2D:DE:61:D1\nAttempting to pair with DC:0C:2D:DE:61:D1\n[CHG] Device DC:0C:2D:DE:61:D1 Connected: yes\n[Wireless Controller]# [CHG] Device DC:0C:2D:DE:61:D1 Bonded: yes\n[Wireless Controller]# [CHG] Device DC:0C:2D:DE:61:D1 WakeAllowed: yes\n[Wireless Controller]# [CHG] Device DC:0C:2D:DE:61:D1 ServicesResolved: yes\n[Wireless Controller]# [CHG] Device DC:0C:2D:DE:61:D1 Paired: yes\n[Wireless Controller]# Pairing successful\n[Wireless Controller]# [CHG] Device DC:0C:2D:DE:61:D1 ServicesResolved: no\n[CHG] Device DC:0C:2D:DE:61:D1 Connected: no\n\n[bluetooth]# trust DC:0C:2D:DE:61:D1\n[bluetooth]# Changing DC:0C:2D:DE:61:D1 trust succeeded\n[bluetooth]# connect DC:0C:2D:DE:61:D1\nAttempting to connect to DC:0C:2D:DE:61:D1\n[bluetooth]# Failed to connect: org.bluez.Error.Failed br-connection-create-socket\n</code></pre> <p>After many failed attempts, and with no clear reason, a system notification popped up asking whether the trust the device, and upon selecting Always Trust (or similar) the connection was established and the controller was finally usable.</p> <p>Other reports recommend using <code>blueoothctl</code> and confirmed it worked, although in some cases it is necessary to re-pair and re-connect.</p> <p>For this purpose this handy <code>repair-bluetooth.sh</code> script is useful:</p> repair-bluetooth.sh<pre><code>DS4DEV=\"DC:0C:2D:DE:61:D1\" \nCMDS=(\"power on\" \"agent on\" \"default-agent\" \"remove ${DS4DEV}\" \"scan on\" \"pair ${DS4DEV}\" \"trust ${DS4DEV}\" \"connect ${DS4DEV}\" \"scan off\") \nfor CMD in \"${CMDS[@]}\"\ndo\n    sh -c echo \"$CMD\" | bluetoothctl\n    sleep 0.5\ndone\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#battery-level-and-charging-status","title":"Battery level and charging status","text":"<p>Once the Dual Shock 4 controller is connected, the battery charging status and current capacity can be checked with <code>upower</code>:</p> <pre><code>$ upower -e\n/org/freedesktop/UPower/devices/battery_hidpp_battery_0\n/org/freedesktop/UPower/devices/battery_ps_controller_battery_dco0co2dodeo61od1\n/org/freedesktop/UPower/devices/DisplayDevice\n\n$ upower -i /org/freedesktop/UPower/devices/battery_ps_controller_battery_dco0co2dodeo61od1\n  native-path:          ps-controller-battery-dc:0c:2d:de:61:d1\n  model:                Wireless Controller\n  serial:               dc:0c:2d:de:61:d1\n  power supply:         no\n  updated:              Thu 14 Nov 2024 08:40:39 PM CET (26 seconds ago)\n  has history:          yes\n  has statistics:       yes\n  touchpad\n    present:             yes\n    rechargeable:        yes\n    state:               charging\n    warning-level:       none\n    percentage:          25%\n    icon-name:          'battery-low-charging-symbolic'\n  History (charge):\n    1731613239  25.000  charging\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#smart-monitoring","title":"S.M.A.R.T. Monitoring","text":"<p>Install Smartmontools to setup up S.M.A.R.T. monitoring:</p> <pre><code># apt install smartmontools gsmartcontrol libnotify-bin -y\n</code></pre> <p>Create <code>/usr/local/bin/smartdnotify</code> to notify when errors are found:</p> /usr/local/bin/smartdnotify<pre><code>#!/bin/sh\ndata=/root/smart-latest-error\necho \"SMARTD_FAILTYPE=$SMARTD_FAILTYPE\" &gt;&gt; $data\necho \"SMARTD_MESSAGE=\u2019$SMARTD_MESSAGE\u2019\" &gt;&gt; $data\nsudo -u coder DISPLAY=:0 DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus notify-send \"S.M.A.R.T Error ($SMARTD_FAILTYPE)\" \"$SMARTD_MESSAGE\"  -i /usr/share/pixmaps/yoshimi.png\n</code></pre> <p>Configure <code>/etc/smartd.conf</code> to run this script:</p> <pre><code>DEVICESCAN -d removable -n standby -m root -M exec /usr/local/bin/smartdnotify\n</code></pre> <p>Restart the <code>smartd</code> service:</p> <pre><code># systemctl restart smartd.service\n</code></pre> <p>Then add a scrip to re-notify: <code>/usr/local/bin/smartd-renotify</code></p> /usr/local/bin/smartd-renotify<pre><code>#!/bin/sh\nlatest=/root/smart-latest-error\nif [ -f $latest ] ; then\n  . $latest\n  if [ -n \"$SMARTD_FAILTYPE\" ]\n  then\n    sudo -u coder DISPLAY=:0 DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus notify-send \"S.M.A.R.T Error ($SMARTD_FAILTYPE)\" \"$SMARTD_MESSAGE\"  -i /usr/share/pixmaps/yoshimi.png\n  fi\nfi\n</code></pre> <pre><code># chmod +x /usr/local/bin/smartdnotify /usr/local/bin/smartd-renotify\n# crontab -e\n*/5 * * * * /usr/local/bin/smartd-renotify\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#errorcount-in-nvme","title":"ErrorCount in NVME","text":"<p>Once in a while, the above monitoring reports an \u201cErrorCount\u201d error.</p> <pre><code># cat smart-latest-error \nSMARTD_FAILTYPE=\nSMARTD_MESSAGE=\nSMARTD_FAILTYPE=\nSMARTD_MESSAGE=\nSMARTD_FAILTYPE=\nSMARTD_MESSAGE=\n...\nSMARTD_FAILTYPE=ErrorCount\nSMARTD_MESSAGE='Device: /dev/nvme0, number of Error Log entries increased from 1000 to 1020'\nSMARTD_FAILTYPE=ErrorCount\nSMARTD_MESSAGE='Device: /dev/nvme1, number of Error Log entries increased from 338 to 368'\n</code></pre> <code># smartctl -a /dev/nvme0</code> <pre><code># smartctl -a /dev/nvme0\nsmartctl 7.4 2023-08-01 r5530 [x86_64-linux-6.8.0-47-lowlatency] (local build)\nCopyright (C) 2002-23, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nModel Number:                       Samsung SSD 970 EVO Plus 2TB\nSerial Number:                      S4J4NX0T216141L\nFirmware Version:                   2B2QEXM7\nPCI Vendor/Subsystem ID:            0x144d\nIEEE OUI Identifier:                0x002538\nTotal NVM Capacity:                 2,000,398,934,016 [2.00 TB]\nUnallocated NVM Capacity:           0\nController ID:                      4\nNVMe Version:                       1.3\nNumber of Namespaces:               1\nNamespace 1 Size/Capacity:          2,000,398,934,016 [2.00 TB]\nNamespace 1 Utilization:            1,235,084,509,184 [1.23 TB]\nNamespace 1 Formatted LBA Size:     512\nNamespace 1 IEEE EUI-64:            002538 5221b08748\nLocal Time is:                      Mon Nov 18 22:35:43 2024 CET\nFirmware Updates (0x16):            3 Slots, no Reset required\nOptional Admin Commands (0x0017):   Security Format Frmw_DL Self_Test\nOptional NVM Commands (0x005f):     Comp Wr_Unc DS_Mngmt Wr_Zero Sav/Sel_Feat Timestmp\nLog Page Attributes (0x03):         S/H_per_NS Cmd_Eff_Lg\nMaximum Data Transfer Size:         512 Pages\nWarning  Comp. Temp. Threshold:     85 Celsius\nCritical Comp. Temp. Threshold:     85 Celsius\n\nSupported Power States\nSt Op     Max   Active     Idle   RL RT WL WT  Ent_Lat  Ex_Lat\n0 +     7.50W       -        -    0  0  0  0        0       0\n1 +     5.90W       -        -    1  1  1  1        0       0\n2 +     3.60W       -        -    2  2  2  2        0       0\n3 -   0.0700W       -        -    3  3  3  3      210    1200\n4 -   0.0050W       -        -    4  4  4  4     2000    8000\n\nSupported LBA Sizes (NSID 0x1)\nId Fmt  Data  Metadt  Rel_Perf\n0 +     512       0         0\n\n=== START OF SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nSMART/Health Information (NVMe Log 0x02)\nCritical Warning:                   0x00\nTemperature:                        41 Celsius\nAvailable Spare:                    100%\nAvailable Spare Threshold:          10%\nPercentage Used:                    1%\nData Units Read:                    279,103,719 [142 TB]\nData Units Written:                 118,255,818 [60.5 TB]\nHost Read Commands:                 552,324,221\nHost Write Commands:                1,104,801,710\nController Busy Time:               10,076\nPower Cycles:                       800\nPower On Hours:                     4,834\nUnsafe Shutdowns:                   7\nMedia and Data Integrity Errors:    0\nError Information Log Entries:      1,048\nWarning  Comp. Temperature Time:    0\nCritical Comp. Temperature Time:    0\nTemperature Sensor 1:               41 Celsius\nTemperature Sensor 2:               44 Celsius\n\nError Information (NVMe Log 0x01, 16 of 64 entries)\nNum   ErrCount  SQId   CmdId  Status  PELoc          LBA  NSID    VS  Message\n  0       1048     0  0x0008  0x4004      -            0     0     -  Invalid Field in Command\n\nSelf-test Log (NVMe Log 0x06)\nSelf-test status: No self-test in progress\nNo Self-tests Logged\n</code></pre> <code># smartctl -a /dev/nvme1</code> <pre><code># smartctl -a /dev/nvme1\nsmartctl 7.4 2023-08-01 r5530 [x86_64-linux-6.8.0-47-lowlatency] (local build)\nCopyright (C) 2002-23, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nModel Number:                       KINGSTON SFYRD4000G\nSerial Number:                      50026B76866DA9CA\nFirmware Version:                   EIFK31.6\nPCI Vendor/Subsystem ID:            0x2646\nIEEE OUI Identifier:                0x0026b7\nTotal NVM Capacity:                 4,000,787,030,016 [4.00 TB]\nUnallocated NVM Capacity:           0\nController ID:                      1\nNVMe Version:                       1.4\nNumber of Namespaces:               1\nNamespace 1 Size/Capacity:          4,000,787,030,016 [4.00 TB]\nNamespace 1 Formatted LBA Size:     512\nNamespace 1 IEEE EUI-64:            0026b7 6866da9ca5\nLocal Time is:                      Mon Nov 18 22:36:00 2024 CET\nFirmware Updates (0x12):            1 Slot, no Reset required\nOptional Admin Commands (0x0017):   Security Format Frmw_DL Self_Test\nOptional NVM Commands (0x005d):     Comp DS_Mngmt Wr_Zero Sav/Sel_Feat Timestmp\nLog Page Attributes (0x08):         Telmtry_Lg\nMaximum Data Transfer Size:         512 Pages\nWarning  Comp. Temp. Threshold:     84 Celsius\nCritical Comp. Temp. Threshold:     89 Celsius\n\nSupported Power States\nSt Op     Max   Active     Idle   RL RT WL WT  Ent_Lat  Ex_Lat\n0 +     8.80W       -        -    0  0  0  0        0       0\n1 +     7.10W       -        -    1  1  1  1        0       0\n2 +     5.20W       -        -    2  2  2  2        0       0\n3 -   0.0620W       -        -    3  3  3  3     2500    7500\n4 -   0.0620W       -        -    4  4  4  4     2500    7500\n\nSupported LBA Sizes (NSID 0x1)\nId Fmt  Data  Metadt  Rel_Perf\n0 +     512       0         2\n1 -    4096       0         1\n\n=== START OF SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nSMART/Health Information (NVMe Log 0x02)\nCritical Warning:                   0x00\nTemperature:                        41 Celsius\nAvailable Spare:                    100%\nAvailable Spare Threshold:          10%\nPercentage Used:                    0%\nData Units Read:                    154,185,434 [78.9 TB]\nData Units Written:                 15,777,901 [8.07 TB]\nHost Read Commands:                 219,252,510\nHost Write Commands:                109,668,538\nController Busy Time:               653\nPower Cycles:                       174\nPower On Hours:                     2,079\nUnsafe Shutdowns:                   1\nMedia and Data Integrity Errors:    0\nError Information Log Entries:      472\nWarning  Comp. Temperature Time:    0\nCritical Comp. Temperature Time:    0\nTemperature Sensor 2:               73 Celsius\nThermal Temp. 1 Total Time:         6429\n\nError Information (NVMe Log 0x01, 16 of 63 entries)\nNum   ErrCount  SQId   CmdId  Status  PELoc          LBA  NSID    VS  Message\n  0        472     0  0x101b  0x4004      -            0     1     -  Invalid Field in Command\n  1        471     0  0x3001  0x4004      -            0     1     -  Invalid Field in Command\n  2        470     0  0x0003  0x4004  0x028            0     0     -  Invalid Field in Command\n  3        469     0  0x0011  0x4004      -            0     0     -  Invalid Field in Command\n\nSelf-test Log (NVMe Log 0x06)\nSelf-test status: No self-test in progress\nNo Self-tests Logged\n</code></pre> <p>However, most of the entries are the same: not an error at all:</p> <pre><code># nvme error-log /dev/nvme0 | grep status_field | sort | uniq -c\n    63 status_field    : 0(Successful Completion: The command completed without error)\n      1 status_field    : 0x2002(Invalid Field in Command: A reserved coded value or an unsupported value in a defined field)\n\n# nvme error-log /dev/nvme1 | grep status_field | sort | uniq -c\n    59 status_field    : 0(Successful Completion: The command completed without error)\n      4 status_field    : 0x2002(Invalid Field in Command: A reserved coded value or an unsupported value in a defined field)\n</code></pre> <p>Update <code>/usr/local/bin/smartdnotify</code> and <code>/usr/local/bin/smartd-renotify</code> to ignore only those entries that are not errors (<code>status_field: 0</code>):</p> /usr/local/bin/smartdnotify<pre><code>#!/bin/sh\n\n# Ignore NVME \"error\" entries that are not errors.\nelog=/root/smart-latest-error-log-entry\nnvme error-log /dev/nvme0 | tail -16 &gt; $elog\nnvme error-log /dev/nvme1 | tail -16 &gt;&gt; $elog\ngrep -iq 'status_field[[:blank:]]*: 0.SUCCESS' $elog &amp;&amp; exit 0\n\n# Otherwise, log and notify the error.\ndata=/root/smart-latest-error\necho \"SMARTD_FAILTYPE=$SMARTD_FAILTYPE\" &gt;&gt; $data\necho \"SMARTD_MESSAGE=\u2019$SMARTD_MESSAGE\u2019\" &gt;&gt; $data\nsudo -u coder DISPLAY=:0 DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus notify-send \"S.M.A.R.T Error ($SMARTD_FAILTYPE)\" \"$SMARTD_MESSAGE\"  -i /usr/share/pixmaps/yoshimi.png\n</code></pre> /usr/local/bin/smartd-renotify<pre><code>#!/bin/sh\n\n# Ignore NVME \"error\" entries that are not errors.\nelog=/root/smart-latest-error-log-entry\nnvme error-log /dev/nvme0 | tail -16 &gt; $elog\nnvme error-log /dev/nvme1 | tail -16 &gt;&gt; $elog\ngrep -iq 'status_field[[:blank:]]*: 0.SUCCESS' $elog &amp;&amp; exit 0\n\n# Otherwise, re-notify.\nlatest=/root/smart-latest-error\nif [ -f $latest ] ; then\n  . $latest\n  if [ -n \"$SMARTD_FAILTYPE\" ]\n  then\n    sudo -u coder DISPLAY=:0 DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus notify-send \"S.M.A.R.T Error ($SMARTD_FAILTYPE)\" \"$SMARTD_MESSAGE\"  -i /usr/share/pixmaps/yoshimi.png\n  fi\nfi\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#appendix","title":"Appendix","text":""},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#2nd-new-nvme-ssd","title":"2nd New NVMe SSD","text":"<p>Once the new system had been a reliable daily driver and there was no need to boot back into the old 2TB NVMe SSD, it was time to replace it with a new 4TB NVMe SSD (and, this time, one with a headsink).</p> <p>This new new SSD gets the same Partitions as the old new SSD; the goal is to be able to boot from either of these.</p> <pre><code># parted /dev/nvme0n1 --script -- mklabel gpt\n# parted -a optimal /dev/nvme0n1 mkpart primary fat32 0% 260MiB\n# parted -a optimal /dev/nvme0n1 mkpart primary ext4 260MiB 75GiB\n# parted -a optimal /dev/nvme0n1 mkpart primary ext4 75GiB 150GiB\n# parted -a optimal /dev/nvme0n1 mkpart primary btrfs 150GiB 100%\n# parted /dev/nvme0n1 toggle 1 boot\n# parted /dev/nvme0n1 print                         \nModel: KINGSTON SFYRDK4000G (nvme)\nDisk /dev/nvme0n1: 4001GB\nSector size (logical/physical): 512B/512B\nPartition Table: gpt\nDisk Flags: \n\nNumber  Start   End     Size    File system  Name     Flags\n 1      1049kB  273MB   272MB                primary  boot, esp\n 2      273MB   80.5GB  80.3GB               primary\n 3      80.5GB  161GB   80.5GB               primary\n 4      161GB   4001GB  3840GB               primary\n\n# fdisk -l /dev/nvme0n1 \nDisk /dev/nvme0n1: 3.64 TiB, 4000787030016 bytes, 7814037168 sectors\nDisk model: KINGSTON SFYRDK4000G                    \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisklabel type: gpt\nDisk identifier: 97991032-0F17-48B4-B658-C1D033F89E73\n\nDevice             Start        End    Sectors  Size Type\n/dev/nvme0n1p1      2048     532479     530432  259M EFI System\n/dev/nvme0n1p2    532480  157286399  156753920 74.7G Linux filesystem\n/dev/nvme0n1p3 157286400  314572799  157286400   75G Linux filesystem\n/dev/nvme0n1p4 314572800 7814035455 7499462656  3.5T Linux filesystem\n\n# mkfs.ext4 /dev/nvme0n1p2\n# mkfs.ext4 /dev/nvme0n1p3\n# mkfs.btrfs /dev/nvme0n1p4\n</code></pre> <p>This finally allows taking the old new 4TB NVMe SSD down from 81% to a more comfortable 69% so that installing big Steam games is not a problem.</p> <pre><code>$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme1n1p3   74G   33G   38G  47% /\n/dev/nvme1n1p1  259M  6.2M  253M   3% /boot/efi\n/dev/nvme1n1p2   74G   51G   19G  73% /jammy\n/dev/nvme1n1p4  3.5T  2.9T  680G  81% /home\n/dev/sdc        3.7T  2.9T  831G  78% /home/ssd\n/dev/sdb        3.7T  2.0T  1.7T  55% /home/new-ssd\n/dev/sda        5.5T  5.1T  425G  93% /home/raid\n/dev/nvme0n1p4  3.5T  5.8M  3.5T   1% /home/new-m2\n\n$ time rsync -ua /home/Fotos/ /home/new-m2/Fotos/\n\nsent 1,621,093,379,224 bytes  received 3,164,279 bytes  711,475,331.80 bytes/sec\ntotal size is 1,620,950,812,470  speedup is 1.00\n\nreal    37m57.967s\nuser    11m33.723s\nsys     31m42.640s\n\n$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme1n1p3   74G   33G   37G  47% /\n/dev/nvme1n1p1  259M  6.2M  253M   3% /boot/efi\n/dev/nvme1n1p2   74G   51G   19G  73% /jammy\n/dev/nvme1n1p4  3.5T  2.9T  680G  81% /home\n/dev/sdc        3.7T  2.9T  831G  78% /home/ssd\n/dev/sdb        3.7T  2.0T  1.7T  55% /home/new-ssd\n/dev/sda        5.5T  5.1T  425G  93% /home/raid\n/dev/nvme0n1p4  3.5T  1.5T  2.1T  43% /home/new-m2\n</code></pre> <p>There goes 1.5T of photos that can now be removed from the old new SSD, but as an additional precaution let <code>rsync</code> check that files really are identical on both disks now, using the <code>-c</code> flag to  skip based on checksum, not mod-time &amp; size:</p> <pre><code>$ time rsync -cuva /home/Fotos/ /home/new-m2/Fotos/\nsending incremental file list\n\nsent 6,297,699 bytes  received 4,541 bytes  4,783.48 bytes/sec\ntotal size is 1,620,950,812,470  speedup is 257,202.33\n\nreal    21m57.232s\nuser    2m20.143s\nsys     18m48.906s\n</code></pre> <p>At this point the old copy can be confidently removed; there are other copies elsewhere too, the checking of the checksum was intended to check the file integrity in the new new disk because it will now become the canonical copy that is backed up everywhere else:</p> <pre><code>$ rm -rf /home/Fotos/\n$ ln -sf /home/new-m2/Fotos/ $HOME/\n$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme1n1p3   74G   35G   35G  50% /\n/dev/nvme1n1p1  259M  6.2M  253M   3% /boot/efi\n/dev/nvme1n1p2   74G   51G   19G  73% /jammy\n/dev/nvme1n1p4  3.5T  1.4T  2.2T  39% /home\n/dev/sdc        3.7T  2.9T  831G  78% /home/ssd\n/dev/sdb        3.7T  2.0T  1.7T  55% /home/new-ssd\n/dev/sda        5.5T  5.1T  425G  93% /home/raid\n/dev/nvme0n1p4  3.5T  1.5T  2.1T  43% /home/new-m2\n</code></pre> <p>Once both NVMe drives are filled in, the  weekly btrfs scrub shows very different disk I/O and drastically different temperatures:</p> <p></p> <p></p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#10gbps-nic","title":"10Gbps NIC","text":"<p>When the Internet connexion was upgraded to 10Gbps fiber and, once the migration was finished, a Ethernet 10Gbps port turned out to be available in the router and very close to Rapture, it seemd like a great idea to upgrade Rapture by installing an Intel X550-T2 10Gbps Ethernet card and connect it directly to the 10Gbps port on the router.</p> <p>It works well, when it works, but it also constantly drops the link and spams <code>dmesg</code> with:</p> <pre><code>[24546.650824] ixgbe 0000:05:00.0 enp5s0f0: NIC Link is Down\n[24561.169494] ixgbe 0000:05:00.0 enp5s0f0: NIC Link is Up 10 Gbps, Flow Control: RX/TX\n[24721.246550] ixgbe 0000:05:00.0 enp5s0f0: NIC Link is Down\n[24731.241700] ixgbe 0000:05:00.0 enp5s0f0: NIC Link is Up 10 Gbps, Flow Control: RX/TX\n[24731.772858] ixgbe 0000:05:00.0 enp5s0f0: NIC Link is Down\n[24741.361649] ixgbe 0000:05:00.0 enp5s0f0: NIC Link is Up 10 Gbps, Flow Control: RX/TX\n</code></pre> <p>Installing a Noctua 80mm 5V did lower the NIC's temperature from around 50\u00b0C to 42\u00b0C, which would seem enough to stabilize the link, but the issue persists.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#disable-pcie-aspm","title":"Disable PCIe ASPM","text":"<p>PCIe Power Management (ASPM) attempts to save power by putting PCIe links into a low-power state when idle. High-performance cards like the X550-T2 can trigger aggressive power-saving on the entire PCIe bus, which often causes the <code>igc</code> (2.5G) or <code>ixgbe</code> (10G) drivers to lose the link during state transitions.</p> <p>Update <code>/etc/default/grub</code> to add the following parameters:</p> <ul> <li><code>pcie_aspm=off</code> tells the kernel not to manage the power states of individual PCIe     devices.</li> <li><code>pcie_port_pm=off</code> disables power management for the PCIe Root Ports (the     motherboard controllers that manage the slots themselves). </li> </ul> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"noquiet nosplash pcie_aspm=off pcie_port_pm=off\"\n</code></pre> <p>Then run <code>update-grubp</code> and reboot.</p>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#bonding-nics","title":"Bonding NICs","text":"<p>Disabling ASPM alone does not seem to significantly alleviated the issue, if not fixed it entirely. To add redundancy, the old 2.5Gbps NIC can be added to as a backup interface. This keeps the system using the 10G NIC for as long as the link is Up, and only switches traffic to the old 2.5G NIC when the link is down on the 10G.</p> <p>In addition to Netplan, the <code>ifenslave</code> package is necessary to support bonding:</p> <code># apt install ifenslave -y</code> <pre><code># apt install ifenslave -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  ifupdown\nSuggested packages:\n  rdnssd\nThe following NEW packages will be installed:\n  ifenslave ifupdown\n0 upgraded, 2 newly installed, 0 to remove and 1 not upgraded.\nNeed to get 79.1 kB of archives.\nAfter this operation, 261 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu noble/universe amd64 ifupdown amd64 0.8.41ubuntu1 [65.9 kB]\nGet:2 http://archive.ubuntu.com/ubuntu noble/universe amd64 ifenslave all 2.10ubuntu3 [13.3 kB]\nFetched 79.1 kB in 0s (482 kB/s)     \nSelecting previously unselected package ifupdown.\n(Reading database ... 511358 files and directories currently installed.)\nPreparing to unpack .../ifupdown_0.8.41ubuntu1_amd64.deb ...\nUnpacking ifupdown (0.8.41ubuntu1) ...\nSelecting previously unselected package ifenslave.\nPreparing to unpack .../ifenslave_2.10ubuntu3_all.deb ...\nUnpacking ifenslave (2.10ubuntu3) ...\nSetting up ifupdown (0.8.41ubuntu1) ...\nCreating /etc/network/interfaces.\nCreated symlink /etc/systemd/system/multi-user.target.wants/networking.service \u2192 /usr/lib/systemd/system/networking.service.\nCreated symlink /etc/systemd/system/network-online.target.wants/networking.service \u2192 /usr/lib/systemd/system/networking.service.\nSetting up ifenslave (2.10ubuntu3) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\n</code></pre> <p>In this system, the old 2.5Gbps NIC is <code>enp6s0</code> and the new 10Gbps NIC has two ports, each of them showing up as a separate NIC, and the one currently connected to the router is <code>enp5s0f0</code>. The following Netplan config bonds these together:</p> <p><code>/etc/netplan/01-network-manager-all.yaml</code></p> <pre><code># Dual static IP on LAN, nothing else.\nnetwork:\n  version: 2\n  renderer: networkd  # Use 'networkd' for system-level bonding\n  ethernets:\n    enp6s0: {}\n    enp5s0f0: {}\n  bonds:\n    bond0:\n      interfaces:\n        - enp5s0f0\n        - enp6s0\n      parameters:\n        mode: active-backup        # Stay on the 10G NIC while it's UP.\n        primary: enp5s0f0          # Use the 10G NIC as primary.\n        mii-monitor-interval: 250  # Increased to help with flapping.\n      optional: true               # Prevents 2-minute boot hang if one NIC is down.\n      dhcp4: no\n      dhcp6: no\n      # Ser IP address &amp; subnet mask\n      addresses: [ 10.0.0.2/24, 192.168.0.2/24 ]\n      # Set default gateway\n      routes:\n      - to: default\n        via: 192.168.0.1\n      # Set DNS name servers\n      nameservers:\n        addresses: [ 77.109.128.2, 213.144.129.20 ]\n</code></pre> <p>This bonding configuration uses the <code>active-backup</code> mode so that only one interface in the bond is active at a time. A different interface becomes active if, and only if, the active interface fails. The bond\u2019s MAC address is externally visible on only one port (network adapter) to avoid confusing the switch.</p> <p>For other available modes, see the Linux Ethernet Bonding Driver HOWTO.</p> <p>Apply the changes and check that the bond is active:</p> <pre><code># netplan apply\n\n# cat /proc/net/bonding/bond0\nEthernet Channel Bonding Driver: v6.8.0-90-lowlatency\n\nBonding Mode: fault-tolerance (active-backup)\nPrimary Slave: enp5s0f0 (primary_reselect always)\nCurrently Active Slave: enp5s0f0\nMII Status: up\nMII Polling Interval (ms): 250\nUp Delay (ms): 0\nDown Delay (ms): 0\nPeer Notification Delay (ms): 0\n\nSlave Interface: enp6s0\nMII Status: up\nSpeed: 2500 Mbps\nDuplex: full\nLink Failure Count: 0\nPermanent HW addr: 04:42:1a:97:4e:47\nSlave queue ID: 0\n\nSlave Interface: enp5s0f0\nMII Status: up\nSpeed: 10000 Mbps\nDuplex: full\nLink Failure Count: 0\nPermanent HW addr: ec:e7:a7:19:54:a8\nSlave queue ID: 0\n</code></pre> <p>When this bonding configuration is applied, the following shows in <code>dmesg</code> logs:</p> <pre><code>[ 8022.642137] bond0: (slave enp6s0): Enslaving as a backup interface with a down link\n[ 8022.652079] ixgbe 0000:05:00.0: removed PHC on enp5s0f0\n[ 8023.077888] ixgbe 0000:05:00.0: registered PHC device on enp5s0f0\n[ 8023.200826] bond0: (slave enp5s0f0): Enslaving as a backup interface with a down link\n[ 8027.912118] ixgbe 0000:05:00.0 enp5s0f0: NIC Link is Up 10 Gbps, Flow Control: RX/TX\n[ 8028.063109] bond0: (slave enp5s0f0): link status definitely up, 10000 Mbps full duplex\n[ 8028.063119] bond0: (slave enp5s0f0): making interface the new active one\n[ 8028.065169] bond0: active interface up!\n</code></pre> <p>Now the IP address is assigned to the <code>bond0</code> interface:</p> <pre><code>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n       valid_lft forever preferred_lft forever\n2: enp6s0: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc mq master bond0 state UP group default qlen 1000\n    link/ether 3e:ad:61:e4:20:8e brd ff:ff:ff:ff:ff:ff permaddr 04:42:1a:97:4e:47\n3: enp5s0f0: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc mq master bond0 state UP group default qlen 1000\n    link/ether 3e:ad:61:e4:20:8e brd ff:ff:ff:ff:ff:ff permaddr ec:e7:a7:19:54:a8\n4: enp5s0f1: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc mq state DOWN group default qlen 1000\n    link/ether ec:e7:a7:19:54:a9 brd ff:ff:ff:ff:ff:ff\n6: tailscale0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1280 qdisc fq_codel state UNKNOWN group default qlen 500\n    link/none \n    inet 100.97.190.45/32 scope global tailscale0\n       valid_lft forever preferred_lft forever\n    inet6 fd7a:115c:a1e0::d601:be2e/128 scope global \n       valid_lft forever preferred_lft forever\n    inet6 fe80::ce6a:5ca:4f73:134e/64 scope link stable-privacy \n       valid_lft forever preferred_lft forever\n7: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 3e:ad:61:e4:20:8e brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.2/24 brd 10.0.0.255 scope global bond0\n       valid_lft forever preferred_lft forever\n    inet 192.168.0.2/24 brd 192.168.0.255 scope global bond0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::3cad:61ff:fee4:208e/64 scope link \n       valid_lft forever preferred_lft forever\n8: fake0: &lt;BROADCAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/ether ce:56:b8:58:f8:32 brd ff:ff:ff:ff:ff:ff\n    inet 1.2.3.4/24 brd 1.2.3.255 scope global noprefixroute fake0\n       valid_lft forever preferred_lft forever\n</code></pre> <p>Now, when the 10G NIC link goes down, the 2.5G NIC takes over until the 10G is back up:</p> <pre><code>[13135.476859] ixgbe 0000:05:00.0 enp5s0f0: NIC Link is Down\n[13135.481044] bond0: (slave enp5s0f0): link status definitely down, disabling slave\n[13135.481049] bond0: (slave enp6s0): making interface the new active one\n[13140.125754] ixgbe 0000:05:00.0 enp5s0f0: NIC Link is Up 10 Gbps, Flow Control: RX/TX\n[13140.344048] bond0: (slave enp5s0f0): link status definitely up, 10000 Mbps full duplex\n[13140.344061] bond0: (slave enp5s0f0): making interface the new active one\n</code></pre>"},{"location":"blog/2024/11/03/ubuntu-studio-2404-on-rapture-gaming-pc-and-more/#do-not-update-to-hwe-kernel-614","title":"DO NOT update to HWE kernel 6.14","text":"<p>Updating to 6.14 was once recommended to resolve the PCIe link flapping, as it contains significantly improved Intel 10G/2.5G driver stability and better power state  management. However, the 6.14 kernel made the desktop feel a log more \"laggy\" with UIs and mouse pointer freezing for quite a perceptible time, sometimes more than a second, and playing audio via USB devices suffers a lot when transferring large files. Eventually reverting to the 6.8 kernel resolved those issues immediately.</p> <p>For the record, this is how the kernel 6.14 was tested; first install it:</p> <code># apt install --install-recommends linux-generic-hwe-24.04 -y</code> <pre><code># apt install --install-recommends linux-generic-hwe-24.04 -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  amd64-microcode linux-headers-6.14.0-37-generic linux-headers-generic-hwe-24.04\n  linux-hwe-6.14-headers-6.14.0-37 linux-hwe-6.14-tools-6.14.0-37 linux-image-6.14.0-37-generic\n  linux-image-generic-hwe-24.04 linux-modules-6.14.0-37-generic linux-modules-extra-6.14.0-37-generic\n  linux-tools-6.14.0-37-generic\nSuggested packages:\n  linux-hwe-6.14-tools\nThe following NEW packages will be installed:\n  amd64-microcode linux-generic-hwe-24.04 linux-headers-6.14.0-37-generic linux-headers-generic-hwe-24.04\n  linux-hwe-6.14-headers-6.14.0-37 linux-hwe-6.14-tools-6.14.0-37 linux-image-6.14.0-37-generic\n  linux-image-generic-hwe-24.04 linux-modules-6.14.0-37-generic linux-modules-extra-6.14.0-37-generic\n  linux-tools-6.14.0-37-generic\n0 upgraded, 11 newly installed, 0 to remove and 1 not upgraded.\nNeed to get 0 B/199 MB of archives.\nAfter this operation, 320 MB of additional disk space will be used.\nSelecting previously unselected package linux-modules-6.14.0-37-generic.\n(Reading database ... 511408 files and directories currently installed.)\nPreparing to unpack .../00-linux-modules-6.14.0-37-generic_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-modules-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-image-6.14.0-37-generic.\nPreparing to unpack .../01-linux-image-6.14.0-37-generic_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-image-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-modules-extra-6.14.0-37-generic.\nPreparing to unpack .../02-linux-modules-extra-6.14.0-37-generic_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-modules-extra-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package amd64-microcode.\nPreparing to unpack .../03-amd64-microcode_3.20250311.1ubuntu0.24.04.1_amd64.deb ...\nUnpacking amd64-microcode (3.20250311.1ubuntu0.24.04.1) ...\nSelecting previously unselected package linux-image-generic-hwe-24.04.\nPreparing to unpack .../04-linux-image-generic-hwe-24.04_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-image-generic-hwe-24.04 (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-hwe-6.14-headers-6.14.0-37.\nPreparing to unpack .../05-linux-hwe-6.14-headers-6.14.0-37_6.14.0-37.37~24.04.1_all.deb ...\nUnpacking linux-hwe-6.14-headers-6.14.0-37 (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-headers-6.14.0-37-generic.\nPreparing to unpack .../06-linux-headers-6.14.0-37-generic_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-headers-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-headers-generic-hwe-24.04.\nPreparing to unpack .../07-linux-headers-generic-hwe-24.04_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-headers-generic-hwe-24.04 (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-generic-hwe-24.04.\nPreparing to unpack .../08-linux-generic-hwe-24.04_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-generic-hwe-24.04 (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-hwe-6.14-tools-6.14.0-37.\nPreparing to unpack .../09-linux-hwe-6.14-tools-6.14.0-37_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-hwe-6.14-tools-6.14.0-37 (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-tools-6.14.0-37-generic.\nPreparing to unpack .../10-linux-tools-6.14.0-37-generic_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-tools-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSetting up amd64-microcode (3.20250311.1ubuntu0.24.04.1) ...\nupdate-initramfs: deferring update (trigger activated)\namd64-microcode: microcode will be updated at next boot\nSetting up linux-hwe-6.14-headers-6.14.0-37 (6.14.0-37.37~24.04.1) ...\nSetting up linux-modules-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSetting up linux-hwe-6.14-tools-6.14.0-37 (6.14.0-37.37~24.04.1) ...\nSetting up linux-headers-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\n/etc/kernel/header_postinst.d/dkms:\n* dkms: running auto installation service for kernel 6.14.0-37-generic\nSign command: /usr/bin/kmodsign\nSigning key: /var/lib/shim-signed/mok/MOK.priv\nPublic certificate (MOK): /var/lib/shim-signed/mok/MOK.der\n\nBuilding module:\nCleaning build area...\nunset ARCH; [ ! -h /usr/bin/cc ] &amp;&amp; export CC=/usr/bin/gcc; env NV_VERBOSE=1 'make' -j16 NV_EXCLUDE_BUILD_MODULES='' KERNEL_UNAME=6.14.0-37-generic IGNORE_XEN_PRESENCE=1 IGNORE_CC_MISMATCH=1 SYSSRC=/lib/modules/6.14.0-37-generic/build LD=/usr/bin/ld.bfd CONFIG_X86_KERNEL_IBT= modules...........\nSigning module /var/lib/dkms/nvidia/580.95.05/build/nvidia.ko\nSigning module /var/lib/dkms/nvidia/580.95.05/build/nvidia-modeset.ko\nSigning module /var/lib/dkms/nvidia/580.95.05/build/nvidia-drm.ko\nSigning module /var/lib/dkms/nvidia/580.95.05/build/nvidia-uvm.ko\nSigning module /var/lib/dkms/nvidia/580.95.05/build/nvidia-peermem.ko\nCleaning build area...\n\nnvidia.ko.zst:\nRunning module version sanity check.\n- Original module\n  - No original module exists within this kernel\n- Installation\n  - Installing to /lib/modules/6.14.0-37-generic/updates/dkms/\n\nnvidia-modeset.ko.zst:\nRunning module version sanity check.\n- Original module\n  - No original module exists within this kernel\n- Installation\n  - Installing to /lib/modules/6.14.0-37-generic/updates/dkms/\n\nnvidia-drm.ko.zst:\nRunning module version sanity check.\n- Original module\n  - No original module exists within this kernel\n- Installation\n  - Installing to /lib/modules/6.14.0-37-generic/updates/dkms/\n\nnvidia-uvm.ko.zst:\nRunning module version sanity check.\n- Original module\n  - No original module exists within this kernel\n- Installation\n  - Installing to /lib/modules/6.14.0-37-generic/updates/dkms/\n\nnvidia-peermem.ko.zst:\nRunning module version sanity check.\n- Original module\n  - No original module exists within this kernel\n- Installation\n  - Installing to /lib/modules/6.14.0-37-generic/updates/dkms/\ndepmod...\ndkms autoinstall on 6.14.0-37-generic/x86_64 succeeded for nvidia\n* dkms: autoinstall for kernel 6.14.0-37-generic\n  ...done.\nSetting up linux-image-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nI: /boot/vmlinuz.old is now a symlink to vmlinuz-6.8.0-90-lowlatency\nI: /boot/initrd.img.old is now a symlink to initrd.img-6.8.0-90-lowlatency\nI: /boot/vmlinuz is now a symlink to vmlinuz-6.14.0-37-generic\nI: /boot/initrd.img is now a symlink to initrd.img-6.14.0-37-generic\nSetting up linux-modules-extra-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSetting up linux-tools-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSetting up linux-headers-generic-hwe-24.04 (6.14.0-37.37~24.04.1) ...\nSetting up linux-image-generic-hwe-24.04 (6.14.0-37.37~24.04.1) ...\nSetting up linux-generic-hwe-24.04 (6.14.0-37.37~24.04.1) ...\nProcessing triggers for initramfs-tools (0.142ubuntu25.5) ...\nupdate-initramfs: Generating /boot/initrd.img-6.8.0-90-lowlatency\nProcessing triggers for linux-image-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\n/etc/kernel/postinst.d/dkms:\n* dkms: running auto installation service for kernel 6.14.0-37-generic\n* dkms: autoinstall for kernel 6.14.0-37-generic\n  ...done.\n/etc/kernel/postinst.d/initramfs-tools:\nupdate-initramfs: Generating /boot/initrd.img-6.14.0-37-generic\n/etc/kernel/postinst.d/zz-update-grub:\nSourcing file `/etc/default/grub'\nSourcing file `/etc/default/grub.d/ubuntustudio.cfg'\nGenerating grub configuration file ...\nFound linux image: /boot/vmlinuz-6.8.0-90-lowlatency\nFound initrd image: /boot/initrd.img-6.8.0-90-lowlatency\nFound linux image: /boot/vmlinuz-6.8.0-88-lowlatency\nFound initrd image: /boot/initrd.img-6.8.0-88-lowlatency\nFound linux image: /boot/vmlinuz-6.14.0-37-generic\nFound initrd image: /boot/initrd.img-6.14.0-37-generic\nFound memtest86+ 64bit EFI image: /boot/memtest86+x64.efi\nWarning: os-prober will be executed to detect other bootable partitions.\nIts output will be used to detect bootable binaries on them and create new boot entries.\nFound Ubuntu 22.04.5 LTS (22.04) on /dev/nvme1n1p2\nAdding boot menu entry for UEFI Firmware Settings ...\ndone\n</code></pre> <p>In case the new kernel does not become the new default, adjust <code>/etc/default/grub</code> so that it will show the menu to allow choosing a specific kernel and then remember that choice as the default for subsequent reboots:</p> /etc/default/grub<pre><code># Use these options to pick a specific kernel.\nGRUB_DEFAULT=saved\nGRUB_TIMEOUT=10\nGRUB_TIMEOUT_STYLE=menu\nGRUB_SAVEDEFAULT=true\n\n# Use these options to directly boot the first kernel.\n#GRUB_DEFAULT=0\n#GRUB_TIMEOUT=0\n#GRUB_TIMEOUT_STYLE=hidden\n</code></pre> <p>Update GRUB and restart, then choose the new kernel.</p> <pre><code>Sourcing file `/etc/default/grub'\nSourcing file `/etc/default/grub.d/ubuntustudio.cfg'\nGenerating grub configuration file ...\nFound linux image: /boot/vmlinuz-6.8.0-90-lowlatency\nFound initrd image: /boot/initrd.img-6.8.0-90-lowlatency\nFound linux image: /boot/vmlinuz-6.8.0-88-lowlatency\nFound initrd image: /boot/initrd.img-6.8.0-88-lowlatency\nFound linux image: /boot/vmlinuz-6.14.0-37-generic\nFound initrd image: /boot/initrd.img-6.14.0-37-generic\nFound memtest86+ 64bit EFI image: /boot/memtest86+x64.efi\nWarning: os-prober will be executed to detect other bootable partitions.\nIts output will be used to detect bootable binaries on them and create new boot entries.\nFound Ubuntu 22.04.5 LTS (22.04) on /dev/nvme1n1p2\nAdding boot menu entry for UEFI Firmware Settings ...\ndone\n</code></pre> <p>To make the new kernel the all-time default, it should be possible to set <code>GRUB_DEFAULT</code> to the index number of the desired option as obtained from <code>/boot/grub/grub.cfg</code>; e.g. <code>0</code> for the old 6.8 kernel or <code>4</code> for the 6.14 kernel.</p> <pre><code>$ sudo awk -F\"'\" '/menuentry / &amp;&amp; /with Linux/ {print i++ \" : \" $2}' /boot/grub/grub.cfg\n0 : Ubuntu, with Linux 6.8.0-90-lowlatency\n1 : Ubuntu, with Linux 6.8.0-90-lowlatency (recovery mode)\n2 : Ubuntu, with Linux 6.8.0-88-lowlatency\n3 : Ubuntu, with Linux 6.8.0-88-lowlatency (recovery mode)\n4 : Ubuntu, with Linux 6.14.0-37-generic\n5 : Ubuntu, with Linux 6.14.0-37-generic (recovery mode)\n6 : Ubuntu, with Linux 5.15.0-124-lowlatency (on /dev/nvme1n1p2)\n7 : Ubuntu, with Linux 5.15.0-124-lowlatency (recovery mode) (on /dev/nvme1n1p2)\n8 : Ubuntu, with Linux 5.15.0-122-lowlatency (on /dev/nvme1n1p2)\n9 : Ubuntu, with Linux 5.15.0-122-lowlatency (recovery mode) (on /dev/nvme1n1p2)\n10 : Ubuntu, with Linux 5.15.0-60-lowlatency (on /dev/nvme1n1p2)\n11 : Ubuntu, with Linux 5.15.0-60-lowlatency (recovery mode) (on /dev/nvme1n1p2)\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/","title":"Ubuntu Studio 24.04 on Super Tuna (NUC PC)","text":"<p>This would be a very minimal journey of installing Ubuntu Studio 24.04 on a relatively new mid-range Intel\u00ae NUC 13 Pro mini PC, had it not gone wahoonie-shaped...</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#preparation","title":"Preparation","text":"<p>The following components are chosen for a most compact, possibly portable desktop setup, to be running 24x7 without taking much space:</p> <ul> <li>ASUS Arena Canyon NUC13ANKI5.   with a 13th Generation Intel\u00ae Core\u2122 i5-1340P and integrated Intel\u00ae UHD Graphics.</li> <li>Kingston FURY Impact DDR4 Memory   (32 GB, 3200 MHz, DDR4).</li> <li>Kingston FURY Renegade PCIe 4.0 NVMe M.2 SSD.</li> <li>14\" Full HD 1080p Portable Monitor    Verbatim PM-14.</li> <li>Logitech   K400 Plus Wireless Touch Keyboard.</li> </ul>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#install-ubuntu-studio-2404","title":"Install Ubuntu Studio 24.04","text":"<p>Prepared the USB stick with <code>usb-creator-kde</code> and booted into it, then used the \u201cInstall Ubuntu\u201d launcher on the desktop.</p> <ol> <li>Plug the USB stick and turn the PC on.</li> <li>Press <code>F8</code> to select the boot device and choose the     UEFI: ... option.</li> <li>In the Grub menu, choose to Try or Install Ubuntu.</li> <li>Select language (English) and then Install Ubuntu.</li> <li>Select keyboard layout (can be from a different language).</li> <li>Select the appropriate wireless or wireless network.</li> <li>Select Try Ubuntu Studio.</li> <li>Disable screen locking to prevent     KDE Plasma live lock screen rendering the session useless:<ul> <li>Press Alt-Space to invoke Krunner and type \u201cSystem Settings\u201d.</li> <li>From there, search for \u201cScreen Locking\u201d and</li> <li>deactivate \u201cLock automatically after\u2026\u201d.</li> </ul> </li> <li>Select Type of install: Interactive Installation.</li> <li>Enable the options to<ul> <li>Install third-party software for graphics and Wifi hardware and</li> <li>Download and install support for additional media formats.</li> </ul> </li> <li>Select Manual Installation<ul> <li>Use the arrow keys to navigate down to the nvme0n1 disk.</li> <li>Set nvme0n1p1 (300 MB) as EFI System Partition mounted     on <code>/boot/efi</code></li> <li>Set nvme0n1p2 (60 GB) as ext4 mounted on <code>/</code></li> <li>Leave nvme0n1p3 (60 GB) alone (to be used for Ubuntu 26.04)</li> <li>Set nvme0n1p4 (3.88 TB) as Leave formatted as Btrfs     mounted on <code>/home</code></li> <li>Set Device for boot loader installation to nvme0n1</li> </ul> </li> <li>Click on Next to confirm the partition selection.</li> <li>Confirm first non-root user name (<code>coder</code>) and computer     name (<code>super-tuna</code>).</li> <li>Select time zone (seems to be detected correctly).</li> <li>Review the choices and click on Install to start copying files.</li> <li>Once it's done, select Restart     (remove install media and hit <code>Enter</code>).</li> </ol> <p>Note</p> <p>60 GB should be a enough for the root partition for the amount of software that tends to be installed in this PC.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#first-boot-into-ubuntu-studio-2404","title":"First boot into Ubuntu Studio 24.04","text":""},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#disable-power-saving-suspend","title":"Disable Power Saving / Suspend","text":"<p>This desktop will need to continue running as normal even when left alone for extended periods of time. To avoid interruptions, open the Energy Saving section in System Settings and disable Screen Energy Saving and Suspend session.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#ssh-server","title":"SSH Server","text":"<p>Ubuntu Studio doesn't enable the SSH server by default, but we want this to adjust the system remotely:</p> <pre><code># apt install ssh -y\n# sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config\n# systemctl enable --now ssh\n# systemctl daemon-reload\n# systemctl restart sshd.service\n</code></pre> <p>Enable SSH from trusted hosts by adding them to <code>.ssh/authorized_keys</code> and copy content of <code>/etc/hosts</code> from a relevant host (e.g. <code>pi-z2</code>).</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#check-ip-addresses-on-lan-wlan","title":"Check IP addresses on LAN &amp; WLAN","text":"<p>This system will normally not be connected to the local wired network, so it is important to take note of both IP addresses it has obtained from the DHCP server:</p> <pre><code># ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n       valid_lft forever preferred_lft forever\n2: enp86s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 48:21:0b:57:51:92 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.0.68/24 brd 192.168.0.255 scope global dynamic noprefixroute enp86s0\n       valid_lft 85819sec preferred_lft 85819sec\n    inet6 fe80::4a21:bff:fe57:5192/64 scope link \n       valid_lft forever preferred_lft forever\n3: enx5c857e3e1129: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000\n    link/ether 5c:85:7e:3e:11:29 brd ff:ff:ff:ff:ff:ff\n4: wlo1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether e0:c2:64:39:17:73 brd ff:ff:ff:ff:ff:ff\n    altname wlp0s20f3\n    inet 192.168.0.17/24 brd 192.168.0.255 scope global dynamic noprefixroute wlo1\n       valid_lft 85820sec preferred_lft 85820sec\n    inet6 fe80::56d1:3cea:139:daaf/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#install-essential-packages","title":"Install Essential Packages","text":"<p>Start by installing a subset of the essential packages, plus a few more  that have been found necessary later (e.g. <code>auditd</code> to stop apparmor spew in the logs):</p> <code># apt install ...</code> <pre><code># apt install gdebi-core wget vim curl geeqie playonlinux \\\n  exfat-fuse clementine id3v2 htop vnstat sox wine smem \\\n  python-is-python3 exiv2 rename scrot xcalib python3-pip \\\n  netcat-openbsd python3-selenium lm-sensors sysstat tor \\\n  unrar ttf-mscorefonts-installer winetricks icc-profiles \\\n  ffmpeg iotop-c xdotool inxi mpv screen libxxf86vm-dev \\\n  displaycal intel-gpu-tools redshift-qt auditd -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  cabextract chromium-browser chromium-chromedriver exiftran fonts-wine\n  fuseiso geeqie-common icoutils libasound2-plugins libaudit-common\n  libaudit1 libauparse0t64 libcapi20-3t64 libegl-mesa0 libegl1-mesa-dev\n  libgbm1 libgl1-mesa-dri libglapi-mesa libglx-mesa0 libgpod-common\n  libgpod4t64 liblastfm5-1 liblua5.3-0 libmspack0t64 libmygpo-qt5-1\n  libosmesa6 libpython3-dev libpython3.12-dev libsgutils2-1.46-2\n  libutempter0 libwine libxatracker2 libxdo3 libxkbregistry0 libz-mingw-w64\n  mesa-va-drivers mesa-vdpau-drivers mesa-vulkan-drivers python3-dev\n  python3-exceptiongroup python3-h11 python3-natsort python3-outcome\n  python3-sniffio python3-trio python3-trio-websocket python3-wsproto\n  python3.12-dev redshift tor-geoipdb torsocks tree vim-common vim-runtime\n  vim-tiny webp-pixbuf-loader wine64 xxd\nSuggested packages:\n  audispd-plugins xpaint libjpeg-progs libterm-readline-gnu-perl\n  | libterm-readline-perl-perl libxml-dumper-perl sg3-utils fancontrol\n  read-edid i2c-tools libcuda1 winbind python-natsort-doc byobu | screenie\n  | iselect libsox-fmt-all mixmaster torbrowser-launcher apparmor-utils nyx\n  obfs4proxy ctags vim-doc vim-scripts indent vnstati q4wine wine-binfmt\n  dosbox wine64-preloader\nRecommended packages:\n  wine32\nThe following NEW packages will be installed:\n  auditd cabextract chromium-browser chromium-chromedriver clementine curl\n  exfat-fuse exiftran exiv2 fonts-wine fuseiso gdebi-core geeqie\n  geeqie-common htop icc-profiles icoutils id3v2 intel-gpu-tools inxi\n  iotop-c libasound2-plugins libauparse0t64 libcapi20-3t64 libgpod-common\n  libgpod4t64 liblastfm5-1 liblua5.3-0 libmspack0t64 libmygpo-qt5-1\n  libosmesa6 libpython3-dev libpython3.12-dev libsgutils2-1.46-2\n  libutempter0 libwine libxdo3 libxkbregistry0 libxxf86vm-dev libz-mingw-w64\n  lm-sensors mpv playonlinux python-is-python3 python3-dev\n  python3-exceptiongroup python3-h11 python3-natsort python3-outcome\n  python3-pip python3-selenium python3-sniffio python3-trio\n  python3-trio-websocket python3-wsproto python3.12-dev redshift redshift-qt\n  rename screen scrot sox tor tor-geoipdb torsocks tree\n  ttf-mscorefonts-installer unrar vim vim-runtime vnstat webp-pixbuf-loader\n  wine wine64 winetricks xcalib xdotool\nThe following packages will be upgraded:\n  libaudit-common libaudit1 libegl-mesa0 libegl1-mesa-dev libgbm1\n  libgl1-mesa-dri libglapi-mesa libglx-mesa0 libxatracker2 mesa-va-drivers\n  mesa-vdpau-drivers mesa-vulkan-drivers vim-common vim-tiny xxd\n15 upgraded, 77 newly installed, 0 to remove and 71 not upgraded.\nNeed to get 247 MB of archives.\nAfter this operation, 965 MB of additional disk space will be used.\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#brave-browser","title":"Brave browser","text":"<p>Install from the Release Channel:</p> <pre><code># curl -fsSLo \\\n  /usr/share/keyrings/brave-browser-archive-keyring.gpg \\\n  https://brave-browser-apt-release.s3.brave.com/brave-browser-archive-keyring.gpg\n\n# echo \"deb [signed-by=/usr/share/keyrings/brave-browser-archive-keyring.gpg] https://brave-browser-apt-release.s3.brave.com/ stable main\" \\\n| tee /etc/apt/sources.list.d/brave-browser-release.list\n\n# apt update &amp;&amp; apt install brave-browser -y\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#google-chrome","title":"Google Chrome","text":"<p>Installing Google Chrome is as simple as downloading the Debian package and installing it:</p> <pre><code># dpkg -i google-chrome-stable_current_amd64.deb\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#continuous-monitoring","title":"Continuous Monitoring","text":"<p>Install the multi-thread version of the <code>conmon</code> script as <code>/usr/local/bin/conmon</code> and run it as a service; create <code>/etc/systemd/system/conmon.service</code> as follows:</p> <pre><code>[Unit]\nDescription=Continuous Monitoring\n\n[Service]\nExecStart=/usr/local/bin/conmon\nRestart=on-failure\nStandardOutput=null\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Then enable and start the services in <code>systemd</code>:</p> <pre><code># systemctl enable conmon.service\n# systemctl daemon-reload\n# systemctl start conmon.service\n# systemctl status conmon.service\n</code></pre> <p>If not already adjusted, add at least the following lines to <code>/etc/hosts</code> so that <code>lexicon</code> is reachable:</p> <pre><code>192.168.0.6     lexicon\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#hardware-sensors","title":"Hardware Sensors","text":"<p>All hardware sensors are supported out of the box:</p> <pre><code># sensors -A\niwlwifi_1-virtual-0\ntemp1:        +37.0\u00b0C  \n\nucsi_source_psy_USBC000:001-isa-0000\nin0:           0.00 V  (min =  +0.00 V, max =  +0.00 V)\ncurr1:         0.00 A  (max =  +0.00 A)\n\nacpitz-acpi-0\ntemp1:        +43.0\u00b0C  \n\ncoretemp-isa-0000\nPackage id 0:  +43.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\nCore 0:        +36.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\nCore 4:        +34.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\nCore 8:        +35.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\nCore 12:       +34.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\nCore 16:       +40.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\nCore 17:       +40.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\nCore 18:       +40.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\nCore 19:       +40.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\nCore 20:       +42.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\nCore 21:       +42.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\nCore 22:       +41.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\nCore 23:       +41.0\u00b0C  (high = +100.0\u00b0C, crit = +100.0\u00b0C)\n\nucsi_source_psy_USBC000:002-isa-0000\nin0:           0.00 V  (min =  +0.00 V, max =  +0.00 V)\ncurr1:         3.00 A  (max =  +0.00 A)\n\nnvme-pci-0100\nComposite:    +25.9\u00b0C  (low  = -20.1\u00b0C, high = +83.8\u00b0C)\n                       (crit = +88.8\u00b0C)\nSensor 2:     +64.8\u00b0C  \n</code></pre> <p>There is no need to load the <code>drivetemp</code> kernel module because the only storage in this Nuc PC are NVMe drives, which already show up as the <code>nvme-pci-0100</code> controller above.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#system-configuration","title":"System Configuration","text":"<p>The above having covered installing software, there are still system configurations that need to be tweaked.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#apt-respositories-clean-up","title":"APT respositories clean-up","text":"<p>Ubuntu Studio 24.04 seems to consistently need a little APT respositories clean-up; just comment out the last line in <code>/etc/apt/sources.list.d/dvd.list</code> to let <code>noble-security</code> be defined (only) in <code>ubuntu.sources</code>.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#ubuntu-pro","title":"Ubuntu Pro","text":"<p>When updating the system with <code>apt full-upgrade -y</code> a notice comes up about additional security updates:</p> <pre><code>Get more security updates through Ubuntu Pro with 'esm-apps' enabled:\n  libdcmtk17t64 libcjson1 libavdevice60 ffmpeg libpostproc57 libavcodec60\n  libavutil58 libswscale7 libswresample4 libavformat60 libavfilter9\nLearn more about Ubuntu Pro at https://ubuntu.com/pro\n</code></pre> <p>This being a new system, indeed it's not attached to an Ubuntu Pro account (the old system was):</p> <pre><code># pro security-status\n3131 packages installed:\n     1589 packages from Ubuntu Main/Restricted repository\n     1539 packages from Ubuntu Universe/Multiverse repository\n     3 packages from third parties\n\nTo get more information about the packages, run\n    pro security-status --help\nfor a list of available options.\n\nThis machine is receiving security patching for Ubuntu Main/Restricted\nrepository until 2029.\nThis machine is NOT attached to an Ubuntu Pro subscription.\n\nUbuntu Pro with 'esm-infra' enabled provides security updates for\nMain/Restricted packages until 2034.\n\nUbuntu Pro with 'esm-apps' enabled provides security updates for\nUniverse/Multiverse packages until 2034. There are 11 pending security updates.\n\nTry Ubuntu Pro with a free personal subscription on up to 5 machines.\nLearn more at https://ubuntu.com/pro\n</code></pre> <p>After creating an Ubuntu account a token is available to use with <code>pro attach</code>:</p> <pre><code># pro attach ...\nEnabling Ubuntu Pro: ESM Apps\nUbuntu Pro: ESM Apps enabled\nEnabling Ubuntu Pro: ESM Infra\nUbuntu Pro: ESM Infra enabled\nEnabling Livepatch\nLivepatch enabled\nThis machine is now attached to 'Ubuntu Pro - free personal subscription'\n\nSERVICE          ENTITLED  STATUS       DESCRIPTION\nanbox-cloud      yes       disabled     Scalable Android in the cloud\nesm-apps         yes       enabled      Expanded Security Maintenance for Applications\nesm-infra        yes       enabled      Expanded Security Maintenance for Infrastructure\nlandscape        yes       disabled     Management and administration tool for Ubuntu\nlivepatch        yes       warning      Current kernel is not covered by livepatch\nrealtime-kernel* yes       disabled     Ubuntu kernel with PREEMPT_RT patches integrated\n\n * Service has variants\n\nNOTICES\nOperation in progress: pro attach\nThe current kernel (6.8.0-47-lowlatency, x86_64) is not covered by livepatch.\nCovered kernels are listed here: https://ubuntu.com/security/livepatch/docs/kernels\nEither switch to a covered kernel or `sudo pro disable livepatch` to dismiss this warning.\n\nFor a list of all Ubuntu Pro services and variants, run 'pro status --all'\nEnable services with: pro enable &lt;service&gt;\n\n     Account: ponder.stibbons@uu.am\nSubscription: Ubuntu Pro - free personal subscription\n</code></pre> <p>Now the system can be updated again with <code>apt full-upgrade -y</code> to receive those additional security updates:</p> <pre><code># apt full-upgrade -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\nThe following upgrades have been deferred due to phasing:\n  krb5-locales libboost-chrono1.83.0t64 libboost-filesystem1.83.0\n  libboost-iostreams1.83.0 libboost-locale1.83.0\n  libboost-program-options1.83.0 libboost-thread1.83.0 libgssapi-krb5-2\n  libk5crypto3 libkrb5-3 libkrb5support0 libldap-common libldap2\n  python3-distupgrade ubuntu-release-upgrader-core\n  ubuntu-release-upgrader-qt\nThe following packages will be upgraded:\n  ffmpeg libavcodec60 libavdevice60 libavfilter9 libavformat60 libavutil58\n  libcjson1 libdcmtk17t64 libpostproc57 libswresample4 libswscale7\n11 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n11 esm-apps security updates\nNeed to get 19.3 MB of archives.\nAfter this operation, 9216 B of additional disk space will be used.\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#make-dmesg-non-privileged","title":"Make <code>dmesg</code> non-privileged","text":"<p>Since Ubuntu 22.04, <code>dmesg</code> has become a privileged operation by default:</p> <pre><code>$ dmesg\ndmesg: read kernel buffer failed: Operation not permitted\n</code></pre> <p>This is controlled by </p> <pre><code># sysctl kernel.dmesg_restrict\nkernel.dmesg_restrict = 1\n</code></pre> <p>To revert this default, and make it permanent (source):</p> <pre><code># echo 'kernel.dmesg_restrict=0' | tee -a /etc/sysctl.d/99-sysctl.conf\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#weekly-btrfs-scrub","title":"Weekly btrfs scrub","text":"<p>To keep BTRFS file systems healthy, it is recommended to run a weekly scrub to check everything for consistency. For this, I run the script from crontab every Saturday night.</p> <pre><code># wget -O /usr/local/bin/btrfs-scrub-all \\\n  http://marc.merlins.org/linux/scripts/btrfs-scrub\n\n# crontab -e\n...\n# m h  dom mon dow   command\n50 2 * * 6 /usr/local/bin/btrfs-scrub-all\n</code></pre> <p>Marc MERLIN keeps the script updated, so each systme may benefit from a few modifications, e.g. 1. Remove tests for laptop battery status, when running on a PC. 2. Set the <code>BTRFS_SCRUB_SKIP</code> to filter out partitions to skip.</p> /usr/local/bin/btrfs-scrub-all<pre><code>#! /bin/bash\n\n# By Marc MERLIN &lt;marc_soft@merlins.org&gt; 2014/03/20\n# License: Apache-2.0\n# http://marc.merlins.org/perso/btrfs/post_2014-03-19_Btrfs-Tips_-Btrfs-Scrub-and-Btrfs-Filesystem-Repair.html\n\nwhich btrfs &gt;/dev/null || exit 0\nexport PATH=/usr/local/bin:/sbin:$PATH\n\nFILTER='(^Dumping|balancing, usage)'\nBTRFS_SCRUB_SKIP=\"sda\"\nsource /etc/btrfs_config 2&gt;/dev/null\ntest -n \"$DEVS\" || DEVS=$(grep '\\&lt;btrfs\\&gt;' /proc/mounts | awk '{ print $1 }' | sort -u | grep -v $BTRFS_SCRUB_SKIP)\nfor btrfs in $DEVS\ndo\n    tail -n 0 -f /var/log/syslog | grep \"BTRFS\" | grep -Ev '(disk space caching is enabled|unlinked .* orphans|turning on discard|device label .* devid .* transid|enabling SSD mode|BTRFS: has skinny extents|BTRFS: device label|BTRFS info )' &amp;\n    mountpoint=\"$(grep \"$btrfs\" /proc/mounts | awk '{ print $2 }' | sort | head -1)\"\n    logger -s \"Quick Metadata and Data Balance of $mountpoint ($btrfs)\" &gt;&amp;2\n    # Even in 4.3 kernels, you can still get in places where balance\n    # won't work (no place left, until you run a -m0 one first)\n    # I'm told that proactively rebalancing metadata may not be a good idea.\n    #btrfs balance start -musage=20 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    # but a null rebalance should help corner cases:\n    sleep 10\n    btrfs balance start -musage=0 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    # After metadata, let's do data:\n    sleep 10\n    btrfs balance start -dusage=0 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    sleep 10\n    btrfs balance start -dusage=20 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    # And now we do scrub. Note that scrub can fail with \"no space left\n    # on device\" if you're very out of balance.\n    logger -s \"Starting scrub of $mountpoint\" &gt;&amp;2\n    echo btrfs scrub start -Bd $mountpoint\n    # -r is read only, but won't fix a redundant array.\n    #ionice -c 3 nice -10 btrfs scrub start -Bdr $mountpoint\n    time ionice -c 3 nice -10 btrfs scrub start -Bd $mountpoint\n    pkill -f 'tail -n 0 -f /var/log/syslog'\n    logger \"Ended scrub of $mountpoint\" &gt;&amp;2\ndone\n</code></pre> <p>Note</p> <p>Setting <code>BTRFS_SCRUB_SKIP=\"sda\"</code> prevents Btrfs balancing from running on USB external storage if attached; not really necessary, but the script fails if this variable is left empty.</p> <pre><code># /usr/local/bin/btrfs-scrub-all\n&lt;13&gt;Nov 17 22:23:49 root: Quick Metadata and Data Balance of /home (/dev/nvme0n1p4)\nDone, had to relocate 0 out of 1536 chunks\nDone, had to relocate 0 out of 1536 chunks\nDone, had to relocate 0 out of 1536 chunks\n&lt;13&gt;Nov 17 22:24:19 root: Starting scrub of /home\nbtrfs scrub start -Bd /home\nStarting scrub on devid 1\n\nScrub device /dev/nvme0n1p4 (id 1) done\nScrub started:    Sun Nov 17 22:24:19 2024\nStatus:           finished\nDuration:         0:06:50\nTotal to scrub:   1.48TiB\nRate:             3.71GiB/s\nError summary:    no errors found\n\nreal    6m49.944s\nuser    0m0.002s\nsys     4m41.299s\n</code></pre> <p>The whole process takes about 7 minutes with the 4TB NVMe SSD about 43% full, with 1.5T used.</p> <p></p> <p>Note</p> <p>The weekly Btrfs scrub doesn't seem to really need <code>inn</code> or any of its dependencies, so this installation step was skipped:</p> <pre><code># apt install inn -y\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#make-sddm-look-good","title":"Make SDDM Look Good","text":"<p>Ubuntu Studio 24.04 uses  Simple Desktop Display Manager (SDDM) (sddm/sddm in GitHub) which is quite good looking out of the box, but I like to customize this for each computer.</p> <p>For most computers my favorite SDDM theme is Breeze-Noir-Dark, which I like to install system-wide.</p> <pre><code># unzip -d /usr/share/sddm/themes Breeze-Noir-Dark.zip\n</code></pre> <p>Warning</p> <p>Action icons won\u2019t render if the directory name is changed. If needed, change the directory name in the <code>iconSource</code> fields in <code>Main.qml</code> to match final directory name so icons show. This is not the only thing that breaks when changing the directory name.</p> <p>Other than installing this theme, all I really change in it is the background image to use  Super Tuna (1920x1080).</p> <p></p> <pre><code># mv Super-Tuna.jpg \\\n  /usr/share/sddm/themes/Breeze-Noir-Dark/\n# cd /usr/share/sddm/themes/Breeze-Noir-Dark/\n\n# vi theme.conf\n[General]\ntype=image\ncolor=#132e43\nbackground=/usr/share/sddm/themes/Breeze-Noir-Dark/Super-Tuna.jpg\n\n# vi theme.conf.user\n[General]\ntype=image\nbackground=Super-Tuna.jpg\n</code></pre> <p>Additionally, as this is new in Ubuntu 24.04, the theme has to be selected by adding a <code>[Theme]</code> section in the system config in <code>/usr/lib/sddm/sddm.conf.d/ubuntustudio.conf</code></p> <pre><code>[General]\nInputMethod=\n\n[Theme]\nCurrent=\"Breeze-Noir-Dark\"\nEnableAvatars=True\n</code></pre> <p>Reportedly, you have to create the <code>/etc/sddm.conf.d</code> directory to add the Local configuration file that allows setting the theme:</p> <pre><code># mkdir /etc/sddm.conf.d\n# vi /etc/sddm.conf.d/ubuntustudio.conf\n</code></pre> <p>Besides setting the theme, it is also good to limit the range of user ids so that only human users show up:</p> <pre><code>[Theme]\nCurrent=Breeze-Noir-Dark\n\n[Users]\nMaximumUid=1001\nMinimumUid=1000\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#bluetooth-controller","title":"Bluetooth controller","text":"<p>The following shows up in <code>dmesg</code>:</p> <pre><code>Bluetooth: Core ver 2.22\nNET: Registered PF_BLUETOOTH protocol family\nBluetooth: HCI device and connection manager initialized\nBluetooth: HCI socket layer initialized\nBluetooth: L2CAP socket layer initialized\nBluetooth: SCO socket layer initialized\nBluetooth: hci0: Device revision is 0\nBluetooth: hci0: Secure boot is enabled\nBluetooth: hci0: OTP lock is enabled\nBluetooth: hci0: API lock is enabled\nBluetooth: hci0: Debug lock is disabled\nBluetooth: hci0: Minimum firmware build 1 week 10 2014\nBluetooth: hci0: Bootloader timestamp 2019.40 buildtype 1 build 38\nBluetooth: hci0: DSM reset method type: 0x00\nBluetooth: hci0: Found device firmware: intel/ibt-0040-0041.sfi\nBluetooth: hci0: Boot Address: 0x100800\nBluetooth: hci0: Firmware Version: 60-48.23\nBluetooth: BNEP (Ethernet Emulation) ver 1.3\nBluetooth: BNEP filters: protocol multicast\nBluetooth: BNEP socket layer initialized\nBluetooth: hci0: Waiting for firmware download to complete\nBluetooth: hci0: Firmware loaded in 1536493 usecs\nBluetooth: hci0: Waiting for device to boot\nBluetooth: hci0: Device booted in 15625 usecs\nBluetooth: hci0: Malformed MSFT vendor event: 0x02\nBluetooth: hci0: Found Intel DDC parameters: intel/ibt-0040-0041.ddc\nBluetooth: hci0: Applying Intel DDC parameters completed\nBluetooth: hci0: Firmware timestamp 2023.48 buildtype 1 build 75324\nBluetooth: hci0: Firmware SHA1: 0x23bac558\nBluetooth: MGMT ver 1.22\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#bluetooth-headphones","title":"Bluetooth headphones","text":"<p>Pairing headphones (Bose QuietComfort SE) was quick and easy. As a fun bonus, every time the PC connects to the headphones an audible notification announced connected to super tuna which is fun.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#kernel-memory-leak","title":"Kernel memory leak","text":"<p>Only in this Nuc PC is Ubuntu Studio 24.04 showing a huge memory leak and it seems to be in the kernel rather than in any application/s:</p> <pre><code># free -h\n               total        used        free      shared  buff/cache   available\nMem:            30Gi        25Gi       1.6Gi       190Mi       4.2Gi       5.0Gi\nSwap:          8.0Gi       256Ki       8.0Gi\n</code></pre> <p>This could have been caused by transferring 1.5 TB of data over the network, but normally that would increase only the buff/cache usage and that is not the case here. It is the actually <code>used</code> memory that has been growing steadily over hours and is only released upon rebooting:</p> <p></p> Red herring: <code>fluidsynth</code> <p>Killing the most memory intensive process had previously made no dent:</p> <pre><code># ps -ax -eo user,pid,pcpu:5,pmem:5,rss:8=R-MEM,vsz:10=V-MEM,tty:6=TTY,stat:5,bsdstart:7,bsdtime:7,args | (read h; echo \"$h\"; sort -nr -k 4) | head -10 | less -SEX\nUSER         PID  %CPU  %MEM    R-MEM      V-MEM TTY    STAT    START    TIME COMMAND\nroot     2661652   0.5   0.7   246360    1568376 ?      SLsl    07:57    0:13 /usr/bin/fluidsynth -is /usr/share/sounds/sf3/default-GM.sf3 HOME=/ro&gt;\nsddm     2405392   0.2   0.4   158060    1140264 ?      Sl      00:48    1:03 /usr/bin/sddm-greeter --socket /tmp/sddm-:0-KPzxxr --theme /usr/share&gt;\ncolord     12644   0.0   0.3   120748     425728 ?      Ssl     00:40    0:00 /usr/libexec/colord LANG=es_ES.UTF-8 PATH=/usr/local/sbin:/usr/local/&gt;\nroot     2381522   0.0   0.2    83072     763252 tty2   Ssl+    00:47    0:14 /usr/lib/xorg/Xorg -nolisten tcp -background none -seat seat0 vt2 -au&gt;\nroot         481   0.0   0.2    75596     137064 ?      S&lt;s     00:40    0:19 /usr/lib/systemd/systemd-journald LANG=es_ES.UTF-8 PATH=/usr/local/sb&gt;\nroot       14132   0.0   0.1    40788     477384 ?      Ssl     00:54    0:01 /usr/libexec/fwupd/fwupd LANG=es_ES.UTF-8 PATH=/usr/local/sbin:/usr/l&gt;\nroot        1680   0.0   0.1    33252    2731468 ?      Ssl     00:40    0:02 /usr/lib/snapd/snapd LANG=es_ES.UTF-8 PATH=/usr/local/sbin:/usr/local&gt;\nminidlna    1645   0.0   0.1    36224     336280 ?      SLsl    00:40    0:00 /usr/sbin/minidlnad -f /etc/minidlna.conf -P /run/minidlna/minidlna.p&gt;\ndebian-+   30046   0.3   0.1    61244    1252000 ?      Ssl     00:40    1:26 /usr/bin/tor --defaults-torrc /usr/share/tor/tor-service-defaults-tor&gt;\n\n# killall /usr/bin/fluidsynth\n# ps -ax -eo user,pid,pcpu:5,pmem:5,rss:8=R-MEM,vsz:10=V-MEM,tty:6=TTY,stat:5,bsdstart:7,bsdtime:7,args | (read h; echo \"$h\"; sort -nr -k 4) | head -10 | less -SEX\nUSER         PID  %CPU  %MEM    R-MEM      V-MEM TTY    STAT    START    TIME COMMAND\nsddm     2405392   0.2   0.4   158060    1140328 ?      Sl      00:48    1:04 /usr/bin/sddm-greeter --socket /tmp/sddm-:0-KPzxxr --theme /usr/share&gt;\ncolord     12644   0.0   0.3   120748     425728 ?      Ssl     00:40    0:00 /usr/libexec/colord LANG=es_ES.UTF-8 PATH=/usr/local/sbin:/usr/local/&gt;\nroot     2381522   0.0   0.2    83072     763252 tty2   Ssl+    00:47    0:14 /usr/lib/xorg/Xorg -nolisten tcp -background none -seat seat0 vt2 -au&gt;\nroot         481   0.0   0.2    76236     137064 ?      S&lt;s     00:40    0:19 /usr/lib/systemd/systemd-journald LANG=es_ES.UTF-8 PATH=/usr/local/sb&gt;\nroot       14132   0.0   0.1    40788     477384 ?      Ssl     00:54    0:01 /usr/libexec/fwupd/fwupd LANG=es_ES.UTF-8 PATH=/usr/local/sbin:/usr/l&gt;\nroot        1680   0.0   0.1    33252    2731468 ?      Ssl     00:40    0:02 /usr/lib/snapd/snapd LANG=es_ES.UTF-8 PATH=/usr/local/sbin:/usr/local&gt;\nminidlna    1645   0.0   0.1    36224     336280 ?      SLsl    00:40    0:00 /usr/sbin/minidlnad -f /etc/minidlna.conf -P /run/minidlna/minidlna.p&gt;\ndebian-+   30046   0.3   0.1    61244    1252000 ?      Ssl     00:40    1:27 /usr/bin/tor --defaults-torrc /usr/share/tor/tor-service-defaults-tor&gt;\nvnstat      5498   0.0   0.0     3584       5476 ?      Ss      00:40    0:03 /usr/sbin/vnstatd -n LANG=es_ES.UTF-8 PATH=/usr/local/sbin:/usr/local&gt;\n\n# free -h\n              total        used        free      shared  buff/cache   available\nMem:            30Gi        27Gi       3.6Gi       188Mi       1.0Gi       3.8Gi\nSwap:          8.0Gi       256Ki       8.0Gi\n</code></pre> <p>That <code>fluidsynth</code> process may have been a leftover from having started <code>ardour</code> earlier, but this was not really a problematic case of Fluidsynth Starting Automatically On Boot; Login Freezing.</p> Red herring: <code>drop_caches</code> <p>The trick in linuxatemyram.com to drop caches did not help:</p> <pre><code># echo 3 | sudo tee /proc/sys/vm/drop_caches\n3\n\n# free -h\n              total        used        free      shared  buff/cache   available\nMem:            30Gi        26Gi       4.5Gi       188Mi       781Mi       4.6Gi\nSwap:          8.0Gi       256Ki       8.0Gi\n\n# free -m\n              total        used        free      shared  buff/cache   available\nMem:           31645       27081        4200         186        1013        4563\nSwap:           8191           0        8191\n</code></pre> <p>Indeed this only releases the <code>buff/cache</code> memory, not that which is <code>used</code>:</p> <p></p> <p>There was another hint somewhere (Red Hat Oracle documentation) about the number of \"huge pages\" but that is already set to zero:</p> <pre><code># cat /proc/sys/vm/nr_hugepages \n0\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#suspect-driver-i915","title":"Suspect driver: <code>i915</code>","text":"<p>After a wider and deeper investigation (see ~1500 lines in this log) it looks like a memory leak bug in the <code>i915</code> driver:</p> <pre><code># apt install smem -y\n\n# smem -twk\nArea                           Used      Cache   Noncache \nfirmware/hardware                 0          0          0 \nkernel image                      0          0          0 \nkernel dynamic memory         18.7G       3.1G      15.5G \nuserspace memory               2.4G       1.2G       1.3G \nfree memory                    9.8G       9.8G          0 \n----------------------------------------------------------\n                              30.9G      14.1G      16.8G \n\n# smem -wp\nArea                           Used      Cache   Noncache \nfirmware/hardware             0.00%      0.00%      0.00% \nkernel image                  0.00%      0.00%      0.00% \nkernel dynamic memory        58.94%     10.13%     48.81% \nuserspace memory              7.24%      3.05%      4.19% \nfree memory                  33.81%     33.81%      0.00% \n\n# lsmod | sort -nr -k 2 | head\ni915                 4284416  69\nxe                   2711552  0\nbtrfs                2015232  1\nmac80211             1720320  1 iwlmvm\nkvm                  1409024  1 kvm_intel\ncfg80211             1323008  3 iwlmvm,iwlwifi,mac80211\nbluetooth            1028096  71 btrtl,btmtk,btintel,btbcm,bnep,btusb,rfcomm\niwlmvm                868352  0\niwlwifi               598016  1 iwlmvm\nthunderbolt           516096  0\n\n# uname -a\nLinux super-tuna 6.8.0-47-lowlatency #47.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Oct  7 15:01:17 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n\n# lspci -nnk\n00:00.0 Host bridge [0600]: Intel Corporation Raptor Lake-P/U 4p+8e cores Host Bridge/DRAM Controller [8086:a707]\n        DeviceName: Onboard - Other\n        Subsystem: Intel Corporation Raptor Lake-P/U 4p+8e cores Host Bridge/DRAM Controller [8086:3037]\n        Kernel driver in use: igen6_edac\n        Kernel modules: igen6_edac\n00:02.0 VGA compatible controller [0300]: Intel Corporation Raptor Lake-P [UHD Graphics] [8086:a720] (rev 04)\n        DeviceName: Onboard - Video\n        Subsystem: Intel Corporation Raptor Lake-P [UHD Graphics] [8086:3037]\n        Kernel driver in use: i915\n        Kernel modules: i915, xe\n</code></pre> <p>Since the <code>i915</code> and <code>xe</code> drivers are taking more memory than all others, it looked like this could be a case of memory leak in the driver (e.g. like strongtz/i915-sriov-dkms issue #137). However, this system is using the \"vanilla\" <code>i915</code> drivers and, after the memory has been released by rebooting the system, the drivers still show about the same size:</p> <pre><code># smem -twk\nArea                           Used      Cache   Noncache \nfirmware/hardware                 0          0          0 \nkernel image                      0          0          0 \nkernel dynamic memory          3.6G       2.1G       1.5G \nuserspace memory               1.6G     499.5M       1.1G \nfree memory                   25.7G      25.7G          0 \n----------------------------------------------------------\n                              30.9G      28.3G       2.6G \n\n# smem -wp\nArea                           Used      Cache   Noncache \nfirmware/hardware             0.00%      0.00%      0.00% \nkernel image                  0.00%      0.00%      0.00% \nkernel dynamic memory        11.84%      6.79%      5.05% \nuserspace memory              5.08%      1.58%      3.50% \nfree memory                  83.07%     83.07%      0.00%\n\n# lsmod | sort -nr -k 2 | head\ni915                 4284416  60\nxe                   2711552  0\nbtrfs                2015232  1\nmac80211             1720320  1 iwlmvm\nkvm                  1409024  1 kvm_intel\ncfg80211             1323008  3 iwlmvm,iwlwifi,mac80211\nbluetooth            1028096  44 btrtl,btmtk,btintel,btbcm,bnep,btusb,rfcomm\niwlmvm                868352  0\niwlwifi               598016  1 iwlmvm\nthunderbolt           516096  0\n</code></pre> <p>The fix for strongtz/i915-sriov-dkms issue #137 seems to be strongtz/i915-sriov-dkms PR #204 but that looks like a non-trivial change.</p> <p>There is also a one-line patch to Fix memory leak by correcting cache object name in error handler from May 2024 and is not yet applied to the source file <code>drivers/gpu/drm/i915/i915_scheduler.c</code> in the <code>linux-source-6.8.0</code> package. This one seems a more likely fix, and a much simpler change to implement.</p> <p>The patch apperas to have been resent in late November, 2024, committed in early December, and that may be related to the patch not being (yet) available in the 6.8.0-47 and 6.8.0-49 kernels. The patch may be included in later kernels, but after an update in early January, 2025 the kernel updated to 6.8.0-51 and yet the linux-source-6.8.0 package on version 6.8.0-51.52 still does not have the patched applied.</p> <p>Todo</p> <p>BuildYourOwnKernel with the patched applied, see if that helps.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#maybe-somewhere-else","title":"Maybe somewhere else?","text":"<p>If the <code>i915</code> is not behind this memory leak, the one other lead from this log is, by far, the largest user of kernel memory is <code>kmalloc-rnd-07-8k</code>:</p> <pre><code># slabtop -o | head -10\n Active / Total Objects (% used)    : 3424282 / 3461230 (98,9%)\n Active / Total Slabs (% used)      : 507294 / 507294 (100,0%)\n Active / Total Caches (% used)     : 341 / 386 (88,3%)\n Active / Total Size (% used)       : 15543458,94K / 15549019,20K (100,0%)\n Minimum / Average / Maximum Object : 0,01K / 4,49K / 10,25K\n\n  OBJS ACTIVE USE OBJ SIZE SLABS OBJ/SLAB CACHE SIZE NAME                     \n1901280 1901280 100%    8,00K 475320        4  15210240K kmalloc-rnd-07-8k\n 170310  170106  99%    0,19K   4055       42     32440K dentry\n 162825  162299  99%    0,10K   4175       39     16700K buffer_head\n</code></pre> <p>That's a whooping 14.5 GB used only by <code>kmalloc-rnd-07-8k</code>, which is released only after a reboot (although soon enough back up to nearly 2 GB):</p> <pre><code># slabtop -o | head -10\n Active / Total Objects (% used)    : 1413878 / 1435036 (98.5%)\n Active / Total Slabs (% used)      : 87550 / 87550 (100.0%)\n Active / Total Caches (% used)     : 338 / 386 (87.6%)\n Active / Total Size (% used)       : 2300293.48K / 2306032.96K (99.8%)\n Minimum / Average / Maximum Object : 0.01K / 1.61K / 10.25K\n\n  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME                   \n256376 256376 100%    8.00K  64094        4   2051008K kmalloc-rnd-12-8k      \n114996 114394  99%    0.19K   2738       42     21904K dentry                 \n 73100  72807  99%    0.05K    860       85      3440K shared_policy_node     \n</code></pre> <p>There is nothing like this on <code>rapture</code>, where <code>kmalloc-rnd-07-8k</code> is only 512K:</p> <pre><code>root@rapture:~# slabtop -o | head -10\n Active / Total Objects (% used)    : 7453076 / 7724162 (96.5%)\n Active / Total Slabs (% used)      : 178099 / 178099 (100.0%)\n Active / Total Caches (% used)     : 338 / 385 (87.8%)\n Active / Total Size (% used)       : 2520903.59K / 2613929.42K (96.4%)\n Minimum / Average / Maximum Object : 0.01K / 0.34K / 32.54K\n\n  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME                   \n1435392 1434726  99%    0.19K  34176       42    273408K dentry                 \n1334925 1332417  99%    0.05K  15705       85     62820K shared_policy_node     \n1063179 1063179 100%    1.06K  35928       30   1149696K btrfs_inode            \n\n# slabtop -o | grep kmalloc-rnd-12-8k\n      64     64 100%    8.00K     16        4       512K kmalloc-rnd-12-8k \n</code></pre> <p>To compare again, the biggest user of kernel dynamic memory is</p> <ul> <li><code>btrfs_inode</code> in <code>rapture</code>, with 1,149,792K after a good 20 hours up</li> <li><code>kmalloc-rnd-12-8k</code> in <code>super-tuna</code>, with 2,391,360K after barely 1 hour up</li> </ul> <p>5 hours later <code>kmalloc-rnd-12-8k</code> in <code>super-tuna</code> is up 13.71 GB again, which amounts for most of the <code>SUnreclaim</code> memory.</p> <code>kmalloc-rnd-12-8k</code> up 13.7 GB, <code>SUnreclaim</code> up 15.3 GB. <pre><code># smem -twk\nArea                           Used      Cache   Noncache \nfirmware/hardware                 0          0          0 \nkernel image                      0          0          0 \nkernel dynamic memory         16.4G       2.2G      14.2G \nuserspace memory               1.6G     534.1M       1.1G \nfree memory                   12.9G      12.9G          0 \n----------------------------------------------------------\n                              30.9G      15.6G      15.3G \n# smem -wp\nArea                           Used      Cache   Noncache \nfirmware/hardware             0.00%      0.00%      0.00% \nkernel image                  0.00%      0.00%      0.00% \nkernel dynamic memory        53.19%      7.08%     46.11% \nuserspace memory              5.24%      1.69%      3.55% \nfree memory                  41.57%     41.57%      0.00% \n\n# lsmod | sort -nr -k 2 | head -3\ni915                 4284416  59\nxe                   2711552  0\nbtrfs                2015232  1\n\n# slabtop -o | sort -r -n -k 7 | head -10\n1797400 1797400 100%    8.00K 449350        4  14379200K kmalloc-rnd-12-8k      \n22302  22302 100%    1.16K    826       27     26432K ext4_inode_cache       \n117432 116955  99%    0.19K   2796       42     22368K dentry                 \n22900  22835  99%    0.62K    916       25     14656K inode_cache            \n20944  20925  99%    0.57K    748       28     11968K radix_tree_node        \n72324  69525  96%    0.14K   2583       28     10332K kernfs_node_cache      \n  954    946  99%   10.25K    318        3     10176K task_struct            \n13754  13041  94%    0.70K    299       46      9568K proc_inode_cache       \n48510  47450  97%    0.19K   1155       42      9240K cred_jar               \n85137  85137 100%    0.10K   2183       39      8732K buffer_head     \n\n# cat /proc/slabinfo | grep \"kmalloc-rnd-12-8k\"\nkmalloc-rnd-12-8k 1962444 1962444   8192    4    8 : tunables    0    0    0 : slabdata 490611 490611      0\n\n# head -3 /proc/slabinfo; cat /proc/slabinfo | grep \"kmalloc-rnd-12-8k\"\nslabinfo - version: 2.1\n# name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;\nQIPCRTR               78     78    832   39    8 : tunables    0    0    0 : slabdata      2      2      0\nkmalloc-rnd-12-8k 1963068 1963068   8192    4    8 : tunables    0    0    0 : slabdata 490767 490767      0\n\n# free -h\n              total        used        free      shared  buff/cache   available\nMem:            30Gi        17Gi        11Gi       748Mi       2.7Gi        13Gi\nSwap:          8.0Gi          0B       8.0Gi\n\n# cat /proc/meminfo \nMemTotal:       32404656 kB\nMemFree:        12123032 kB\nMemAvailable:   13785908 kB\nBuffers:          137656 kB\nCached:          2610480 kB\nSwapCached:            0 kB\nActive:          1825832 kB\nInactive:        1126328 kB\nActive(anon):     970752 kB\nInactive(anon):        0 kB\nActive(file):     855080 kB\nInactive(file):  1126328 kB\nUnevictable:      940008 kB\nMlocked:          212356 kB\nSwapTotal:       8388604 kB\nSwapFree:        8388604 kB\nZswap:                 0 kB\nZswapped:              0 kB\nDirty:               256 kB\nWriteback:             0 kB\nAnonPages:       1143976 kB\nMapped:           548812 kB\nShmem:            766728 kB\nKReclaimable:     101908 kB\nSlab:           16161044 kB\nSReclaimable:     101908 kB\nSUnreclaim:     16059136 kB\nKernelStack:       13280 kB\nPageTables:        27176 kB\n...\n</code></pre> <p>Google web search shows a single search result for <code>\"kmalloc-rnd-12-8k\"</code> which does not lead to a conclusive solution.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#maybe-gone-after-reinstalling","title":"Maybe gone after reinstalling?","text":"<p>Out of frustation with this opaque memory leak and a few other details, decided to reinstall Ubuntu Studio 24.04 anew.</p> <p>Along the way had to update the BIOS (to version 33) because until that update the systen would not boot from the USB start-up disk, even though it had booted from it only minutes ago.</p> <p>Eventually, most to fhe above setup was restored and yet the memory leak was no longer happening:</p> <p></p> <p>Moreover, it seems the CPU cores are running visibly cooler now:</p> <p></p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#but-why","title":"But Why?","text":"<p>A few things were different when reinstalling the system.</p> <p>First, the system language was left as its default value: English. This should be irrelevant, but when the system locazation affects everything down to the output from commands, it might just happen that different number format and/or translated messages may trigger bugs.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#not-gone-after-reinstalling","title":"Not gone after reinstalling","text":"<p>The (or a) memory leak is still there, even if only it grows slowly enough that it takes about 3 days to reach the point where almost all 32 GB of RAM are taken up by <code>SUnreclaim</code> and the biggest memory hoarder is, by far, <code>kmalloc-rnd-11-8k</code>:</p> <pre><code># cat /proc/meminfo\n...\nSlab:           31814260 kB\nSReclaimable:      50868 kB\nSUnreclaim:     31763392 kB\n...\n\n# slabtop -o | head -10\n Active / Total Objects (% used)    : 4817156 / 5002245 (96.3%)\n Active / Total Slabs (% used)      : 1032290 / 1032290 (100.0%)\n Active / Total Caches (% used)     : 342 / 386 (88.6%)\n Active / Total Size (% used)       : 31637718.69K / 31682510.83K (99.9%)\n Minimum / Average / Maximum Object : 0.01K / 6.33K / 10.25K\n\n  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME                   \n3931151 3931151 100%    8.00K 1010822        4  32346304K kmalloc-rnd-11-8k      \n 71145  36213  50%    0.05K    837       85      3348K shared_policy_node     \n 67928  67790  99%    0.14K   2426       28      9704K kernfs_node_cache      \n\n# cat /proc/slabinfo | grep \"kmalloc-rnd-11-8k\"\nkmalloc-rnd-11-8k 3931772 3931772   8192    4    8 : tunables    0    0    0 : slabdata 1011434 1011434      0\n\n# head -3 /proc/slabinfo; cat /proc/slabinfo | grep kmalloc-rnd-11-8k\nslabinfo - version: 2.1\n# name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;\nQIPCRTR               78     78    832   39    8 : tunables    0    0    0 : slabdata      2      2      0\nkmalloc-rnd-11-8k 3931887 3931887   8192    4    8 : tunables    0    0    0 : slabdata 1011528 1011528      0\n</code></pre> Repeat run or debug commands above shows about the same. <pre><code># free -h\n              total        used        free      shared  buff/cache   available\nMem:            30Gi        30Gi       291Mi        61Mi       152Mi        73Mi\nSwap:          8.0Gi       271Mi       7.7Gi\n\n# ps -ax -eo user,pid,pcpu:5,pmem:5,rss:8=R-MEM,vsz:10=V-MEM,tty:6=TTY,stat:5,bsdstart:7,bsdtime:7,args | (read h; echo \"$h\"; sort -nr -k 4) | head -10 | less -SEX\nUSER         PID  %CPU  %MEM    R-MEM      V-MEM TTY    STAT    START    TIME COMMAND\nvnstat      2707   0.0   0.0     1536       5516 ?      Ss     Jan  5    0:17 /usr/sbin/vnstatd -n LANG=en_US.UTF-8 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/snap/bin USER=vnstat LOGNAME=vnstat HOME=/var/lib/vnstat INVOCATION_ID=ac8c9f302e5444b691575435a6157b99 JOURNAL_STREAM=8:29855 STATE_DIRECTORY=/var/lib/vnstat SYSTEMD_EXEC_PID=2707 MEMORY_PRESSURE_WATCH=/sy&gt;\nsystemd+    1201   0.0   0.0     1152      91044 ?      Ssl    Jan  5    0:01 /usr/lib/systemd/systemd-timesyncd LANG=en_US.UTF-8 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/snap/bin NOTIFY_SOCKET=/run/systemd/notify WATCHDOG_PID=1201 WATCHDOG_USEC=180000000 USER=systemd-timesync LOGNAME=systemd-timesync INVOCATION_ID=9f337859e19249b2977af8bb586f2c94 JOURNAL_STREA&gt;\nsystemd+    1196   0.0   0.0      384      22008 ?      Ss     Jan  5    0:48 /usr/lib/systemd/systemd-resolved LANG=en_US.UTF-8 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/snap/bin NOTIFY_SOCKET=/run/systemd/notify WATCHDOG_PID=1196 WATCHDOG_USEC=180000000 USER=systemd-resolve LOGNAME=systemd-resolve INVOCATION_ID=5707dd899b2541a289e315ed6616d70c JOURNAL_STREAM=8&gt;\nsyslog      1787   0.0   0.0      640     222508 ?      Ssl    Jan  5    0:02 /usr/sbin/rsyslogd -n -iNONE LANG=en_US.UTF-8 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/snap/bin NOTIFY_SOCKET=/run/systemd/notify LISTEN_PID=1787 LISTEN_FDS=1 LISTEN_FDNAMES=syslog.socket USER=root INVOCATION_ID=94eeaa8f349e456fa5df905f17729693 JOURNAL_STREAM=8:29727 SYSTEMD_EXEC_PID=&gt;\nrtkit       3291   0.0   0.0     1024      22940 ?      SNsl   Jan  5    0:04 /usr/libexec/rtkit-daemon LANG=en_US.UTF-8 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/snap/bin NOTIFY_SOCKET=/run/systemd/notify USER=root INVOCATION_ID=6e9a99b6e2ab44019c666b2ae4b886cc JOURNAL_STREAM=8:21269 SYSTEMD_EXEC_PID=3291 MEMORY_PRESSURE_WATCH=/sys/fs/cgroup/system.slice/rtkit-&gt;\nroot          99   0.2   0.0        0          0 ?      S      Jan  5   11:40 [ksoftirqd/15]\nroot         984   0.0   0.0        0          0 ?      I&lt;     Jan  5    0:00 [kworker/R-btrfs]\nroot         983   0.0   0.0        0          0 ?      I&lt;     Jan  5    0:00 [kworker/R-btrfs]\nroot         982   0.0   0.0        0          0 ?      I&lt;     Jan  5    0:00 [kworker/R-btrfs]\n\n# echo 3 | tee /proc/sys/vm/drop_caches\n3\n\n# free -h\n              total        used        free      shared  buff/cache   available\nMem:            30Gi        30Gi       245Mi        87Mi       187Mi        32Mi\nSwap:          8.0Gi       257Mi       7.7Gi\n\n# free -m\n              total        used        free      shared  buff/cache   available\nMem:           31645       31594         269          79         166          50\nSwap:           8191         265        7926\n\n# sync\n\n# cat /proc/sys/vm/nr_hugepages\n0\n\n# smem -twk\nArea                           Used      Cache   Noncache \nfirmware/hardware                 0          0          0 \nkernel image                      0          0          0 \nkernel dynamic memory         30.6G     155.9M      30.4G \nuserspace memory              39.4M      19.6M      19.8M \nfree memory                  278.6M     278.6M          0 \n----------------------------------------------------------\n                              30.9G     454.1M      30.5G \n\n# smem -wp\nArea                           Used      Cache   Noncache \nfirmware/hardware             0.00%      0.00%      0.00% \nkernel image                  0.00%      0.00%      0.00% \nkernel dynamic memory        99.00%      0.43%     98.57% \nuserspace memory              0.13%      0.06%      0.07% \nfree memory                   0.86%      0.86%      0.00% \n\n# lsmod | sort -nr -k 2 | head\ni915                 4292608  27\nxe                   2715648  0\nbtrfs                2019328  1\nmac80211             1720320  1 iwlmvm\nkvm                  1409024  1 kvm_intel\ncfg80211             1323008  3 iwlmvm,iwlwifi,mac80211\nbluetooth            1028096  34 btrtl,btmtk,btintel,btbcm,bnep,btusb,rfcomm\niwlmvm                872448  0\niwlwifi               598016  1 iwlmvm\nthunderbolt           516096  0\n\n# uname -a\nLinux super-tuna 6.8.0-49-lowlatency #49.1-Ubuntu SMP PREEMPT_DYNAMIC Sun Nov 10 10:00:36 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n\n# slabtop -o | head -10\nActive / Total Objects (% used)    : 4817156 / 5002245 (96.3%)\nActive / Total Slabs (% used)      : 1032290 / 1032290 (100.0%)\nActive / Total Caches (% used)     : 342 / 386 (88.6%)\nActive / Total Size (% used)       : 31637718.69K / 31682510.83K (99.9%)\nMinimum / Average / Maximum Object : 0.01K / 6.33K / 10.25K\n\n  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME                   \n3931151 3931151 100%    8.00K 1010822        4  32346304K kmalloc-rnd-11-8k      \n71145  36213  50%    0.05K    837       85      3348K shared_policy_node     \n67928  67790  99%    0.14K   2426       28      9704K kernfs_node_cache      \n\n# cat /proc/slabinfo | grep \"kmalloc-rnd-11-8k\"\nkmalloc-rnd-11-8k 3931772 3931772   8192    4    8 : tunables    0    0    0 : slabdata 1011434 1011434      0\n\n# head -3 /proc/slabinfo; cat /proc/slabinfo | grep kmalloc-rnd-11-8k\nslabinfo - version: 2.1\n# name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;\nQIPCRTR               78     78    832   39    8 : tunables    0    0    0 : slabdata      2      2      0\nkmalloc-rnd-11-8k 3931887 3931887   8192    4    8 : tunables    0    0    0 : slabdata 1011528 1011528      0\n\n# cat /proc/meminfo\nMemTotal:       32404500 kB\nMemFree:          283740 kB\nMemAvailable:      60448 kB\nBuffers:            2388 kB\nCached:           102356 kB\nSwapCached:         9336 kB\nActive:            21592 kB\nInactive:          33280 kB\nActive(anon):       7320 kB\nInactive(anon):     6100 kB\nActive(file):      14272 kB\nInactive(file):    27180 kB\nUnevictable:       62500 kB\nMlocked:              16 kB\nSwapTotal:       8388604 kB\nSwapFree:        8105176 kB\nZswap:                 0 kB\nZswapped:              0 kB\nDirty:               552 kB\nWriteback:             0 kB\nAnonPages:         10912 kB\nMapped:            18372 kB\nShmem:             63280 kB\nKReclaimable:      50868 kB\nSlab:           31814260 kB\nSReclaimable:      50868 kB\nSUnreclaim:     31763392 kB\nKernelStack:        9344 kB\nPageTables:        10260 kB\nSecPageTables:         0 kB\nNFS_Unstable:          0 kB\nBounce:                0 kB\nWritebackTmp:          0 kB\nCommitLimit:    24590852 kB\nCommitted_AS:    1989136 kB\nVmallocTotal:   34359738367 kB\nVmallocUsed:       50920 kB\nVmallocChunk:          0 kB\nPercpu:            21888 kB\nHardwareCorrupted:     0 kB\nAnonHugePages:         0 kB\nShmemHugePages:     8192 kB\nShmemPmdMapped:        0 kB\nFileHugePages:         0 kB\nFilePmdMapped:         0 kB\nUnaccepted:            0 kB\nHugePages_Total:       0\nHugePages_Free:        0\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nHugetlb:               0 kB\nDirectMap4k:      315792 kB\nDirectMap2M:     8658944 kB\nDirectMap1G:    25165824 kB\n</code></pre> Understanding Linux Kernel Memory Statistics explains these values:"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#byte-level-memory-vmalloc","title":"Byte-level memory: VMalloc","text":""},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#slab-allocator","title":"Slab allocator","text":"<p>The kernel uses various data structures in different kernel components and drivers. Some examples are <code>struct mm_struct</code> used for each process's address space and <code>struct buffer_head</code> used in filesystem I/O. The Slab allocator manages caches of commonly used objects kept in an initialized state available to be used later. This saves time for allocating, initialising, and freeing objects.</p> <p>The following statistics offer insights into the Slab allocator\u2019s impact.</p> <ul> <li><code>Slab</code> is the total memory used by all Slab caches.</li> <li><code>SReclaimable is</code> the amount of the Slab that might be reclaimed      (such as cache objects like dentry).</li> <li><code>SUnreclaim</code> is the opposite. When unreclaimable slab memory takes up a     high percentage of the total memory, system performance may be affected.</li> </ul> <pre><code>Slab:            1622084 kB\nSReclaimable:    1000692 kB\nSUnreclaim:       621392 kB\n</code></pre> <p>Slab memory can grow when a kernel component or a driver requests memory but fails to release the memory properly, which is a typical memory leak case. Delving deeper into the specifics of <code>/proc/slabinfo</code> is crucial for identifying the particular slab object that has accumulated substantial memory usage.</p> <p>...</p> <p>For byte-level memory, <code>kmalloc</code> and <code>vmalloc</code> are involved. <code>kmalloc</code> allocates physically contiguous memory regions, while <code>vmalloc</code> handles memory blocks that may not be available in contiguous memory regions.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#debug-memory-leak","title":"Debug memory leak","text":"<p>If applying the <code>i915</code> patch does not help, the next step would be to do a Kernel dynamic memory analysis to find out what is leaking memory.</p> <p>This should be relatively easy since <code>/sys/kernel/debug/tracing/trace</code> is already present, although it only contains the headers. To enable tracing of memory events, add this to kernel parameters:</p> <pre><code>trace_event=kmem:kmalloc,kmem:kmem_cache_alloc,kmem:kfree,kmem:kmem_cache_free\n</code></pre> <p>This can be done by adding <code>/etc/default/grub.d/trace.cfg</code> with</p> <pre><code># Enable tracing of memory events\nGRUB_CMDLINE_LINUX_DEFAULT=\"$GRUB_CMDLINE_LINUX_DEFAULT trace_event=kmem:kmalloc,kmem:kmem_cache_alloc,kmem:kfree,kmem:kmem_cache_free\"\n</code></pre> <p>After 2 days running with this, there are 745,033 lines in <code>/sys/kernel/debug/tracing/trace</code> but it is not entirely clear what callsites they point to:</p> <pre><code># grep alloc kmem.log | grep --color=no -o 'call_site=[a-zA-Z_0-9]*' | sed 's/call_site=//' &gt; kmem.txt\n# sort kmem.txt | uniq -c | sort -nr | head\n 138686 vm_area_dup\n  92207 anon_vma_clone\n  77047 getname_flags\n  75840 alloc_buffer_head\n  64657 vm_area_alloc\n  58102 mas_alloc_nodes\n  49660 __alloc_skb\n  44111 security_file_alloc\n  44111 alloc_empty_file\n  42981 anon_vma_fork\n  26055 __anon_vma_prepare\n  19385 i915_gem_do_execbuffer\n  10631 kvmalloc_node\n   9379 seq_open\n   8660 single_open\n   7014 drm_syncobj_array_wait_timeout\n   7014 drm_syncobj_array_find\n   6537 jbd2__journal_start\n   3071 __d_alloc\n   2765 prepare_creds\n   2763 security_prepare_creds\n   2648 security_inode_alloc\n   2527 kmalloc_reserve\n   2518 load_elf_binary\n   2517 load_elf_phdrs\n   2430 alloc_pipe_info\n   2361 krealloc\n   2102 alloc_extent_state\n   1883 genl_family_rcv_msg_attrs_parse\n   1804 memcg_alloc_slab_cgroups\n   1512 dup_task_struct\n   1509 security_task_alloc\n   1509 audit_alloc\n   1507 mas_dup_build\n   1507 dup_mm\n   1507 copy_signal\n   1507 copy_sighand\n   1506 dup_fd\n   1506 copy_fs_struct\n   1494 alloc_pid\n   1400 alloc_vmap_area\n   1350 proc_alloc_inode\n   1302 getname_kernel\n   1264 xas_alloc\n   1253 mm_alloc\n   1253 alloc_bprm\n   1215 alloc_inode\n   1173 __i915_request_create\n   1171 drm_syncobj_create\n   1171 add_fence_array\n    865 sg_kmalloc\n    831 mempool_alloc_slab\n    804 i915_vma_resource_alloc\n    798 drm_atomic_state_init\n    797 vma_create\n</code></pre> <p>It seems one would really need to wrestle against gcc to obtain accurate callsites. Moreover, the script to analyze this file (<code>trace_analyze.py</code>) is from 2012 and writen in Python 2.</p> <p>Alternative methods:</p> <ul> <li>Kernel Memory Leak Detector would require enabling <code>CONFIG_DEBUG_KMEMLEAK</code>     when compling a new kernel, because <code>/sys/kernel/debug/kmemleakls</code>     is not present.</li> <li>euspectre/kedr seems more     involved, based on its     step-by-step tutorial.</li> </ul>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#disable-swap","title":"Disable swap","text":"<p>Swap was disabled by commenting out the corresponding line in <code>/etc/fstab</code>.</p> <p>The memory leak was detected when the Chrome browser started crashing consistently despite running only one tab on one web application, and eventually even SSH sessions where crashing and at that point it seems the availability of swap lead the system to go into thrashing, because at that point essentially all the RAM was taken up and Chrome could no longer allocate enough memory:</p> <p></p> <p>In the 82 hours the system had been running until it was no longer stable, the memory leak had squeezed Chrome (and eventually everything else) out of all available memory:</p> <p> </p> <p>This could have been detected earlier, since the used RAM had already grown to 14 GB in the first 24 hours:</p> <p></p> <p>Such a memory leak did not afect Raven even afte 7 days running the same Ubuntu Studio 24.04, installed from the same USB media following the same process, running the same desktop and even more applications; but that PC has an NVidia card, like most others.</p> <p></p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#remove-cups","title":"Remove CUPS","text":"<p>On several occassions <code>cupsd</code> had suddently gone up to 200% CPU usage and stayed up there for 30+ minutes with no sign of and end to come.</p> <pre><code>root        5512  0.0  0.0   2892  1664 ?        Ss   Nov16   0:00 /bin/sh /snap/cups/1067/scripts/run-cupsd\nroot        9582  0.0  0.0  63780 12032 ?        S    Nov16   0:00 cupsd -f -s /var/snap/cups/common/etc/cups/cups-files.conf -c /var/snap/cups/common/etc/cups/cupsd.conf\nroot     3659053  0.0  0.0   7160  2048 pts/2    S+   00:29   0:00 grep --color=auto cupsd\nroot     3689314  194  0.0 184064 12416 ?        Rsl  00:00  56:44 /usr/sbin/cupsd -l\n</code></pre> <p>This PC won't have any printers connected to it, so CUPS can be removed:</p> <pre><code># snap services\nService                                              Startup   Current   Notes\ncanonical-livepatch.canonical-livepatchd             enabled   active    -\nchromium.daemon                                      disabled  inactive  -\ncups.cups-browsed                                    enabled   active    -\ncups.cupsd                                           enabled   active    -\nfirmware-updater.firmware-notifier                   enabled   -         user,timer-activated\nfirmware-updater.firmware-updater-app                enabled   -         user,dbus-activated\nsnapd-desktop-integration.snapd-desktop-integration  enabled   -         user\n\n# snap stop cups\nStopped.\n\n# snap disable cups\ncups disabled\n\n# ps aux | grep cupsd\nroot     1865839  0.0  0.0  39304 12544 ?        Ss   00:00   0:00 /usr/sbin/cupsd -l\nroot     2208813  0.0  0.0   9144  1920 pts/1    S+   00:24   0:00 grep --color=auto cupsd\n\n# tail -f /var/log/cups/*log  | grep -v pam_unix\n==&gt; /var/log/cups/access_log &lt;==\nlocalhost - - [09/Jan/2025:00:00:00 +0100] \"POST / HTTP/1.1\" 200 357 Create-Printer-Subscriptions successful-ok\nlocalhost - - [09/Jan/2025:00:00:00 +0100] \"POST / HTTP/1.1\" 200 176 Create-Printer-Subscriptions successful-ok\nlocalhost - - [09/Jan/2025:00:00:05 +0100] \"POST /admin/ HTTP/1.1\" 401 0 - -\nlocalhost - cups-browsed [09/Jan/2025:00:00:05 +0100] \"POST /admin/ HTTP/1.1\" 200 181 CUPS-Delete-Printer client-error-not-found\nlocalhost - - [09/Jan/2025:00:23:29 +0100] \"POST / HTTP/1.1\" 401 120 Cancel-Subscription successful-ok\nlocalhost - root [09/Jan/2025:00:23:29 +0100] \"POST / HTTP/1.1\" 200 120 Cancel-Subscription successful-ok\n\n==&gt; /var/log/cups/error_log &lt;==\n\n# snap services\nService                                              Startup   Current   Notes\ncanonical-livepatch.canonical-livepatchd             enabled   active    -\nchromium.daemon                                      disabled  inactive  -\ncups.cups-browsed                                    disabled  inactive  -\ncups.cupsd                                           disabled  inactive  -\nfirmware-updater.firmware-notifier                   enabled   -         user,timer-activated\nfirmware-updater.firmware-updater-app                enabled   -         user,dbus-activated\nsnapd-desktop-integration.snapd-desktop-integration  enabled   -         user\n\n# systemctl stop cups.service\n\n# ps aux | grep cupsd\nroot     2271225  0.0  0.0   9144  2176 pts/1    S+   00:29   0:00 grep --color=auto cupsd\n</code></pre> <p>This issue repeated after reinstalling, lasting for up to 90 minutes:</p> <p></p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#december-2025-follow-up","title":"December 2025 Follow-up","text":"<p>Over time not only needs the system a daily reboot to flush the memory leak, but also the whole system feels laggy and sluggish and specially playing videos on YouTube and switching to/from full-screen. There are two main routes to try: the newer <code>xe</code> driver, or if that leads to worse problems, updating BIOS and kernel to much newer versions.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#tweak-google-chrome","title":"Tweak Google Chrome","text":"<p>The following adjustments to Google Chrome are recommended independently of which driver is used (<code>i915</code> or <code>xe</code>).</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#disable-gpu-override","title":"Disable GPU override","text":"<p>Google Chrome tends to avoid GPUs that are not deemed stable or mature enough, even though oftentime they actually are quite good enough. To disable this behavior:</p> <ul> <li>Check <code>chrome://gpu</code> and see if Canvas is reported as     Software only, hardware acceleration unavailable, which usually means Chrome has     blocklisted the GPU.</li> <li>Go to <code>chrome://flags</code> and set the flat for Override software rendering list     to Enabled; restart Chrome.</li> </ul>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#install-the-h264ify-extension","title":"Install the <code>h264ify</code> extension","text":"<p>After installing the h264ify extension in Chrome, YouTube \"Stats for nerds\" shows the codecs used are avc and mp4a. This does seem to make playback smoother than previously with codecs av01 and  opus.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#switching-to-the-xe-driver","title":"Switching to the <code>xe</code> driver","text":"<p>The above behavior with system-wide a steady climb in kernel dynamic memory\u2014strongly suggested a kernel-level memory leak associated with the Intel graphics driver. In Ubuntu 24.04 (Noble Numbat), 13th Generation Intel CPUs (i5-1340P) can use either the legacy <code>i915</code> driver or the newer, modern <code>xe</code> driver. The <code>lsmod</code> output shows both are loaded, but <code>i915</code> has 60 active references while <code>xe</code> has 0, confirming the system is relying on the older driver.</p> <p>The <code>xe</code> driver was designed from the ground up for modern Intel hardware (Tiger Lake and newer) and is increasingly the focus for performance and stability fixes in 2025. Switching the driver should help resolve the memory leak.</p> <p>Identify the PCI ID of the GPU:</p> <pre><code>$ lspci -nn | grep VGA\n00:02.0 VGA compatible controller [0300]: Intel Corporation Raptor Lake-P [UHD Graphics] [8086:a720] (rev 04)\n</code></pre> <p>The ID is the second part of the hex code in brackets: <code>a720</code>.</p> <p>Update <code>/etc/default/grub</code> to add the options to <code>GRUB_CMDLINE_LINUX_DEFAULT</code> to force the <code>xe</code> driver to detect the GPU instead of the <code>i915</code> driver:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash i915.force_probe=!a720 xe.force_probe=a720\"\n</code></pre> <p>Update GRUB and reboot:</p> <pre><code># update-grub\n# shutdowh -r now \n</code></pre> <p>After rebooting, <code>lsmod</code> shows the <code>xe</code> driver is now the one with active usage, while <code>i915</code> has 0:</p> <pre><code># lsmod | grep -E 'i915|xe'\nxe                   2732032  47\ndrm_gpuvm              45056  1 xe\ndrm_exec               12288  2 drm_gpuvm,xe\ngpu_sched              61440  1 xe\ndrm_suballoc_helper    16384  1 xe\ndrm_ttm_helper         12288  1 xe\ni915                 4300800  0\ndrm_buddy              20480  2 xe,i915\nttm                   110592  3 drm_ttm_helper,xe,i915\ndrm_display_helper    237568  2 xe,i915\ncec                    94208  3 drm_display_helper,xe,i915\ni2c_algo_bit           16384  2 xe,i915\nvideo                  77824  4 asus_wmi,asus_nb_wmi,xe,i915\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#gpu-monitoring","title":"GPU monitoring","text":"<p>After switching to the <code>xe</code> driver <code>intel_gpu_top</code> no longer works:</p> <pre><code># intel_gpu_top \nNo device filter specified and no discrete/integrated i915 devices found\n</code></pre> <p>The <code>intel_gpu_top</code> utility is hard-coded to work with the legacy <code>i915</code> driver and does not natively support the newer <code>xe</code> driver. Instead, <code>nvtop</code> (Neat Videocard TOP) can be used as the most effective replacement for monitoring your GPU while using the <code>xe</code> driver. However, even <code>nvtop</code> needs to be pretty recent to detect this GPU, so it must be installed from <code>snap</code> rather than the Ubuntu repository.</p> <p>By default <code>nvtop</code> shows a nice UI with charts, but it can also be used to export data in JSON format:</p> <pre><code># nvtop -s\n[\n  {\n   \"device_name\": \"Raptor Lake-P (UHD Graphics)\",\n   \"gpu_clock\": \"333MHz\",\n   \"mem_clock\": null,\n   \"temp\": null,\n   \"fan_speed\": \"CPU Fan\",\n   \"power_draw\": null,\n   \"gpu_util\": null,\n   \"mem_util\": \"3%\"\n  }\n]\n</code></pre> <p>The only useful metric available are <code>gpu_clock</code> and <code>mem_util</code>; the <code>fan_speed</code> seems to merely refer to the fact that there is no separate fan for the GPU and the other metrics are not available.</p> <p>Unlike <code>nvtop</code>, <code>vainfo</code> remains unable to find this GPU:</p> <pre><code># export LIBVA_DRIVER_NAME=iHD\n# vainfo --display drm --device /dev/dri/renderD128\nlibva info: VA-API version 1.20.0\nlibva info: User environment variable requested driver 'iHD'\nlibva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so\nlibva info: Found init function __vaDriverInit_1_20\nlibva error: /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so init failed\nlibva info: va_openDriver() returns 18\nvaInitialize failed with error code 18 (invalid parameter),exit\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#install-va-driver-non-free","title":"Install VA driver non-free","text":"<p>There is a lot of lagging in the browser, because the system is currently using Software Rendering (CPU-only) to decode and display video. Even though the <code>xe</code> kernel driver is loaded, the userspace software (Chrome, <code>vainfo</code>, <code>nvtop</code>) cannot access the GPU's hardware acceleration features because the driver interface is not correctly initialized, and this was because Ubuntu did not install the <code>non-free</code> driver.</p> <p>Install the <code>intel-media-va-driver-non-free</code> driver and reboot:</p> <pre><code># apt install intel-media-va-driver-non-free -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  intel-media-va-driver\nThe following NEW packages will be installed:\n  intel-media-va-driver-non-free\n0 upgraded, 1 newly installed, 1 to remove and 0 not upgraded.\nNeed to get 0 B/8,784 kB of archives.\nAfter this operation, 22.5 MB of additional disk space will be used.\ndpkg: intel-media-va-driver:amd64: dependency problems, but removing anyway as you requested:\n va-driver-all:amd64 depends on intel-media-va-driver | intel-media-va-driver-non-free; however:\n  Package intel-media-va-driver:amd64 is to be removed.\n  Package intel-media-va-driver-non-free is not installed.\n\n(Reading database ... 470408 files and directories currently installed.)\nRemoving intel-media-va-driver:amd64 (24.1.0+dfsg1-1ubuntu0.1) ...\nSelecting previously unselected package intel-media-va-driver-non-free:amd64.\n(Reading database ... 470404 files and directories currently installed.)\nPreparing to unpack .../intel-media-va-driver-non-free_24.1.0+ds1-1_amd64.deb ...\nUnpacking intel-media-va-driver-non-free:amd64 (24.1.0+ds1-1) ...\nSetting up intel-media-va-driver-non-free:amd64 (24.1.0+ds1-1) ...\n</code></pre> <p>Note</p> <p>\"Non-free\" in this context refers to licensing, not cost or security risk. This package includes proprietary binary blobs (media kernels) required to fully unlock hardware acceleration for modern Intel features like AV1 and improved H.264/HEVC performance on 13th Gen chips. Standard Ubuntu/Debian repositories provide the \"free\" version by default, which often lacks the firmware or features needed to initialize the iHD driver correctly on newer Xe-based hardware. </p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#reinstall-linux-firmware","title":"Reinstall <code>linux-firmware</code>","text":"<p>Ensure firmware is current, the <code>xe</code> driver depends on specific GUC/HUC firmware blobs:</p> <pre><code># apt install --reinstall linux-firmware.\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#grant-access-to-devdri","title":"Grant access to /dev/dri","text":"<p>Confirm Driver Ownership: Run ls -l /dev/dri. Ensure your user is part of the video and render groups: sudo usermod -aG video,render $USER (re-log after this).</p> <pre><code># ls -l /dev/dri\ntotal 0\ndrwxr-xr-x  2 root root         80 Dec 23 14:02 by-path\ncrw-rw----+ 1 root video  226,   1 Dec 23 14:02 card1\ncrw-rw----+ 1 root render 226, 128 Dec 23 14:02 renderD128\n\n# usermod -aG video,render $USER\n</code></pre>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#graphical-glitches","title":"Graphical glitches","text":"<p>Unfortunately, once the XE driver was finally in full effect, it lead to a terrible issue with flickering colors and a reduced palette. It started relativel subtle but then became a lot more prominent and started affecting even the geometry of windows borders.</p> <p>The issue started happening after adding the user to the <code>video,render</code> groups and reinstalling <code>linux-firmware</code>, both followed by a single reboot. While Chrome appeared to be working better, with several features using GPU acceleration, the graphical glitches rendered the system essentiall unusable.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#disable-panel-self-refresh","title":"Disable Panel Self Refresh","text":"<p>Panel Self Refresh (PSR) is a common cause of flickering on Intel Xe/UHD graphics. This can be disabled by adding <code>xe.enable_psr=0</code> to the kernel parameters in <code>/etc/default/grub</code>:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash i915.force_probe=!a720 xe.force_probe=a720 xe.enable_psr=0\"\n</code></pre> <p>After updating GRUB (<code>update-grub</code>) and rebooting, the issue was only partially mitigated; it stopped the worst color flickering, which was happening on the login screen, but when playing YouTube videos it showed washed out colors with some contour lines and subtle gradients being replaced with very saturated colors.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#toggle-iommu-for-graphics","title":"Toggle IOMMU for Graphics","text":"<p>Some users have reported that disabling IOMMU specifically for the integrated graphics solves persistent flickering in Ubuntu 24.04. This can be done by adding  <code>intel_iommu=igfx_off</code> to the kernel parameters in <code>/etc/default/grub</code>:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash i915.force_probe=!a720 xe.force_probe=a720 xe.enable_psr=0 intel_iommu=igfx_off\"\n</code></pre> <p>After updating GRUB (<code>update-grub</code>) and rebooting, the issue of color flickering seemed to have been fully mitited. However, the output from <code>xrandr --prop</code> showed the display (<code>DP-1</code>) was using an Automatic Broadcast RGB setting. On many monitors, especially portable ones like the connected to the USB-C/DisplayPort (a Verbatim M14), \"Automatic\" defaults to Limited range (16-235) and this causes the washed-out colors and banding.</p> Output from <code>xrandr --prop</code> for <code>DP-1</code> <pre><code>$ xrandr --prop\nScreen 0: minimum 320 x 200, current 1920 x 1080, maximum 16384 x 16384\n...\nDP-1 connected primary 1920x1080+0+0 (normal left inverted right x axis y axis) 230mm x 270mm\n        _KDE_SCREEN_INDEX: 1 \n        EDID: \n                00ffffffffffff004a8b010001000000\n                08210104b5171b783bee91a3544c9926\n                0f5054210800d1c08180a9c0d1c00101\n                010101010101023a801871382d40582c\n                2500e60e0100001e000000ff0064656d\n                6f7365742d310a202020000000fc0056\n                6572626174696d204d31340a000000fd\n                00283d545411010a202020202020011f\n                02031ff2440104021023097f07830100\n                00e305c000e200c0e606050162622802\n                3a801871382d40582c4500e60e010000\n                1e000000000000000000000000000000\n                00000000000000000000000000000000\n                00000000000000000000000000000000\n                00000000000000000000000000000000\n                0000000000000000000000000000006b\n        HDCP Content Type: HDCP Type0 \n                supported: HDCP Type0, HDCP Type1\n        Content Protection: Undesired \n                supported: Undesired, Desired, Enabled\n        vrr_capable: 0 \n                range: (0, 1)\n        Colorspace: Default \n                supported: Default, BT709_YCC, XVYCC_601, XVYCC_709, SYCC_601, opYCC_601, opRGB, BT2020_CYCC, BT2020_RGB, BT2020_YCC, DCI-P3_RGB_D65, RGB_WIDE_FIXED, RGB_WIDE_FLOAT, BT601_YCC\n        max bpc: 12 \n                range: (6, 12)\n        Broadcast RGB: Automatic \n                supported: Automatic, Full, Limited 16:235\n        audio: auto \n                supported: force-dvi, off, auto, on\n        subconnector: Native \n                supported: Unknown, VGA, DVI-D, HDMI, DP, Wireless, Native\n        link-status: Good \n                supported: Good, Bad\n        CTM: 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 \n                0 1 \n        CONNECTOR_ID: 250 \n                supported: 250\n        non-desktop: 0 \n                range: (0, 1)\n  1920x1080     60.00*+  60.00    59.94  \n  1600x900      60.00  \n  1280x1024     60.02  \n  1280x720      60.00    59.94  \n  1024x768      60.00  \n  800x600       60.32  \n  720x480       60.00    59.94  \n  640x480       60.00    59.94  \n...\n</code></pre> <p>These issues should be mitigated immediately with the following commands:</p> <pre><code>$ xrandr --output DP-1 --set \"Broadcast RGB\" \"Full\"\n$ xrandr --output DP-1 --set \"max bpc\" 8\n</code></pre> <p>To make the changes permanent, create <code>/etc/X11/xorg.conf.d/20-intel.conf</code> with:</p> <pre><code>Section \"Device\"\n    Identifier  \"Intel Graphics\"\n    Driver      \"intel\"\n    Option      \"AccelMethod\"  \"sna\"\n    Option      \"BroadcastRGB\" \"Full\"\nEndSection\n</code></pre> <p>After this change colors finally looked okay, but it came at the cost of a huge drop in performance; it was now absolutely terrible when playing videos on YouTube, the UI was sluggish again, and CPU usage was a lot higher while GPU usage seemed to be zero.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#roolback-to-i915","title":"Roolback to <code>i915</code>","text":"<p>Having run out of workarounds to make the <code>xe</code> driver work well, it was time to revert back to the <code>i915</code> driver, by updating the kernel command line to:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash i915.force_probe=a720 intel_iommu=igfx_off\"\n</code></pre> <p>After updating GRUB and rebooting, there are still more changes to make, to ensure the correct kernel and Xorg drivers are used at all times.</p> <p>To ensure the <code>xe</code> driver does not intefer by traing to claim harware elements, prevent it from loading by creating <code>/etc/modprobe.d/blacklist-xe.conf</code> with</p> <pre><code>blacklist xe\n</code></pre> <p>This change requires first running <code>update-initramfs</code> before rebooting.</p> <p>To always use the correct <code>iHD</code> driver, add the following to <code>/etc/environment</code>:</p> <pre><code>LIBVA_DRIVER_NAME=iHD\n</code></pre> <p>After reverting to the <code>i915</code> driver, the system falls back to software rendering (<code>llvmpipe</code> as reported by <code>glxinfo | grep \"OpenGL renderer\"</code>) instead of the standard hardware-accelerated <code>i915</code> driver.</p> <p>Because the system has been switching drivers, the compiled shader cache for <code>llvmpipe</code> or <code>xe</code> might be interfering with the <code>i915</code> driver initialization. To avoid this, clear the Mesa Cache:</p> <pre><code>rm -rf ~/.cache/mesa_shader_cache\n</code></pre> <p>Remove the Manual X11 Config previosly added:</p> <pre><code>rm -rf /etc/X11/xorg.conf.d/20-intel.conf\n</code></pre> <p>After all the above changes are in effect, <code>glxinfo | grep \"OpenGL renderer\"</code> shows again the intel driver is used for hardware-accelerated rendering. Google Chrome feels a lot snappier too, no longer constantly sluggish. AS a plus, <code>intel_gpu_top</code> works again.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#summary-of-changes","title":"Summary of changes","text":"<p>At this point it seems only the following changes remain in effect:</p> <ol> <li>Kernel command line includes the <code>intel_iommu=igfx_off</code> parameter (via GRUB config).</li> <li>The <code>intel-media-va-driver-non-free</code> driver is installed and in use.</li> <li>The <code>xe</code> kernel driver is not loaded at all.</li> <li>Chrome is no longer blocklisting the GPU.</li> <li>The h264ify extension is installed in Google Chrome.</li> <li>The user belongs to the <code>video</code> and <code>render</code> groups.</li> </ol> <p>Back to the original problem (memory leak in the i915 driver), would now be the time to disable GUC/HUC? (adding i915.enable_guc=0 to GRUB)</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#future-plans","title":"Future Plans","text":"<p>Having restored performance with the <code>i915</code> driver, it is not recommended time to disable GuC/HuC (with the <code>i915.enable_guc=0</code> kernel parameter) unless the memory leak returns.</p> <p>On 13th Gen Intel i5-1340P (Raptor Lake), the GuC (Graphics MicroController) is  responsible for critical power management and scheduling. Disabling it may re-introduce the sluggishness or high CPU usage, by shifting tasks from the GPU back to the CPU.</p> <p>If the memory leak is no longer observed, one explanation may be that those \"leak-like\" symptoms were actually DMA/IOMMU memory fragmentation issues. These would then have been resolved by loading the kernel with the <code>intel_iommu=igfx_off</code> parameter.</p> <p>In contrast, disabling the GuC/HuC (with <code>enable_guc=0</code>) is, on Raptor Lake and newer CPUS, \"legacy\" troubleshooting that may cause the system to lose hardware-accelerated video decoding (VA-API) in Chrome, even if the driver is loaded.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#disable-panel-self-refresh_1","title":"Disable Panel Self Refresh","text":"<p>If the leak persists, adding <code>i915.enable_psr=0</code> to the kernel command line in GRUB to disable PSR, while leaving GuC enabled, may be a better workaround. Panel Self Refresh is a more frequent cause of \"ghost\" memory usage than GuC on 13th Gen hardware.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#update-bios-microcode","title":"Update BIOS &amp; Microcode","text":"<p>For 13th Gen NUCs, the most critical fix for \"unusual\" driver behavior and stability is the <code>0x12F</code> microcode update (or newer). Updating the BIOS to the latest version should mitigate hardware-level voltage instability that can mimic driver memory leaks.</p> <p>Product Support For NUC 13 Pro Mini PC shows the latest BIOS Version release is v0039 from September 4, 2025. The current BIOS Version release is v0033 from August 7, 2024:</p> <pre><code># dmidecode -s bios-version\nANRPL357.0033.2024.0716.1113\n</code></pre> <p>To update the firmware, all it takes is a USB stick and a few minutes:</p> <ol> <li>Download the file from https://www.asus.com/supportonly/nuc13anhi7/helpdesk_bios/</li> <li>Extract the <code>ANRPL357.0039.CAP</code> file and write it to a FAT32-formatted USB drive.</li> <li>Restart the NUC and press F7 screen to enter the Flash Update tool.</li> <li>Select the file and confirm. Do not power off during the 2\u20135 minute process.</li> </ol> <p>The latest Microcode for Raptor Lake is <code>0x4129</code> and that is the one already in the system:</p> <pre><code># dmesg | grep -i microcode\n[    1.810277] microcode: Current revision: 0x00004129\n[    1.810278] microcode: Updated early from: 0x00004121\n</code></pre> <p>The <code>0x4129</code> revision is a recent release addressing high-severity security vulnerabilities documented in Intel-SA-01249/01313.</p>"},{"location":"blog/2024/11/15/ubuntu-studio-2404-on-super-tuna-nuc-pc/#2026-update-linux-kernel-614","title":"2026 Update: Linux kernel 6.14","text":"<p>In the end, the solution was a lot simpler than all the above, after enough time passed.</p> <p>Ubuntu 24.04 defaults to a very old kernel (6.8.0) but at this time, nearly a year later, the system can be upgraded to the much newer kernel 6.14 which had several memory management fixes for Raptor Lake backported.</p> 3 methods to update the kernel on a NUC. <p>To update the kernel on a NUC, where secure boot cannot be disabled, thus requiring the kernel to be signed, there are two easy options and a hard one:</p> <ol> <li> <p>Use the Official HWE Kernel (Easiest &amp; Safest).     The Hardware Enablement (HWE) stack provides newer kernels that are fully signed and officially supported by Canonical. Currently the HWE stack for Ubuntu 24.04 has     advanced to Linux the 6.14 branch (<code>6.14.0-37.37~24.04.1</code>):</p> <pre><code># apt install --install-recommends linux-generic-hwe-24.04\n</code></pre> <p>This kernel is automatically signed by Ubuntu, meaning it will boot immediately with Secure Boot active. </p> </li> <li> <p>Use the OEM Kernel (For NUC Stability).     NUC hardware often qualifies for the \"OEM\" track, which carries earlier access     to drivers and hardware-specific fixes. These are also fully signed by     Canonical. Currently the OEM track for Ubuntu 24.04 offers slighly newer     patches in the 6.14 branch (<code>6.14.0-1018.18</code>):</p> <pre><code># apt install linux-oem-24.04\n</code></pre> </li> <li> <p>Manual Signing of Mainline Kernels (Advanced).     If a vanilla kernel like 6.8.12 or a bleeding-edge version not yet in HWE is     needed, this must be done using a tool like <code>mainline</code> and then manually     signing it so the UEFI firmware trusts it.</p> <ol> <li> <p>Install the Mainline Tool:</p> <pre><code># add-apt-repository ppa:cappelikan/ppa\n# apt update &amp;&amp; sudo apt install mainline\n</code></pre> </li> <li> <p>Generate a Signing Key (MOK):</p> <pre><code># openssl req -new -x509 -newkey rsa:2048 -keyout MOK.priv -outform DER -out MOK.der -nodes -days 36500 -subj \"/CN=Your Name/\"\n# mokutil --import MOK.der\n</code></pre> <p>This will prompt for a password. After rebooting, this must be selected via Enroll MOK in the blue MOKManager screen and enter the password.</p> </li> <li> <p>Install and Sign the Kernel:</p> <p>After installing a kernel via the <code>mainline</code> app, sign the <code>vmlinuz</code> binary:</p> <p>Convert key to PEM for <code>sbsign</code></p> <pre><code># openssl x509 -in MOK.der -inform DER -outform PEM -out MOK.pem\n</code></pre> <p>Sign the kernel (replace <code>[VERSION]</code>)</p> <pre><code># sbsign --key MOK.priv --cert MOK.pem /boot/vmlinuz-[VERSION]-generic --output /boot/vmlinuz-[VERSION]-generic.signed\n</code></pre> <p>Overwrite the original kernel with the signed version</p> <pre><code># mv /boot/vmlinuz-[VERSION]-generic.signed /boot/vmlinuz-[VERSION]-generic\n# update-grub\n</code></pre> </li> </ol> </li> </ol> <p>The easiest method turned out to be a good solution; install the HWE kernel:</p> <pre><code># apt install linux-generic-hwe-24.04 -y\n</code></pre> <p>In case the new kernel does not become the new default, adjust <code>/etc/default/grub</code> so that it will show the menu to allow choosing a specific kernel and then remember that choice as the default for subsequent reboots:</p> /etc/default/grub<pre><code>GRUB_DEFAULT=saved\nGRUB_SAVEDEFAULT=true\nGRUB_TIMEOUT_STYLE=menu\nGRUB_TIMEOUT=10\nGRUB_DISTRIBUTOR=`( . /etc/os-release; echo ${NAME:-Ubuntu} ) 2&gt;/dev/null || echo Ubuntu`\nGRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash i915.force_probe=a720 intel_iommu=igfx_off\"\nGRUB_CMDLINE_LINUX=\"\"\n</code></pre> <p>Update GRUB and restart, then choose the new kernel.</p> <pre><code># vi /etc/default/grub\n# update-grub\nSourcing file `/etc/default/grub'\nSourcing file `/etc/default/grub.d/trace.cfg'\nSourcing file `/etc/default/grub.d/ubuntustudio.cfg'\nGenerating grub configuration file ...\nFound linux image: /boot/vmlinuz-6.8.0-90-lowlatency\nFound initrd image: /boot/initrd.img-6.8.0-90-lowlatency\nFound linux image: /boot/vmlinuz-6.14.0-37-generic\nFound initrd image: /boot/initrd.img-6.14.0-37-generic\nFound memtest86+ 64bit EFI image: /boot/memtest86+x64.efi\nWarning: os-prober will not be executed to detect other bootable partitions.\nSystems on them will not be added to the GRUB boot configuration.\nCheck GRUB_DISABLE_OS_PROBER documentation entry.\nAdding boot menu entry for UEFI Firmware Settings ...\ndone\n</code></pre> <p>To make the new kernel the all-time default, set <code>GRUB_DEFAULT</code> to the index number of the desired option as obtained from <code>/boot/grub/grub.cfg</code></p> <pre><code># awk -F\"'\" '/menuentry / &amp;&amp; /with Linux/ {print i++ \" : \" $2}' /boot/grub/grub.cfg\n0 : Ubuntu, with Linux 6.8.0-90-lowlatency\n1 : Ubuntu, with Linux 6.8.0-90-lowlatency (recovery mode)\n2 : Ubuntu, with Linux 6.14.0-37-generic\n3 : Ubuntu, with Linux 6.14.0-37-generic (recovery mode)\n</code></pre>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/","title":"Starting a blog with mkdocs-material on GitHub pages","text":"<p>After about a year since starting a blog with Jekyll on GitHub pages, the time has come to step this blog's game up. Jekyll is good, but it makes copying text a bit hard sometimes (hard to see) and code blocks are lacking some features like showing file names and highlighting specific lines. These features, and more, are available in Material for MkDocs, another Markdown-based documentation framwork that looks really good!</p>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#get-started-with-material-for-mkdocs","title":"Get Started with Material for MkDocs","text":"<p>For a tutorial, see Material for MkDocs: Full Tutorial To Build And Deploy Your Docs Portal.</p> <p> </p> <p>For more inspiration, see also Beautiful Pages on GitLab with mkdocs.</p>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#installation","title":"Installation","text":"<p>Installation of Material for MkDocs using <code>pip</code> but this is not recommended in Ubuntu Studio:</p> <pre><code>$ pip install mkdocs-material\nerror: externally-managed-environment\n\n\u00d7 This environment is externally managed\n\u2570\u2500&gt; To install Python packages system-wide, try apt install\n    python3-xyz, where xyz is the package you are trying to\n    install.\n</code></pre> <p>Install using <code>apt install</code> instead:</p> <code># apt install mkdocs-material mkdocs-material-extensions -y</code> <pre><code># apt install mkdocs-material mkdocs-material-extensions -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  ghp-import libjs-bootstrap4 libjs-lunr libjs-modernizr libjs-popper.js libjs-sizzle mkdocs\n  node-jquery python3-joblib python3-livereload python3-lunr python3-mergedeep python3-nltk\n  python3-pathspec python3-platformdirs python3-pymdownx python3-pyyaml-env-tag python3-regex\n  python3-tqdm\nSuggested packages:\n  libjs-es5-shim mkdocs-doc nodejs coffeescript node-less node-uglify python-livereload-doc\n  python3-django python3-slimmer python-lunr-doc\nRecommended packages:\n  prover9\nThe following NEW packages will be installed:\n  ghp-import libjs-bootstrap4 libjs-lunr libjs-modernizr libjs-popper.js libjs-sizzle mkdocs\n  mkdocs-material mkdocs-material-extensions node-jquery python3-joblib python3-livereload\n  python3-lunr python3-mergedeep python3-nltk python3-pathspec python3-platformdirs\n  python3-pymdownx python3-pyyaml-env-tag python3-regex python3-tqdm\n0 upgraded, 21 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 8,721 kB of archives.\nAfter this operation, 41.4 MB of additional disk space will be used.\n</code></pre> <p>Note</p> <p>Material for MkDocs relies on <code>watchmedo</code> from  gorakhargosh/watchdog so wherever that is installed must be added to the <code>PATH</code> variable, e.g. in <code>.bashrc</code> </p><pre><code>export PATH=\"$HOME/.local/bin:$PATH\"\n</code></pre><p></p>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#creating-a-new-site","title":"Creating a new site","text":"<p>Create a new directory for the new site, and initialize it:</p> <pre><code>$ mkdocs new .\nINFO    -  Writing config file: ./mkdocs.yml\nINFO    -  Writing initial docs: ./docs/index.md\n</code></pre> <p>Setup the site name and URL in <code>mkdocs.yml</code></p> mkdocs.yml<pre><code>site_name: Inadvisably Applied Magic\ntheme:\n  name: material\n</code></pre> <p>Run the local server and follow the link provided to see the new site:</p> <code>$ mkdocs serve</code> <pre><code>$ mkdocs serve\n...\nINFO    -  Building documentation...\nINFO    -  [macros] - Macros arguments: {'module_name': 'main',\n          'modules': [], 'render_by_default': True, 'include_dir': '',\n          'include_yaml': [], 'j2_block_start_string': '',\n          'j2_block_end_string': '', 'j2_variable_start_string': '',\n          'j2_variable_end_string': '', 'on_undefined': 'strict',\n          'on_error_fail': False, 'verbose': False}\nINFO    -  [macros] - Extra filters (module): ['pretty']\nINFO    -  Cleaning site directory\nINFO    -  Documentation built in 6.55 seconds\nINFO    -  [21:53:31] Watching paths for changes: 'docs', 'mkdocs.yml'\nINFO    -  [21:53:31] Serving on http://127.0.0.1:8000/\n</code></pre> <p>Content can be added now to <code>docs/index.md</code> and any new files. Changes are picked up as soon as the files are saved to disk and the site will refresh automatically.</p>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#creating-a-blog","title":"Creating a Blog","text":"<p>Set up a blog using the built-in <code>blog</code> plugin.</p> <p>Alternatively, create a new repository with the Use this template button in the Material for Mkdocs Blog Template.</p> <p>To create a blog with the built-in <code>blog</code> plugin, add it to the <code>mkdocs.yml</code>:</p> mkdocs.yml<pre><code>site_name: Inadvisably Applied Magic\nplugins:\n  - blog\ntheme:\n  name: material\n</code></pre> <p>Now, as soon as the <code>blog</code> plugin is loaded, the server crashes and refuses to start again, because the  <code>paginate</code> module cannot be found:</p> <p><code>$ mkdocs serve</code></p> <pre><code>...\n  File \"/usr/lib/python3/dist-packages/material/plugins/blog/plugin.py\", line 38, in &lt;module&gt;\n    from paginate import Page as Pagination\nModuleNotFoundError: No module named 'paginate'\n</code></pre> <p>There is no system package in Ubuntu 24.04 that provides this module, so it must be installed via <code>pip</code> (forced with <code>--break-system-packages</code>):</p> <code>$ sudo pip3 install --break-system-packages paginate</code> <pre><code>$ sudo pip3 install --break-system-packages paginate\nCollecting paginate\n  Downloading paginate-0.5.7-py2.py3-none-any.whl.metadata (11 kB)\nDownloading paginate-0.5.7-py2.py3-none-any.whl (13 kB)\nInstalling collected packages: paginate\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmkdocs-material 9.4.0 requires pymdown-extensions~=10.2, but you have pymdown-extensions 9.5 which is incompatible.\nSuccessfully installed paginate-0.5.7\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n</code></pre>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#add-a-post","title":"Add a Post","text":"<p>Posts can be any files under <code>docs/blog/posts/</code> so long as they have a <code>date</code> in their metadata section. It is convenient to add the date to the file name, e.g. <code>docs/blog/posts/2024-11-01-this-is-only-a-test.md</code>, but the <code>date</code> in the metadata will take precedence over that in the file name.</p> 2024-11-01-this-is-only-a-test.md<pre><code>---\ndate: 2024-11-01\n---\n\n# This is only a test\n\nReally, just a test.\n</code></pre> <p>Other than the <code>date</code>, each post can have other metadata define in the metadata block, but the title is taken either from the first top-level heading (<code># Title</code>) or, otherwise, the file name.</p> <p>Other interesting metadata is the <code>categories</code>, to classify and group posts under the Archive section, and the <code>slug</code> in case of really long titles getting URLs too long.</p> 2024-11-01-this-is-only-a-test.md<pre><code>---\ndate: 2024-11-01\nslug: this-is-only-a-test\ncategories:\n  - Search\n  - Performance\n---\n\n# This is only a test. Honest! Really, just a test, nothing else, promise!\n\nReally, just a test.\n</code></pre>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#arrange-content","title":"Arrange Content","text":"<p>By default, a blog using <code>mkdocs</code> built-in plugin will be under the main page.</p> <p>That is, under the main directory (created with <code>mkdocs new</code>) the files are as follows:</p> <pre><code>mkdocs.yml              # The configuration file.\ndocs/\n    index.md            # The homepage.\n    ...                 # Other markdown pages and files.\n    blog/\n        index.md        # The blog's homepage.\n            posts/\n                2024-12-01-test.md   # A post\n</code></pre> <p>This structure leads to the following URLs:</p> <pre><code>example.com/            # The homepage.\nexample.com/blog/       # The blog's homepage.\nexample.com/blog/2024/11/01/this-is-only-a-test/  # A post\n</code></pre> <p>If the main directory is not at the root of a (sub)domain, as is the case with GitHub Pages, the URLs will more verbose:</p> <pre><code>user.github.io/repo/            # The homepage.\nuser.github.io/repo/blog/       # The blog's homepage.\nuser.github.io/repo/blog/2024/11/01/this-is-only-a-test/  # A post\n</code></pre> <p>If the <code>repo</code> repository is meant to host only a blog, it would be desirable to use the <code>same-dir</code> plugin. This is not used in this site, because there is more than a blog here; there are also static pages such as Continuous Monitoring.</p> <p>It can also be good to have a clear separation between the blog, as an otherwise unorganized timeline of events, and the more (if not better) organized content in static pages.</p>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#site-agnostic-urls","title":"Site-agnostic URLs","text":"<p>To make links to other pages in the same site, and image URLs, agnostic of where the site is hostead, and whether the blog is using the <code>same-dir</code> plugin or not, <code>extra</code> data can be added to the <code>mkdocs.yml</code> configuration and used in content generation via the <code>macros</code> plugin:</p> mkdocs.yml<pre><code>extra:\n  site:\n    baseurl: http://127.0.0.1:8000/\nplugins:\n  - blog\n  - macros:\n      on_undefined: strict\nsite_name: Inadvisably Applied Magic\n</code></pre> <p>This may require installing the <code>macros</code> pluging in the system; in Ubuntu this requires installing <code>mkdocs-macros-plugin</code>:</p> <code># sudo apt install mkdocs-macros-plugin</code> <pre><code># sudo apt install mkdocs-macros-plugin\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  python3-termcolor\nThe following NEW packages will be installed:\n  mkdocs-macros-plugin python3-termcolor\n0 upgraded, 2 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 29.1 kB of archives.\nAfter this operation, 124 kB of additional disk space will be used.\n</code></pre>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#glightbox","title":"GLightbox","text":"<p>MkDocs GLightbox is a nice plugin that supports image lightbox with GLightbox, a pure javascript lightbox library with mobile support.</p> <p>Again, there is no system package in Ubuntu 24.04 so it must be installed via <code>pip</code>.</p> <code>$ sudo pip3 install --break-system-packages mkdocs-glightbox</code> <pre><code>$ sudo pip install --break-system-packages mkdocs-glightbox\nCollecting mkdocs-glightbox\n  Downloading mkdocs_glightbox-0.4.0-py3-none-any.whl.metadata (6.1 kB)\nDownloading mkdocs_glightbox-0.4.0-py3-none-any.whl (31 kB)\nInstalling collected packages: mkdocs-glightbox\nSuccessfully installed mkdocs-glightbox-0.4.0\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n</code></pre>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#publish-to-github-pages","title":"Publish to GitHub Pages","text":"<p>To publish the new site to GitHub Pages, replacing the previous site based on Jekyll, first undo some of the settings required by Jekyll, then setup the new flow.</p> <p>Replace the current flow with the new one</p> <ol> <li>Under your repository name, click Settings.    If you cannot see the \"Settings\" tab, select the  dropdown menu,    then click Settings.</li> <li>In the \"Code and automation\" section of the sidebar,     click Pages.</li> <li>Under \"Build and deployment\", under \"Source\", select     GitHub Actions.</li> <li>Follow the link to create your own.</li> <li>Name it <code>ci.yml</code> and paste the YAML from    Publishing your site with GitHub Actions.</li> <li>Go back to \"Build and deployment\", and under \"Source\", switch     back to Deploy from a branch and make sure the selected    branch is gh-pages (and the forder is / (root)).</li> <li>Click on Commit changes..., then Commit changes.</li> </ol> <p>Tip</p> <p>It would have been enough to simply create the <code>.github/workflows/ci.yml</code> file and the \"Build and deployment\" settings under \"Source\" as they were, so long as the selected branch is gh-pages (and the forder is / (root)).</p> <p>After this commit the workflow will start and, understandably, fail with <code>Error: Config file 'mkdocs.yml' does not exist.</code> because the required files are missing. The next commit should include at least</p> <pre><code>mkdocs.yml              # The configuration file.\ndocs/\n    index.md            # The homepage.\n    ...                 # Other markdown pages and files.\n    blog/\n        index.md        # The blog's homepage.\n            posts/\n                2024-12-01-test.md   # A post\n</code></pre> <p>When trying to use the <code>macros</code> plugin, the workflow will also fail, this time with</p> <pre><code>ERROR   -  Config value 'plugins': The \"macros\" plugin is not installed\n</code></pre> <p>To fix this, add a step to install the plugin in the <code>.github/workflows/ci.yml</code> just before deploying:</p> .github/workflows/ci.yml<pre><code>      - run: pip install mkdocs-material \n      - run: pip install mkdocs-macros-plugin\n      - run: pip install mkdocs-glightbox\n      - run: mkdocs gh-deploy --force\n</code></pre> <p>Now the workflow finishes successfully, and the site is updated, which means it's mostly empty because the old content needs to be moved under <code>docs/blog/posts</code>.</p>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#migrate-old-content","title":"Migrate old content","text":"<p>The files under <code>_posts</code> will need a number of modifications, once copied under <code>docs/blog/posts</code>:</p> <ul> <li>metadata <code>date</code> must be only a date (e.g. <code>2024-12-03</code>)</li> <li>metadata <code>categories</code> must be a list in YAML format</li> <li>media files are moved under <code>docs/blog/media/</code> using the command line <code>git mv</code>.</li> <li>media files URLs are then relative to blog posts: <code>../media/</code></li> </ul> <pre><code>$ mkdir docs/blog/media\n$ git mv assets/media/* docs/blog/media/\n$ git ci -m \"Move media files to mkdocs blog\" .\n$ git push\nUsername for 'https://github.com': stibbons1990\nPassword for 'https://stibbons1990@github.com': \n...\n</code></pre> <p>The files under <code>docs/blog/posts</code> can also be created by moving the files already existing under <code>_posts</code>, then making the necessary modifications in follow-up commits:</p> <pre><code>$ mkdir docs/blog/posts\n$ git mv _posts/*.md docs/blog/posts/\n$ git ci -m \"Move posts to mkdocs blog\" .\n$ git push\nUsername for 'https://github.com': stibbons1990\nPassword for 'https://stibbons1990@github.com': \n</code></pre> <p>This second <code>git mv</code> operation will lead to many <code>WARNING</code> messages in the Action run of <code>mkdocs gh-deploy</code>, which can be used to track down all stray references that may need fixing.</p> <p>Tip</p> <p>Instead of the password, use a fine-grain token for personal access.</p>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#final-tweaks","title":"Final Tweaks","text":"<p>Many features were added to <code>mkdocs.yml</code> during the above process, such as adding Google Analytics, adding a Dark / Light themes switch, changing the colors and logo, etc. By the times this post was finished, this is what <code>mkdocs.yml</code> looked like:</p> mkdocs.yml<pre><code>copyright: Copyright &amp;copy; 2019-2025 Ponder Stibbons\nextra:\n  analytics:\n    provider: google\n    property: G-Y167L3ZVGY\nextra_css:\n  - stylesheets/extra.css\nmarkdown_extensions:\n  - admonition\n  - attr_list\n  - md_in_html\n  - pymdownx.details\n  - pymdownx.emoji:\n      emoji_index: !!python/name:material.extensions.emoji.twemoji\n      emoji_generator: !!python/name:material.extensions.emoji.to_svg\n  - pymdownx.highlight:\n      anchor_linenums: true\n      line_spans: __span\n      pygments_lang_class: true\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.superfences\n  - pymdownx.tabbed:\n      alternate_style: true\nnav:\n  - About This Place: 'index.md'\n  - Continuous Monitoring: 'conmon.md'\n  - Unexpected Linux Adventures: 'blog/index.md'\nplugins:\n  - blog:\n      draft: false\n      draft_on_serve: false\n      pagination: true\n      pagination_per_page: 10\n  - macros:\n      on_undefined: strict\nsite_name: Inadvisably Applied Magic\nsite_url: https://stibbons1990.github.io/hex/\ntheme:\n  name: material\n  features:\n    - content.code.copy\n  icon:\n    logo: fontawesome/solid/hat-wizard\n  palette:\n    # Dark mode\n    - scheme: slate\n      primary: deep purple\n      accent: green\n      toggle:\n        icon: material/weather-sunny\n        name: Dark mode\n    # Light mode\n    - scheme: default\n      primary: deep purple\n      accent: green\n      toggle:\n        icon: material/weather-night\n        name: Light mode\n</code></pre>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#custom-admonitions","title":"Custom Admonitions","text":"<p>This blog contains many long sections capturing the output from running shell commands in a terminal emulator. These are often long, even very long, and most of the times need not be read, or even visible by default.</p> <p>There are also long samples of source code and configuration files, such as Xorg, Systemd, Kubernetes and many more languages supported by Pygments.</p> <p>To show these blocks differently, a custom <code>terminal</code> admonitions is used, based on the <code>pied-piper</code> example in Custom admonitions, using the SVG from the  emoji for the icon:</p> stylesheets/extra.css<pre><code>.md-typeset .admonition,\n.md-typeset details {\n  font-size: 16px\n}\n:root {\n    --md-admonition-icon--terminal: url('data:image/svg+xml;charset=utf-8,&lt;svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 576 512\"&gt;&lt;path d=\"M9.4 86.6c-12.5-12.5-12.5-32.7 0-45.2s32.8-12.5 45.3 0l192 192c12.5 12.5 12.5 32.8 0 45.3l-192 192c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L178.7 256 9.4 86.6zM256 416h288c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32z\"/&gt;&lt;/svg&gt;')\n  }\n  .md-typeset .admonition.terminal,\n  .md-typeset details.terminal {\n    border-color: rgb(43, 155, 70);\n  }\n  .md-typeset .terminal &gt; .admonition-title,\n  .md-typeset .terminal &gt; summary {\n    background-color: rgba(43, 155, 70, 0.1);\n  }\n  .md-typeset .terminal &gt; .admonition-title::before,\n  .md-typeset .terminal &gt; summary::before {\n    background-color: rgb(43, 155, 70);\n    -webkit-mask-image: var(--md-admonition-icon--terminal);\n            mask-image: var(--md-admonition-icon--terminal);\n  }\n</code></pre> <p>The admonition can be collapsed by default when the output is long. One down-side is that it's hard to copy the title of the admonition (the command to run), it is much easier to copy it from either the content (inside the admonition) and/or a separate code block outside the admonition.</p> <code>$ date -R</code> <pre><code>$ date -R\nFri, 13 Dec 2024 23:05:53 +0100\n</code></pre> <p>The first 4 lines in the above CSS change the font-size of the admonition block so that the text, output or code is more easily readable.</p> <p>News</p> <p>More custom admonitions are added as the need turms up, what follows is the full <code>extra.css</code> file with all of these.</p> Here goes the entire <code>extra.css</code> file: stylesheets/extra.css<pre><code>.md-typeset .admonition,\n.md-typeset details {\n  font-size: 16px\n}\n\n:root {\n    --md-admonition-icon--terminal: url('data:image/svg+xml;charset=utf-8,&lt;svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 576 512\"&gt;&lt;path d=\"M9.4 86.6c-12.5-12.5-12.5-32.7 0-45.2s32.8-12.5 45.3 0l192 192c12.5 12.5 12.5 32.8 0 45.3l-192 192c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L178.7 256 9.4 86.6zM256 416h288c17.7 0 32 14.3 32 32s-14.3 32-32 32H256c-17.7 0-32-14.3-32-32s14.3-32 32-32z\"/&gt;&lt;/svg&gt;')\n  }\n  .md-typeset .admonition.terminal,\n  .md-typeset details.terminal {\n    border-color: rgb(43, 155, 70);\n  }\n  .md-typeset .terminal &gt; .admonition-title,\n  .md-typeset .terminal &gt; summary {\n    background-color: rgba(43, 155, 70, 0.1);\n  }\n  .md-typeset .terminal &gt; .admonition-title::before,\n  .md-typeset .terminal &gt; summary::before {\n    background-color: rgb(43, 155, 70);\n    -webkit-mask-image: var(--md-admonition-icon--terminal);\n            mask-image: var(--md-admonition-icon--terminal);\n  }\n\n:root {\n    --md-admonition-icon--todo: url('data:image/svg+xml;charset=utf-8,&lt;svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"&gt;&lt;path d=\"M11 9h2V7h-2m1 13c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8m0-18A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10A10 10 0 0 0 12 2m-1 15h2v-6h-2v6Z\"/&gt;&lt;/svg&gt;')\n  }\n  .md-typeset .admonition.todo,\n  .md-typeset details.todo {\n    border-color: rgb(188, 128, 128);\n  }\n  .md-typeset .todo &gt; .admonition-title,\n  .md-typeset .todo &gt; summary {\n    background-color: rgba(188, 128, 128, 0.1);\n  }\n  .md-typeset .todo &gt; .admonition-title::before,\n  .md-typeset .todo &gt; summary::before {\n    background-color: rgb(188, 128, 128);\n    -webkit-mask-image: var(--md-admonition-icon--todo);\n            mask-image: var(--md-admonition-icon--todo);\n  }\n\n:root {\n    --md-admonition-icon--news: url('data:image/svg+xml;charset=utf-8,&lt;svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"&gt;&lt;path d=\"M20 11H4V8h16m0 7h-7v-2h7m0 6h-7v-2h7m-9 2H4v-6h7m9.33-8.33L18.67 3 17 4.67 15.33 3l-1.66 1.67L12 3l-1.67 1.67L8.67 3 7 4.67 5.33 3 3.67 4.67 2 3v16a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V3l-1.67 1.67Z\"/&gt;&lt;/svg&gt;')\n  }\n  .md-typeset .admonition.news,\n  .md-typeset details.news {\n    border-color: rgb(128, 128, 188);\n  }\n  .md-typeset .news &gt; .admonition-title,\n  .md-typeset .news &gt; summary {\n    background-color: rgba(128, 128, 188, 0.1);\n  }\n  .md-typeset .news &gt; .admonition-title::before,\n  .md-typeset .news &gt; summary::before {\n    background-color: rgb(128, 128, 188);\n    -webkit-mask-image: var(--md-admonition-icon--news);\n            mask-image: var(--md-admonition-icon--news);\n  }\n\n:root {\n    --md-admonition-icon--k8s: url('data:image/svg+xml;charset=utf-8,&lt;svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"&gt;&lt;path d=\"m10.204 14.35.007.01-.999 2.413a5.171 5.171 0 0 1-2.075-2.597l2.578-.437.004.005a.44.44 0 0 1 .484.606zm-.833-2.129a.44.44 0 0 0 .173-.756l.002-.011L7.585 9.7a5.143 5.143 0 0 0-.73 3.255l2.514-.725.002-.009zm1.145-1.98a.44.44 0 0 0 .699-.337l.01-.005.15-2.62a5.144 5.144 0 0 0-3.01 1.442l2.147 1.523.004-.002zm.76 2.75.723.349.722-.347.18-.78-.5-.623h-.804l-.5.623.179.779zm1.5-3.095a.44.44 0 0 0 .7.336l.008.003 2.134-1.513a5.188 5.188 0 0 0-2.992-1.442l.148 2.615.002.001zm10.876 5.97-5.773 7.181a1.6 1.6 0 0 1-1.248.594l-9.261.003a1.6 1.6 0 0 1-1.247-.596l-5.776-7.18a1.583 1.583 0 0 1-.307-1.34L2.1 5.573c.108-.47.425-.864.863-1.073L11.305.513a1.606 1.606 0 0 1 1.385 0l8.345 3.985c.438.209.755.604.863 1.073l2.062 8.955c.108.47-.005.963-.308 1.34zm-3.289-2.057c-.042-.01-.103-.026-.145-.034-.174-.033-.315-.025-.479-.038-.35-.037-.638-.067-.895-.148-.105-.04-.18-.165-.216-.216l-.201-.059a6.45 6.45 0 0 0-.105-2.332 6.465 6.465 0 0 0-.936-2.163c.052-.047.15-.133.177-.159.008-.09.001-.183.094-.282.197-.185.444-.338.743-.522.142-.084.273-.137.415-.242.032-.024.076-.062.11-.089.24-.191.295-.52.123-.736-.172-.216-.506-.236-.745-.045-.034.027-.08.062-.111.088-.134.116-.217.23-.33.35-.246.25-.45.458-.673.609-.097.056-.239.037-.303.033l-.19.135a6.545 6.545 0 0 0-4.146-2.003l-.012-.223c-.065-.062-.143-.115-.163-.25-.022-.268.015-.557.057-.905.023-.163.061-.298.068-.475.001-.04-.001-.099-.001-.142 0-.306-.224-.555-.5-.555-.275 0-.499.249-.499.555l.001.014c0 .041-.002.092 0 .128.006.177.044.312.067.475.042.348.078.637.056.906a.545.545 0 0 1-.162.258l-.012.211a6.424 6.424 0 0 0-4.166 2.003 8.373 8.373 0 0 1-.18-.128c-.09.012-.18.04-.297-.029-.223-.15-.427-.358-.673-.608-.113-.12-.195-.234-.329-.349a2.691 2.691 0 0 0-.111-.088.594.594 0 0 0-.348-.132.481.481 0 0 0-.398.176c-.172.216-.117.546.123.737l.007.005.104.083c.142.105.272.159.414.242.299.185.546.338.743.522.076.082.09.226.1.288l.16.143a6.462 6.462 0 0 0-1.02 4.506l-.208.06c-.055.072-.133.184-.215.217-.257.081-.546.11-.895.147-.164.014-.305.006-.48.039-.037.007-.09.02-.133.03l-.004.002-.007.002c-.295.071-.484.342-.423.608.061.267.349.429.645.365l.007-.001.01-.003.129-.029c.17-.046.294-.113.448-.172.33-.118.604-.217.87-.256.112-.009.23.069.288.101l.217-.037a6.5 6.5 0 0 0 2.88 3.596l-.09.218c.033.084.069.199.044.282-.097.252-.263.517-.452.813-.091.136-.185.242-.268.399-.02.037-.045.095-.064.134-.128.275-.034.591.213.71.248.12.556-.007.69-.282v-.002c.02-.039.046-.09.062-.127.07-.162.094-.301.144-.458.132-.332.205-.68.387-.897.05-.06.13-.082.215-.105l.113-.205a6.453 6.453 0 0 0 4.609.012l.106.192c.086.028.18.042.256.155.136.232.229.507.342.84.05.156.074.295.145.457.016.037.043.09.062.129.133.276.442.402.69.282.247-.118.341-.435.213-.71-.02-.039-.045-.096-.065-.134-.083-.156-.177-.261-.268-.398-.19-.296-.346-.541-.443-.793-.04-.13.007-.21.038-.294-.018-.022-.059-.144-.083-.202a6.499 6.499 0 0 0 2.88-3.622c.064.01.176.03.213.038.075-.05.144-.114.28-.104.266.039.54.138.87.256.154.06.277.128.448.173.036.01.088.019.13.028l.009.003.007.001c.297.064.584-.098.645-.365.06-.266-.128-.537-.423-.608zM16.4 9.701l-1.95 1.746v.005a.44.44 0 0 0 .173.757l.003.01 2.526.728a5.199 5.199 0 0 0-.108-1.674A5.208 5.208 0 0 0 16.4 9.7zm-4.013 5.325a.437.437 0 0 0-.404-.232.44.44 0 0 0-.372.233h-.002l-1.268 2.292a5.164 5.164 0 0 0 3.326.003l-1.27-2.296h-.01zm1.888-1.293a.44.44 0 0 0-.27.036.44.44 0 0 0-.214.572l-.003.004 1.01 2.438a5.15 5.15 0 0 0 2.081-2.615l-2.6-.44-.004.005z\"/&gt;&lt;/svg&gt;')\n  }\n  .md-typeset .admonition.k8s,\n  .md-typeset details.k8s {\n    border-color: rgb(61, 109, 223);\n  }\n  .md-typeset .k8s &gt; .admonition-title,\n  .md-typeset .k8s &gt; summary {\n    background-color: rgba(61, 109, 223, 0.1);\n  }\n  .md-typeset .k8s &gt; .admonition-title::before,\n  .md-typeset .k8s &gt; summary::before {\n    background-color: rgb(61, 109, 223);\n    -webkit-mask-image: var(--md-admonition-icon--k8s);\n            mask-image: var(--md-admonition-icon--k8s);\n  }\n\n:root {\n    --md-admonition-icon--code: url('data:image/svg+xml;charset=utf-8,&lt;svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"&gt;&lt;path d=\"M5.59 3.41L7 4.82L3.82 8L7 11.18L5.59 12.6L1 8L5.59 3.41M11.41 3.41L16 8L11.41 12.6L10 11.18L13.18 8L10 4.82L11.41 3.41M22 6V18C22 19.11 21.11 20 20 20H4C2.9 20 2 19.11 2 18V14H4V18H20V6H17.03V4H20C21.11 4 22 4.89 22 6Z\"/&gt;&lt;/svg&gt;')\n  }\n  .md-typeset .admonition.code,\n  .md-typeset details.code {\n    border-color: rgb(43, 155, 70);\n  }\n  .md-typeset .code &gt; .admonition-title,\n  .md-typeset .code &gt; summary {\n    background-color: rgba(43, 155, 70, 0.1);\n  }\n  .md-typeset .code &gt; .admonition-title::before,\n  .md-typeset .code &gt; summary::before {\n    background-color: rgb(43, 155, 70);\n    -webkit-mask-image: var(--md-admonition-icon--code);\n            mask-image: var(--md-admonition-icon--code);\n  }\n\n:root {\n    --md-admonition-icon--json: url('data:image/svg+xml;charset=utf-8,&lt;svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"&gt;&lt;path d=\"M5 3C3.9 3 3 3.9 3 5S2.1 7 1 7V9C2.1 9 3 9.9 3 11S3.9 13 5 13H7V11H5V10C5 8.9 4.1 8 3 8C4.1 8 5 7.1 5 6V5H7V3M11 3C12.1 3 13 3.9 13 5S13.9 7 15 7V9C13.9 9 13 9.9 13 11S12.1 13 11 13H9V11H11V10C11 8.9 11.9 8 13 8C11.9 8 11 7.1 11 6V5H9V3H11M22 6V18C22 19.11 21.11 20 20 20H4C2.9 20 2 19.11 2 18V15H4V18H20V6H17.03V4H20C21.11 4 22 4.89 22 6Z\"/&gt;&lt;/svg&gt;')\n  }\n  .md-typeset .admonition.json,\n  .md-typeset details.json {\n    border-color: rgb(199, 146, 234);\n  }\n  .md-typeset .json &gt; .admonition-title,\n  .md-typeset .json &gt; summary {\n    background-color: rgba(199, 146, 234, 0.1);\n  }\n  .md-typeset .json &gt; .admonition-title::before,\n  .md-typeset .json &gt; summary::before {\n    background-color: rgb(199, 146, 234);\n    -webkit-mask-image: var(--md-admonition-icon--json);\n            mask-image: var(--md-admonition-icon--json);\n  }\n</code></pre> <p>Note</p> <p>The <code>&lt;svg&gt;</code> tags in each <code>--md-admonition-icon</code> URL are the result of adding Icons, Emojis to a page, then grabbing the <code>&lt;svg&gt;</code> HTML element using browser inspection tools.</p> <p>Find a way to embed the <code>extra.css</code> file directly.</p>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#custom-domain","title":"Custom domain","text":""},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#setup-in-github-pages","title":"Setup in GitHub Pages","text":"<p>Custom domains for GitHub Pages are easy to setup but in my experience not so reliable; I was able to set this up to make this blog available at https://hex.very-very-dark-gray.top/blog/ but then a few hours later the custom domain was removed, even though DNS challenge verification had been successful.</p>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#setup-in-cloudflare-pages","title":"Setup in Cloudflare Pages","text":"<p>DNS for <code>very-very-dark-gray.top</code> are managed in Cloudflare, because this domain was set up for experimenting with Cloudflare Tunnels, and while those don't get so much use it would make sense to use it for Cloudflare Pages now.</p> <p>Git integration connects the Cloudflare account with a GitHub account, and access can be restricted to the one specific repository to use for pages, and then MkDocs is available as a support framework so the entire setup can be completed in one go. However, the first time running the <code>mkdocs build</code> command it fails because dependencies are missing.</p> <p>Deploying MkDocs on Cloudflare Pages: Troubleshooting Build failures explains how to address this; a <code>requirements.txt</code> in the root directory of the repository must be submitted (and pushed) to include all dependencies:</p> requirements.txt<pre><code>mkdocs==1.5.3\nmkdocs-glightbox\nmkdocs-macros-plugin\nmkdocs-material\nmkdocs-material-extensions\npaginate\n</code></pre> <p>Before pushing this, the build command in the Cloudflare Pages settings must be updated to install requirements:</p> <pre><code>pip install -r requirements.txt &amp;&amp; mkdocs build\n</code></pre> <p>Once this change is saved, push the <code>requirements.txt</code> to the remote origin in GitHub and start the site build. It should now succeed and the new site should be available at a new URL under <code>pages.dev</code> (e.g. <code>hex-brs.pages.dev</code>).</p> <p>Note</p> <p>Select the <code>main</code> branch from the repository, not <code>gh-pages</code>.</p> <p>Add a custom domain directly from Cloudflare Pages settings to redirect https://hex.very-very-dark-gray.top/ to <code>hex-brs.pages.dev</code> and wait a bit for DNS to propagate. This shouldn't take long, it should be ready in a few minutes.</p>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#hide-pagesdev-subdomain","title":"Hide <code>pages.dev</code> subdomain","text":"<p>For a more seamless experience, and to keep crawlers from (incorrectly) counting both URLs as different/duplicate sites, implement this solution prevent direct access to <code>hex-brs.pages.dev</code> and only allow access through the custom domain. This is done by setting up bulk redirects, to redirect requests for all URLs to <code>hex-brs.pages.dev</code> to https://hex.very-very-dark-gray.top/ (preserving all URL parameters).</p>"},{"location":"blog/2024/12/03/starting-a-blog-with-mkdocs-material-on-github-pages/#comments","title":"Comments","text":"<p>Giscus integration is a very easy to setup way to enable comments on blog posts.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/","title":"Ubuntu Studio 24.04 on Raven, Gaming PC (and more)","text":"<p>The time has came to update my other PC, which I use also for gaming, coding, media production and just about everything, to  Ubuntu Studio 24.04.</p> <p>This is a smaller PC build based on a lower TDP CPU (AMD Ryzen 5 2600X) to fit in a Silverstone RVZ03 Mini ITX case with the very compact Noctua NH-L9x65 cooler.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#preparation","title":"Preparation","text":"<p>This PC was already prepared to install Ubuntu Studio 24.04 as part of installing 22.04 on a new M.2 SSD a year ago, with partitions as follows:</p> <ol> <li>300 MB EFI System for the EFI boot.</li> <li>72 GB Linux filesystem for the root (ext4).</li> <li>72 GB Linux filesystem for the alternative root (ext4).</li> <li>1.7 TB Linux filesystem for <code>/home</code> (btrfs).</li> </ol> <pre><code># parted /dev/nvme0n1 print\nModel: Samsung SSD 970 EVO Plus 2TB (nvme)\nDisk /dev/nvme0n1: 2000GB\nSector size (logical/physical): 512B/512B\nPartition Table: gpt\nDisk Flags: \n\nNumber  Start   End     Size    File system  Name  Flags\n 1      1049kB  316MB   315MB   fat32              boot, esp\n 2      316MB   79.0GB  78.6GB  ext4\n 3      79.0GB  158GB   78.6GB  ext4\n 4      158GB   2000GB  1843GB  btrfs\n</code></pre> <p>Note</p> <p>78 GB has proven to be a reasonable size for the root partition for the amount of software that tends to be installed in my PC, including 20 GB in <code>/usr</code> and 13 GB in <code>/snap</code>.</p> <p>Ubuntu Studio 22.04 is already installed in <code>/dev/nvme0n1p2</code> and now Ubuntu Studio 24.04 will be installed in <code>/dev/nvme0n1p3</code>.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#installation","title":"Installation","text":"<p>With the above partitions prepared well in advance, to Install Ubuntu Studio 24.04 the process should be as simple, easy and smooth as it was with other systems.</p> <p>And this time it was, thanks to the lesson learned when installing Ubuntu Studio 24.04 on Super Tuna (NUC PC) to Disable screen locking to prevent KDE Plasma live lock screen rendering the session useless. With that in mind, and the USB stick already prepared with <code>usb-creator-kde</code>, booted  into it and used \u201cInstall Ubuntu\u201d launcher on the desktop after disabling screen locking.</p> <ol> <li>Plug the USB stick and turn the PC on. This PC already defaults to     boot from USB when present.</li> <li>In the Grub menu, choose to Try or Install Ubuntu.</li> <li>Select language (English) and then Install Ubuntu.</li> <li>Connect to a Wi-Fi network because there is no LAN here.</li> <li>Select keyboard layout (can be from a different language).</li> <li>Select Try Ubuntu Studio.</li> <li>Disable screen locking to prevent     KDE Plasma live lock screen rendering the session useless:<ul> <li>Press <code>Alt+Space</code> to invoke Krunner and type <code>System Settings</code>.</li> <li>From there, search for Screen Locking and</li> <li>deactivate Lock automatically after\u2026.</li> </ul> </li> <li>Launch Install Ubuntu Studio from the desktop.</li> <li>Select Type of install: Interactive Installation.</li> <li>Enable the options to<ul> <li>Install third-party software for graphics and Wifi hardware and</li> <li>Download and install support for additional media formats.</li> </ul> </li> <li>Select Manual Installation<ul> <li>Use the arrow keys to navigate down to the nvme0n1 disk.</li> <li>Set nvme0n1p1 (300 MB) as EFI System Partition mounted    on <code>/boot/efi</code></li> <li>Leave nvme0n1p2 (78 GB) alone (here lives Ubuntu 22.04)</li> <li>Set nvme0n1p3 (78 GB) as ext4 mounted on <code>/</code></li> <li>Set nvme0n1p4 (1.7 TB) as Leave formatted as Btrfs    mounted on <code>/home</code></li> <li>Set Device for boot loader installation to nvme0n1</li> </ul> </li> <li>Click on Next to confirm the partition selection.</li> <li>Confirm first non-root user name (<code>coder</code>) and computer     name (<code>raven</code>).</li> <li>Select time zone (seems to be detected correctly).</li> <li>Review the choices and click on Install to start copying files.</li> <li>Once it's done, select Restart and, once prompted later     remove the USB stick and hit <code>Enter</code>.</li> </ol>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#first-boot-into-ubuntu-studio-2404","title":"First boot into Ubuntu Studio 24.04","text":"<p>The first time booting into the new system, right after login for the first time an additional reboot is required for the Ubuntu Studio Audio Configuration.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#second-boot-into-ubuntu-studio-2404","title":"Second boot into Ubuntu Studio 24.04","text":""},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#mount-ubuntu-studio-2204-root","title":"Mount Ubuntu Studio 22.04 root","text":"<p>To have easy access to files from the old Ubuntu Studio 22.04 root partition, mount it on <code>/jammy</code> and add that to <code>/etc/fstab</code></p> <pre><code># ls -l /dev/disk/by-uuid/ | grep nvme0n1p2\nlrwxrwxrwx 1 root root 15 Dec 27 10:44 ffeabdbb-ac78-46db-8cf8-1ef6efe7db88 -&gt; ../../nvme0n1p2\n\n# vi /etc/fstab\n...\n/dev/disk/by-uuid/ffeabdbb-ac78-46db-8cf8-1ef6efe7db88 /jammy ext4 defaults 0 1\n...\n\n# systemctl daemon-reload\n# mount /jammy\n# df -h\nFilesystem      Size  Used Avail Use% Mounted on\n...\n/dev/nvme0n1p2   72G   32G   37G  47% /jammy\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#tweak-grub","title":"Tweak Grub","text":"<p>Make sure Grub will show the menu and wait a bit, by tweaking <code>/etc/default/grub</code> as follows (and updating Grub):</p> /etc/default/grub<pre><code>GRUB_DEFAULT=0\nGRUB_TIMEOUT_STYLE=menu\nGRUB_TIMEOUT=10\nGRUB_DISTRIBUTOR=`( . /etc/os-release; echo ${NAME:-Ubuntu} ) 2&gt;/dev/null || echo Ubuntu`\nGRUB_CMDLINE_LINUX_DEFAULT=\"noquiet nosplash\"\nGRUB_CMDLINE_LINUX=\"\"\n</code></pre> <pre><code># update-grub2\nSourcing file `/etc/default/grub'\nSourcing file `/etc/default/grub.d/ubuntustudio.cfg'\nGenerating grub configuration file ...\nFound linux image: /boot/vmlinuz-6.8.0-50-lowlatency\nFound initrd image: /boot/initrd.img-6.8.0-50-lowlatency\nFound memtest86+ 64bit EFI image: /boot/memtest86+x64.efi\nWarning: os-prober will be executed to detect other bootable partitions.\nIts output will be used to detect bootable binaries on them and create new boot entries.\nFound Ubuntu 22.04.5 LTS (22.04) on /dev/nvme0n1p2\nAdding boot menu entry for UEFI Firmware Settings ...\ndone\n</code></pre> The latest NVidia driver is already installed. <p>Ubuntu Studio 24.04 already installed the latest NVidia driver so there is no need to reboot just yet.</p> <pre><code># nvidia-smi \nFri Dec 27 11:43:33 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce GTX 1660 Ti     Off |   00000000:29:00.0  On |                  N/A |\n| 25%   27C    P8              7W /  120W |     503MiB /   6144MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n</code></pre> <p>Since a reboot is not really necessary just yet, start by updating the system and installing essential packages.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#apt-respositories-clean-up","title":"APT respositories clean-up","text":"<p>Ubuntu Studio 24.04 seems to consistently need a little APT respositories clean-up; just comment out the last line in <code>/etc/apt/sources.list.d/dvd.list</code> to let <code>noble-security</code> be defined (only) in <code>ubuntu.sources</code>.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#update-installed-packages","title":"Update Installed Packages","text":"<p>Updates will be available after installing from the USB installer.</p> <code># apt update &amp;&amp; apt full-upgrade -y</code> <pre><code># apt update &amp;&amp; apt full-upgrade -y\nHit:1 http://es.archive.ubuntu.com/ubuntu noble InRelease\nHit:2 http://security.ubuntu.com/ubuntu noble-security InRelease\nHit:3 http://es.archive.ubuntu.com/ubuntu noble-updates InRelease\nHit:4 http://es.archive.ubuntu.com/ubuntu noble-backports InRelease\nHit:5 http://archive.ubuntu.com/ubuntu noble InRelease\nHit:6 https://dl.google.com/linux/chrome/deb stable InRelease\nHit:7 http://archive.ubuntu.com/ubuntu noble-updates InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\n105 packages can be upgraded. Run 'apt list --upgradable' to see them.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\nGet more security updates through Ubuntu Pro with 'esm-apps' enabled:\n  libdcmtk17t64 python3-waitress libcjson1 libavdevice60 ffmpeg libpostproc57\n  libavcodec60 libavutil58 libswscale7 libswresample4 libavformat60\n  libavfilter9\nLearn more about Ubuntu Pro at https://ubuntu.com/pro\nThe following upgrades have been deferred due to phasing:\n  python3-distupgrade ubuntu-release-upgrader-core ubuntu-release-upgrader-qt\nThe following packages will be upgraded:\n  acl alsa-ucm-conf apparmor apport apport-core-dump-handler apport-kde cloud-init\n  cryptsetup cryptsetup-bin cryptsetup-initramfs digikam digikam-data digikam-private-libs\n  distro-info-data dmidecode dmsetup fwupd gir1.2-gtk-3.0 gir1.2-packagekitglib-1.0\n  gir1.2-udisks-2.0 gtk-update-icon-cache initramfs-tools initramfs-tools-bin\n  initramfs-tools-core krb5-locales libacl1 libapparmor1 libastro1 libaudit-common\n  libaudit1 libboost-chrono1.83.0t64 libboost-filesystem1.83.0 libboost-iostreams1.83.0\n  libboost-locale1.83.0 libboost-program-options1.83.0 libboost-thread1.83.0\n  libcryptsetup12 libdevmapper1.02.1 libegl-mesa0 libegl1-mesa-dev libfwupd2 libgbm1\n  libgl1-mesa-dri libglapi-mesa libglx-mesa0 libgssapi-krb5-2 libgtk-3-0t64 libgtk-3-bin\n  libgtk-3-common libgtk-3-dev libk5crypto3 libkrb5-3 libkrb5support0 libldap-common\n  libldap2 libmarblewidget-qt5-28 libnm0 libpackagekit-glib2-18 libpipewire-0.3-0t64\n  libpipewire-0.3-common libpipewire-0.3-modules libproc2-0 libspa-0.2-bluetooth\n  libspa-0.2-modules libspeex1 libudisks2-0 libxatracker2 login lp-solve marble-plugins\n  marble-qt-data mesa-va-drivers mesa-vdpau-drivers mesa-vulkan-drivers mtr-tiny\n  network-manager packagekit packagekit-tools passwd pipewire pipewire-alsa pipewire-audio\n  pipewire-bin pipewire-jack pipewire-pulse plasma-distro-release-notifier procps\n  python3-apport python3-problem-report python3-software-properties python3-update-manager\n  snapd software-properties-common software-properties-qt systemd-hwe-hwdb\n  ubuntu-drivers-common ubuntu-pro-client ubuntu-pro-client-l10n udisks2\n  update-manager-core xdg-desktop-portal zip\n102 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 0 B/133 MB of archives.\nAfter this operation, 5,865 kB of additional disk space will be used.\nExtracting templates from packages: 100%\nPreconfiguring packages ...\n(Reading database ... 417845 files and directories currently installed.)\nPreparing to unpack .../login_1%3a4.13+dfsg1-4ubuntu3.2_amd64.deb ...\nUnpacking login (1:4.13+dfsg1-4ubuntu3.2) over (1:4.13+dfsg1-4ubuntu3) ...\nSetting up login (1:4.13+dfsg1-4ubuntu3.2) ...\n(Reading database ... 417845 files and directories currently installed.)\nPreparing to unpack .../0-python3-problem-report_2.28.1-0ubuntu3.3_all.deb ...\nUnpacking python3-problem-report (2.28.1-0ubuntu3.3) over (2.28.1-0ubuntu3.1) ...\nPreparing to unpack .../1-python3-apport_2.28.1-0ubuntu3.3_all.deb ...\nUnpacking python3-apport (2.28.1-0ubuntu3.3) over (2.28.1-0ubuntu3.1) ...\nPreparing to unpack .../2-apport-core-dump-handler_2.28.1-0ubuntu3.3_all.deb ...\nUnpacking apport-core-dump-handler (2.28.1-0ubuntu3.3) over (2.28.1-0ubuntu3.1) ...\nPreparing to unpack .../3-apport_2.28.1-0ubuntu3.3_all.deb ...\nUnpacking apport (2.28.1-0ubuntu3.3) over (2.28.1-0ubuntu3.1) ...\nPreparing to unpack .../4-ubuntu-drivers-common_1%3a0.9.7.6ubuntu3.1_amd64.deb ...\nUnpacking ubuntu-drivers-common (1:0.9.7.6ubuntu3.1) over (1:0.9.7.6ubuntu3) ...\nPreparing to unpack .../5-acl_2.3.2-1build1.1_amd64.deb ...\nUnpacking acl (2.3.2-1build1.1) over (2.3.2-1build1) ...\nPreparing to unpack .../6-libacl1_2.3.2-1build1.1_amd64.deb ...\nUnpacking libacl1:amd64 (2.3.2-1build1.1) over (2.3.2-1build1) ...\nSetting up libacl1:amd64 (2.3.2-1build1.1) ...\n(Reading database ... 417845 files and directories currently installed.)\nPreparing to unpack .../libaudit-common_1%3a3.1.2-2.1build1.1_all.deb ...\nUnpacking libaudit-common (1:3.1.2-2.1build1.1) over (1:3.1.2-2.1build1) ...\nSetting up libaudit-common (1:3.1.2-2.1build1.1) ...\n(Reading database ... 417845 files and directories currently installed.)\nPreparing to unpack .../libaudit1_1%3a3.1.2-2.1build1.1_amd64.deb ...\nUnpacking libaudit1:amd64 (1:3.1.2-2.1build1.1) over (1:3.1.2-2.1build1) ...\nSetting up libaudit1:amd64 (1:3.1.2-2.1build1.1) ...\n(Reading database ... 417845 files and directories currently installed.)\nPreparing to unpack .../passwd_1%3a4.13+dfsg1-4ubuntu3.2_amd64.deb ...\nUnpacking passwd (1:4.13+dfsg1-4ubuntu3.2) over (1:4.13+dfsg1-4ubuntu3) ...\nSetting up passwd (1:4.13+dfsg1-4ubuntu3.2) ...\n(Reading database ... 417845 files and directories currently installed.)\nPreparing to unpack .../00-libproc2-0_2%3a4.0.4-4ubuntu3.2_amd64.deb ...\nUnpacking libproc2-0:amd64 (2:4.0.4-4ubuntu3.2) over (2:4.0.4-4ubuntu3) ...\nPreparing to unpack .../01-procps_2%3a4.0.4-4ubuntu3.2_amd64.deb ...\nUnpacking procps (2:4.0.4-4ubuntu3.2) over (2:4.0.4-4ubuntu3) ...\nPreparing to unpack .../02-distro-info-data_0.60ubuntu0.2_all.deb ...\nUnpacking distro-info-data (0.60ubuntu0.2) over (0.60ubuntu0.1) ...\nPreparing to unpack .../03-libdevmapper1.02.1_2%3a1.02.185-3ubuntu3.1_amd64.deb ...\nUnpacking libdevmapper1.02.1:amd64 (2:1.02.185-3ubuntu3.1) over (2:1.02.185-3ubuntu3) ...\nPreparing to unpack .../04-dmsetup_2%3a1.02.185-3ubuntu3.1_amd64.deb ...\nUnpacking dmsetup (2:1.02.185-3ubuntu3.1) over (2:1.02.185-3ubuntu3) ...\nPreparing to unpack .../05-krb5-locales_1.20.1-6ubuntu2.2_all.deb ...\nUnpacking krb5-locales (1.20.1-6ubuntu2.2) over (1.20.1-6ubuntu2.1) ...\nPreparing to unpack .../06-libapparmor1_4.0.1really4.0.1-0ubuntu0.24.04.3_amd64.deb ...\nUnpacking libapparmor1:amd64 (4.0.1really4.0.1-0ubuntu0.24.04.3) over (4.0.1really4.0.0-beta3-0ubuntu0.1) ...\nPreparing to unpack .../07-libcryptsetup12_2%3a2.7.0-1ubuntu4.1_amd64.deb ...\nUnpacking libcryptsetup12:amd64 (2:2.7.0-1ubuntu4.1) over (2:2.7.0-1ubuntu4) ...\nPreparing to unpack .../08-libgssapi-krb5-2_1.20.1-6ubuntu2.2_amd64.deb ...\nUnpacking libgssapi-krb5-2:amd64 (1.20.1-6ubuntu2.2) over (1.20.1-6ubuntu2.1) ...\nPreparing to unpack .../09-libkrb5-3_1.20.1-6ubuntu2.2_amd64.deb ...\nUnpacking libkrb5-3:amd64 (1.20.1-6ubuntu2.2) over (1.20.1-6ubuntu2.1) ...\nPreparing to unpack .../10-libkrb5support0_1.20.1-6ubuntu2.2_amd64.deb ...\nUnpacking libkrb5support0:amd64 (1.20.1-6ubuntu2.2) over (1.20.1-6ubuntu2.1) ...\nPreparing to unpack .../11-libk5crypto3_1.20.1-6ubuntu2.2_amd64.deb ...\nUnpacking libk5crypto3:amd64 (1.20.1-6ubuntu2.2) over (1.20.1-6ubuntu2.1) ...\nPreparing to unpack .../12-systemd-hwe-hwdb_255.1.4_all.deb ...\nUnpacking systemd-hwe-hwdb (255.1.4) over (255.1.3) ...\nPreparing to unpack .../13-ubuntu-pro-client-l10n_34~24.04_amd64.deb ...\nUnpacking ubuntu-pro-client-l10n (34~24.04) over (32.3.1~24.04) ...\nPreparing to unpack .../14-ubuntu-pro-client_34~24.04_amd64.deb ...\nUnpacking ubuntu-pro-client (34~24.04) over (32.3.1~24.04) ...\nPreparing to unpack .../15-apparmor_4.0.1really4.0.1-0ubuntu0.24.04.3_amd64.deb ...\nUnpacking apparmor (4.0.1really4.0.1-0ubuntu0.24.04.3) over (4.0.1really4.0.0-beta3-0ubuntu0.1) ...\nPreparing to unpack .../16-dmidecode_3.5-3ubuntu0.1_amd64.deb ...\nUnpacking dmidecode (3.5-3ubuntu0.1) over (3.5-3build1) ...\nPreparing to unpack .../17-mtr-tiny_0.95-1.1ubuntu0.1_amd64.deb ...\nUnpacking mtr-tiny (0.95-1.1ubuntu0.1) over (0.95-1.1build2) ...\nPreparing to unpack .../18-python3-update-manager_1%3a24.04.9_all.deb ...\nUnpacking python3-update-manager (1:24.04.9) over (1:24.04.6) ...\nPreparing to unpack .../19-update-manager-core_1%3a24.04.9_all.deb ...\nUnpacking update-manager-core (1:24.04.9) over (1:24.04.6) ...\nPreparing to unpack .../20-alsa-ucm-conf_1.2.10-1ubuntu5.3_all.deb ...\nUnpacking alsa-ucm-conf (1.2.10-1ubuntu5.3) over (1.2.10-1ubuntu5) ...\nPreparing to unpack .../21-apport-kde_2.28.1-0ubuntu3.3_all.deb ...\nUnpacking apport-kde (2.28.1-0ubuntu3.3) over (2.28.1-0ubuntu3.1) ...\nPreparing to unpack .../22-initramfs-tools_0.142ubuntu25.4_all.deb ...\nUnpacking initramfs-tools (0.142ubuntu25.4) over (0.142ubuntu25.1) ...\nPreparing to unpack .../23-initramfs-tools-core_0.142ubuntu25.4_all.deb ...\nUnpacking initramfs-tools-core (0.142ubuntu25.4) over (0.142ubuntu25.1) ...\nPreparing to unpack .../24-initramfs-tools-bin_0.142ubuntu25.4_amd64.deb ...\nUnpacking initramfs-tools-bin (0.142ubuntu25.4) over (0.142ubuntu25.1) ...\nPreparing to unpack .../25-cryptsetup-initramfs_2%3a2.7.0-1ubuntu4.1_all.deb ...\nUnpacking cryptsetup-initramfs (2:2.7.0-1ubuntu4.1) over (2:2.7.0-1ubuntu4) ...\nPreparing to unpack .../26-cryptsetup-bin_2%3a2.7.0-1ubuntu4.1_amd64.deb ...\nUnpacking cryptsetup-bin (2:2.7.0-1ubuntu4.1) over (2:2.7.0-1ubuntu4) ...\nPreparing to unpack .../27-cryptsetup_2%3a2.7.0-1ubuntu4.1_amd64.deb ...\nUnpacking cryptsetup (2:2.7.0-1ubuntu4.1) over (2:2.7.0-1ubuntu4) ...\nPreparing to unpack .../28-marble-plugins_4%3a23.08.5-0ubuntu3.2_amd64.deb ...\nUnpacking marble-plugins:amd64 (4:23.08.5-0ubuntu3.2) over (4:23.08.5-0ubuntu3) ...\nPreparing to unpack .../29-libmarblewidget-qt5-28_4%3a23.08.5-0ubuntu3.2_amd64.deb ...\nUnpacking libmarblewidget-qt5-28:amd64 (4:23.08.5-0ubuntu3.2) over (4:23.08.5-0ubuntu3) ...\nPreparing to unpack .../30-libastro1_4%3a23.08.5-0ubuntu3.2_amd64.deb ...\nUnpacking libastro1:amd64 (4:23.08.5-0ubuntu3.2) over (4:23.08.5-0ubuntu3) ...\nPreparing to unpack .../31-marble-qt-data_4%3a23.08.5-0ubuntu3.2_all.deb ...\nUnpacking marble-qt-data (4:23.08.5-0ubuntu3.2) over (4:23.08.5-0ubuntu3) ...\nPreparing to unpack .../32-digikam_4%3a8.2.0-0ubuntu6.2_amd64.deb ...\nUnpacking digikam (4:8.2.0-0ubuntu6.2) over (4:8.2.0-0ubuntu6) ...\nPreparing to unpack .../33-digikam-private-libs_4%3a8.2.0-0ubuntu6.2_amd64.deb ...\nUnpacking digikam-private-libs (4:8.2.0-0ubuntu6.2) over (4:8.2.0-0ubuntu6) ...\nPreparing to unpack .../34-digikam-data_4%3a8.2.0-0ubuntu6.2_all.deb ...\nUnpacking digikam-data (4:8.2.0-0ubuntu6.2) over (4:8.2.0-0ubuntu6) ...\nPreparing to unpack .../35-libfwupd2_1.9.27-0ubuntu1~24.04.1_amd64.deb ...\nUnpacking libfwupd2:amd64 (1.9.27-0ubuntu1~24.04.1) over (1.9.16-1) ...\nPreparing to unpack .../36-fwupd_1.9.27-0ubuntu1~24.04.1_amd64.deb ...\nUnpacking fwupd (1.9.27-0ubuntu1~24.04.1) over (1.9.16-1) ...\nPreparing to unpack .../37-libgtk-3-common_3.24.41-4ubuntu1.2_all.deb ...\nUnpacking libgtk-3-common (3.24.41-4ubuntu1.2) over (3.24.41-4ubuntu1.1) ...\nPreparing to unpack .../38-libegl1-mesa-dev_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libegl1-mesa-dev:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.1) ...\nPreparing to unpack .../39-libgtk-3-dev_3.24.41-4ubuntu1.2_amd64.deb ...\nUnpacking libgtk-3-dev:amd64 (3.24.41-4ubuntu1.2) over (3.24.41-4ubuntu1.1) ...\nPreparing to unpack .../40-libgtk-3-0t64_3.24.41-4ubuntu1.2_amd64.deb ...\nUnpacking libgtk-3-0t64:amd64 (3.24.41-4ubuntu1.2) over (3.24.41-4ubuntu1.1) ...\nPreparing to unpack .../41-gir1.2-gtk-3.0_3.24.41-4ubuntu1.2_amd64.deb ...\nUnpacking gir1.2-gtk-3.0:amd64 (3.24.41-4ubuntu1.2) over (3.24.41-4ubuntu1.1) ...\nPreparing to unpack .../42-libpackagekit-glib2-18_1.2.8-2ubuntu1.1_amd64.deb ...\nUnpacking libpackagekit-glib2-18:amd64 (1.2.8-2ubuntu1.1) over (1.2.8-2build3) ...\nPreparing to unpack .../43-gir1.2-packagekitglib-1.0_1.2.8-2ubuntu1.1_amd64.deb ...\nUnpacking gir1.2-packagekitglib-1.0 (1.2.8-2ubuntu1.1) over (1.2.8-2build3) ...\nPreparing to unpack .../44-udisks2_2.10.1-6ubuntu1_amd64.deb ...\nUnpacking udisks2 (2.10.1-6ubuntu1) over (2.10.1-6build1) ...\nPreparing to unpack .../45-libudisks2-0_2.10.1-6ubuntu1_amd64.deb ...\nUnpacking libudisks2-0:amd64 (2.10.1-6ubuntu1) over (2.10.1-6build1) ...\nPreparing to unpack .../46-gir1.2-udisks-2.0_2.10.1-6ubuntu1_amd64.deb ...\nUnpacking gir1.2-udisks-2.0:amd64 (2.10.1-6ubuntu1) over (2.10.1-6build1) ...\nPreparing to unpack .../47-gtk-update-icon-cache_3.24.41-4ubuntu1.2_amd64.deb ...\nUnpacking gtk-update-icon-cache (3.24.41-4ubuntu1.2) over (3.24.41-4ubuntu1.1) ...\nPreparing to unpack .../48-libboost-chrono1.83.0t64_1.83.0-2.1ubuntu3.1_amd64.deb ...\nUnpacking libboost-chrono1.83.0t64:amd64 (1.83.0-2.1ubuntu3.1) over (1.83.0-2.1ubuntu3) ...\nPreparing to unpack .../49-libboost-filesystem1.83.0_1.83.0-2.1ubuntu3.1_amd64.deb ...\nUnpacking libboost-filesystem1.83.0:amd64 (1.83.0-2.1ubuntu3.1) over (1.83.0-2.1ubuntu3) ...\nPreparing to unpack .../50-libboost-iostreams1.83.0_1.83.0-2.1ubuntu3.1_amd64.deb ...\nUnpacking libboost-iostreams1.83.0:amd64 (1.83.0-2.1ubuntu3.1) over (1.83.0-2.1ubuntu3) ...\nPreparing to unpack .../51-libboost-thread1.83.0_1.83.0-2.1ubuntu3.1_amd64.deb ...\nUnpacking libboost-thread1.83.0:amd64 (1.83.0-2.1ubuntu3.1) over (1.83.0-2.1ubuntu3) ...\nPreparing to unpack .../52-libboost-locale1.83.0_1.83.0-2.1ubuntu3.1_amd64.deb ...\nUnpacking libboost-locale1.83.0:amd64 (1.83.0-2.1ubuntu3.1) over (1.83.0-2.1ubuntu3) ...\nPreparing to unpack .../53-libboost-program-options1.83.0_1.83.0-2.1ubuntu3.1_amd64.deb ...\nUnpacking libboost-program-options1.83.0:amd64 (1.83.0-2.1ubuntu3.1) over (1.83.0-2.1ubuntu3) ...\nPreparing to unpack .../54-libegl-mesa0_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libegl-mesa0:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.1) ...\nPreparing to unpack .../55-libgbm1_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libgbm1:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.1) ...\nPreparing to unpack .../56-libgl1-mesa-dri_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libgl1-mesa-dri:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.1) ...\nPreparing to unpack .../57-libglx-mesa0_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libglx-mesa0:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.1) ...\nPreparing to unpack .../58-libglapi-mesa_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libglapi-mesa:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.1) ...\nPreparing to unpack .../59-libgtk-3-bin_3.24.41-4ubuntu1.2_amd64.deb ...\nUnpacking libgtk-3-bin (3.24.41-4ubuntu1.2) over (3.24.41-4ubuntu1.1) ...\nPreparing to unpack .../60-libldap-common_2.6.7+dfsg-1~exp1ubuntu8.1_all.deb ...\nUnpacking libldap-common (2.6.7+dfsg-1~exp1ubuntu8.1) over (2.6.7+dfsg-1~exp1ubuntu8) ...\nPreparing to unpack .../61-libldap2_2.6.7+dfsg-1~exp1ubuntu8.1_amd64.deb ...\nUnpacking libldap2:amd64 (2.6.7+dfsg-1~exp1ubuntu8.1) over (2.6.7+dfsg-1~exp1ubuntu8) ...\nPreparing to unpack .../62-network-manager_1.46.0-1ubuntu2.2_amd64.deb ...\nUnpacking network-manager (1.46.0-1ubuntu2.2) over (1.46.0-1ubuntu2) ...\nPreparing to unpack .../63-libnm0_1.46.0-1ubuntu2.2_amd64.deb ...\nUnpacking libnm0:amd64 (1.46.0-1ubuntu2.2) over (1.46.0-1ubuntu2) ...\nPreparing to unpack .../64-pipewire-pulse_1.0.5-1ubuntu2_amd64.deb ...\nUnpacking pipewire-pulse (1.0.5-1ubuntu2) over (1.0.5-1ubuntu1) ...\nPreparing to unpack .../65-pipewire-jack_1.0.5-1ubuntu2_amd64.deb ...\nUnpacking pipewire-jack:amd64 (1.0.5-1ubuntu2) over (1.0.5-1ubuntu1) ...\nPreparing to unpack .../66-pipewire-alsa_1.0.5-1ubuntu2_amd64.deb ...\nUnpacking pipewire-alsa:amd64 (1.0.5-1ubuntu2) over (1.0.5-1ubuntu1) ...\nPreparing to unpack .../67-libpipewire-0.3-modules_1.0.5-1ubuntu2_amd64.deb ...\nUnpacking libpipewire-0.3-modules:amd64 (1.0.5-1ubuntu2) over (1.0.5-1ubuntu1) ...\nPreparing to unpack .../68-libpipewire-0.3-0t64_1.0.5-1ubuntu2_amd64.deb ...\nUnpacking libpipewire-0.3-0t64:amd64 (1.0.5-1ubuntu2) over (1.0.5-1ubuntu1) ...\nPreparing to unpack .../69-libspa-0.2-bluetooth_1.0.5-1ubuntu2_amd64.deb ...\nUnpacking libspa-0.2-bluetooth:amd64 (1.0.5-1ubuntu2) over (1.0.5-1ubuntu1) ...\nPreparing to unpack .../70-libspa-0.2-modules_1.0.5-1ubuntu2_amd64.deb ...\nUnpacking libspa-0.2-modules:amd64 (1.0.5-1ubuntu2) over (1.0.5-1ubuntu1) ...\nPreparing to unpack .../71-pipewire_1.0.5-1ubuntu2_amd64.deb ...\nUnpacking pipewire:amd64 (1.0.5-1ubuntu2) over (1.0.5-1ubuntu1) ...\nPreparing to unpack .../72-pipewire-bin_1.0.5-1ubuntu2_amd64.deb ...\nUnpacking pipewire-bin (1.0.5-1ubuntu2) over (1.0.5-1ubuntu1) ...\nPreparing to unpack .../73-libpipewire-0.3-common_1.0.5-1ubuntu2_all.deb ...\nUnpacking libpipewire-0.3-common (1.0.5-1ubuntu2) over (1.0.5-1ubuntu1) ...\nPreparing to unpack .../74-libspeex1_1.2.1-2ubuntu2.24.04.1_amd64.deb ...\nUnpacking libspeex1:amd64 (1.2.1-2ubuntu2.24.04.1) over (1.2.1-2ubuntu2) ...\nPreparing to unpack .../75-libxatracker2_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libxatracker2:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.1) ...\nPreparing to unpack .../76-lp-solve_5.5.2.5-2ubuntu0.1_amd64.deb ...\nUnpacking lp-solve (5.5.2.5-2ubuntu0.1) over (5.5.2.5-2build4) ...\nPreparing to unpack .../77-mesa-va-drivers_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking mesa-va-drivers:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.1) ...\nPreparing to unpack .../78-mesa-vdpau-drivers_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking mesa-vdpau-drivers:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.1) ...\nPreparing to unpack .../79-mesa-vulkan-drivers_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking mesa-vulkan-drivers:amd64 (24.0.9-0ubuntu0.3) over (24.0.9-0ubuntu0.1) ...\nPreparing to unpack .../80-packagekit-tools_1.2.8-2ubuntu1.1_amd64.deb ...\nUnpacking packagekit-tools (1.2.8-2ubuntu1.1) over (1.2.8-2build3) ...\nPreparing to unpack .../81-packagekit_1.2.8-2ubuntu1.1_amd64.deb ...\nUnpacking packagekit (1.2.8-2ubuntu1.1) over (1.2.8-2build3) ...\nPreparing to unpack .../82-pipewire-audio_1.0.5-1ubuntu2_all.deb ...\nUnpacking pipewire-audio (1.0.5-1ubuntu2) over (1.0.5-1ubuntu1) ...\nPreparing to unpack .../83-plasma-distro-release-notifier_20220915-0ubuntu6.1_amd64.deb ...\nUnpacking plasma-distro-release-notifier (20220915-0ubuntu6.1) over (20220915-0ubuntu6) ...\nPreparing to unpack .../84-software-properties-common_0.99.49.1_all.deb ...\nUnpacking software-properties-common (0.99.49.1) over (0.99.48) ...\nPreparing to unpack .../85-software-properties-qt_0.99.49.1_all.deb ...\nUnpacking software-properties-qt (0.99.49.1) over (0.99.48.1) ...\nPreparing to unpack .../86-python3-software-properties_0.99.49.1_all.deb ...\nUnpacking python3-software-properties (0.99.49.1) over (0.99.48) ...\nPreparing to unpack .../87-snapd_2.66.1+24.04_amd64.deb ...\nUnpacking snapd (2.66.1+24.04) over (2.63.1+24.04) ...\nPreparing to unpack .../88-xdg-desktop-portal_1.18.4-1ubuntu2.24.04.1_amd64.deb ...\nUnpacking xdg-desktop-portal (1.18.4-1ubuntu2.24.04.1) over (1.18.4-1ubuntu2) ...\nPreparing to unpack .../89-zip_3.0-13ubuntu0.1_amd64.deb ...\nUnpacking zip (3.0-13ubuntu0.1) over (3.0-13build1) ...\nPreparing to unpack .../90-cloud-init_24.4-0ubuntu1~24.04.2_all.deb ...\nUnpacking cloud-init (24.4-0ubuntu1~24.04.2) over (24.1.3-0ubuntu3.3) ...\ndpkg: warning: unable to delete old directory '/etc/systemd/system/sshd-keygen@.service.d': Directory not empty\nSetting up libpipewire-0.3-common (1.0.5-1ubuntu2) ...\nSetting up marble-qt-data (4:23.08.5-0ubuntu3.2) ...\nSetting up libboost-program-options1.83.0:amd64 (1.83.0-2.1ubuntu3.1) ...\nSetting up mesa-vulkan-drivers:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up mesa-vdpau-drivers:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up gtk-update-icon-cache (3.24.41-4ubuntu1.2) ...\nSetting up libapparmor1:amd64 (4.0.1really4.0.1-0ubuntu0.24.04.3) ...\nSetting up libspeex1:amd64 (1.2.1-2ubuntu2.24.04.1) ...\nSetting up ubuntu-drivers-common (1:0.9.7.6ubuntu3.1) ...\nSetting up libgbm1:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up alsa-ucm-conf (1.2.10-1ubuntu5.3) ...\nSetting up python3-problem-report (2.28.1-0ubuntu3.3) ...\nSetting up distro-info-data (0.60ubuntu0.2) ...\nSetting up libfwupd2:amd64 (1.9.27-0ubuntu1~24.04.1) ...\nSetting up lp-solve (5.5.2.5-2ubuntu0.1) ...\nSetting up libboost-thread1.83.0:amd64 (1.83.0-2.1ubuntu3.1) ...\nSetting up libpackagekit-glib2-18:amd64 (1.2.8-2ubuntu1.1) ...\nSetting up krb5-locales (1.20.1-6ubuntu2.2) ...\nSetting up libboost-filesystem1.83.0:amd64 (1.83.0-2.1ubuntu3.1) ...\nSetting up libldap-common (2.6.7+dfsg-1~exp1ubuntu8.1) ...\nSetting up libxatracker2:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up python3-apport (2.28.1-0ubuntu3.3) ...\nSetting up acl (2.3.2-1build1.1) ...\nSetting up libkrb5support0:amd64 (1.20.1-6ubuntu2.2) ...\nSetting up apparmor (4.0.1really4.0.1-0ubuntu0.24.04.3) ...\nInstalling new version of config file /etc/apparmor.d/abstractions/authentication ...\nInstalling new version of config file /etc/apparmor.d/abstractions/samba ...\nInstalling new version of config file /etc/apparmor.d/firefox ...\nReloading AppArmor profiles \nSetting up gir1.2-packagekitglib-1.0 (1.2.8-2ubuntu1.1) ...\nSetting up zip (3.0-13ubuntu0.1) ...\nSetting up python3-software-properties (0.99.49.1) ...\nSetting up mtr-tiny (0.95-1.1ubuntu0.1) ...\nSetting up libboost-chrono1.83.0t64:amd64 (1.83.0-2.1ubuntu3.1) ...\nSetting up libspa-0.2-modules:amd64 (1.0.5-1ubuntu2) ...\nSetting up libboost-iostreams1.83.0:amd64 (1.83.0-2.1ubuntu3.1) ...\nSetting up libastro1:amd64 (4:23.08.5-0ubuntu3.2) ...\nSetting up libproc2-0:amd64 (2:4.0.4-4ubuntu3.2) ...\nSetting up libk5crypto3:amd64 (1.20.1-6ubuntu2.2) ...\nSetting up libglapi-mesa:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up libnm0:amd64 (1.46.0-1ubuntu2.2) ...\nSetting up systemd-hwe-hwdb (255.1.4) ...\nSetting up libdevmapper1.02.1:amd64 (2:1.02.185-3ubuntu3.1) ...\nSetting up dmsetup (2:1.02.185-3ubuntu3.1) ...\nSetting up python3-update-manager (1:24.04.9) ...\nSetting up procps (2:4.0.4-4ubuntu3.2) ...\nSetting up libspa-0.2-bluetooth:amd64 (1.0.5-1ubuntu2) ...\nSetting up libcryptsetup12:amd64 (2:2.7.0-1ubuntu4.1) ...\nSetting up packagekit (1.2.8-2ubuntu1.1) ...\nSetting up digikam-data (4:8.2.0-0ubuntu6.2) ...\nSetting up libkrb5-3:amd64 (1.20.1-6ubuntu2.2) ...\nSetting up mesa-va-drivers:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up dmidecode (3.5-3ubuntu0.1) ...\nSetting up libpipewire-0.3-0t64:amd64 (1.0.5-1ubuntu2) ...\nSetting up ubuntu-pro-client (34~24.04) ...\nInstalling new version of config file /etc/apparmor.d/ubuntu_pro_apt_news ...\nInstalling new version of config file /etc/apt/apt.conf.d/20apt-esm-hook.conf ...\nSetting up libldap2:amd64 (2.6.7+dfsg-1~exp1ubuntu8.1) ...\nSetting up fwupd (1.9.27-0ubuntu1~24.04.1) ...\nfwupd-offline-update.service is a disabled or a static unit not running, not starting it.\nfwupd-refresh.service is a disabled or a static unit not running, not starting it.\nfwupd.service is a disabled or a static unit not running, not starting it.\nSetting up libgtk-3-common (3.24.41-4ubuntu1.2) ...\nSetting up libudisks2-0:amd64 (2.10.1-6ubuntu1) ...\nSetting up initramfs-tools-bin (0.142ubuntu25.4) ...\nSetting up libegl1-mesa-dev:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up libmarblewidget-qt5-28:amd64 (4:23.08.5-0ubuntu3.2) ...\nSetting up cryptsetup-bin (2:2.7.0-1ubuntu4.1) ...\nSetting up ubuntu-pro-client-l10n (34~24.04) ...\nSetting up snapd (2.66.1+24.04) ...\nInstalling new version of config file /etc/apparmor.d/usr.lib.snapd.snap-confine.real ...\nsnapd.failure.service is a disabled or a static unit not running, not starting it.\nsnapd.snap-repair.service is a disabled or a static unit not running, not starting it.\nSetting up packagekit-tools (1.2.8-2ubuntu1.1) ...\nSetting up udisks2 (2.10.1-6ubuntu1) ...\nSetting up xdg-desktop-portal (1.18.4-1ubuntu2.24.04.1) ...\nSetting up cloud-init (24.4-0ubuntu1~24.04.2) ...\nInstalling new version of config file /etc/cloud/cloud.cfg ...\nInstalling new version of config file /etc/cloud/templates/sources.list.ubuntu.deb822.tmpl ...\nSetting up cryptsetup (2:2.7.0-1ubuntu4.1) ...\nSetting up gir1.2-udisks-2.0:amd64 (2.10.1-6ubuntu1) ...\nSetting up libgl1-mesa-dri:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up libboost-locale1.83.0:amd64 (1.83.0-2.1ubuntu3.1) ...\nSetting up plasma-distro-release-notifier (20220915-0ubuntu6.1) ...\nSetting up network-manager (1.46.0-1ubuntu2.2) ...\nWarning: The unit file, source configuration file or drop-ins of NetworkManager.service changed on disk. Run 'systemctl daemon-reload' to reload units.\nSetting up software-properties-common (0.99.49.1) ...\nSetting up libpipewire-0.3-modules:amd64 (1.0.5-1ubuntu2) ...\nSetting up libegl-mesa0:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up libgssapi-krb5-2:amd64 (1.20.1-6ubuntu2.2) ...\nSetting up marble-plugins:amd64 (4:23.08.5-0ubuntu3.2) ...\nSetting up update-manager-core (1:24.04.9) ...\nSetting up digikam-private-libs (4:8.2.0-0ubuntu6.2) ...\nSetting up initramfs-tools-core (0.142ubuntu25.4) ...\nSetting up software-properties-qt (0.99.49.1) ...\nSetting up libglx-mesa0:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up initramfs-tools (0.142ubuntu25.4) ...\nupdate-initramfs: deferring update (trigger activated)\nSetting up pipewire-bin (1.0.5-1ubuntu2) ...\nSetting up digikam (4:8.2.0-0ubuntu6.2) ...\nInstalling new version of config file /etc/apparmor.d/usr.bin.digikam ...\nSetting up pipewire:amd64 (1.0.5-1ubuntu2) ...\nSetting up cryptsetup-initramfs (2:2.7.0-1ubuntu4.1) ...\nupdate-initramfs: deferring update (trigger activated)\nSetting up pipewire-jack:amd64 (1.0.5-1ubuntu2) ...\nSetting up pipewire-alsa:amd64 (1.0.5-1ubuntu2) ...\nSetting up pipewire-pulse (1.0.5-1ubuntu2) ...\nSetting up pipewire-audio (1.0.5-1ubuntu2) ...\nSetting up apport (2.28.1-0ubuntu3.3) ...\napport-autoreport.service is a disabled or a static unit not running, not starting it.\nSetting up apport-kde (2.28.1-0ubuntu3.3) ...\nSetting up apport-core-dump-handler (2.28.1-0ubuntu3.3) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for libglib2.0-0t64:amd64 (2.80.0-6ubuntu3.2) ...\nProcessing triggers for dbus (1.14.10-4ubuntu4.1) ...\nSetting up libgtk-3-0t64:amd64 (3.24.41-4ubuntu1.2) ...\nProcessing triggers for udev (255.4-1ubuntu8.4) ...\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.3) ...\nProcessing triggers for rsyslog (8.2312.0-3ubuntu9) ...\nWarning: The unit file, source configuration file or drop-ins of rsyslog.service changed on disk. Run 'systemctl daemon-reload' to reload units.\nSetting up gir1.2-gtk-3.0:amd64 (3.24.41-4ubuntu1.2) ...\nSetting up libgtk-3-bin (3.24.41-4ubuntu1.2) ...\nSetting up libgtk-3-dev:amd64 (3.24.41-4ubuntu1.2) ...\nProcessing triggers for initramfs-tools (0.142ubuntu25.4) ...\nupdate-initramfs: Generating /boot/initrd.img-6.8.0-50-lowlatency\n</code></pre> <p>After this the system will require a reboot, but before that a few more essential packages can be installed.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#install-essential-packages","title":"Install Essential Packages","text":"<p>Start by installing a few essential packages, plus a few more  that have been found necessary later (e.g. <code>auditd</code> to stop apparmor spew in the logs):</p> <code># apt install ...</code> <pre><code># apt install gdebi-core wget gkrellm vim curl gkrellm-leds \\\ngkrellm-xkb gkrellm-cpufreq geeqie playonlinux exfat-fuse \\\nclementine id3v2 htop vnstat neofetch tigervnc-viewer sox \\\nscummvm wine gamemode python-is-python3 exiv2 rename scrot \\\nspeedtest-cli xcalib python3-pip netcat-openbsd jstest-gtk \\\netherwake python3-selenium lm-sensors sysstat tor unrar \\\nttf-mscorefonts-installer winetricks icc-profiles ffmpeg \\\niotop-c xdotool redshift-qt inxi vainfo vdpauinfo mpv xsane \\\ntigervnc-tools screen lutris libxxf86vm-dev displaycal \\\npython3-absl python3-unidecode auditd -y\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nwget is already the newest version (1.21.4-1ubuntu4.1).\nwget set to manually installed.\nnetcat-openbsd is already the newest version (1.226-1ubuntu2).\nnetcat-openbsd set to manually installed.\nsysstat is already the newest version (12.6.1-2).\nsysstat set to manually installed.\nffmpeg is already the newest version (7:6.1.1-3ubuntu5).\nffmpeg set to manually installed.\ndisplaycal is already the newest version (3.9.11-2ubuntu0.24.04.1).\ndisplaycal set to manually installed.\nThe following additional packages will be installed:\n  auditd cabextract caca-utils chafa chromium-browser chromium-chromedriver evemu-tools\n  evtest exiftran fluid-soundfont-gs fonts-wine fuseiso gamemode-daemon geeqie-common\n  gir1.2-javascriptcoregtk-4.1 gir1.2-soup-3.0 gir1.2-webkit2-4.1 icoutils joystick jp2a \n  libasound2-plugins libcapi20-3t64 libchafa0t64 libcpufreq0\n  libevemu3t64 libgamemode0 libgamemodeauto0 libgnutls-openssl27t64 libgpod-common\n  libgpod4t64 libinih1 libjavascriptcoregtk-4.1-0 liblastfm5-1 liblua5.3-0\n  libmanette-0.2-0 libmikmod3 libmspack0t64 libmygpo-qt5-1 libntlm0 libosmesa6\n  libpython3-dev libpython3.12-dev libsdl2-net-2.0-0 libsgutils2-1.46-2\n  libsixel-bin libsonivox3 libutempter0 libwebkit2gtk-4.1-0 libwine libxdo3\n  libxkbregistry0 libz-mingw-w64 python3-dev python3-evdev python3-exceptiongroup\n  python3-h11 python3-magic python3-natsort python3-outcome python3-setproctitle\n  python3-sniffio python3-trio python3-trio-websocket python3-wsproto python3.12-dev\n  redshift scummvm-data toilet toilet-fonts tor-geoipdb torsocks tree vim-runtime w3m\n  w3m-img wakeonlan webp-pixbuf-loader wine64 xdg-dbus-proxy xsane-common\nSuggested packages:\n  audispd-plugins gnome-shell-extension-gamemode xpaint libjpeg-progs\n  libterm-readline-gnu-perl | libterm-readline-perl-perl\n  libxml-dumper-perl sg3-utils fancontrol read-edid i2c-tools gamescope libcuda1 winbind\n  python-evdev-doc python-natsort-doc byobu | screenie | iselect ncurses-term\n  beneath-a-steel-sky drascula flight-of-the-amazon-queen lure-of-the-temptress\n  libsox-fmt-all figlet mixmaster torbrowser-launcher apparmor-utils nyx obfs4proxy\n  ctags vim-doc vim-scripts vnstati brotli cmigemo compface dict dict-wn dictd mailcap\n  w3m-el xsel q4wine wine-binfmt dosbox wine64-preloader\n  gocr | cuneiform | tesseract-ocr | ocrad gv hylafax-client | mgetty-fax\nRecommended packages:\n  libgamemode0:i386 libgamemodeauto0:i386 wine32\nThe following NEW packages will be installed:\n  auditd cabextract caca-utils chafa chromium-browser chromium-chromedriver clementine\n  etherwake evemu-tools evtest exfat-fuse exiftran exiv2 fluid-soundfont-gs fonts-wine\n  fuseiso gamemode gamemode-daemon gdebi-core geeqie geeqie-common\n  gir1.2-javascriptcoregtk-4.1 gir1.2-soup-3.0 gir1.2-webkit2-4.1 gkrellm\n  gkrellm-cpufreq gkrellm-leds gkrellm-xkb htop icc-profiles icoutils id3v2 inxi iotop-c\n  joystick jp2a jstest-gtk libasound2-plugins libauparse0t64 libcapi20-3t64 libchafa0t64\n  libcpufreq0 libevemu3t64 libgamemode0 libgamemodeauto0 libgnutls-openssl27t64\n  libgpod-common libgpod4t64 libinih1 libjavascriptcoregtk-4.1-0 liblastfm5-1 liblua5.3-0\n  libmanette-0.2-0 libmikmod3 libmspack0t64 libmygpo-qt5-1 libntlm0 libosmesa6\n  libpython3-dev libpython3.12-dev libsdl2-net-2.0-0 libsgutils2-1.46-2 libsixel-bin\n  libsonivox3 libutempter0 libwebkit2gtk-4.1-0 libwine libxdo3 libxkbregistry0\n  libxxf86vm-dev libz-mingw-w64 lm-sensors lutris mpv neofetch playonlinux\n  python-is-python3 python3-absl python3-dev python3-evdev python3-exceptiongroup\n  python3-h11 python3-magic python3-natsort python3-outcome python3-pip python3-selenium\n  python3-setproctitle python3-sniffio python3-trio python3-trio-websocket\n  python3-unidecode auditd python3-wsproto python3.12-dev redshift\n  redshift-qt rename screen scrot scummvm scummvm-data sox speedtest-cli tigervnc-tools\n  tigervnc-viewer toilet toilet-fonts tor tor-geoipdb torsocks tree\n  ttf-mscorefonts-installer unrar vainfo vdpauinfo vim vim-runtime vnstat w3m w3m-img\n  wakeonlan webp-pixbuf-loader wine wine64 winetricks xcalib xdg-dbus-proxy xdotool\n  xsane xsane-common\n0 upgraded, 128 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 0 B/353 MB of archives.\nAfter this operation, 1,259 MB of additional disk space will be used.\nExtracting templates from packages: 100%\nPreconfiguring packages ...\nSelecting previously unselected package chromium-browser.\n(Reading database ... 417887 files and directories currently installed.)\nPreparing to unpack .../000-chromium-browser_2%3a1snap1-0ubuntu2_amd64.deb ...\n=&gt; Installing the chromium snap\n==&gt; Checking connectivity with the snap store\n==&gt; Installing the chromium snap\nWarning: /snap/bin was not found in your $PATH. If you've not restarted your session since you\n        installed snapd, try doing that. Please see https://forum.snapcraft.io/t/9469 for more\n        details.\n\nchromium 131.0.6778.139 from Canonical\u2713 installed\n=&gt; Snap installation complete\n\nUnpacking chromium-browser (2:1snap1-0ubuntu2) ...\nSelecting previously unselected package libmspack0t64:amd64.\nPreparing to unpack .../001-libmspack0t64_0.11-1.1build1_amd64.deb ...\nUnpacking libmspack0t64:amd64 (0.11-1.1build1) ...\nSelecting previously unselected package cabextract.\nPreparing to unpack .../002-cabextract_1.11-2_amd64.deb ...\nUnpacking cabextract (1.11-2) ...\nSelecting previously unselected package ttf-mscorefonts-installer.\nPreparing to unpack .../003-ttf-mscorefonts-installer_3.8.1ubuntu1_all.deb ...\nUnpacking ttf-mscorefonts-installer (3.8.1ubuntu1) ...\nSelecting previously unselected package vnstat.\nPreparing to unpack .../004-vnstat_2.12-1_amd64.deb ...\nUnpacking vnstat (2.12-1) ...\nSelecting previously unselected package caca-utils.\nPreparing to unpack .../005-caca-utils_0.99.beta20-4build2_amd64.deb ...\nUnpacking caca-utils (0.99.beta20-4build2) ...\nSelecting previously unselected package libchafa0t64:amd64.\nPreparing to unpack .../006-libchafa0t64_1.14.0-1.1build1_amd64.deb ...\nUnpacking libchafa0t64:amd64 (1.14.0-1.1build1) ...\nSelecting previously unselected package chafa.\nPreparing to unpack .../007-chafa_1.14.0-1.1build1_amd64.deb ...\nUnpacking chafa (1.14.0-1.1build1) ...\nSelecting previously unselected package chromium-chromedriver.\nPreparing to unpack .../008-chromium-chromedriver_2%3a1snap1-0ubuntu2_amd64.deb ...\nUnpacking chromium-chromedriver (2:1snap1-0ubuntu2) ...\nSelecting previously unselected package libgpod4t64:amd64.\nPreparing to unpack .../009-libgpod4t64_0.8.3-19.1ubuntu4_amd64.deb ...\nUnpacking libgpod4t64:amd64 (0.8.3-19.1ubuntu4) ...\nSelecting previously unselected package liblastfm5-1:amd64.\nPreparing to unpack .../010-liblastfm5-1_1.1.0-5build3_amd64.deb ...\nUnpacking liblastfm5-1:amd64 (1.1.0-5build3) ...\nSelecting previously unselected package libmygpo-qt5-1:amd64.\nPreparing to unpack .../011-libmygpo-qt5-1_1.1.0-4.1build3_amd64.deb ...\nUnpacking libmygpo-qt5-1:amd64 (1.1.0-4.1build3) ...\nSelecting previously unselected package clementine.\nPreparing to unpack .../012-clementine_1.4.0~rc1+git867-g9ef681b0e+dfsg-1ubuntu4_amd64.deb ...\nUnpacking clementine (1.4.0~rc1+git867-g9ef681b0e+dfsg-1ubuntu4) ...\nSelecting previously unselected package etherwake.\nPreparing to unpack .../013-etherwake_1.09-4build1_amd64.deb ...\nUnpacking etherwake (1.09-4build1) ...\nSelecting previously unselected package libevemu3t64:amd64.\nPreparing to unpack .../014-libevemu3t64_2.7.0-4build1_amd64.deb ...\nUnpacking libevemu3t64:amd64 (2.7.0-4build1) ...\nSelecting previously unselected package evemu-tools.\nPreparing to unpack .../015-evemu-tools_2.7.0-4build1_amd64.deb ...\nUnpacking evemu-tools (2.7.0-4build1) ...\nSelecting previously unselected package exfat-fuse.\nPreparing to unpack .../016-exfat-fuse_1.4.0-2_amd64.deb ...\nUnpacking exfat-fuse (1.4.0-2) ...\nSelecting previously unselected package exiftran.\nPreparing to unpack .../017-exiftran_2.10-4ubuntu4_amd64.deb ...\nUnpacking exiftran (2.10-4ubuntu4) ...\nSelecting previously unselected package exiv2.\nPreparing to unpack .../018-exiv2_0.27.6-1build1_amd64.deb ...\nUnpacking exiv2 (0.27.6-1build1) ...\nSelecting previously unselected package fluid-soundfont-gs.\nPreparing to unpack .../019-fluid-soundfont-gs_3.1-5.3_all.deb ...\nUnpacking fluid-soundfont-gs (3.1-5.3) ...\nSelecting previously unselected package fonts-wine.\nPreparing to unpack .../020-fonts-wine_9.0~repack-4build3_all.deb ...\nUnpacking fonts-wine (9.0~repack-4build3) ...\nSelecting previously unselected package fuseiso.\nPreparing to unpack .../021-fuseiso_20070708-3.2build3_amd64.deb ...\nUnpacking fuseiso (20070708-3.2build3) ...\nSelecting previously unselected package libinih1:amd64.\nPreparing to unpack .../022-libinih1_55-1ubuntu2_amd64.deb ...\nUnpacking libinih1:amd64 (55-1ubuntu2) ...\nSelecting previously unselected package gamemode-daemon.\nPreparing to unpack .../023-gamemode-daemon_1.8.1-2build1_amd64.deb ...\nUnpacking gamemode-daemon (1.8.1-2build1) ...\nSelecting previously unselected package libgamemode0:amd64.\nPreparing to unpack .../024-libgamemode0_1.8.1-2build1_amd64.deb ...\nUnpacking libgamemode0:amd64 (1.8.1-2build1) ...\nSelecting previously unselected package libgamemodeauto0:amd64.\nPreparing to unpack .../025-libgamemodeauto0_1.8.1-2build1_amd64.deb ...\nUnpacking libgamemodeauto0:amd64 (1.8.1-2build1) ...\nSelecting previously unselected package gamemode.\nPreparing to unpack .../026-gamemode_1.8.1-2build1_amd64.deb ...\nUnpacking gamemode (1.8.1-2build1) ...\nSelecting previously unselected package gdebi-core.\nPreparing to unpack .../027-gdebi-core_0.9.5.7+nmu7_all.deb ...\nUnpacking gdebi-core (0.9.5.7+nmu7) ...\nSelecting previously unselected package liblua5.3-0:amd64.\nPreparing to unpack .../028-liblua5.3-0_5.3.6-2build2_amd64.deb ...\nUnpacking liblua5.3-0:amd64 (5.3.6-2build2) ...\nSelecting previously unselected package geeqie-common.\nPreparing to unpack .../029-geeqie-common_1%3a2.2-2build4_all.deb ...\nUnpacking geeqie-common (1:2.2-2build4) ...\nSelecting previously unselected package webp-pixbuf-loader:amd64.\nPreparing to unpack .../030-webp-pixbuf-loader_0.2.4-2build2_amd64.deb ...\nUnpacking webp-pixbuf-loader:amd64 (0.2.4-2build2) ...\nSelecting previously unselected package geeqie.\nPreparing to unpack .../031-geeqie_1%3a2.2-2build4_amd64.deb ...\nUnpacking geeqie (1:2.2-2build4) ...\nSelecting previously unselected package libjavascriptcoregtk-4.1-0:amd64.\nPreparing to unpack .../032-libjavascriptcoregtk-4.1-0_2.46.4-0ubuntu0.24.04.1_amd64.deb ...\nUnpacking libjavascriptcoregtk-4.1-0:amd64 (2.46.4-0ubuntu0.24.04.1) ...\nSelecting previously unselected package gir1.2-javascriptcoregtk-4.1:amd64.\nPreparing to unpack .../033-gir1.2-javascriptcoregtk-4.1_2.46.4-0ubuntu0.24.04.1_amd64.deb ...\nUnpacking gir1.2-javascriptcoregtk-4.1:amd64 (2.46.4-0ubuntu0.24.04.1) ...\nSelecting previously unselected package gir1.2-soup-3.0:amd64.\nPreparing to unpack .../034-gir1.2-soup-3.0_3.4.4-5ubuntu0.1_amd64.deb ...\nUnpacking gir1.2-soup-3.0:amd64 (3.4.4-5ubuntu0.1) ...\nSelecting previously unselected package xdg-dbus-proxy.\nPreparing to unpack .../035-xdg-dbus-proxy_0.1.5-1build2_amd64.deb ...\nUnpacking xdg-dbus-proxy (0.1.5-1build2) ...\nSelecting previously unselected package libmanette-0.2-0:amd64.\nPreparing to unpack .../036-libmanette-0.2-0_0.2.7-1build2_amd64.deb ...\nUnpacking libmanette-0.2-0:amd64 (0.2.7-1build2) ...\nSelecting previously unselected package libwebkit2gtk-4.1-0:amd64.\nPreparing to unpack .../037-libwebkit2gtk-4.1-0_2.46.4-0ubuntu0.24.04.1_amd64.deb ...\nUnpacking libwebkit2gtk-4.1-0:amd64 (2.46.4-0ubuntu0.24.04.1) ...\nSelecting previously unselected package gir1.2-webkit2-4.1:amd64.\nPreparing to unpack .../038-gir1.2-webkit2-4.1_2.46.4-0ubuntu0.24.04.1_amd64.deb ...\nUnpacking gir1.2-webkit2-4.1:amd64 (2.46.4-0ubuntu0.24.04.1) ...\nSelecting previously unselected package libgnutls-openssl27t64:amd64.\nPreparing to unpack .../039-libgnutls-openssl27t64_3.8.3-1.1ubuntu3.2_amd64.deb ...\nUnpacking libgnutls-openssl27t64:amd64 (3.8.3-1.1ubuntu3.2) ...\nSelecting previously unselected package libntlm0:amd64.\nPreparing to unpack .../040-libntlm0_1.7-1build1_amd64.deb ...\nUnpacking libntlm0:amd64 (1.7-1build1) ...\nSelecting previously unselected package gkrellm.\nPreparing to unpack .../041-gkrellm_2.3.11-2build2_amd64.deb ...\nUnpacking gkrellm (2.3.11-2build2) ...\nSelecting previously unselected package gkrellm-xkb.\nPreparing to unpack .../042-gkrellm-xkb_1.05-5.1build2_amd64.deb ...\nUnpacking gkrellm-xkb (1.05-5.1build2) ...\nSelecting previously unselected package htop.\nPreparing to unpack .../043-htop_3.3.0-4build1_amd64.deb ...\nUnpacking htop (3.3.0-4build1) ...\nSelecting previously unselected package icc-profiles.\nPreparing to unpack .../044-icc-profiles_2.1-2_all.deb ...\nUnpacking icc-profiles (2.1-2) ...\nSelecting previously unselected package icoutils.\nPreparing to unpack .../045-icoutils_0.32.3-4build2_amd64.deb ...\nUnpacking icoutils (0.32.3-4build2) ...\nSelecting previously unselected package id3v2.\nPreparing to unpack .../046-id3v2_0.1.12+dfsg-7_amd64.deb ...\nUnpacking id3v2 (0.1.12+dfsg-7) ...\nSelecting previously unselected package iotop-c.\nPreparing to unpack .../047-iotop-c_1.26-1_amd64.deb ...\nUnpacking iotop-c (1.26-1) ...\nSelecting previously unselected package jp2a.\nPreparing to unpack .../048-jp2a_1.1.1-2ubuntu2_amd64.deb ...\nUnpacking jp2a (1.1.1-2ubuntu2) ...\nSelecting previously unselected package jstest-gtk.\nPreparing to unpack .../049-jstest-gtk_0.1.1~git20180602-2build2_amd64.deb ...\nUnpacking jstest-gtk (0.1.1~git20180602-2build2) ...\nSelecting previously unselected package libasound2-plugins:amd64.\nPreparing to unpack .../050-libasound2-plugins_1.2.7.1-1ubuntu5_amd64.deb ...\nUnpacking libasound2-plugins:amd64 (1.2.7.1-1ubuntu5) ...\nSelecting previously unselected package libcapi20-3t64:amd64.\nPreparing to unpack .../051-libcapi20-3t64_1%3a3.27-3.1build1_amd64.deb ...\nUnpacking libcapi20-3t64:amd64 (1:3.27-3.1build1) ...\nSelecting previously unselected package libcpufreq0.\nPreparing to unpack .../052-libcpufreq0_008-2build2_amd64.deb ...\nUnpacking libcpufreq0 (008-2build2) ...\nSelecting previously unselected package libsgutils2-1.46-2:amd64.\nPreparing to unpack .../053-libsgutils2-1.46-2_1.46-3ubuntu4_amd64.deb ...\nUnpacking libsgutils2-1.46-2:amd64 (1.46-3ubuntu4) ...\nSelecting previously unselected package libgpod-common.\nPreparing to unpack .../054-libgpod-common_0.8.3-19.1ubuntu4_amd64.deb ...\nUnpacking libgpod-common (0.8.3-19.1ubuntu4) ...\nSelecting previously unselected package libmikmod3:amd64.\nPreparing to unpack .../055-libmikmod3_3.3.11.1-7build1_amd64.deb ...\nUnpacking libmikmod3:amd64 (3.3.11.1-7build1) ...\nSelecting previously unselected package libpython3.12-dev:amd64.\nPreparing to unpack .../056-libpython3.12-dev_3.12.3-1ubuntu0.3_amd64.deb ...\nUnpacking libpython3.12-dev:amd64 (3.12.3-1ubuntu0.3) ...\nSelecting previously unselected package libpython3-dev:amd64.\nPreparing to unpack .../057-libpython3-dev_3.12.3-0ubuntu2_amd64.deb ...\nUnpacking libpython3-dev:amd64 (3.12.3-0ubuntu2) ...\nSelecting previously unselected package libsdl2-net-2.0-0:amd64.\nPreparing to unpack .../058-libsdl2-net-2.0-0_2.2.0+dfsg-2_amd64.deb ...\nUnpacking libsdl2-net-2.0-0:amd64 (2.2.0+dfsg-2) ...\nSelecting previously unselected package libsonivox3:amd64.\nPreparing to unpack .../059-libsonivox3_3.6.12-1_amd64.deb ...\nUnpacking libsonivox3:amd64 (3.6.12-1) ...\nSelecting previously unselected package libutempter0:amd64.\nPreparing to unpack .../060-libutempter0_1.2.1-3build1_amd64.deb ...\nUnpacking libutempter0:amd64 (1.2.1-3build1) ...\nSelecting previously unselected package libxkbregistry0:amd64.\nPreparing to unpack .../061-libxkbregistry0_1.6.0-1build1_amd64.deb ...\nUnpacking libxkbregistry0:amd64 (1.6.0-1build1) ...\nSelecting previously unselected package libz-mingw-w64..................................] \nPreparing to unpack .../062-libz-mingw-w64_1.3.1+dfsg-1_all.deb ........................] \nUnpacking libz-mingw-w64 (1.3.1+dfsg-1) ...\nSelecting previously unselected package libwine:amd64.\nPreparing to unpack .../063-libwine_9.0~repack-4build3_amd64.deb ...\nUnpacking libwine:amd64 (9.0~repack-4build3) ...\nSelecting previously unselected package libxxf86vm-dev:amd64.\nPreparing to unpack .../064-libxxf86vm-dev_1%3a1.1.4-1build4_amd64.deb ...\nUnpacking libxxf86vm-dev:amd64 (1:1.1.4-1build4) ...\nSelecting previously unselected package python3-setproctitle:amd64.\nPreparing to unpack .../065-python3-setproctitle_1.3.3-1build2_amd64.deb ...\nUnpacking python3-setproctitle:amd64 (1.3.3-1build2) ...\nSelecting previously unselected package python3-magic.\nPreparing to unpack .../066-python3-magic_2%3a0.4.27-3_all.deb ...\nUnpacking python3-magic (2:0.4.27-3) ...\nSelecting previously unselected package lutris.\nPreparing to unpack .../067-lutris_0.5.14-2_all.deb ...\nUnpacking lutris (0.5.14-2) ...\nSelecting previously unselected package mpv.\nPreparing to unpack .../068-mpv_0.37.0-1ubuntu4_amd64.deb ...\nUnpacking mpv (0.37.0-1ubuntu4) ...\nSelecting previously unselected package neofetch.\nPreparing to unpack .../069-neofetch_7.1.0-4_all.deb ...\nUnpacking neofetch (7.1.0-4) ...\nSelecting previously unselected package python3-natsort.\nPreparing to unpack .../070-python3-natsort_8.0.2-2_all.deb ...\nUnpacking python3-natsort (8.0.2-2) ...\nSelecting previously unselected package wine64.\nPreparing to unpack .../071-wine64_9.0~repack-4build3_amd64.deb ...\nUnpacking wine64 (9.0~repack-4build3) ...\nSelecting previously unselected package wine.\nPreparing to unpack .../072-wine_9.0~repack-4build3_all.deb ...\nUnpacking wine (9.0~repack-4build3) ...\nSelecting previously unselected package playonlinux.\nPreparing to unpack .../073-playonlinux_4.3.4-3_all.deb ...\nUnpacking playonlinux (4.3.4-3) ...\nSelecting previously unselected package python-is-python3.\nPreparing to unpack .../074-python-is-python3_3.11.4-1_all.deb ...\nUnpacking python-is-python3 (3.11.4-1) ...\nSelecting previously unselected package python3-absl.\nPreparing to unpack .../075-python3-absl_2.1.0-1_all.deb ...\nUnpacking python3-absl (2.1.0-1) ...\nSelecting previously unselected package python3.12-dev.\nPreparing to unpack .../076-python3.12-dev_3.12.3-1ubuntu0.3_amd64.deb ...\nUnpacking python3.12-dev (3.12.3-1ubuntu0.3) ...\nSelecting previously unselected package python3-dev.\nPreparing to unpack .../077-python3-dev_3.12.3-0ubuntu2_amd64.deb ...\nUnpacking python3-dev (3.12.3-0ubuntu2) ...\nSelecting previously unselected package python3-evdev.\nPreparing to unpack .../078-python3-evdev_1.7.0+dfsg-1build1_amd64.deb ...\nUnpacking python3-evdev (1.7.0+dfsg-1build1) ...\nSelecting previously unselected package python3-exceptiongroup.\nPreparing to unpack .../079-python3-exceptiongroup_1.2.0-1_all.deb ...\nUnpacking python3-exceptiongroup (1.2.0-1) ...\nSelecting previously unselected package python3-h11.\nPreparing to unpack .../080-python3-h11_0.14.0-1_all.deb ...\nUnpacking python3-h11 (0.14.0-1) ...\nSelecting previously unselected package python3-outcome.\nPreparing to unpack .../081-python3-outcome_1.2.0-1.1_all.deb ...\nUnpacking python3-outcome (1.2.0-1.1) ...\nSelecting previously unselected package python3-pip.\nPreparing to unpack .../082-python3-pip_24.0+dfsg-1ubuntu1.1_all.deb ...\nUnpacking python3-pip (24.0+dfsg-1ubuntu1.1) ...\nSelecting previously unselected package python3-sniffio.\nPreparing to unpack .../083-python3-sniffio_1.3.0-2_all.deb ...\nUnpacking python3-sniffio (1.3.0-2) ...\nSelecting previously unselected package python3-trio.\nPreparing to unpack .../084-python3-trio_0.24.0-1ubuntu1_all.deb ...\nUnpacking python3-trio (0.24.0-1ubuntu1) ...\nSelecting previously unselected package python3-wsproto.\nPreparing to unpack .../085-python3-wsproto_1.2.0-1_all.deb ...\nUnpacking python3-wsproto (1.2.0-1) ...\nSelecting previously unselected package python3-trio-websocket.\nPreparing to unpack .../086-python3-trio-websocket_0.11.1-1_all.deb ...\nUnpacking python3-trio-websocket (0.11.1-1) ...\nSelecting previously unselected package python3-selenium.\nPreparing to unpack .../087-python3-selenium_4.18.1+dfsg-1_all.deb ...\nUnpacking python3-selenium (4.18.1+dfsg-1) ...\nSelecting previously unselected package python3-unidecode.\nPreparing to unpack .../088-python3-unidecode_1.3.8-1_all.deb ...\nUnpacking python3-unidecode (1.3.8-1) ...\nSelecting previously unselected package redshift.\nPreparing to unpack .../089-redshift_1.12-4.2ubuntu4_amd64.deb ...\nUnpacking redshift (1.12-4.2ubuntu4) ...\nSelecting previously unselected package redshift-qt.\nPreparing to unpack .../090-redshift-qt_0.6-3build3_amd64.deb ...\nUnpacking redshift-qt (0.6-3build3) ...\nSelecting previously unselected package rename.\nPreparing to unpack .../091-rename_2.02-1_all.deb ...\nUnpacking rename (2.02-1) ...\nSelecting previously unselected package screen.\nPreparing to unpack .../092-screen_4.9.1-1build1_amd64.deb ...\nUnpacking screen (4.9.1-1build1) ...\nSelecting previously unselected package scrot.\nPreparing to unpack .../093-scrot_1.10-1build2_amd64.deb ...\nUnpacking scrot (1.10-1build2) ...\nSelecting previously unselected package scummvm-data.\nPreparing to unpack .../094-scummvm-data_2.8.0+dfsg-1build5_all.deb ...\nUnpacking scummvm-data (2.8.0+dfsg-1build5) ...\nSelecting previously unselected package scummvm.\nPreparing to unpack .../095-scummvm_2.8.0+dfsg-1build5_amd64.deb ...\nUnpacking scummvm (2.8.0+dfsg-1build5) ...\nSelecting previously unselected package sox.\nPreparing to unpack .../096-sox_14.4.2+git20190427-4build4_amd64.deb ...\nUnpacking sox (14.4.2+git20190427-4build4) ...\nSelecting previously unselected package speedtest-cli.\nPreparing to unpack .../097-speedtest-cli_2.1.3-2_all.deb ...\nUnpacking speedtest-cli (2.1.3-2) ...\nSelecting previously unselected package tigervnc-tools.\nPreparing to unpack .../098-tigervnc-tools_1.13.1+dfsg-2build2_amd64.deb ...\nUnpacking tigervnc-tools (1.13.1+dfsg-2build2) ...\nSelecting previously unselected package tigervnc-viewer.\nPreparing to unpack .../099-tigervnc-viewer_1.13.1+dfsg-2build2_amd64.deb ...\nUnpacking tigervnc-viewer (1.13.1+dfsg-2build2) ...\nSelecting previously unselected package toilet-fonts.\nPreparing to unpack .../100-toilet-fonts_0.3-1.4build1_all.deb ...\nUnpacking toilet-fonts (0.3-1.4build1) ...\nSelecting previously unselected package toilet.\nPreparing to unpack .../101-toilet_0.3-1.4build1_amd64.deb ...\nUnpacking toilet (0.3-1.4build1) ...\nSelecting previously unselected package tor.\nPreparing to unpack .../102-tor_0.4.8.10-1build2_amd64.deb ...\nUnpacking tor (0.4.8.10-1build2) ...\nSelecting previously unselected package torsocks.\nPreparing to unpack .../103-torsocks_2.4.0-1_amd64.deb ...\nUnpacking torsocks (2.4.0-1) ...\nSelecting previously unselected package tree.\nPreparing to unpack .../104-tree_2.1.1-2ubuntu3_amd64.deb ...\nUnpacking tree (2.1.1-2ubuntu3) ...\nSelecting previously unselected package unrar.\nPreparing to unpack .../105-unrar_1%3a7.0.7-1build1_amd64.deb ...\nUnpacking unrar (1:7.0.7-1build1) ...\nSelecting previously unselected package vainfo.\nPreparing to unpack .../106-vainfo_2.12.0+ds1-1_amd64.deb ...\nUnpacking vainfo (2.12.0+ds1-1) ...\nSelecting previously unselected package vim-runtime.\nPreparing to unpack .../107-vim-runtime_2%3a9.1.0016-1ubuntu7.5_all.deb ...\nAdding 'diversion of /usr/share/vim/vim91/doc/help.txt to /usr/share/vim/vim91/doc/help.txt.vim-tiny by vim-runtime'\nAdding 'diversion of /usr/share/vim/vim91/doc/tags to /usr/share/vim/vim91/doc/tags.vim-tiny by vim-runtime'\nUnpacking vim-runtime (2:9.1.0016-1ubuntu7.5) ...\nSelecting previously unselected package vim.\nPreparing to unpack .../108-vim_2%3a9.1.0016-1ubuntu7.5_amd64.deb ...\nUnpacking vim (2:9.1.0016-1ubuntu7.5) ...\nSelecting previously unselected package w3m.\nPreparing to unpack .../109-w3m_0.5.3+git20230121-2ubuntu5_amd64.deb ...\nUnpacking w3m (0.5.3+git20230121-2ubuntu5) ...\nSelecting previously unselected package w3m-img.\nPreparing to unpack .../110-w3m-img_0.5.3+git20230121-2ubuntu5_amd64.deb ...\nUnpacking w3m-img (0.5.3+git20230121-2ubuntu5) ...\nSelecting previously unselected package wakeonlan.\nPreparing to unpack .../111-wakeonlan_0.41-12.1_all.deb ...\nUnpacking wakeonlan (0.41-12.1) ...\nSelecting previously unselected package winetricks.\nPreparing to unpack .../112-winetricks_20240105-2_all.deb ...\nUnpacking winetricks (20240105-2) ...\nSelecting previously unselected package xsane-common.\nPreparing to unpack .../113-xsane-common_0.999-12ubuntu4_all.deb ...\nUnpacking xsane-common (0.999-12ubuntu4) ...\nSelecting previously unselected package xsane.\nPreparing to unpack .../114-xsane_0.999-12ubuntu4_amd64.deb ...\nUnpacking xsane (0.999-12ubuntu4) ...\nSelecting previously unselected package evtest.\nPreparing to unpack .../115-evtest_1%3a1.35-1_amd64.deb ...\nUnpacking evtest (1:1.35-1) ...\nSelecting previously unselected package gkrellm-cpufreq.\nPreparing to unpack .../116-gkrellm-cpufreq_0.6.4-4_amd64.deb ...\nUnpacking gkrellm-cpufreq (0.6.4-4) ...\nSelecting previously unselected package gkrellm-leds.\nPreparing to unpack .../117-gkrellm-leds_0.8.0-2build2_amd64.deb ...\nUnpacking gkrellm-leds (0.8.0-2build2) ...\nSelecting previously unselected package inxi.\nPreparing to unpack .../118-inxi_3.3.34-1-1_all.deb ...\nUnpacking inxi (3.3.34-1-1) ...\nSelecting previously unselected package joystick.\nPreparing to unpack .../119-joystick_1%3a1.8.1-2build1_amd64.deb ...\nUnpacking joystick (1:1.8.1-2build1) ...\nSelecting previously unselected package libosmesa6:amd64.\nPreparing to unpack .../120-libosmesa6_24.0.9-0ubuntu0.3_amd64.deb ...\nUnpacking libosmesa6:amd64 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package libsixel-bin.\nPreparing to unpack .../121-libsixel-bin_1.10.3-3build1_amd64.deb ...\nUnpacking libsixel-bin (1.10.3-3build1) ...\nSelecting previously unselected package libxdo3:amd64.\nPreparing to unpack .../122-libxdo3_1%3a3.20160805.1-5build1_amd64.deb ...\nUnpacking libxdo3:amd64 (1:3.20160805.1-5build1) ...\nSelecting previously unselected package lm-sensors.\nPreparing to unpack .../123-lm-sensors_1%3a3.6.0-9build1_amd64.deb ...\nUnpacking lm-sensors (1:3.6.0-9build1) ...\nSelecting previously unselected package tor-geoipdb.\nPreparing to unpack .../124-tor-geoipdb_0.4.8.10-1build2_all.deb ...\nUnpacking tor-geoipdb (0.4.8.10-1build2) ...\nSelecting previously unselected package vdpauinfo.\nPreparing to unpack .../125-vdpauinfo_1.5-2_amd64.deb ...\nUnpacking vdpauinfo (1.5-2) ...\nSelecting previously unselected package xcalib.\nPreparing to unpack .../126-xcalib_0.8.dfsg1-3_amd64.deb ...\nUnpacking xcalib (0.8.dfsg1-3) ...\nSelecting previously unselected package xdotool.\nPreparing to unpack .../127-xdotool_1%3a3.20160805.1-5build1_amd64.deb ...\nUnpacking xdotool (1:3.20160805.1-5build1) ...\nSelecting previously unselected package libauparse0t64:amd64.\n(Reading database ... 426177 files and directories currently installed.)\nPreparing to unpack .../libauparse0t64_1%3a3.1.2-2.1build1.1_amd64.deb ...\nAdding 'diversion of /lib/x86_64-linux-gnu/libauparse.so.0 to /lib/x86_64-linux-gnu/libauparse.so.0.usr-is-merged by libauparse0t64'\nAdding 'diversion of /lib/x86_64-linux-gnu/libauparse.so.0.0.0 to /lib/x86_64-linux-gnu/libauparse.so.0.0.0.usr-is-merged by libauparse0t64'\nUnpacking libauparse0t64:amd64 (1:3.1.2-2.1build1.1) ...\nSelecting previously unselected package auditd.\nPreparing to unpack .../auditd_1%3a3.1.2-2.1build1.1_amd64.deb ...\nUnpacking auditd (1:3.1.2-2.1build1.1) ...\nSetting up libauparse0t64:amd64 (1:3.1.2-2.1build1.1) ...\nSetting up auditd (1:3.1.2-2.1build1.1) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/auditd.service \u2192 /usr/lib/systemd/system/auditd.service.\nSetting up libsonivox3:amd64 (3.6.12-1) ...\nSetting up libgnutls-openssl27t64:amd64 (3.8.3-1.1ubuntu3.2) ...\nSetting up python3-sniffio (1.3.0-2) ...\nSetting up python3-outcome (1.2.0-1.1) ...\nSetting up libasound2-plugins:amd64 (1.2.7.1-1ubuntu5) ...\nSetting up toilet-fonts (0.3-1.4build1) ...\nSetting up gdebi-core (0.9.5.7+nmu7) ...\n/usr/share/gdebi/GDebi/GDebiCli.py:159: SyntaxWarning: invalid escape sequence '\\S'\nc = findall(\"[[(](\\S+)/\\S+[])]\", msg)[0].lower()\nSetting up caca-utils (0.99.beta20-4build2) ...\nSetting up toilet (0.3-1.4build1) ...\nupdate-alternatives: using /usr/bin/figlet-toilet to provide /usr/bin/figlet (figlet) in auto mode\nSetting up libgpod4t64:amd64 (0.8.3-19.1ubuntu4) ...\nSetting up libsdl2-net-2.0-0:amd64 (2.2.0+dfsg-2) ...\nSetting up htop (3.3.0-4build1) ...\nSetting up vdpauinfo (1.5-2) ...\nSetting up libinih1:amd64 (55-1ubuntu2) ...\nSetting up joystick (1:1.8.1-2build1) ...\nSetting up geeqie-common (1:2.2-2build4) ...\nSetting up wakeonlan (0.41-12.1) ...\nSetting up libmanette-0.2-0:amd64 (0.2.7-1build2) ...\nSetting up libxxf86vm-dev:amd64 (1:1.1.4-1build4) ...\nSetting up libmygpo-qt5-1:amd64 (1.1.0-4.1build3) ...\nSetting up libmspack0t64:amd64 (0.11-1.1build1) ...\nSetting up jp2a (1.1.1-2ubuntu2) ...\nSetting up libmikmod3:amd64 (3.3.11.1-7build1) ...\nSetting up libsgutils2-1.46-2:amd64 (1.46-3ubuntu4) ...\nSetting up redshift (1.12-4.2ubuntu4) ...\nSetting up inxi (3.3.34-1-1) ...\nSetting up rename (2.02-1) ...\nupdate-alternatives: using /usr/bin/file-rename to provide /usr/bin/rename (rename) in auto mode\nSetting up libpython3.12-dev:amd64 (3.12.3-1ubuntu0.3) ...\nSetting up unrar (1:7.0.7-1build1) ...\nupdate-alternatives: using /usr/bin/unrar-nonfree to provide /usr/bin/unrar (unrar) in auto mode\nSetting up libchafa0t64:amd64 (1.14.0-1.1build1) ...\nSetting up libz-mingw-w64 (1.3.1+dfsg-1) ...\nSetting up libsixel-bin (1.10.3-3build1) ...\nSetting up jstest-gtk (0.1.1~git20180602-2build2) ...\nSetting up neofetch (7.1.0-4) ...\nSetting up python3-natsort (8.0.2-2) ...\nSetting up gamemode-daemon (1.8.1-2build1) ...\nSetting up webp-pixbuf-loader:amd64 (0.2.4-2build2) ...\nSetting up exiftran (2.10-4ubuntu4) ...\nSetting up python3-absl (2.1.0-1) ...\nSetting up libxdo3:amd64 (1:3.20160805.1-5build1) ...\nSetting up fonts-wine (9.0~repack-4build3) ...\nSetting up scummvm-data (2.8.0+dfsg-1build5) ...\nSetting up tigervnc-tools (1.13.1+dfsg-2build2) ...\nupdate-alternatives: using /usr/bin/tigervncpasswd to provide /usr/bin/vncpasswd (vncpasswd) in auto mode\nSetting up libjavascriptcoregtk-4.1-0:amd64 (2.46.4-0ubuntu0.24.04.1) ...\nSetting up fuseiso (20070708-3.2build3) ...\nSetting up w3m (0.5.3+git20230121-2ubuntu5) ...\nSetting up libxkbregistry0:amd64 (1.6.0-1build1) ...\nSetting up libntlm0:amd64 (1.7-1build1) ...\nSetting up icoutils (0.32.3-4build2) ...\nSetting up tree (2.1.1-2ubuntu3) ...\nSetting up exiv2 (0.27.6-1build1) ...\nSetting up python3-setproctitle:amd64 (1.3.3-1build2) ...\nSetting up python3.12-dev (3.12.3-1ubuntu0.3) ...\nSetting up python3-h11 (0.14.0-1) ...\nSetting up python3-evdev (1.7.0+dfsg-1build1) ...\nSetting up libevemu3t64:amd64 (2.7.0-4build1) ...\nSetting up python3-pip (24.0+dfsg-1ubuntu1.1) ...\nSetting up icc-profiles (2.1-2) ...\nSetting up libcapi20-3t64:amd64 (1:3.27-3.1build1) ...\nSetting up id3v2 (0.1.12+dfsg-7) ...\nSetting up vainfo (2.12.0+ds1-1) ...\nSetting up xcalib (0.8.dfsg1-3) ...\nSetting up tigervnc-viewer (1.13.1+dfsg-2build2) ...\nupdate-alternatives: using /usr/bin/xtigervncviewer to provide /usr/bin/vncviewer (vncviewer) in auto mode\nSetting up lm-sensors (1:3.6.0-9build1) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/lm-sensors.service \u2192 /usr/lib/systemd/system/lm-sensors.service.\nSetting up gir1.2-soup-3.0:amd64 (3.4.4-5ubuntu0.1) ...\nSetting up libutempter0:amd64 (1.2.1-3build1) ...\nSetting up sox (14.4.2+git20190427-4build4) ...\nSetting up mpv (0.37.0-1ubuntu4) ...\nSetting up xdg-dbus-proxy (0.1.5-1build2) ...\nSetting up libcpufreq0 (008-2build2) ...\nSetting up chromium-browser (2:1snap1-0ubuntu2) ...\nSetting up fluid-soundfont-gs (3.1-5.3) ...\nSetting up speedtest-cli (2.1.3-2) ...\nSetting up tor (0.4.8.10-1build2) ...\nSomething or somebody made /var/lib/tor disappear.\nCreating one for you again.\nSomething or somebody made /var/log/tor disappear.\nCreating one for you again.\nCreated symlink /etc/systemd/system/multi-user.target.wants/tor.service \u2192 /usr/lib/systemd/system/tor.service.\nSetting up w3m-img (0.5.3+git20230121-2ubuntu5) ...\nSetting up liblua5.3-0:amd64 (5.3.6-2build2) ...\nSetting up etherwake (1.09-4build1) ...\nSetting up python3-exceptiongroup (1.2.0-1) ...\nSetting up iotop-c (1.26-1) ...\nupdate-alternatives: using /usr/sbin/iotop-c to provide /usr/sbin/iotop (iotop) in auto mode\nSetting up exfat-fuse (1.4.0-2) ...\nSetting up libwine:amd64 (9.0~repack-4build3) ...\nSetting up vim-runtime (2:9.1.0016-1ubuntu7.5) ...\nSetting up xsane-common (0.999-12ubuntu4) ...\nSetting up redshift-qt (0.6-3build3) ...\nSetting up evtest (1:1.35-1) ...\nSetting up python3-magic (2:0.4.27-3) ...\nSetting up libgamemodeauto0:amd64 (1.8.1-2build1) ...\nSetting up torsocks (2.4.0-1) ...\nSetting up python-is-python3 (3.11.4-1) ...\nSetting up scrot (1.10-1build2) ...\nSetting up vnstat (2.12-1) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/vnstat.service \u2192 /usr/lib/systemd/system/vnstat.service.\nSetting up libgamemode0:amd64 (1.8.1-2build1) ...\nSetting up python3-unidecode (1.3.8-1) ...\nSetting up liblastfm5-1:amd64 (1.1.0-5build3) ...\nSetting up scummvm (2.8.0+dfsg-1build5) ...\nSetting up libosmesa6:amd64 (24.0.9-0ubuntu0.3) ...\nSetting up evemu-tools (2.7.0-4build1) ...\nSetting up libwebkit2gtk-4.1-0:amd64 (2.46.4-0ubuntu0.24.04.1) ...\nSetting up geeqie (1:2.2-2build4) ...\nSetting up xsane (0.999-12ubuntu4) ...\nSetting up vim (2:9.1.0016-1ubuntu7.5) ...\nupdate-alternatives: using /usr/bin/vim.basic to provide /usr/bin/ex (ex) in auto mode\nupdate-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rview (rview) in auto mode\nupdate-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rvim (rvim) in auto mode\nupdate-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vi (vi) in auto mode\nupdate-alternatives: using /usr/bin/vim.basic to provide /usr/bin/view (view) in auto mode\nupdate-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vim (vim) in auto mode\nupdate-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vimdiff (vimdiff) in auto mode\nSetting up libpython3-dev:amd64 (3.12.3-0ubuntu2) ...\nSetting up python3-wsproto (1.2.0-1) ...\nSetting up cabextract (1.11-2) ...\nSetting up xdotool (1:3.20160805.1-5build1) ...\nSetting up libgpod-common (0.8.3-19.1ubuntu4) ...\nSetting up chafa (1.14.0-1.1build1) ...\nSetting up screen (4.9.1-1build1) ...\nSetting up wine64 (9.0~repack-4build3) ...\nSetting up gir1.2-javascriptcoregtk-4.1:amd64 (2.46.4-0ubuntu0.24.04.1) ...\nSetting up gkrellm (2.3.11-2build2) ...\nSetting up tor-geoipdb (0.4.8.10-1build2) ...\nSetting up chromium-chromedriver (2:1snap1-0ubuntu2) ...\nSetting up python3-trio (0.24.0-1ubuntu1) ...\nSetting up python3-dev (3.12.3-0ubuntu2) ...\nSetting up gkrellm-cpufreq (0.6.4-4) ...\nSetting up clementine (1.4.0~rc1+git867-g9ef681b0e+dfsg-1ubuntu4) ...\nSetting up ttf-mscorefonts-installer (3.8.1ubuntu1) ...\nSetting up gamemode (1.8.1-2build1) ...\nSetting up wine (9.0~repack-4build3) ...\nSetting up gir1.2-webkit2-4.1:amd64 (2.46.4-0ubuntu0.24.04.1) ...\nSetting up playonlinux (4.3.4-3) ...\n/usr/share/playonlinux/python/configurewindow/ConfigureWindowNotebook.py:463: SyntaxWarning: invalid escape sequence '\\|'\nself.supported_files = \"All|*.exe;*.EXE;*.msi;*.MSI\\\n/usr/share/playonlinux/python/mainwindow.py:710: SyntaxWarning: invalid escape sequence '\\|'\nself.SupprotedIconExt = \"All|*.xpm;*.XPM;*.png;*.PNG;*.ico;*.ICO;*.jpg;*.JPG;*.jpeg;*.JPEG;*.bmp;*.BMP\\\nSetting up python3-trio-websocket (0.11.1-1) ...\nSetting up gkrellm-xkb (1.05-5.1build2) ...\nSetting up gkrellm-leds (0.8.0-2build2) ...\nSetting up winetricks (20240105-2) ...\nSetting up lutris (0.5.14-2) ...\nSetting up python3-selenium (4.18.1+dfsg-1) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.3) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.10+dfsg-3ubuntu3.1) ...\nProcessing triggers for debianutils (5.17build1) ...\nProcessing triggers for install-info (7.1-3build2) ...\nProcessing triggers for fontconfig (2.15.0-1.1ubuntu2) ...\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for update-notifier-common (3.192.68build3) ...\nttf-mscorefonts-installer: processing...\nttf-mscorefonts-installer: downloading http://downloads.sourceforge.net/corefonts/andale32.exe\nGet:1 http://downloads.sourceforge.net/corefonts/andale32.exe [198 kB]\nFetched 198 kB in 1s (200 kB/s)                                                              \nttf-mscorefonts-installer: downloading http://downloads.sourceforge.net/corefonts/arial32.exe\nGet:1 http://downloads.sourceforge.net/corefonts/arial32.exe [554 kB]\nFetched 554 kB in 1s (431 kB/s)                                                             \nttf-mscorefonts-installer: downloading http://downloads.sourceforge.net/corefonts/arialb32.exe\nGet:1 http://downloads.sourceforge.net/corefonts/arialb32.exe [168 kB]\nFetched 168 kB in 4s (43.0 kB/s)                                                              \nttf-mscorefonts-installer: downloading http://downloads.sourceforge.net/corefonts/comic32.exe\nGet:1 http://downloads.sourceforge.net/corefonts/comic32.exe [246 kB]\nFetched 246 kB in 1s (271 kB/s)                                                             \nttf-mscorefonts-installer: downloading http://downloads.sourceforge.net/corefonts/courie32.exe\nGet:1 http://downloads.sourceforge.net/corefonts/courie32.exe [646 kB]\nFetched 646 kB in 6s (103 kB/s)                                                                                                                                                                                                                                                                                            \nttf-mscorefonts-installer: downloading http://downloads.sourceforge.net/corefonts/georgi32.exe\nGet:1 http://downloads.sourceforge.net/corefonts/georgi32.exe [392 kB]\nFetched 392 kB in 1s (347 kB/s)                                                              \nttf-mscorefonts-installer: downloading http://downloads.sourceforge.net/corefonts/impact32.exe\nGet:1 http://downloads.sourceforge.net/corefonts/impact32.exe [173 kB]\nFetched 173 kB in 1s (202 kB/s)                                                              \nttf-mscorefonts-installer: downloading http://downloads.sourceforge.net/corefonts/times32.exe\nGet:1 http://downloads.sourceforge.net/corefonts/times32.exe [662 kB]\nFetched 662 kB in 1s (623 kB/s)                                                             \nttf-mscorefonts-installer: downloading http://downloads.sourceforge.net/corefonts/trebuc32.exe\nGet:1 http://downloads.sourceforge.net/corefonts/trebuc32.exe [357 kB]\nFetched 357 kB in 1s (382 kB/s)                                                              \nttf-mscorefonts-installer: downloading http://downloads.sourceforge.net/corefonts/verdan32.exe\nGet:1 http://downloads.sourceforge.net/corefonts/verdan32.exe [352 kB]\nFetched 352 kB in 1s (345 kB/s)                                                              \nttf-mscorefonts-installer: downloading http://downloads.sourceforge.net/corefonts/webdin32.exe\nGet:1 http://downloads.sourceforge.net/corefonts/webdin32.exe [185 kB]\nFetched 185 kB in 1s (187 kB/s)                                                              \n\nThese fonts were provided by Microsoft \"in the interest of cross-\nplatform compatibility\".  This is no longer the case, but they are\nstill available from third parties.\n\nYou are free to download these fonts and use them for your own use,\nbut you may not redistribute them in modified form, including changes\nto the file name or packaging format.\n\nExtracting cabinet: /var/lib/update-notifier/package-data-downloads/partial/andale32.exe\nextracting fontinst.inf\nextracting andale.inf\nextracting fontinst.exe\nextracting AndaleMo.TTF\nextracting ADVPACK.DLL\nextracting W95INF32.DLL\nextracting W95INF16.DLL\n\nAll done, no errors.\nExtracting cabinet: /var/lib/update-notifier/package-data-downloads/partial/arial32.exe\nextracting FONTINST.EXE\nextracting fontinst.inf\nextracting Ariali.TTF\nextracting Arialbd.TTF\nextracting Arialbi.TTF\nextracting Arial.TTF\n\nAll done, no errors.\nExtracting cabinet: /var/lib/update-notifier/package-data-downloads/partial/arialb32.exe\nextracting fontinst.exe\nextracting fontinst.inf\nextracting AriBlk.TTF\n\nAll done, no errors.\nExtracting cabinet: /var/lib/update-notifier/package-data-downloads/partial/comic32.exe\nextracting fontinst.inf\nextracting Comicbd.TTF\nextracting Comic.TTF\nextracting fontinst.exe\n\nAll done, no errors.\nExtracting cabinet: /var/lib/update-notifier/package-data-downloads/partial/courie32.exe\nextracting cour.ttf\nextracting courbd.ttf\nextracting courbi.ttf\nextracting fontinst.inf\nextracting couri.ttf\nextracting fontinst.exe\n\nAll done, no errors.\nExtracting cabinet: /var/lib/update-notifier/package-data-downloads/partial/georgi32.exe\nextracting fontinst.inf\nextracting Georgiaz.TTF\nextracting Georgiab.TTF\nextracting Georgiai.TTF\nextracting Georgia.TTF\nextracting fontinst.exe\n\nAll done, no errors.\nExtracting cabinet: /var/lib/update-notifier/package-data-downloads/partial/impact32.exe\nextracting fontinst.exe\nextracting Impact.TTF\nextracting fontinst.inf\n\nAll done, no errors.\nExtracting cabinet: /var/lib/update-notifier/package-data-downloads/partial/times32.exe\nextracting fontinst.inf\nextracting Times.TTF\nextracting Timesbd.TTF\nextracting Timesbi.TTF\nextracting Timesi.TTF\nextracting FONTINST.EXE\n\nAll done, no errors.\nExtracting cabinet: /var/lib/update-notifier/package-data-downloads/partial/trebuc32.exe\nextracting FONTINST.EXE\nextracting trebuc.ttf\nextracting Trebucbd.ttf\nextracting trebucbi.ttf\nextracting trebucit.ttf\nextracting fontinst.inf\n\nAll done, no errors.\nExtracting cabinet: /var/lib/update-notifier/package-data-downloads/partial/verdan32.exe\nextracting fontinst.exe\nextracting fontinst.inf\nextracting Verdanab.TTF\nextracting Verdanai.TTF\nextracting Verdanaz.TTF\nextracting Verdana.TTF\n\nAll done, no errors.\nExtracting cabinet: /var/lib/update-notifier/package-data-downloads/partial/webdin32.exe\nextracting fontinst.exe\nextracting Webdings.TTF\nextracting fontinst.inf\nextracting Licen.TXT\n\nAll done, no errors.\nAll fonts downloaded and installed.\nProcessing triggers for wine (9.0~repack-4build3) ...\n</code></pre> <p>After installing these, Redshift is immediately available.</p> <p>Even before installing any packages KDE Connect is already running, which comes in very handy if it was already setup.</p> <p>At this point a reboot would be recommended, but we can do one  more thing before that...</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#restore-software-raid-5","title":"Restore Software RAID 5","text":"<p>This PC has a software RAID 5 on 3 old 1TB HDD (not SSD) disks that were already set up as a RAID 5 array in the old system.</p> <p>To restore this RAID, start by preserving the information reported in the RAID superblocks on each device:</p> <code># apt install mdadm -y</code> <pre><code># apt install mdadm -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\nfinalrd\nSuggested packages:\ndefault-mta | mail-transport-agent\nThe following NEW packages will be installed:\nfinalrd mdadm\n0 upgraded, 2 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 471 kB of archives.\nAfter this operation, 1,188 kB of additional disk space will be used.\nGet:1 http://es.archive.ubuntu.com/ubuntu noble/main amd64 finalrd all 9build1 [7,306 B]\nGet:2 http://es.archive.ubuntu.com/ubuntu noble-updates/main amd64 mdadm amd64 4.3-1ubuntu2.1 [464 kB]\nFetched 471 kB in 1s (495 kB/s)\nPreconfiguring packages ...\nSelecting previously unselected package finalrd.\n(Reading database ... 425709 files and directories currently installed.)\nPreparing to unpack .../finalrd_9build1_all.deb ...\nUnpacking finalrd (9build1) ...\nSelecting previously unselected package mdadm.\nPreparing to unpack .../mdadm_4.3-1ubuntu2.1_amd64.deb ...\nUnpacking mdadm (4.3-1ubuntu2.1) ...\nSetting up finalrd (9build1) ...\nCreated symlink /etc/systemd/system/sysinit.target.wants/finalrd.service \u2192 /usr/lib/systemd/system/finalrd.service.\nSetting up mdadm (4.3-1ubuntu2.1) ...\nGenerating mdadm.conf... done.\nupdate-initramfs: deferring update (trigger activated)\nSourcing file `/etc/default/grub'\nSourcing file `/etc/default/grub.d/ubuntustudio.cfg'\nGenerating grub configuration file ...\nFound linux image: /boot/vmlinuz-6.8.0-50-lowlatency\nFound initrd image: /boot/initrd.img-6.8.0-50-lowlatency\nFound memtest86+ 64bit EFI image: /boot/memtest86+x64.efi\nWarning: os-prober will be executed to detect other bootable partitions.\nIts output will be used to detect bootable binaries on them and create new boot entries.\nFound Ubuntu 22.04.5 LTS (22.04) on /dev/nvme0n1p2\nAdding boot menu entry for UEFI Firmware Settings ...\ndone\nCreated symlink /etc/systemd/system/mdmonitor.service.wants/mdcheck_continue.timer \u2192 /usr/lib/systemd/system/mdcheck_continue.timer.\nCreated symlink /etc/systemd/system/mdmonitor.service.wants/mdcheck_start.timer \u2192 /usr/lib/systemd/system/mdcheck_start.timer.\nCreated symlink /etc/systemd/system/mdmonitor.service.wants/mdmonitor-oneshot.timer \u2192 /usr/lib/systemd/system/mdmonitor-oneshot.timer.\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for initramfs-tools (0.142ubuntu25.4) ...\nupdate-initramfs: Generating /boot/initrd.img-6.8.0-50-lowlatency\n</code></pre> <p>Note</p> <p>Installing <code>mdadm</code> already updates Grub so there is no need to manually run <code>update-grub2</code> afterwards.</p> <code># mdadm --examine /dev/sd[a-c] | tee raid.status</code> <pre><code># mdadm --examine /dev/sd[a-c] | tee raid.status\n/dev/sda:\n        Magic : a92b4efc\n        Version : 1.2\n    Feature Map : 0x1\n    Array UUID : ba946432:20362047:d8bfa70b:615817a1\n        Name : rapture:0\nCreation Time : Fri Jul 26 16:06:17 2019\n    Raid Level : raid5\nRaid Devices : 3\n\nAvail Dev Size : 1953260976 sectors (931.39 GiB 1000.07 GB)\n    Array Size : 1953260544 KiB (1862.77 GiB 2000.14 GB)\nUsed Dev Size : 1953260544 sectors (931.39 GiB 1000.07 GB)\n    Data Offset : 264192 sectors\nSuper Offset : 8 sectors\nUnused Space : before=264112 sectors, after=432 sectors\n        State : clean\n    Device UUID : 0a40bde1:a7089d96:744e726d:5f700b85\n\nInternal Bitmap : 8 sectors from superblock\n    Update Time : Fri Dec 27 09:46:03 2024\nBad Block Log : 512 entries available at offset 16 sectors\n    Checksum : 96af7161 - correct\n        Events : 3986\n\n        Layout : left-symmetric\n    Chunk Size : 512K\n\nDevice Role : Active device 0\nArray State : AAA ('A' == active, '.' == missing, 'R' == replacing)\n/dev/sdb:\n        Magic : a92b4efc\n        Version : 1.2\n    Feature Map : 0x1\n    Array UUID : ba946432:20362047:d8bfa70b:615817a1\n        Name : rapture:0\nCreation Time : Fri Jul 26 16:06:17 2019\n    Raid Level : raid5\nRaid Devices : 3\n\nAvail Dev Size : 1953260976 sectors (931.39 GiB 1000.07 GB)\n    Array Size : 1953260544 KiB (1862.77 GiB 2000.14 GB)\nUsed Dev Size : 1953260544 sectors (931.39 GiB 1000.07 GB)\n    Data Offset : 264192 sectors\nSuper Offset : 8 sectors\nUnused Space : before=264112 sectors, after=432 sectors\n        State : clean\n    Device UUID : 64dff6fe:57336347:4853f110:1aa160c1\n\nInternal Bitmap : 8 sectors from superblock\n    Update Time : Fri Dec 27 09:46:03 2024\nBad Block Log : 512 entries available at offset 16 sectors\n    Checksum : 4483655b - correct\n        Events : 3986\n\n        Layout : left-symmetric\n    Chunk Size : 512K\n\nDevice Role : Active device 1\nArray State : AAA ('A' == active, '.' == missing, 'R' == replacing)\n/dev/sdc:\n        Magic : a92b4efc\n        Version : 1.2\n    Feature Map : 0x1\n    Array UUID : ba946432:20362047:d8bfa70b:615817a1\n        Name : rapture:0\nCreation Time : Fri Jul 26 16:06:17 2019\n    Raid Level : raid5\nRaid Devices : 3\n\nAvail Dev Size : 1953260976 sectors (931.39 GiB 1000.07 GB)\n    Array Size : 1953260544 KiB (1862.77 GiB 2000.14 GB)\nUsed Dev Size : 1953260544 sectors (931.39 GiB 1000.07 GB)\n    Data Offset : 264192 sectors\nSuper Offset : 8 sectors\nUnused Space : before=264112 sectors, after=432 sectors\n        State : clean\n    Device UUID : bcf0c123:37b5deb8:74af5640:37aa2be4\n\nInternal Bitmap : 8 sectors from superblock\n    Update Time : Fri Dec 27 09:46:03 2024\nBad Block Log : 512 entries available at offset 16 sectors\n    Checksum : 2cfa5dde - correct\n        Events : 3986\n\n        Layout : left-symmetric\n    Chunk Size : 512K\n\nDevice Role : Active device 2\nArray State : AAA ('A' == active, '.' == missing, 'R' == replacing)\n</code></pre> <p>All disks show as <code>clean</code>, <code>active</code> and with the same <code>Events</code> count; that\u2019s good! At this point we want to Assemble your array, not create it, and with the same <code>Events</code> count it shouldn\u2019t even be necessary to <code>--force</code> it:</p> <pre><code># mdadm /dev/md0 --assemble /dev/sda /dev/sdb /dev/sdc\nmdadm: /dev/md0 has been started with 3 drives.\n</code></pre> <p>Success! The RAID is now started, as seen in <code>dmesg</code>:</p> <pre><code>[ 9649.589111] md: md0 stopped.\n[ 9649.593929]  sda:\n[ 9649.596065]  sdb:\n[ 9649.598005]  sdc:\n[ 9649.649447] async_tx: api initialized (async)\n[ 9649.661057] md/raid:md0: device sda operational as raid disk 0\n[ 9649.661065] md/raid:md0: device sdc operational as raid disk 2\n[ 9649.661068] md/raid:md0: device sdb operational as raid disk 1\n[ 9649.661956] md/raid:md0: raid level 5 active with 3 out of 3 devices, algorithm 2\n[ 9649.691338] md0: detected capacity change from 0 to 3906521088\n</code></pre> <p>Although not shown in <code>dmesg</code>, the BTRFS in it is also found:</p> <pre><code># btrfs filesystem show\nLabel: none  uuid: c8d05650-0cbf-48c3-8616-7e942534ab4a\n        Total devices 1 FS bytes used 1.12TiB\n        devid    1 size 1.68TiB used 1.24TiB path /dev/nvme0n1p4\n\nLabel: none  uuid: abd8f19b-aff8-4c27-bbe9-d5b7f18bc18e\n        Total devices 1 FS bytes used 1.51TiB\n        devid    1 size 1.82TiB used 1.52TiB path /dev/md0\n</code></pre> <p>Now we can mount the array in <code>/home/raid</code> as it used to be:</p> <pre><code># vi /etc/fstab\n/dev/disk/by-uuid/c8d05650-0cbf-48c3-8616-7e942534ab4a /home btrfs defaults 0 1\n/dev/disk/by-uuid/abd8f19b-aff8-4c27-bbe9-d5b7f18bc18e /home/raid btrfs defaults 0 1\n\n# systemctl daemon-reload\n# mount /home/raid\n# df -h \nFilesystem      Size  Used Avail Use% Mounted on\n...\n/dev/md0        1.9T  1.6T  309G  84% /home/raid\n</code></pre> <p>Finally, comment out the line for the swap file (<code>/swap.img</code>), because that won't be necessary in a system with 32GB of RAM.</p> <p>At this point a reboot would be in order, if only to make sure all the above is still working well after a reboot.</p> <p>Note</p> <p>In fact quite a few more things can be done before this second reboot, and indeed everything that follows down to Continuous Monitoring had been done before restoring the RAID.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#install-additional-software","title":"Install Additional Software","text":""},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#brave-browser","title":"Brave browser","text":"<p>Install from the Release Channel:</p> <pre><code># curl -fsSLo \\\n  /usr/share/keyrings/brave-browser-archive-keyring.gpg \\\n  https://brave-browser-apt-release.s3.brave.com/brave-browser-archive-keyring.gpg\n\n# echo \"deb [signed-by=/usr/share/keyrings/brave-browser-archive-keyring.gpg] https://brave-browser-apt-release.s3.brave.com/ stable main\" \\\n| tee /etc/apt/sources.list.d/brave-browser-release.list\n\n# apt update &amp;&amp; apt install brave-browser -y\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#google-chrome","title":"Google Chrome","text":"<p>Installing Google Chrome is as simple as downloading the Debian package and installing it:</p> <pre><code># dpkg -i google-chrome-stable_current_amd64.deb\n</code></pre> <p>This could be done later, but it is convenient for me to easily access services, documents and my self-hosted Visual Studio Code Server to write this blog.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#continuous-monitoring","title":"Continuous Monitoring","text":"<p>Warning</p> <p>This depends on <code>curl</code> being installed as part of the Essential Packages above.</p> <p>Install the multi-thread version of the <code>conmon</code> script as <code>/usr/local/bin/conmon</code> and run it as a service; create <code>/etc/systemd/system/conmon.service</code> as follows:</p> /etc/systemd/system/conmon.service<pre><code>[Unit]\nDescription=Continuous Monitoring\n\n[Service]\nExecStart=/usr/local/bin/conmon\nRestart=on-failure\nStandardOutput=null\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Before starting the service, copy over the configuration files under <code>/jammy/etc/conmon</code> and update the <code>TARGET_HTTP</code> and <code>TARGET_HTTPS</code> in <code>/usr/local/bin/conmon</code> so that metrics are sent to the remote server via HTTPS. Then enable and start the service:</p> <pre><code># vi /usr/local/bin/conmon\nTARGET_HTTPS='https://inf.ssl.uu.am:443'\n\n# cp -a /jammy/etc/conmon /etc/\n# systemctl enable conmon.service\n# systemctl daemon-reload\n# systemctl start conmon.service\n# systemctl status conmon.service\n\u25cf conmon.service - Continuous Monitoring\n     Loaded: loaded (/etc/systemd/system/conmon.service; enabled; preset: enabled)\n     Active: active (running) since Fri 2024-12-27 12:18:57 CET; 3s ago\n   Main PID: 1930626 (conmon)\n      Tasks: 18 (limit: 38354)\n     Memory: 6.0M (peak: 25.3M)\n        CPU: 1.255s\n     CGroup: /system.slice/conmon.service\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#hardware-sensors","title":"Hardware Sensors","text":"<p>Only a few hardware sensors are supported out of the box, sensors work for NVME, WiFi and CPU temperatures:</p> <pre><code># sensors -A\niwlwifi_1-virtual-0\ntemp1:        +50.0\u00b0C  \n\nnvme-pci-0100\nComposite:    +50.9\u00b0C  (low  = -273.1\u00b0C, high = +84.8\u00b0C)\n                       (crit = +84.8\u00b0C)\nSensor 1:     +50.9\u00b0C  (low  = -273.1\u00b0C, high = +65261.8\u00b0C)\nSensor 2:     +53.9\u00b0C  (low  = -273.1\u00b0C, high = +65261.8\u00b0C)\n\nk10temp-pci-00c3\nTctl:         +37.9\u00b0C  \n</code></pre> <p>HDD temperatures are available after loading the <code>drivetemp</code> kernel module:</p> <pre><code># echo drivetemp &gt; /etc/modules-load.d/drivetemp.conf\n# modprobe drivetemp\n# sensors -A\ndrivetemp-scsi-5-0\ntemp1:        +27.0\u00b0C  (low  =  -5.0\u00b0C, high = +80.0\u00b0C)\n                       (crit low = -10.0\u00b0C, crit = +85.0\u00b0C)\n                       (lowest = +16.0\u00b0C, highest = +27.0\u00b0C)\n\ndrivetemp-scsi-0-0\ntemp1:        +25.0\u00b0C  (low  =  -5.0\u00b0C, high = +80.0\u00b0C)\n                       (crit low = -10.0\u00b0C, crit = +85.0\u00b0C)\n                       (lowest = +16.0\u00b0C, highest = +25.0\u00b0C)\n\ndrivetemp-scsi-4-0\ntemp1:        +29.0\u00b0C  (low  =  -5.0\u00b0C, high = +80.0\u00b0C)\n                       (crit low = -10.0\u00b0C, crit = +85.0\u00b0C)\n                       (lowest = +16.0\u00b0C, highest = +29.0\u00b0C)\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#motherboard-sensors-nct6795d","title":"Motherboard Sensors (NCT6795D)","text":"<p>The system board MSI B450I Gaming Plus AC initially shows only the South bridge (<code>k10temp</code>) sensors, but the <code>sensors-detect</code> tool is able to detect that additional sensors are available in a <code>NCT6795D</code> compatible chip.</p> <p>Warning</p> <p>Do NOT attempt scanning the NVidia i2c sensors, they cause errors (pending further investigation).</p> <code># sensors-detect</code> <pre><code>root@raven:~# sensors-detect\n# sensors-detect version 3.6.0\n# System: Micro-Star International Co., Ltd. MS-7A40 [2.0]\n# Board: Micro-Star International Co., Ltd. B450I GAMING PLUS AC (MS-7A40)\n# Kernel: 6.8.0-50-lowlatency x86_64\n# Processor: AMD Ryzen 5 2600X Six-Core Processor (23/8/2)\n\nThis program will help you determine which kernel modules you need\nto load to use lm_sensors most effectively. It is generally safe\nand recommended to accept the default answers to all questions,\nunless you know what you're doing.\n\nSome south bridges, CPUs or memory controllers contain embedded sensors.\nDo you want to scan for them? This is totally safe. (YES/no): \nModule cpuid loaded successfully.\nSilicon Integrated Systems SIS5595...                       No\nVIA VT82C686 Integrated Sensors...                          No\nVIA VT8231 Integrated Sensors...                            No\nAMD K8 thermal sensors...                                   No\nAMD Family 10h thermal sensors...                           No\nAMD Family 11h thermal sensors...                           No\nAMD Family 12h and 14h thermal sensors...                   No\nAMD Family 15h thermal sensors...                           No\nAMD Family 16h thermal sensors...                           No\nAMD Family 17h thermal sensors...                           Success!\n    (driver `k10temp')\nAMD Family 15h power sensors...                             No\nAMD Family 16h power sensors...                             No\nHygon Family 18h thermal sensors...                         No\nIntel digital thermal sensor...                             No\nIntel AMB FB-DIMM thermal sensor...                         No\nIntel 5500/5520/X58 thermal sensor...                       No\nVIA C7 thermal sensor...                                    No\nVIA Nano thermal sensor...                                  No\n\nSome Super I/O chips contain embedded sensors. We have to write to\nstandard I/O ports to probe them. This is usually safe.\nDo you want to scan for Super I/O sensors? (YES/no): \nProbing for Super-I/O at 0x2e/0x2f\nTrying family `National Semiconductor/ITE'...               No\nTrying family `SMSC'...                                     No\nTrying family `VIA/Winbond/Nuvoton/Fintek'...               No\nTrying family `ITE'...                                      No\nProbing for Super-I/O at 0x4e/0x4f\nTrying family `National Semiconductor/ITE'...               No\nTrying family `SMSC'...                                     No\nTrying family `VIA/Winbond/Nuvoton/Fintek'...               Yes\nFound `Nuvoton NCT6795D Super IO Sensors'                   Success!\n    (address 0xa20, driver `nct6775')\n\nSome systems (mainly servers) implement IPMI, a set of common interfaces\nthrough which system health data may be retrieved, amongst other things.\nWe first try to get the information from SMBIOS. If we don't find it\nthere, we have to read from arbitrary I/O ports to probe for such\ninterfaces. This is normally safe. Do you want to scan for IPMI\ninterfaces? (YES/no): \nProbing for `IPMI BMC KCS' at 0xca0...                      No\nProbing for `IPMI BMC SMIC' at 0xca8...                     No\n\nSome hardware monitoring chips are accessible through the ISA I/O ports.\nWe have to write to arbitrary I/O ports to probe them. This is usually\nsafe though. Yes, you do have ISA I/O ports even if you do not have any\nISA slots! Do you want to scan the ISA I/O ports? (yes/NO): \n\nLastly, we can probe the I2C/SMBus adapters for connected hardware\nmonitoring devices. This is the most risky part, and while it works\nreasonably well on most systems, it has been reported to cause trouble\non some systems.\nDo you want to probe the I2C/SMBus adapters now? (YES/no): \nUsing driver `i2c-piix4' for device 0000:00:14.0: AMD KERNCZ SMBus\n\nNext adapter: SMBus PIIX4 adapter port 0 at 0b00 (i2c-0)\nDo you want to scan it? (yes/NO/selectively): \n\nNext adapter: SMBus PIIX4 adapter port 2 at 0b00 (i2c-1)\nDo you want to scan it? (yes/NO/selectively): \n\nNext adapter: SMBus PIIX4 adapter port 1 at 0b20 (i2c-2)\nDo you want to scan it? (yes/NO/selectively): \n\nNext adapter: NVIDIA GPU I2C adapter (i2c-3)\nDo you want to scan it? (YES/no/selectively): no\n\nNext adapter: NVIDIA i2c adapter 1 at 29:00.0 (i2c-4)\nDo you want to scan it? (yes/NO/selectively): no\n\nNext adapter: NVIDIA i2c adapter 3 at 29:00.0 (i2c-5)\nDo you want to scan it? (yes/NO/selectively): no\n\nNext adapter: NVIDIA i2c adapter 4 at 29:00.0 (i2c-6)\nDo you want to scan it? (yes/NO/selectively): no\n\nNext adapter: NVIDIA i2c adapter 5 at 29:00.0 (i2c-7)\nDo you want to scan it? (yes/NO/selectively): no\n\nNext adapter: NVIDIA i2c adapter 6 at 29:00.0 (i2c-8)\nDo you want to scan it? (yes/NO/selectively): no\n\n\nNow follows a summary of the probes I have just done.\nJust press ENTER to continue: \n\nDriver `nct6775':\n* ISA bus, address 0xa20\n    Chip `Nuvoton NCT6795D Super IO Sensors' (confidence: 9)\n\nDriver `k10temp' (autoloaded):\n* Chip `AMD Family 17h thermal sensors' (confidence: 9)\n\nTo load everything that is needed, add this to /etc/modules:\n#----cut here----\n# Chip drivers\nnct6775\n#----cut here----\nIf you have some drivers built into your kernel, the list above will\ncontain too many modules. Skip the appropriate ones!\n\nDo you want to add these lines automatically to /etc/modules? (yes/NO)\n\nUnloading cpuid... OK\n</code></pre> <p>So the only other module that needs to be loaded is <code>nct6775</code>, and with that all sensors are available (including fan speeds):</p> <pre><code># echo nct6775 &gt; /etc/modules-load.d/nct6775.conf\n# modprobe nct6775\n</code></pre> <code># sensors -A</code> <pre><code># sensors -A\ndrivetemp-scsi-5-0\ntemp1:        +27.0\u00b0C  (low  =  -5.0\u00b0C, high = +80.0\u00b0C)\n                    (crit low = -10.0\u00b0C, crit = +85.0\u00b0C)\n                    (lowest = +16.0\u00b0C, highest = +27.0\u00b0C)\n\ndrivetemp-scsi-0-0\ntemp1:        +24.0\u00b0C  (low  =  -5.0\u00b0C, high = +80.0\u00b0C)\n                    (crit low = -10.0\u00b0C, crit = +85.0\u00b0C)\n                    (lowest = +16.0\u00b0C, highest = +25.0\u00b0C)\n\niwlwifi_1-virtual-0\ntemp1:        +49.0\u00b0C  \n\nnvme-pci-0100\nComposite:    +48.9\u00b0C  (low  = -273.1\u00b0C, high = +84.8\u00b0C)\n                    (crit = +84.8\u00b0C)\nSensor 1:     +48.9\u00b0C  (low  = -273.1\u00b0C, high = +65261.8\u00b0C)\nSensor 2:     +52.9\u00b0C  (low  = -273.1\u00b0C, high = +65261.8\u00b0C)\n\nnct6795-isa-0a20\nVcore:                   1.07 V  (min =  +0.00 V, max =  +1.74 V)\nin1:                     1.01 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nAVCC:                    3.39 V  (min =  +2.98 V, max =  +3.63 V)\n+3.3V:                   3.31 V  (min =  +2.98 V, max =  +3.63 V)\nin4:                     1.01 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nin5:                   152.00 mV (min =  +0.00 V, max =  +0.00 V)  ALARM\nin6:                   728.00 mV (min =  +0.00 V, max =  +0.00 V)  ALARM\n3VSB:                    3.38 V  (min =  +2.98 V, max =  +3.63 V)\nVbat:                    3.28 V  (min =  +2.70 V, max =  +3.63 V)\nin9:                     1.84 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nin10:                    0.00 V  (min =  +0.00 V, max =  +0.00 V)\nin11:                  632.00 mV (min =  +0.00 V, max =  +0.00 V)  ALARM\nin12:                  824.00 mV (min =  +0.00 V, max =  +0.00 V)  ALARM\nin13:                  592.00 mV (min =  +0.00 V, max =  +0.00 V)  ALARM\nin14:                    1.53 V  (min =  +0.00 V, max =  +0.00 V)  ALARM\nfan1:                     0 RPM  (min =    0 RPM)\nfan2:                  1143 RPM  (min =    0 RPM)\nfan3:                   925 RPM  (min =    0 RPM)\nfan4:                     0 RPM  (min =    0 RPM)\nfan5:                     0 RPM  (min =    0 RPM)\nSYSTIN:                 +41.0\u00b0C  (high =  +0.0\u00b0C, hyst =  +0.0\u00b0C)  ALARM  sensor = CPU diode\nCPUTIN:                 +36.0\u00b0C  (high = +115.0\u00b0C, hyst = +90.0\u00b0C)  sensor = thermistor\nAUXTIN0:                +41.0\u00b0C  (high = +115.0\u00b0C, hyst = +90.0\u00b0C)  sensor = thermistor\nAUXTIN1:               -128.0\u00b0C    sensor = thermistor\nAUXTIN2:                +47.0\u00b0C    sensor = thermistor\nAUXTIN3:                 -2.0\u00b0C    sensor = thermistor\nSMBUSMASTER 0:          +34.0\u00b0C  \nPCH_CHIP_CPU_MAX_TEMP:   +0.0\u00b0C  \nPCH_CHIP_TEMP:           +0.0\u00b0C  \nPCH_CPU_TEMP:            +0.0\u00b0C  \nPCH_MCH_TEMP:            +0.0\u00b0C  \nAgent0 Dimm0:            +0.0\u00b0C  \nTSI0_TEMP:              +34.4\u00b0C  \nintrusion0:            ALARM\nintrusion1:            ALARM\nbeep_enable:           disabled\n\ndrivetemp-scsi-4-0\ntemp1:        +28.0\u00b0C  (low  =  -5.0\u00b0C, high = +80.0\u00b0C)\n                    (crit low = -10.0\u00b0C, crit = +85.0\u00b0C)\n                    (lowest = +16.0\u00b0C, highest = +29.0\u00b0C)\n\nk10temp-pci-00c3\nTctl:         +34.4\u00b0C\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#speedtest-and-tapo-devices","title":"Speedtest and Tapo devices","text":"<p>While there is no 24x7 system on-site, this PC also has to run some of the additional scripts to monitor internet speeds with <code>conmon-speedtest</code> and TP-Link Tapo devices wit <code>conmon-tapo.py</code>.</p> <pre><code># cp conmon-speedtest conmon-tapo.py /usr/local/bin/\n# crontab -e\n*/3 * * * * /usr/local/bin/conmon-speedtest\n*   * * * * /usr/local/bin/conmon-tapo.py\n</code></pre> <p>Warning</p> <p>Make sure <code>/etc/conmon/tapo.yaml</code> maps to the correct IP addresses and models for each device, otherwise <code>conmon-tapo.py</code> will crash.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#blender","title":"Blender","text":"<p>Blender 4.3 is already available even for Ubuntu 24.04 via  snapcraft.io/blender so there is no reason to install it any other way:</p> <pre><code># snap install blender --classic\nblender 4.3.2 from Blender Foundation (blenderfoundation\u2713) installed\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#steam","title":"Steam","text":"<p>Installing Steam from Snap  couldn't be simpler:</p> <pre><code># snap install steam\nsteam 1.0.0.81 from Canonical\u2713 installed\n</code></pre> <p>This snapcraft.io/steam package is provided by Canonical.</p> <p>When runing the Steam client for the first time, a pop-up shows up advising to install additional 32-bit drivers for best experience</p> <code># apt install libnvidia-gl-550:i386 -y</code> <pre><code># dpkg --add-architecture i386\n# apt update\n# apt install libnvidia-gl-550:i386 -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\ngcc-14-base:i386 libbsd0:i386 libc6:i386 libdrm2:i386 libffi8:i386 libgcc-s1:i386\nlibidn2-0:i386 libmd0:i386 libnvidia-egl-wayland1:i386 libunistring5:i386\nlibwayland-client0:i386 libwayland-server0:i386 libx11-6:i386 libxau6:i386 libxcb1:i386\nlibxdmcp6:i386 libxext6:i386\nSuggested packages:\nglibc-doc:i386 locales:i386 libnss-nis:i386 libnss-nisplus:i386\nThe following NEW packages will be installed:\ngcc-14-base:i386 libbsd0:i386 libc6:i386 libdrm2:i386 libffi8:i386 libgcc-s1:i386\nlibidn2-0:i386 libmd0:i386 libnvidia-egl-wayland1:i386 libnvidia-gl-550:i386\nlibunistring5:i386 libwayland-client0:i386 libwayland-server0:i386 libx11-6:i386\nlibxau6:i386 libxcb1:i386 libxdmcp6:i386 libxext6:i386\n0 upgraded, 18 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 38.6 MB of archives.\nAfter this operation, 155 MB of additional disk space will be used.\n...\nFetched 38.6 MB in 4s (11.0 MB/s)                  \nPreconfiguring packages ...\nSelecting previously unselected package gcc-14-base:i386.\n(Reading database ... 425851 files and directories currently installed.)\nPreparing to unpack .../00-gcc-14-base_14.2.0-4ubuntu2~24.04_i386.deb ...\nUnpacking gcc-14-base:i386 (14.2.0-4ubuntu2~24.04) ...\nSelecting previously unselected package libgcc-s1:i386.\nPreparing to unpack .../01-libgcc-s1_14.2.0-4ubuntu2~24.04_i386.deb ...\nUnpacking libgcc-s1:i386 (14.2.0-4ubuntu2~24.04) ...\nSelecting previously unselected package libc6:i386.\nPreparing to unpack .../02-libc6_2.39-0ubuntu8.3_i386.deb ...\nUnpacking libc6:i386 (2.39-0ubuntu8.3) ...\nSelecting previously unselected package libmd0:i386.\nPreparing to unpack .../03-libmd0_1.1.0-2build1_i386.deb ...\nUnpacking libmd0:i386 (1.1.0-2build1) ...\nSelecting previously unselected package libbsd0:i386.\nPreparing to unpack .../04-libbsd0_0.12.1-1build1_i386.deb ...\nUnpacking libbsd0:i386 (0.12.1-1build1) ...\nSelecting previously unselected package libffi8:i386.\nPreparing to unpack .../05-libffi8_3.4.6-1build1_i386.deb ...\nUnpacking libffi8:i386 (3.4.6-1build1) ...\nSelecting previously unselected package libunistring5:i386.\nPreparing to unpack .../06-libunistring5_1.1-2build1_i386.deb ...\nUnpacking libunistring5:i386 (1.1-2build1) ...\nSelecting previously unselected package libidn2-0:i386.\nPreparing to unpack .../07-libidn2-0_2.3.7-2build1_i386.deb ...\nUnpacking libidn2-0:i386 (2.3.7-2build1) ...\nSelecting previously unselected package libdrm2:i386.\nPreparing to unpack .../08-libdrm2_2.4.120-2build1_i386.deb ...\nUnpacking libdrm2:i386 (2.4.120-2build1) ...\nSelecting previously unselected package libxau6:i386.\nPreparing to unpack .../09-libxau6_1%3a1.0.9-1build6_i386.deb ...\nUnpacking libxau6:i386 (1:1.0.9-1build6) ...\nSelecting previously unselected package libxdmcp6:i386.\nPreparing to unpack .../10-libxdmcp6_1%3a1.1.3-0ubuntu6_i386.deb ...\nUnpacking libxdmcp6:i386 (1:1.1.3-0ubuntu6) ...\nSelecting previously unselected package libxcb1:i386.\nPreparing to unpack .../11-libxcb1_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb1:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libx11-6:i386.\nPreparing to unpack .../12-libx11-6_2%3a1.8.7-1build1_i386.deb ...\nUnpacking libx11-6:i386 (2:1.8.7-1build1) ...\nSelecting previously unselected package libxext6:i386.\nPreparing to unpack .../13-libxext6_2%3a1.3.4-1build2_i386.deb ...\nUnpacking libxext6:i386 (2:1.3.4-1build2) ...\nSelecting previously unselected package libwayland-client0:i386.\nPreparing to unpack .../14-libwayland-client0_1.22.0-2.1build1_i386.deb ...\nUnpacking libwayland-client0:i386 (1.22.0-2.1build1) ...\nSelecting previously unselected package libwayland-server0:i386.\nPreparing to unpack .../15-libwayland-server0_1.22.0-2.1build1_i386.deb ...\nUnpacking libwayland-server0:i386 (1.22.0-2.1build1) ...\nSelecting previously unselected package libnvidia-egl-wayland1:i386.\nPreparing to unpack .../16-libnvidia-egl-wayland1_1%3a1.1.13-1build1_i386.deb ...\nUnpacking libnvidia-egl-wayland1:i386 (1:1.1.13-1build1) ...\nSelecting previously unselected package libnvidia-gl-550:i386.\nPreparing to unpack .../17-libnvidia-gl-550_550.120-0ubuntu0.24.04.1_i386.deb ...\nUnpacking libnvidia-gl-550:i386 (550.120-0ubuntu0.24.04.1) ...\nSetting up gcc-14-base:i386 (14.2.0-4ubuntu2~24.04) ...\nSetting up libgcc-s1:i386 (14.2.0-4ubuntu2~24.04) ...\nSetting up libc6:i386 (2.39-0ubuntu8.3) ...\nSetting up libffi8:i386 (3.4.6-1build1) ...\nSetting up libdrm2:i386 (2.4.120-2build1) ...\nSetting up libmd0:i386 (1.1.0-2build1) ...\nSetting up libbsd0:i386 (0.12.1-1build1) ...\nSetting up libwayland-client0:i386 (1.22.0-2.1build1) ...\nSetting up libwayland-server0:i386 (1.22.0-2.1build1) ...\nSetting up libxau6:i386 (1:1.0.9-1build6) ...\nSetting up libxdmcp6:i386 (1:1.1.3-0ubuntu6) ...\nSetting up libxcb1:i386 (1.15-1ubuntu2) ...\nSetting up libnvidia-egl-wayland1:i386 (1:1.1.13-1build1) ...\nSetting up libunistring5:i386 (1.1-2build1) ...\nSetting up libx11-6:i386 (2:1.8.7-1build1) ...\nSetting up libxext6:i386 (2:1.3.4-1build2) ...\nSetting up libidn2-0:i386 (2.3.7-2build1) ...\nSetting up libnvidia-gl-550:i386 (550.120-0ubuntu0.24.04.1) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.3) ...\n</code></pre> <p>It should also be possible to install the official Steam client, with the non-snap alternative. This doesn't seems necessary anymore.</p> <p>Installing Steam from Snap does have a couple of drawbacks:</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#steam-in-plasma-task-manager","title":"Steam in Plasma Task Manager","text":"<p>Pinning Steam to the Plasma Task Manager doesn't really work; as soon as Steam is closed, clicking on the panel icon will only produce an error messages about Steam not being found. The new fast(est) way to launch the Steam client now is to press <code>Alt+Space</code> to invoke Krunner and type <code>steam</code>, then select the entry that is not at the top.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#steam-games","title":"Steam Games","text":"<p>Perhaps the more annoying drawback of installing Steam from Snap is that the location where games are installed is completely different:</p> <ul> <li>Previously, Steam games taking up 345 GB were installed in     <code>~/.local/share/Steam/steamapps/common/</code></li> <li>Now, Steam snap installs games under (initially mostly empty)     <code>~/snap/steam/common/.local/share/Steam/steamapps/common/</code></li> </ul> <p>Worse Yet: all games previously installed are not found!</p> <p>To restore previously downloaded games, each game can be copied from <code>~/.local/share/Steam/steamapps/common/</code> to <code>~/snap/steam/common/.local/share/Steam/steamapps/common/</code> while Steam is not running:</p> <pre><code>$ cp -a .local/share/Steam/steamapps/common/Undertale \\\n  snap/steam/common/.local/share/Steam/steamapps/common/\n\n$ du -sh snap/steam/common/.local/share/Steam/steamapps/common/*\n4.0K    snap/steam/common/.local/share/Steam/steamapps/common/Steam.dll\n32K     snap/steam/common/.local/share/Steam/steamapps/common/SteamLinuxRuntime\n638M    snap/steam/common/.local/share/Steam/steamapps/common/SteamLinuxRuntime_soldier\n156M    snap/steam/common/.local/share/Steam/steamapps/common/Undertale\n</code></pre> <p>After copying games, Steam will still not detect them until they are manually installed. Upon hitting the INSTALL button for each game, Steam will first verify the local files and, unless they are rendered obsolete by an update, the game should be very shortly ready to play.</p> <p>Even Worse Yet: all games have lost their Properties!</p> <p>Worse than having to re-download double- or triple-digit GB worth of games is having to tinker with each of them again to find the optimal Proton version and command flags to run each game, especially since these have been recorded elsewhere only for very few (recent) games:</p> <ul> <li>Dragon Age: Origins - Ultimate Edition</li> <li>Fallout 3 and 4 GOTY</li> </ul> <p>This loss of setting may be the tipping point to ditck the snap package and install the Steam client directly from its source. Moreover, some games appear to be impossible to run with the Steam snap package (e.g. BioShock Infinite).</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#remove-steam-snap","title":"Remove Steam Snap","text":"<p>Removing steam snap can take a long time because, as explained here, by default on <code>snap remove</code>, <code>snapd</code> creates a snapshot of the data and keeps it around for 31 days. This can take a long time for Steam. To avoid this, follow these steps to disable the snapshot before removing Steam:</p> <pre><code># snap changes\nID   Status  Spawn                   Ready                   Summary\n1    Done    2024-08-27              yesterday at 10:31 CET  Initialize system state\n2    Done    yesterday at 10:31 CET  yesterday at 10:32 CET  Initialize device\n3    Done    yesterday at 12:43 CET  yesterday at 12:45 CET  Auto-refresh 5 snaps\n4    Done    yesterday at 12:48 CET  yesterday at 12:49 CET  Install \"chromium\" snap\n5    Done    yesterday at 19:25 CET  yesterday at 19:25 CET  Install \"steam\" snap\n6    Done    yesterday at 19:32 CET  yesterday at 19:33 CET  Install \"blender\" snap\n7    Done    yesterday at 21:01 CET  yesterday at 21:01 CET  Install \"canonical-livepatch\" snap\n\n# snap set core snapshots.automatic.retention=no\n\n# snap remove steam\nsteam removed\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#install-steam-again","title":"Install Steam Again","text":"<p>Repeating the steps used to install Steam in Ubuntu Studio 22.04, the Steam client requires first installing several <code>i386</code> (32-bit) libraries:</p> <pre><code># apt install libglx-mesa0:i386 libc6:amd64 libc6:i386 libegl1:amd64 libegl1:i386 libgbm1:amd64 libgbm1:i386 \\\n  libgl1-mesa-dri:amd64 libgl1-mesa-dri:i386 libgl1:amd64 libgl1:i386 steam-libs-amd64:amd64 steam-libs-i386:i386 -y\n</code></pre> <p>With those installed, one can download <code>steam_latest.deb</code> from store.steampowered.com/about and install it with <code>gdebi</code>:</p> <pre><code># gdebi steam_latest.deb\n/usr/bin/gdebi:113: SyntaxWarning: invalid escape sequence '\\S'\n  c = findall(\"[[(](\\S+)/\\S+[])]\", msg)[0].lower()\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nReading state information... Done\n\nLauncher for the Steam software distribution service\n Steam is a software distribution service with an online store, automated\n installation, automatic updates, achievements, SteamCloud synchronized\n savegame and screenshot functionality, and many social features.\nDo you want to install the software package? [y/N]:y\n/usr/bin/gdebi:113: FutureWarning: Possible nested set at position 1\n  c = findall(\"[[(](\\S+)/\\S+[])]\", msg)[0].lower()\nSelecting previously unselected package steam-launcher.\n(Reading database ... 429435 files and directories currently installed.)\nPreparing to unpack steam_latest.deb ...\nUnpacking steam-launcher (1:1.0.0.81) ...\nSetting up steam-launcher (1:1.0.0.81) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\n</code></pre> <p>Now, upon running <code>/usr/bin/steam</code> a new terminal window opens and requests installing the missing packages:</p> <code>The packages cache seems to be out of date</code> <pre><code>The packages cache seems to be out of date\n\nPress return to update the list of available packages: \n...................\nPackage libegl1:i386 needs to be installed\nPackage libgbm1:i386 needs to be installed\nPackage libgl1-mesa-dri:i386 needs to be installed\nPackage libgl1:i386 needs to be installed\nPackage steam-libs-amd64:amd64 needs to be installed\nPackage steam-libs-i386:i386 needs to be installed\nPackage xterm needs to be installed\n\nSteam needs to install these additional packages:\nlibc6:amd64 libc6:i386 libegl1:amd64 libegl1:i386 libgbm1:amd64 libgbm1:i386 libgl1-mesa-dri:amd64 libgl1-mesa-dri:i386 libgl1:amd64 libgl1:i386 steam-libs-amd64:amd64 steam-libs-i386:i386 xterm\n\nPress return to proceed with the installation: \n................\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nlibc6 is already the newest version (2.39-0ubuntu8.3).\nlibc6 set to manually installed.\nlibc6:i386 is already the newest version (2.39-0ubuntu8.3).\nlibc6:i386 set to manually installed.\nlibegl1 is already the newest version (1.7.0-1build1).\nlibegl1 set to manually installed.\nlibgbm1 is already the newest version (24.0.9-0ubuntu0.3).\nlibgbm1 set to manually installed.\nlibgl1-mesa-dri is already the newest version (24.0.9-0ubuntu0.3).\nlibgl1-mesa-dri set to manually installed.\nlibgl1 is already the newest version (1.7.0-1build1).\nlibgl1 set to manually installed.\nThe following additional packages will be installed:\ni965-va-driver:i386 intel-media-va-driver:i386 libaom3:i386 libapparmor1:i386\nlibasound2-plugins:i386 libasound2t64:i386 libasyncns0:i386 libatomic1:i386\nlibavcodec60:i386 libavutil58:i386 libblkid1:i386 libbrotli1:i386 libbz2-1.0:i386\nlibcairo-gobject2:i386 libcairo2:i386 libcap2:i386 libcodec2-1.2:i386 libcrypt1:i386\nlibdatrie1:i386 libdav1d7:i386 libdb5.3t64:i386 libdbus-1-3:i386 libdeflate0:i386\nlibdrm-amdgpu1:i386 libdrm-intel1:i386 libdrm-nouveau2:i386 libdrm-radeon1:i386\nlibedit2:i386 libegl-mesa0:i386 libelf1t64:i386 libexpat1:i386 libflac12t64:i386\nlibfontconfig1:i386 libfreetype6:i386 libfribidi0:i386 libgcrypt20:i386\nlibgdk-pixbuf-2.0-0:i386 libglapi-mesa:i386 libglib2.0-0t64:i386 libglvnd0:i386\nlibglx-mesa0:i386 libglx0:i386 libgmp10:i386 libgnutls30t64:i386 libgomp1:i386\nlibgpg-error0:i386 libgraphite2-3:i386 libgsm1:i386 libharfbuzz0b:i386\nlibhogweed6t64:i386 libicu74:i386 libigdgmm12:i386 libjack-jackd2-0:i386 libjbig0:i386\nlibjpeg-turbo8:i386 libjpeg8:i386 libllvm17t64:i386 liblz4-1:i386 liblzma5:i386\nlibmount1:i386 libmp3lame0:i386 libmpg123-0t64:i386 libnettle8t64:i386 libnm0:i386\nlibnuma1:i386 libogg0:i386 libopenjp2-7:i386 libopus0:i386 libp11-kit0:i386\nlibpango-1.0-0:i386 libpangocairo-1.0-0:i386 libpangoft2-1.0-0:i386 libpciaccess0:i386\nlibpcre2-8-0:i386 libpixman-1-0:i386 libpng16-16t64:i386 libpulse0:i386 librsvg2-2:i386\nlibrsvg2-common:i386 libsamplerate0:i386 libselinux1:i386 libsensors5:i386\nlibsharpyuv0:i386 libshine3:i386 libsnappy1v5:i386 libsndfile1:i386 libsoxr0:i386\nlibspeex1:i386 libspeexdsp1:i386 libstdc++6:i386 libsvtav1enc1d1:i386\nlibswresample4:i386 libsystemd0:i386 libtasn1-6:i386 libthai0:i386 libtheora0:i386\nlibtiff6:i386 libtinfo6:i386 libtwolame0:i386 libudev1:i386 libva-drm2:i386 libva-glx2\nlibva-glx2:i386 libva-x11-2:i386 libva2:i386 libvdpau1:i386 libvorbis0a:i386\nlibvorbisenc2:i386 libvpx9:i386 libvulkan1:i386 libwebp7:i386 libwebpmux3:i386\nlibx11-xcb1:i386 libx264-164:i386 libx265-199:i386 libxcb-dri2-0:i386 libxcb-dri3-0:i386\nlibxcb-glx0:i386 libxcb-present0:i386 libxcb-randr0:i386 libxcb-render0:i386\nlibxcb-shm0:i386 libxcb-sync1:i386 libxcb-xfixes0:i386 libxfixes3:i386 libxinerama1:i386\nlibxml2:i386 libxrender1:i386 libxshmfence1:i386 libxss1:i386 libxvidcore4:i386\nlibxxf86vm1:i386 libzstd1:i386 libzvbi0t64:i386 mesa-va-drivers:i386\nmesa-vdpau-drivers:i386 mesa-vulkan-drivers:i386 ocl-icd-libopencl1:i386\nva-driver-all:i386 vdpau-driver-all:i386 zlib1g:i386\nSuggested packages:\ni965-va-driver-shaders:i386 libcuda1:i386 libnvcuvid1:i386 libnvidia-encode1:i386\nrng-tools:i386 low-memory-monitor:i386 jackd2:i386 opus-tools:i386 pulseaudio:i386\nlm-sensors:i386 speex:i386 opencl-icd:i386 nvidia-driver-libs nvidia-vulkan-icd\nnvidia-driver-libs:i386 nvidia-vulkan-icd:i386 libvdpau-va-gl1:i386 xfonts-cyrillic\nRecommended packages:\nlibgl1-amber-dri:i386 luit | x11-utils\nThe following NEW packages will be installed:\ni965-va-driver:i386 intel-media-va-driver:i386 libaom3:i386 libapparmor1:i386\nlibasound2-plugins:i386 libasound2t64:i386 libasyncns0:i386 libatomic1:i386\nlibavcodec60:i386 libavutil58:i386 libblkid1:i386 libbrotli1:i386 libbz2-1.0:i386\nlibcairo-gobject2:i386 libcairo2:i386 libcap2:i386 libcodec2-1.2:i386 libcrypt1:i386\nlibdatrie1:i386 libdav1d7:i386 libdb5.3t64:i386 libdbus-1-3:i386 libdeflate0:i386\nlibdrm-amdgpu1:i386 libdrm-intel1:i386 libdrm-nouveau2:i386 libdrm-radeon1:i386\nlibedit2:i386 libegl-mesa0:i386 libegl1:i386 libelf1t64:i386 libexpat1:i386\nlibflac12t64:i386 libfontconfig1:i386 libfreetype6:i386 libfribidi0:i386 libgbm1:i386\nlibgcrypt20:i386 libgdk-pixbuf-2.0-0:i386 libgl1:i386 libgl1-mesa-dri:i386\nlibglapi-mesa:i386 libglib2.0-0t64:i386 libglvnd0:i386 libglx-mesa0:i386 libglx0:i386\nlibgmp10:i386 libgnutls30t64:i386 libgomp1:i386 libgpg-error0:i386 libgraphite2-3:i386\nlibgsm1:i386 libharfbuzz0b:i386 libhogweed6t64:i386 libicu74:i386 libigdgmm12:i386\nlibjack-jackd2-0:i386 libjbig0:i386 libjpeg-turbo8:i386 libjpeg8:i386 libllvm17t64:i386\nliblz4-1:i386 liblzma5:i386 libmount1:i386 libmp3lame0:i386 libmpg123-0t64:i386\nlibnettle8t64:i386 libnm0:i386 libnuma1:i386 libogg0:i386 libopenjp2-7:i386\nlibopus0:i386 libp11-kit0:i386 libpango-1.0-0:i386 libpangocairo-1.0-0:i386\nlibpangoft2-1.0-0:i386 libpciaccess0:i386 libpcre2-8-0:i386 libpixman-1-0:i386\nlibpng16-16t64:i386 libpulse0:i386 librsvg2-2:i386 librsvg2-common:i386\nlibsamplerate0:i386 libselinux1:i386 libsensors5:i386 libsharpyuv0:i386 libshine3:i386\nlibsnappy1v5:i386 libsndfile1:i386 libsoxr0:i386 libspeex1:i386 libspeexdsp1:i386\nlibstdc++6:i386 libsvtav1enc1d1:i386 libswresample4:i386 libsystemd0:i386\nlibtasn1-6:i386 libthai0:i386 libtheora0:i386 libtiff6:i386 libtinfo6:i386\nlibtwolame0:i386 libudev1:i386 libva-drm2:i386 libva-glx2 libva-glx2:i386\nlibva-x11-2:i386 libva2:i386 libvdpau1:i386 libvorbis0a:i386 libvorbisenc2:i386\nlibvpx9:i386 libvulkan1:i386 libwebp7:i386 libwebpmux3:i386 libx11-xcb1:i386\nlibx264-164:i386 libx265-199:i386 libxcb-dri2-0:i386 libxcb-dri3-0:i386 libxcb-glx0:i386\nlibxcb-present0:i386 libxcb-randr0:i386 libxcb-render0:i386 libxcb-shm0:i386\nlibxcb-sync1:i386 libxcb-xfixes0:i386 libxfixes3:i386 libxinerama1:i386 libxml2:i386\nlibxrender1:i386 libxshmfence1:i386 libxss1:i386 libxvidcore4:i386 libxxf86vm1:i386\nlibzstd1:i386 libzvbi0t64:i386 mesa-va-drivers:i386 mesa-vdpau-drivers:i386\nmesa-vulkan-drivers:i386 ocl-icd-libopencl1:i386 steam-libs-amd64 steam-libs-i386:i386\nva-driver-all:i386 vdpau-driver-all:i386 xterm zlib1g:i386\n0 upgraded, 148 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 29.5 kB/120 MB of archives.\nAfter this operation, 442 MB of additional disk space will be used.\nDo you want to continue? [Y/n] \nGet:1 https://repo.steampowered.com/steam stable/steam amd64 steam-libs-amd64 amd64 1:1.0.0.81 [14.7 kB]\nGet:2 https://repo.steampowered.com/steam stable/steam i386 steam-libs-i386 i386 1:1.0.0.81 [14.7 kB]\nFetched 29.5 kB in 0s (136 kB/s)                \nperl: warning: Setting locale failed.\nperl: warning: Please check that your locale settings:\n        LANGUAGE = \"en_US:en\",\n        LC_ALL = (unset),\n        LC_TIME = \"en_CH.UTF-8\",\n        LC_MONETARY = \"de_CH.UTF-8\",\n        LC_NUMERIC = \"de_CH.UTF-8\",\n        LANG = \"en_US.UTF-8\"\n    are supported and installed on your system.\nperl: warning: Falling back to a fallback locale (\"en_US.UTF-8\").\nlocale: Cannot set LC_ALL to default locale: No such file or directory\nExtracting templates from packages: 100%\nSelecting previously unselected package libblkid1:i386.\n(Reading database ... 429466 files and directories currently installed.)\nPreparing to unpack .../000-libblkid1_2.39.3-9ubuntu6.1_i386.deb ...\nUnpacking libblkid1:i386 (2.39.3-9ubuntu6.1) ...\nSelecting previously unselected package libbz2-1.0:i386.\nPreparing to unpack .../001-libbz2-1.0_1.0.8-5.1build0.1_i386.deb ...\nUnpacking libbz2-1.0:i386 (1.0.8-5.1build0.1) ...\nSelecting previously unselected package libcap2:i386.\nPreparing to unpack .../002-libcap2_1%3a2.66-5ubuntu2_i386.deb ...\nUnpacking libcap2:i386 (1:2.66-5ubuntu2) ...\nSelecting previously unselected package libcrypt1:i386.\nPreparing to unpack .../003-libcrypt1_1%3a4.4.36-4build1_i386.deb ...\nUnpacking libcrypt1:i386 (1:4.4.36-4build1) ...\nSelecting previously unselected package libgpg-error0:i386.\nPreparing to unpack .../004-libgpg-error0_1.47-3build2_i386.deb ...\nUnpacking libgpg-error0:i386 (1.47-3build2) ...\nSelecting previously unselected package libgcrypt20:i386.\nPreparing to unpack .../005-libgcrypt20_1.10.3-2build1_i386.deb ...\nUnpacking libgcrypt20:i386 (1.10.3-2build1) ...\nSelecting previously unselected package libgmp10:i386.\nPreparing to unpack .../006-libgmp10_2%3a6.3.0+dfsg-2ubuntu6_i386.deb ...\nUnpacking libgmp10:i386 (2:6.3.0+dfsg-2ubuntu6) ...\nSelecting previously unselected package liblz4-1:i386.\nPreparing to unpack .../007-liblz4-1_1.9.4-1build1.1_i386.deb ...\nUnpacking liblz4-1:i386 (1.9.4-1build1.1) ...\nSelecting previously unselected package liblzma5:i386.\nPreparing to unpack .../008-liblzma5_5.6.1+really5.4.5-1build0.1_i386.deb ...\nUnpacking liblzma5:i386 (5.6.1+really5.4.5-1build0.1) ...\nSelecting previously unselected package libpcre2-8-0:i386.\nPreparing to unpack .../009-libpcre2-8-0_10.42-4ubuntu2_i386.deb ...\nUnpacking libpcre2-8-0:i386 (10.42-4ubuntu2) ...\nSelecting previously unselected package libselinux1:i386.\nPreparing to unpack .../010-libselinux1_3.5-2ubuntu2_i386.deb ...\nUnpacking libselinux1:i386 (3.5-2ubuntu2) ...\nSelecting previously unselected package libmount1:i386.\nPreparing to unpack .../011-libmount1_2.39.3-9ubuntu6.1_i386.deb ...\nUnpacking libmount1:i386 (2.39.3-9ubuntu6.1) ...\nSelecting previously unselected package libzstd1:i386.\nPreparing to unpack .../012-libzstd1_1.5.5+dfsg2-2build1.1_i386.deb ...\nUnpacking libzstd1:i386 (1.5.5+dfsg2-2build1.1) ...\nSelecting previously unselected package libsystemd0:i386.\nPreparing to unpack .../013-libsystemd0_255.4-1ubuntu8.4_i386.deb ...\nUnpacking libsystemd0:i386 (255.4-1ubuntu8.4) ...\nSelecting previously unselected package libtinfo6:i386.\nPreparing to unpack .../014-libtinfo6_6.4+20240113-1ubuntu2_i386.deb ...\nUnpacking libtinfo6:i386 (6.4+20240113-1ubuntu2) ...\nSelecting previously unselected package libudev1:i386.\nPreparing to unpack .../015-libudev1_255.4-1ubuntu8.4_i386.deb ...\nUnpacking libudev1:i386 (255.4-1ubuntu8.4) ...\nSelecting previously unselected package zlib1g:i386.\nPreparing to unpack .../016-zlib1g_1%3a1.3.dfsg-3.1ubuntu2.1_i386.deb ...\nUnpacking zlib1g:i386 (1:1.3.dfsg-3.1ubuntu2.1) ...\nSelecting previously unselected package libapparmor1:i386.\nPreparing to unpack .../017-libapparmor1_4.0.1really4.0.1-0ubuntu0.24.04.3_i386.deb ...\nUnpacking libapparmor1:i386 (4.0.1really4.0.1-0ubuntu0.24.04.3) ...\nSelecting previously unselected package libdb5.3t64:i386.\nPreparing to unpack .../018-libdb5.3t64_5.3.28+dfsg2-7_i386.deb ...\nUnpacking libdb5.3t64:i386 (5.3.28+dfsg2-7) ...\nSelecting previously unselected package libdbus-1-3:i386.\nPreparing to unpack .../019-libdbus-1-3_1.14.10-4ubuntu4.1_i386.deb ...\nUnpacking libdbus-1-3:i386 (1.14.10-4ubuntu4.1) ...\nSelecting previously unselected package libelf1t64:i386.\nPreparing to unpack .../020-libelf1t64_0.190-1.1build4_i386.deb ...\nUnpacking libelf1t64:i386 (0.190-1.1build4) ...\nSelecting previously unselected package libexpat1:i386.\nPreparing to unpack .../021-libexpat1_2.6.1-2ubuntu0.2_i386.deb ...\nUnpacking libexpat1:i386 (2.6.1-2ubuntu0.2) ...\nSelecting previously unselected package libfribidi0:i386.\nPreparing to unpack .../022-libfribidi0_1.0.13-3build1_i386.deb ...\nUnpacking libfribidi0:i386 (1.0.13-3build1) ...\nSelecting previously unselected package libglib2.0-0t64:i386.\nPreparing to unpack .../023-libglib2.0-0t64_2.80.0-6ubuntu3.2_i386.deb ...\nUnpacking libglib2.0-0t64:i386 (2.80.0-6ubuntu3.2) ...\nSelecting previously unselected package libnettle8t64:i386.\nPreparing to unpack .../024-libnettle8t64_3.9.1-2.2build1.1_i386.deb ...\nUnpacking libnettle8t64:i386 (3.9.1-2.2build1.1) ...\nSelecting previously unselected package libhogweed6t64:i386.\nPreparing to unpack .../025-libhogweed6t64_3.9.1-2.2build1.1_i386.deb ...\nUnpacking libhogweed6t64:i386 (3.9.1-2.2build1.1) ...\nSelecting previously unselected package libp11-kit0:i386.\nPreparing to unpack .../026-libp11-kit0_0.25.3-4ubuntu2.1_i386.deb ...\nUnpacking libp11-kit0:i386 (0.25.3-4ubuntu2.1) ...\nSelecting previously unselected package libtasn1-6:i386.\nPreparing to unpack .../027-libtasn1-6_4.19.0-3build1_i386.deb ...\nUnpacking libtasn1-6:i386 (4.19.0-3build1) ...\nSelecting previously unselected package libgnutls30t64:i386.\nPreparing to unpack .../028-libgnutls30t64_3.8.3-1.1ubuntu3.2_i386.deb ...\nUnpacking libgnutls30t64:i386 (3.8.3-1.1ubuntu3.2) ...\nSelecting previously unselected package libstdc++6:i386.\nPreparing to unpack .../029-libstdc++6_14.2.0-4ubuntu2~24.04_i386.deb ...\nUnpacking libstdc++6:i386 (14.2.0-4ubuntu2~24.04) ...\nSelecting previously unselected package libicu74:i386.\nPreparing to unpack .../030-libicu74_74.2-1ubuntu3.1_i386.deb ...\nUnpacking libicu74:i386 (74.2-1ubuntu3.1) ...\nSelecting previously unselected package libxml2:i386.\nPreparing to unpack .../031-libxml2_2.9.14+dfsg-1.3ubuntu3_i386.deb ...\nUnpacking libxml2:i386 (2.9.14+dfsg-1.3ubuntu3) ...\nSelecting previously unselected package libedit2:i386.\nPreparing to unpack .../032-libedit2_3.1-20230828-1build1_i386.deb ...\nUnpacking libedit2:i386 (3.1-20230828-1build1) ...\nSelecting previously unselected package libnuma1:i386.\nPreparing to unpack .../033-libnuma1_2.0.18-1build1_i386.deb ...\nUnpacking libnuma1:i386 (2.0.18-1build1) ...\nSelecting previously unselected package libpng16-16t64:i386.\nPreparing to unpack .../034-libpng16-16t64_1.6.43-5build1_i386.deb ...\nUnpacking libpng16-16t64:i386 (1.6.43-5build1) ...\nSelecting previously unselected package libsensors5:i386.\nPreparing to unpack .../035-libsensors5_1%3a3.6.0-9build1_i386.deb ...\nUnpacking libsensors5:i386 (1:3.6.0-9build1) ...\nSelecting previously unselected package libva2:i386.\nPreparing to unpack .../036-libva2_2.20.0-2build1_i386.deb ...\nUnpacking libva2:i386 (2.20.0-2build1) ...\nSelecting previously unselected package libigdgmm12:i386.\nPreparing to unpack .../037-libigdgmm12_22.3.17+ds1-1_i386.deb ...\nUnpacking libigdgmm12:i386 (22.3.17+ds1-1) ...\nSelecting previously unselected package intel-media-va-driver:i386.\nPreparing to unpack .../038-intel-media-va-driver_24.1.0+dfsg1-1_i386.deb ...\nUnpacking intel-media-va-driver:i386 (24.1.0+dfsg1-1) ...\nSelecting previously unselected package libaom3:i386.\nPreparing to unpack .../039-libaom3_3.8.2-2ubuntu0.1_i386.deb ...\nUnpacking libaom3:i386 (3.8.2-2ubuntu0.1) ...\nSelecting previously unselected package libasound2t64:i386.\nPreparing to unpack .../040-libasound2t64_1.2.11-1build2_i386.deb ...\nUnpacking libasound2t64:i386 (1.2.11-1build2) ...\nSelecting previously unselected package libva-drm2:i386.\nPreparing to unpack .../041-libva-drm2_2.20.0-2build1_i386.deb ...\nUnpacking libva-drm2:i386 (2.20.0-2build1) ...\nSelecting previously unselected package libx11-xcb1:i386.\nPreparing to unpack .../042-libx11-xcb1_2%3a1.8.7-1build1_i386.deb ...\nUnpacking libx11-xcb1:i386 (2:1.8.7-1build1) ...\nSelecting previously unselected package libxcb-dri3-0:i386.\nPreparing to unpack .../043-libxcb-dri3-0_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb-dri3-0:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libxfixes3:i386.\nPreparing to unpack .../044-libxfixes3_1%3a6.0.0-2build1_i386.deb ...\nUnpacking libxfixes3:i386 (1:6.0.0-2build1) ...\nSelecting previously unselected package libva-x11-2:i386.\nPreparing to unpack .../045-libva-x11-2_2.20.0-2build1_i386.deb ...\nUnpacking libva-x11-2:i386 (2.20.0-2build1) ...\nSelecting previously unselected package libvdpau1:i386.\nPreparing to unpack .../046-libvdpau1_1.5-2build1_i386.deb ...\nUnpacking libvdpau1:i386 (1.5-2build1) ...\nSelecting previously unselected package ocl-icd-libopencl1:i386.\nPreparing to unpack .../047-ocl-icd-libopencl1_2.3.2-1build1_i386.deb ...\nUnpacking ocl-icd-libopencl1:i386 (2.3.2-1build1) ...\nSelecting previously unselected package libavutil58:i386.\nPreparing to unpack .../048-libavutil58_7%3a6.1.1-3ubuntu5+esm2_i386.deb ...\nUnpacking libavutil58:i386 (7:6.1.1-3ubuntu5+esm2) ...\nSelecting previously unselected package libbrotli1:i386.\nPreparing to unpack .../049-libbrotli1_1.1.0-2build2_i386.deb ...\nUnpacking libbrotli1:i386 (1.1.0-2build2) ...\nSelecting previously unselected package libfreetype6:i386.\nPreparing to unpack .../050-libfreetype6_2.13.2+dfsg-1build3_i386.deb ...\nUnpacking libfreetype6:i386 (2.13.2+dfsg-1build3) ...\nSelecting previously unselected package libfontconfig1:i386.\nPreparing to unpack .../051-libfontconfig1_2.15.0-1.1ubuntu2_i386.deb ...\nUnpacking libfontconfig1:i386 (2.15.0-1.1ubuntu2) ...\nSelecting previously unselected package libpixman-1-0:i386.\nPreparing to unpack .../052-libpixman-1-0_0.42.2-1build1_i386.deb ...\nUnpacking libpixman-1-0:i386 (0.42.2-1build1) ...\nSelecting previously unselected package libxcb-render0:i386.\nPreparing to unpack .../053-libxcb-render0_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb-render0:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libxcb-shm0:i386.\nPreparing to unpack .../054-libxcb-shm0_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb-shm0:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libxrender1:i386.\nPreparing to unpack .../055-libxrender1_1%3a0.9.10-1.1build1_i386.deb ...\nUnpacking libxrender1:i386 (1:0.9.10-1.1build1) ...\nSelecting previously unselected package libcairo2:i386.\nPreparing to unpack .../056-libcairo2_1.18.0-3build1_i386.deb ...\nUnpacking libcairo2:i386 (1.18.0-3build1) ...\nSelecting previously unselected package libcodec2-1.2:i386.\nPreparing to unpack .../057-libcodec2-1.2_1.2.0-2build1_i386.deb ...\nUnpacking libcodec2-1.2:i386 (1.2.0-2build1) ...\nSelecting previously unselected package libdav1d7:i386.\nPreparing to unpack .../058-libdav1d7_1.4.1-1build1_i386.deb ...\nUnpacking libdav1d7:i386 (1.4.1-1build1) ...\nSelecting previously unselected package libgsm1:i386.\nPreparing to unpack .../059-libgsm1_1.0.22-1build1_i386.deb ...\nUnpacking libgsm1:i386 (1.0.22-1build1) ...\nSelecting previously unselected package libmp3lame0:i386.\nPreparing to unpack .../060-libmp3lame0_3.100-6build1_i386.deb ...\nUnpacking libmp3lame0:i386 (3.100-6build1) ...\nSelecting previously unselected package libopenjp2-7:i386.\nPreparing to unpack .../061-libopenjp2-7_2.5.0-2ubuntu0.2_i386.deb ...\nUnpacking libopenjp2-7:i386 (2.5.0-2ubuntu0.2) ...\nSelecting previously unselected package libopus0:i386.\nPreparing to unpack .../062-libopus0_1.4-1build1_i386.deb ...\nUnpacking libopus0:i386 (1.4-1build1) ...\nSelecting previously unselected package libcairo-gobject2:i386.\nPreparing to unpack .../063-libcairo-gobject2_1.18.0-3build1_i386.deb ...\nUnpacking libcairo-gobject2:i386 (1.18.0-3build1) ...\nSelecting previously unselected package libjpeg-turbo8:i386.\nPreparing to unpack .../064-libjpeg-turbo8_2.1.5-2ubuntu2_i386.deb ...\nUnpacking libjpeg-turbo8:i386 (2.1.5-2ubuntu2) ...\nSelecting previously unselected package libjpeg8:i386.\nPreparing to unpack .../065-libjpeg8_8c-2ubuntu11_i386.deb ...\nUnpacking libjpeg8:i386 (8c-2ubuntu11) ...\nSelecting previously unselected package libdeflate0:i386.\nPreparing to unpack .../066-libdeflate0_1.19-1build1.1_i386.deb ...\nUnpacking libdeflate0:i386 (1.19-1build1.1) ...\nSelecting previously unselected package libjbig0:i386.\nPreparing to unpack .../067-libjbig0_2.1-6.1ubuntu2_i386.deb ...\nUnpacking libjbig0:i386 (2.1-6.1ubuntu2) ...\nSelecting previously unselected package libsharpyuv0:i386.\nPreparing to unpack .../068-libsharpyuv0_1.3.2-0.4build3_i386.deb ...\nUnpacking libsharpyuv0:i386 (1.3.2-0.4build3) ...\nSelecting previously unselected package libwebp7:i386.\nPreparing to unpack .../069-libwebp7_1.3.2-0.4build3_i386.deb ...\nUnpacking libwebp7:i386 (1.3.2-0.4build3) ...\nSelecting previously unselected package libtiff6:i386.\nPreparing to unpack .../070-libtiff6_4.5.1+git230720-4ubuntu2.2_i386.deb ...\nUnpacking libtiff6:i386 (4.5.1+git230720-4ubuntu2.2) ...\nSelecting previously unselected package libgdk-pixbuf-2.0-0:i386.\nPreparing to unpack .../071-libgdk-pixbuf-2.0-0_2.42.10+dfsg-3ubuntu3.1_i386.deb ...\nUnpacking libgdk-pixbuf-2.0-0:i386 (2.42.10+dfsg-3ubuntu3.1) ...\nSelecting previously unselected package libgraphite2-3:i386.\nPreparing to unpack .../072-libgraphite2-3_1.3.14-2build1_i386.deb ...\nUnpacking libgraphite2-3:i386 (1.3.14-2build1) ...\nSelecting previously unselected package libharfbuzz0b:i386.\nPreparing to unpack .../073-libharfbuzz0b_8.3.0-2build2_i386.deb ...\nUnpacking libharfbuzz0b:i386 (8.3.0-2build2) ...\nSelecting previously unselected package libdatrie1:i386.\nPreparing to unpack .../074-libdatrie1_0.2.13-3build1_i386.deb ...\nUnpacking libdatrie1:i386 (0.2.13-3build1) ...\nSelecting previously unselected package libthai0:i386.\nPreparing to unpack .../075-libthai0_0.1.29-2build1_i386.deb ...\nUnpacking libthai0:i386 (0.1.29-2build1) ...\nSelecting previously unselected package libpango-1.0-0:i386.\nPreparing to unpack .../076-libpango-1.0-0_1.52.1+ds-1build1_i386.deb ...\nUnpacking libpango-1.0-0:i386 (1.52.1+ds-1build1) ...\nSelecting previously unselected package libpangoft2-1.0-0:i386.\nPreparing to unpack .../077-libpangoft2-1.0-0_1.52.1+ds-1build1_i386.deb ...\nUnpacking libpangoft2-1.0-0:i386 (1.52.1+ds-1build1) ...\nSelecting previously unselected package libpangocairo-1.0-0:i386.\nPreparing to unpack .../078-libpangocairo-1.0-0_1.52.1+ds-1build1_i386.deb ...\nUnpacking libpangocairo-1.0-0:i386 (1.52.1+ds-1build1) ...\nSelecting previously unselected package librsvg2-2:i386.\nPreparing to unpack .../079-librsvg2-2_2.58.0+dfsg-1build1_i386.deb ...\nUnpacking librsvg2-2:i386 (2.58.0+dfsg-1build1) ...\nSelecting previously unselected package libshine3:i386.\nPreparing to unpack .../080-libshine3_3.1.1-2build1_i386.deb ...\nUnpacking libshine3:i386 (3.1.1-2build1) ...\nSelecting previously unselected package libsnappy1v5:i386.\nPreparing to unpack .../081-libsnappy1v5_1.1.10-1build1_i386.deb ...\nUnpacking libsnappy1v5:i386 (1.1.10-1build1) ...\nSelecting previously unselected package libspeex1:i386.\nPreparing to unpack .../082-libspeex1_1.2.1-2ubuntu2.24.04.1_i386.deb ...\nUnpacking libspeex1:i386 (1.2.1-2ubuntu2.24.04.1) ...\nSelecting previously unselected package libsvtav1enc1d1:i386.\nPreparing to unpack .../083-libsvtav1enc1d1_1.7.0+dfsg-2build1_i386.deb ...\nUnpacking libsvtav1enc1d1:i386 (1.7.0+dfsg-2build1) ...\nSelecting previously unselected package libgomp1:i386.\nPreparing to unpack .../084-libgomp1_14.2.0-4ubuntu2~24.04_i386.deb ...\nUnpacking libgomp1:i386 (14.2.0-4ubuntu2~24.04) ...\nSelecting previously unselected package libsoxr0:i386.\nPreparing to unpack .../085-libsoxr0_0.1.3-4build3_i386.deb ...\nUnpacking libsoxr0:i386 (0.1.3-4build3) ...\nSelecting previously unselected package libswresample4:i386.\nPreparing to unpack .../086-libswresample4_7%3a6.1.1-3ubuntu5+esm2_i386.deb ...\nUnpacking libswresample4:i386 (7:6.1.1-3ubuntu5+esm2) ...\nSelecting previously unselected package libogg0:i386.\nPreparing to unpack .../087-libogg0_1.3.5-3build1_i386.deb ...\nUnpacking libogg0:i386 (1.3.5-3build1) ...\nSelecting previously unselected package libtheora0:i386.\nPreparing to unpack .../088-libtheora0_1.1.1+dfsg.1-16.1build3_i386.deb ...\nUnpacking libtheora0:i386 (1.1.1+dfsg.1-16.1build3) ...\nSelecting previously unselected package libtwolame0:i386.\nPreparing to unpack .../089-libtwolame0_0.4.0-2build3_i386.deb ...\nUnpacking libtwolame0:i386 (0.4.0-2build3) ...\nSelecting previously unselected package libvorbis0a:i386.\nPreparing to unpack .../090-libvorbis0a_1.3.7-1build3_i386.deb ...\nUnpacking libvorbis0a:i386 (1.3.7-1build3) ...\nSelecting previously unselected package libvorbisenc2:i386.\nPreparing to unpack .../091-libvorbisenc2_1.3.7-1build3_i386.deb ...\nUnpacking libvorbisenc2:i386 (1.3.7-1build3) ...\nSelecting previously unselected package libvpx9:i386.\nPreparing to unpack .../092-libvpx9_1.14.0-1ubuntu2.1_i386.deb ...\nUnpacking libvpx9:i386 (1.14.0-1ubuntu2.1) ...\nSelecting previously unselected package libwebpmux3:i386.\nPreparing to unpack .../093-libwebpmux3_1.3.2-0.4build3_i386.deb ...\nUnpacking libwebpmux3:i386 (1.3.2-0.4build3) ...\nSelecting previously unselected package libx264-164:i386.\nPreparing to unpack .../094-libx264-164_2%3a0.164.3108+git31e19f9-1_i386.deb ...\nUnpacking libx264-164:i386 (2:0.164.3108+git31e19f9-1) ...\nSelecting previously unselected package libx265-199:i386.\nPreparing to unpack .../095-libx265-199_3.5-2build1_i386.deb ...\nUnpacking libx265-199:i386 (3.5-2build1) ...\nSelecting previously unselected package libxvidcore4:i386.\nPreparing to unpack .../096-libxvidcore4_2%3a1.3.7-1build1_i386.deb ...\nUnpacking libxvidcore4:i386 (2:1.3.7-1build1) ...\nSelecting previously unselected package libzvbi0t64:i386.\nPreparing to unpack .../097-libzvbi0t64_0.2.42-2_i386.deb ...\nUnpacking libzvbi0t64:i386 (0.2.42-2) ...\nSelecting previously unselected package libavcodec60:i386.\nPreparing to unpack .../098-libavcodec60_7%3a6.1.1-3ubuntu5+esm2_i386.deb ...\nUnpacking libavcodec60:i386 (7:6.1.1-3ubuntu5+esm2) ...\nSelecting previously unselected package libsamplerate0:i386.\nPreparing to unpack .../099-libsamplerate0_0.2.2-4build1_i386.deb ...\nUnpacking libsamplerate0:i386 (0.2.2-4build1) ...\nSelecting previously unselected package libjack-jackd2-0:i386.\nPreparing to unpack .../100-libjack-jackd2-0_1.9.21~dfsg-3ubuntu3_i386.deb ...\nUnpacking libjack-jackd2-0:i386 (1.9.21~dfsg-3ubuntu3) ...\nSelecting previously unselected package libasyncns0:i386.\nPreparing to unpack .../101-libasyncns0_0.8-6build4_i386.deb ...\nUnpacking libasyncns0:i386 (0.8-6build4) ...\nSelecting previously unselected package libflac12t64:i386.\nPreparing to unpack .../102-libflac12t64_1.4.3+ds-2.1ubuntu2_i386.deb ...\nUnpacking libflac12t64:i386 (1.4.3+ds-2.1ubuntu2) ...\nSelecting previously unselected package libmpg123-0t64:i386.\nPreparing to unpack .../103-libmpg123-0t64_1.32.5-1ubuntu1.1_i386.deb ...\nUnpacking libmpg123-0t64:i386 (1.32.5-1ubuntu1.1) ...\nSelecting previously unselected package libsndfile1:i386.\nPreparing to unpack .../104-libsndfile1_1.2.2-1ubuntu5_i386.deb ...\nUnpacking libsndfile1:i386 (1.2.2-1ubuntu5) ...\nSelecting previously unselected package libpulse0:i386.\nPreparing to unpack .../105-libpulse0_1%3a16.1+dfsg1-2ubuntu10_i386.deb ...\nUnpacking libpulse0:i386 (1:16.1+dfsg1-2ubuntu10) ...\nSelecting previously unselected package libspeexdsp1:i386.\nPreparing to unpack .../106-libspeexdsp1_1.2.1-1ubuntu3_i386.deb ...\nUnpacking libspeexdsp1:i386 (1.2.1-1ubuntu3) ...\nSelecting previously unselected package libasound2-plugins:i386.\nPreparing to unpack .../107-libasound2-plugins_1.2.7.1-1ubuntu5_i386.deb ...\nUnpacking libasound2-plugins:i386 (1.2.7.1-1ubuntu5) ...\nSelecting previously unselected package libatomic1:i386.\nPreparing to unpack .../108-libatomic1_14.2.0-4ubuntu2~24.04_i386.deb ...\nUnpacking libatomic1:i386 (14.2.0-4ubuntu2~24.04) ...\nSelecting previously unselected package libdrm-amdgpu1:i386.\nPreparing to unpack .../109-libdrm-amdgpu1_2.4.120-2build1_i386.deb ...\nUnpacking libdrm-amdgpu1:i386 (2.4.120-2build1) ...\nSelecting previously unselected package libpciaccess0:i386.\nPreparing to unpack .../110-libpciaccess0_0.17-3build1_i386.deb ...\nUnpacking libpciaccess0:i386 (0.17-3build1) ...\nSelecting previously unselected package libdrm-intel1:i386.\nPreparing to unpack .../111-libdrm-intel1_2.4.120-2build1_i386.deb ...\nUnpacking libdrm-intel1:i386 (2.4.120-2build1) ...\nSelecting previously unselected package libdrm-nouveau2:i386.\nPreparing to unpack .../112-libdrm-nouveau2_2.4.120-2build1_i386.deb ...\nUnpacking libdrm-nouveau2:i386 (2.4.120-2build1) ...\nSelecting previously unselected package libdrm-radeon1:i386.\nPreparing to unpack .../113-libdrm-radeon1_2.4.120-2build1_i386.deb ...\nUnpacking libdrm-radeon1:i386 (2.4.120-2build1) ...\nSelecting previously unselected package libxcb-randr0:i386.\nPreparing to unpack .../114-libxcb-randr0_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb-randr0:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libgbm1:i386.\nPreparing to unpack .../115-libgbm1_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking libgbm1:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package libglapi-mesa:i386.\nPreparing to unpack .../116-libglapi-mesa_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking libglapi-mesa:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package libxcb-dri2-0:i386.\nPreparing to unpack .../117-libxcb-dri2-0_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb-dri2-0:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libxcb-present0:i386.\nPreparing to unpack .../118-libxcb-present0_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb-present0:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libxcb-sync1:i386.\nPreparing to unpack .../119-libxcb-sync1_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb-sync1:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libxcb-xfixes0:i386.\nPreparing to unpack .../120-libxcb-xfixes0_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb-xfixes0:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libxshmfence1:i386.\nPreparing to unpack .../121-libxshmfence1_1.3-1build5_i386.deb ...\nUnpacking libxshmfence1:i386 (1.3-1build5) ...\nSelecting previously unselected package libegl-mesa0:i386.\nPreparing to unpack .../122-libegl-mesa0_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking libegl-mesa0:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package libvulkan1:i386.\nPreparing to unpack .../123-libvulkan1_1.3.275.0-1build1_i386.deb ...\nUnpacking libvulkan1:i386 (1.3.275.0-1build1) ...\nSelecting previously unselected package libllvm17t64:i386.\nPreparing to unpack .../124-libllvm17t64_1%3a17.0.6-9ubuntu1_i386.deb ...\nUnpacking libllvm17t64:i386 (1:17.0.6-9ubuntu1) ...\nSelecting previously unselected package libgl1-mesa-dri:i386.\nPreparing to unpack .../125-libgl1-mesa-dri_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking libgl1-mesa-dri:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package libxcb-glx0:i386.\nPreparing to unpack .../126-libxcb-glx0_1.15-1ubuntu2_i386.deb ...\nUnpacking libxcb-glx0:i386 (1.15-1ubuntu2) ...\nSelecting previously unselected package libxxf86vm1:i386.\nPreparing to unpack .../127-libxxf86vm1_1%3a1.1.4-1build4_i386.deb ...\nUnpacking libxxf86vm1:i386 (1:1.1.4-1build4) ...\nSelecting previously unselected package libglx-mesa0:i386.\nPreparing to unpack .../128-libglx-mesa0_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking libglx-mesa0:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package libnm0:i386.\nPreparing to unpack .../129-libnm0_1.46.0-1ubuntu2.2_i386.deb ...\nUnpacking libnm0:i386 (1.46.0-1ubuntu2.2) ...\nSelecting previously unselected package librsvg2-common:i386.\nPreparing to unpack .../130-librsvg2-common_2.58.0+dfsg-1build1_i386.deb ...\nUnpacking librsvg2-common:i386 (2.58.0+dfsg-1build1) ...\nSelecting previously unselected package libglvnd0:i386.\nPreparing to unpack .../131-libglvnd0_1.7.0-1build1_i386.deb ...\nUnpacking libglvnd0:i386 (1.7.0-1build1) ...\nSelecting previously unselected package libglx0:i386.\nPreparing to unpack .../132-libglx0_1.7.0-1build1_i386.deb ...\nUnpacking libglx0:i386 (1.7.0-1build1) ...\nSelecting previously unselected package libgl1:i386.\nPreparing to unpack .../133-libgl1_1.7.0-1build1_i386.deb ...\nUnpacking libgl1:i386 (1.7.0-1build1) ...\nSelecting previously unselected package libva-glx2:i386.\nPreparing to unpack .../134-libva-glx2_2.20.0-2build1_i386.deb ...\nUnpacking libva-glx2:i386 (2.20.0-2build1) ...\nSelecting previously unselected package libva-glx2:amd64.\nPreparing to unpack .../135-libva-glx2_2.20.0-2build1_amd64.deb ...\nUnpacking libva-glx2:amd64 (2.20.0-2build1) ...\nSelecting previously unselected package libxinerama1:i386.\nPreparing to unpack .../136-libxinerama1_2%3a1.1.4-3build1_i386.deb ...\nUnpacking libxinerama1:i386 (2:1.1.4-3build1) ...\nSelecting previously unselected package libxss1:i386.\nPreparing to unpack .../137-libxss1_1%3a1.2.3-1build3_i386.deb ...\nUnpacking libxss1:i386 (1:1.2.3-1build3) ...\nSelecting previously unselected package mesa-va-drivers:i386.\nPreparing to unpack .../138-mesa-va-drivers_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking mesa-va-drivers:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package mesa-vdpau-drivers:i386.\nPreparing to unpack .../139-mesa-vdpau-drivers_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking mesa-vdpau-drivers:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package mesa-vulkan-drivers:i386.\nPreparing to unpack .../140-mesa-vulkan-drivers_24.0.9-0ubuntu0.3_i386.deb ...\nUnpacking mesa-vulkan-drivers:i386 (24.0.9-0ubuntu0.3) ...\nSelecting previously unselected package steam-libs-amd64.\nPreparing to unpack .../141-steam-libs-amd64_1%3a1.0.0.81_amd64.deb ...\nUnpacking steam-libs-amd64 (1:1.0.0.81) ...\nSelecting previously unselected package libegl1:i386.\nPreparing to unpack .../142-libegl1_1.7.0-1build1_i386.deb ...\nUnpacking libegl1:i386 (1.7.0-1build1) ...\nSelecting previously unselected package steam-libs-i386:i386.\nPreparing to unpack .../143-steam-libs-i386_1%3a1.0.0.81_i386.deb ...\nUnpacking steam-libs-i386:i386 (1:1.0.0.81) ...\nSelecting previously unselected package i965-va-driver:i386.\nPreparing to unpack .../144-i965-va-driver_2.4.1+dfsg1-1build2_i386.deb ...\nUnpacking i965-va-driver:i386 (2.4.1+dfsg1-1build2) ...\nSelecting previously unselected package va-driver-all:i386.\nPreparing to unpack .../145-va-driver-all_2.20.0-2build1_i386.deb ...\nUnpacking va-driver-all:i386 (2.20.0-2build1) ...\nSelecting previously unselected package vdpau-driver-all:i386.\nPreparing to unpack .../146-vdpau-driver-all_1.5-2build1_i386.deb ...\nUnpacking vdpau-driver-all:i386 (1.5-2build1) ...\nSelecting previously unselected package xterm.\nPreparing to unpack .../147-xterm_390-1ubuntu3_amd64.deb ...\nUnpacking xterm (390-1ubuntu3) ...\nSetting up libexpat1:i386 (2.6.1-2ubuntu0.2) ...\nSetting up libgraphite2-3:i386 (1.3.14-2build1) ...\nSetting up libxcb-dri3-0:i386 (1.15-1ubuntu2) ...\nSetting up libpixman-1-0:i386 (0.42.2-1build1) ...\nSetting up libsharpyuv0:i386 (1.3.2-0.4build3) ...\nSetting up libzstd1:i386 (1.5.5+dfsg2-2build1.1) ...\nSetting up libaom3:i386 (3.8.2-2ubuntu0.1) ...\nSetting up libx11-xcb1:i386 (2:1.8.7-1build1) ...\nSetting up libapparmor1:i386 (4.0.1really4.0.1-0ubuntu0.24.04.3) ...\nSetting up libdrm-nouveau2:i386 (2.4.120-2build1) ...\nSetting up libxcb-xfixes0:i386 (1.15-1ubuntu2) ...\nSetting up libogg0:i386 (1.3.5-3build1) ...\nSetting up libspeex1:i386 (1.2.1-2ubuntu2.24.04.1) ...\nSetting up libshine3:i386 (3.1.1-2build1) ...\nSetting up libx264-164:i386 (2:0.164.3108+git31e19f9-1) ...\nSetting up libtwolame0:i386 (0.4.0-2build3) ...\nSetting up liblzma5:i386 (5.6.1+really5.4.5-1build0.1) ...\nSetting up libgpg-error0:i386 (1.47-3build2) ...\nSetting up libxrender1:i386 (1:0.9.10-1.1build1) ...\nSetting up libdatrie1:i386 (0.2.13-3build1) ...\nSetting up libgsm1:i386 (1.0.22-1build1) ...\nSetting up libxcb-render0:i386 (1.15-1ubuntu2) ...\nSetting up libdrm-radeon1:i386 (2.4.120-2build1) ...\nSetting up liblz4-1:i386 (1.9.4-1build1.1) ...\nSetting up libglvnd0:i386 (1.7.0-1build1) ...\nSetting up libcodec2-1.2:i386 (1.2.0-2build1) ...\nSetting up libxcb-glx0:i386 (1.15-1ubuntu2) ...\nSetting up libbrotli1:i386 (1.1.0-2build2) ...\nSetting up libdeflate0:i386 (1.19-1build1.1) ...\nSetting up libgcrypt20:i386 (1.10.3-2build1) ...\nSetting up zlib1g:i386 (1:1.3.dfsg-3.1ubuntu2.1) ...\nSetting up libcrypt1:i386 (1:4.4.36-4build1) ...\nSetting up libsvtav1enc1d1:i386 (1.7.0+dfsg-2build1) ...\nSetting up libxcb-shm0:i386 (1.15-1ubuntu2) ...\nSetting up libmpg123-0t64:i386 (1.32.5-1ubuntu1.1) ...\nSetting up libgomp1:i386 (14.2.0-4ubuntu2~24.04) ...\nSetting up libxvidcore4:i386 (2:1.3.7-1build1) ...\nSetting up libjbig0:i386 (2.1-6.1ubuntu2) ...\nSetting up libcap2:i386 (1:2.66-5ubuntu2) ...\nSetting up libelf1t64:i386 (0.190-1.1build4) ...\nSetting up libxxf86vm1:i386 (1:1.1.4-1build4) ...\nSetting up libxcb-present0:i386 (1.15-1ubuntu2) ...\nSetting up libthai0:i386 (0.1.29-2build1) ...\nSetting up libnettle8t64:i386 (3.9.1-2.2build1.1) ...\nSetting up libasound2t64:i386 (1.2.11-1build2) ...\nSetting up libva2:i386 (2.20.0-2build1) ...\nSetting up libxfixes3:i386 (1:6.0.0-2build1) ...\nSetting up libxcb-sync1:i386 (1.15-1ubuntu2) ...\nSetting up libgmp10:i386 (2:6.3.0+dfsg-2ubuntu6) ...\nSetting up libfribidi0:i386 (1.0.13-3build1) ...\nSetting up libopus0:i386 (1.4-1build1) ...\nSetting up libp11-kit0:i386 (0.25.3-4ubuntu2.1) ...\nSetting up libxinerama1:i386 (2:1.1.4-3build1) ...\nSetting up libpng16-16t64:i386 (1.6.43-5build1) ...\nSetting up libatomic1:i386 (14.2.0-4ubuntu2~24.04) ...\nSetting up libvorbis0a:i386 (1.3.7-1build3) ...\nSetting up libsensors5:i386 (1:3.6.0-9build1) ...\nSetting up libpcre2-8-0:i386 (10.42-4ubuntu2) ...\nSetting up libjpeg-turbo8:i386 (2.1.5-2ubuntu2) ...\nSetting up libglapi-mesa:i386 (24.0.9-0ubuntu0.3) ...\nSetting up libvulkan1:i386 (1.3.275.0-1build1) ...\nSetting up libwebp7:i386 (1.3.2-0.4build3) ...\nSetting up libdb5.3t64:i386 (5.3.28+dfsg2-7) ...\nSetting up libudev1:i386 (255.4-1ubuntu8.4) ...\nSetting up libxcb-dri2-0:i386 (1.15-1ubuntu2) ...\nSetting up libnuma1:i386 (2.0.18-1build1) ...\nSetting up libvpx9:i386 (1.14.0-1ubuntu2.1) ...\nSetting up libdav1d7:i386 (1.4.1-1build1) ...\nSetting up libhogweed6t64:i386 (3.9.1-2.2build1.1) ...\nSetting up libva-drm2:i386 (2.20.0-2build1) ...\nSetting up ocl-icd-libopencl1:i386 (2.3.2-1build1) ...\nSetting up libasyncns0:i386 (0.8-6build4) ...\nSetting up libxshmfence1:i386 (1.3-1build5) ...\nSetting up libvdpau1:i386 (1.5-2build1) ...\nSetting up libxcb-randr0:i386 (1.15-1ubuntu2) ...\nSetting up libspeexdsp1:i386 (1.2.1-1ubuntu3) ...\nSetting up libtasn1-6:i386 (4.19.0-3build1) ...\nSetting up steam-libs-amd64 (1:1.0.0.81) ...\nSetting up libopenjp2-7:i386 (2.5.0-2ubuntu0.2) ...\nSetting up libflac12t64:i386 (1.4.3+ds-2.1ubuntu2) ...\nSetting up libxss1:i386 (1:1.2.3-1build3) ...\nSetting up xterm (390-1ubuntu3) ...\nSetting up libbz2-1.0:i386 (1.0.8-5.1build0.1) ...\nSetting up libsamplerate0:i386 (0.2.2-4build1) ...\nSetting up libva-x11-2:i386 (2.20.0-2build1) ...\nSetting up libwebpmux3:i386 (1.3.2-0.4build3) ...\nSetting up libva-glx2:amd64 (2.20.0-2build1) ...\nSetting up libblkid1:i386 (2.39.3-9ubuntu6.1) ...\nSetting up libstdc++6:i386 (14.2.0-4ubuntu2~24.04) ...\nSetting up libmp3lame0:i386 (3.100-6build1) ...\nSetting up libvorbisenc2:i386 (1.3.7-1build3) ...\nSetting up libtinfo6:i386 (6.4+20240113-1ubuntu2) ...\nSetting up libdrm-amdgpu1:i386 (2.4.120-2build1) ...\nSetting up libjpeg8:i386 (8c-2ubuntu11) ...\nSetting up libgnutls30t64:i386 (3.8.3-1.1ubuntu3.2) ...\nSetting up libpciaccess0:i386 (0.17-3build1) ...\nSetting up libzvbi0t64:i386 (0.2.42-2) ...\nSetting up libgbm1:i386 (24.0.9-0ubuntu0.3) ...\nSetting up libsoxr0:i386 (0.1.3-4build3) ...\nSetting up libedit2:i386 (3.1-20230828-1build1) ...\nSetting up libdrm-intel1:i386 (2.4.120-2build1) ...\nSetting up libavutil58:i386 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up libigdgmm12:i386 (22.3.17+ds1-1) ...\nSetting up libsystemd0:i386 (255.4-1ubuntu8.4) ...\nSetting up libsnappy1v5:i386 (1.1.10-1build1) ...\nSetting up libselinux1:i386 (3.5-2ubuntu2) ...\nSetting up libegl-mesa0:i386 (24.0.9-0ubuntu0.3) ...\nSetting up libfreetype6:i386 (2.13.2+dfsg-1build3) ...\nSetting up libswresample4:i386 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up libdbus-1-3:i386 (1.14.10-4ubuntu4.1) ...\nSetting up intel-media-va-driver:i386 (24.1.0+dfsg1-1) ...\nSetting up libx265-199:i386 (3.5-2build1) ...\nSetting up libicu74:i386 (74.2-1ubuntu3.1) ...\nSetting up libjack-jackd2-0:i386 (1.9.21~dfsg-3ubuntu3) ...\nSetting up libmount1:i386 (2.39.3-9ubuntu6.1) ...\nSetting up libtiff6:i386 (4.5.1+git230720-4ubuntu2.2) ...\nSetting up libegl1:i386 (1.7.0-1build1) ...\nSetting up libfontconfig1:i386 (2.15.0-1.1ubuntu2) ...\nSetting up libsndfile1:i386 (1.2.2-1ubuntu5) ...\nSetting up libxml2:i386 (2.9.14+dfsg-1.3ubuntu3) ...\nSetting up i965-va-driver:i386 (2.4.1+dfsg1-1build2) ...\nSetting up libpulse0:i386 (1:16.1+dfsg1-2ubuntu10) ...\nSetting up libcairo2:i386 (1.18.0-3build1) ...\nSetting up libglib2.0-0t64:i386 (2.80.0-6ubuntu3.2) ...\nSetting up libllvm17t64:i386 (1:17.0.6-9ubuntu1) ...\nSetting up libtheora0:i386 (1.1.1+dfsg.1-16.1build3) ...\nSetting up mesa-va-drivers:i386 (24.0.9-0ubuntu0.3) ...\nSetting up mesa-vulkan-drivers:i386 (24.0.9-0ubuntu0.3) ...\nSetting up mesa-vdpau-drivers:i386 (24.0.9-0ubuntu0.3) ...\nSetting up libgl1-mesa-dri:i386 (24.0.9-0ubuntu0.3) ...\nSetting up va-driver-all:i386 (2.20.0-2build1) ...\nSetting up vdpau-driver-all:i386 (1.5-2build1) ...\nSetting up libglx-mesa0:i386 (24.0.9-0ubuntu0.3) ...\nSetting up libglx0:i386 (1.7.0-1build1) ...\nSetting up libgl1:i386 (1.7.0-1build1) ...\nSetting up libva-glx2:i386 (2.20.0-2build1) ...\nSetting up steam-libs-i386:i386 (1:1.0.0.81) ...\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.3) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for libglib2.0-0t64:amd64 (2.80.0-6ubuntu3.2) ...\nSetting up libnm0:i386 (1.46.0-1ubuntu2.2) ...\nSetting up libharfbuzz0b:i386 (8.3.0-2build2) ...\nSetting up libgdk-pixbuf-2.0-0:i386 (2.42.10+dfsg-3ubuntu3.1) ...\nSetting up libcairo-gobject2:i386 (1.18.0-3build1) ...\nSetting up libpango-1.0-0:i386 (1.52.1+ds-1build1) ...\nSetting up libpangoft2-1.0-0:i386 (1.52.1+ds-1build1) ...\nSetting up libpangocairo-1.0-0:i386 (1.52.1+ds-1build1) ...\nSetting up librsvg2-2:i386 (2.58.0+dfsg-1build1) ...\nSetting up librsvg2-common:i386 (2.58.0+dfsg-1build1) ...\nSetting up libavcodec60:i386 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up libasound2-plugins:i386 (1.2.7.1-1ubuntu5) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.3) ...\nProcessing triggers for libgdk-pixbuf-2.0-0:i386 (2.42.10+dfsg-3ubuntu3.1) ...\n\nPress return to continue: \n</code></pre> <p>The final list of packages is very close to what had been downloaded already with the <code>apt install</code> command from Ubuntu 22.04, after replacing <code>libgl1-mesa-glx:i386</code> with <code>libglx-mesa0:i386</code>:</p> <pre><code># apt install libglx-mesa0:i386 libc6:amd64 libc6:i386 libegl1:amd64 \\\n  libegl1:i386 libgbm1:amd64 libgbm1:i386   libgl1-mesa-dri:amd64 \\\n  libgl1-mesa-dri:i386 libgl1:amd64 libgl1:i386 steam-libs-amd64:amd64 \\\n  steam-libs-i386:i386 -y\n</code></pre> <p>Pressing return launches the Steam client and it is immediately clear that all games still have their properties as the should.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#glorious-eggroll","title":"Glorious Eggroll","text":"<p>Many games depend on gloriouseggroll/proton-ge-custom to run and this needs to be installed separately.</p> <p>This was previously installed in Rapture when modding Skyrim Special Edition, and the old versions may still be available under <code>~/.steam/root/compatibilitytools.d/</code></p> <p>The latest version can be installed using the Bash script provided as the Manual method for Native Steam:</p> <code>~/bin/install-latest-protonge.sh</code> <pre><code>#!/bin/bash\nset -euo pipefail\n\n# make temp working directory\necho \"Creating temporary working directory...\"\nrm -rf /tmp/proton-ge-custom\nmkdir /tmp/proton-ge-custom\ncd /tmp/proton-ge-custom\n\n# download tarball\necho \"Fetching tarball URL...\"\ntarball_url=$(curl -s https://api.github.com/repos/GloriousEggroll/proton-ge-custom/releases/latest | grep browser_download_url | cut -d\\\" -f4 | grep .tar.gz)\ntarball_name=$(basename $tarball_url)\necho \"Downloading tarball: $tarball_name...\"\ncurl -# -L $tarball_url -o $tarball_name --no-progress-meter\n\n# download checksum\necho \"Fetching checksum URL...\"\nchecksum_url=$(curl -s https://api.github.com/repos/GloriousEggroll/proton-ge-custom/releases/latest | grep browser_download_url | cut -d\\\" -f4 | grep .sha512sum)\nchecksum_name=$(basename $checksum_url)\necho \"Downloading checksum: $checksum_name...\"\ncurl -# -L $checksum_url -o $checksum_name --no-progress-meter\n\n# check tarball with checksum\necho \"Verifying tarball $tarball_name with checksum $checksum_name...\"\nsha512sum -c $checksum_name\n# if result is ok, continue\n\n# make steam directory if it does not exist\necho \"Creating Steam directory if it does not exist...\"\nmkdir -p ~/.steam/root/compatibilitytools.d\n\n# extract proton tarball to steam directory\necho \"Extracting $tarball_name to Steam directory...\"\ntar -xf $tarball_name -C ~/.steam/root/compatibilitytools.d/\necho \"All done :)\"\n</code></pre> <pre><code>$ ~/bin/install-latest-protonge.sh\nCreating temporary working directory...\nFetching tarball URL...\nDownloading tarball: GE-Proton9-22.tar.gz...\nFetching checksum URL...\nDownloading checksum: GE-Proton9-22.sha512sum...\nVerifying tarball GE-Proton9-22.tar.gz with checksum GE-Proton9-22.sha512sum...\nGE-Proton9-22.tar.gz: OK\nCreating Steam directory if it does not exist...\nExtracting GE-Proton9-22.tar.gz to Steam directory...\nAll done :)\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#itchio","title":"Itch.io","text":"<p>There is a binary in <code>.itch/itch</code> but it doesn\u2019t work, it seem to have launched the app but the app itself is nowhere to be seen:</p> <code>$ .itch/itch</code> <pre><code>$ .itch/itch\n2024/12/27 20:26:33 itch-setup will log to /tmp/itch-setup-log.txt\n2024/12/27 20:26:33 =========================================\n2024/12/27 20:26:33 itch-setup \"v1.26.0, built on Apr 22 2021 @ 03:48:12, ref 48f97b3e7b0b065a2478811b8d0ebcae414845fd\" starting up at \"2024-12-27 20:26:33.155012082 +0100 CET m=+0.004615449\" with arguments:\n2024/12/27 20:26:33 \"/home/coder/.itch/itch-setup\"\n2024/12/27 20:26:33 \"--prefer-launch\"\n2024/12/27 20:26:33 \"--appname\"\n2024/12/27 20:26:33 \"itch\"\n2024/12/27 20:26:33 \"--\"\n2024/12/27 20:26:33 =========================================\n2024/12/27 20:26:33 App name specified on command-line: itch\n2024/12/27 20:26:33 Locale:  en-US\n2024/12/27 20:26:33 Initializing installer GUI...\n2024/12/27 20:26:33 Using GTK UI\n\n(process:748360): Gtk-WARNING **: 20:26:33.157: Locale not supported by C library.\n        Using the fallback 'C' locale.\n2024/12/27 20:26:33 Initializing (itch) multiverse @ (/home/coder/.itch)\n2024/12/27 20:26:33 (/home/coder/.itch)(current = \"26.1.9\", ready = \"\")\n2024/12/27 20:26:33 Launch preferred, attempting...\n2024/12/27 20:26:33 Launching (26.1.9) from (/home/coder/.itch/app-26.1.9)\n2024/12/27 20:26:33 Kernel should support SUID sandboxing, leaving it enabled\n2024/12/27 20:26:33 App launched, getting out of the way\n</code></pre> <p>The solution, albeit possibly only a temporary one, is to disable sandboxing (source) by adding the <code>--no-sandbox</code> in <code>.itch/itch</code>:</p> .itch/itch<pre><code>#!/bin/sh\n/home/coder/.itch/itch-setup \\\n  --prefer-launch --appname itch \\\n  -- --no-sandbox \"$@\"\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#minecraft-java-edition","title":"Minecraft Java Edition","text":"<p>To avoid taking chances, copy the Minecraft launcher from the previous system:</p> <pre><code># cp -a /jammy/opt/minecraft-launcher/ /opt/\n</code></pre> <p>It works perfectly right after installing and logging in again.</p> <p>In contrast, trying to re-download Minecraft Java Edition seems to lead nowhere good.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#minecraft-bedrock-edition","title":"Minecraft Bedrock Edition","text":"<p>There is an unofficial Minecraft Bedrock Launcher, including smiple steps to install it on Debian / Ubuntu. This has not seemed necessary so far, since the family enjoys playing the Java edition more.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#arduino-ide","title":"Arduino IDE","text":"<p>There no Arduino IDE in Ubuntu 22.04 (in <code>/jammy/opt/arduino</code>) and  even if there was it would be out of date, so it pays to install the latest/nightly version:</p> <pre><code># wget https://downloads.arduino.cc/arduino-ide/nightly/arduino-ide_nightly-latest_Linux_64bit.zip\n# unzip arduino-ide_nightly-latest_Linux_64bit.zip\n# mv arduino-ide_nightly-20241212_Linux_64bit/ /opt/arduino/\n# chmod 4755 /opt/arduino/chrome-sandbox\n</code></pre> <p>Upon launching the Arduino IDE, a notification card offers updating installed libraries, which comes in vary handy to update them all.</p> Without the <code>chmod 4755</code> command, the IDE refuses to run. <pre><code>$ /opt/arduino/arduino-ide\n[1917080:1107/234610.122185:FATAL:setuid_sandbox_host.cc(158)] The SUID sandbox helper binary was found, but is not configured correctly. Rather than run without sandboxing I'm aborting now. You need to make sure that /opt/arduino/chrome-sandbox is owned by root and has mode 4755.\nTrace/breakpoint trap (core dumped)\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#arduino-failed-updates","title":"Arduino failed updates","text":"<p>Unfortunately at least one of the libraries in the Arduino IDE hit a snag and got stuck with an exception, seemingly because NVidia cards and/or drivers are not supported.</p> Arduino IDE stuck updating libraries. <pre><code>$ /opt/arduino/arduino-ide\nArduino IDE 2.3.5-nightly-20241212\n...\n\n2024-12-27T19:33:51.252Z daemon INFO time=\"2024-12-27T20:33:51+01:00\" level=info msg=\"Loaded tool\" tool=\"esp8266:mklittlefs@3.1.0-gcc10.3-e5f9fec\"\ntime=\"2024-12-27T20:33:51+01:00\" level=info msg=\"Loaded tool\" tool=\"esp8266:mkspiffs@3.1.0-gcc10.3-e5f9fec\"\ntime=\"2024-12-27T20:33:51+01:00\" level=info msg=\"Loaded tool\" tool=\"esp8266:python3@3.7.2-post1\"\n2024-12-27T19:33:51.252Z daemon INFO time=\"2024-12-27T20:33:51+01:00\" level=info msg=\"Loaded tool\" tool=\"esp8266:xtensa-lx106-elf-gcc@3.1.0-gcc10.3-e5f9fec\"\nWarning: loader_get_json: Failed to open JSON file virtio_icd.x86_64.json\nWarning: loader_scanned_icd_add: Could not get 'vkCreateInstance' via 'vk_icdGetInstanceProcAddr' for ICD libGLX_nvidia.so.0\nWarning: loader_get_json: Failed to open JSON file lvp_icd.x86_64.json\nWarning: /usr/lib/x86_64-linux-gnu/libvulkan_intel.so: cannot open shared object file: Permission denied\nWarning: loader_icd_scan: Failed loading library associated with ICD JSON /usr/lib/x86_64-linux-gnu/libvulkan_intel.so. Ignoring this JSON\nWarning: /usr/lib/x86_64-linux-gnu/libvulkan_radeon.so: cannot open shared object file: Permission denied\nWarning: loader_icd_scan: Failed loading library associated with ICD JSON /usr/lib/x86_64-linux-gnu/libvulkan_radeon.so. Ignoring this JSON\nWarning: loader_get_json: Failed to open JSON file intel_hasvk_icd.x86_64.json\nWarning: vkCreateInstance: Found no drivers!\nWarning: vkCreateInstance failed with VK_ERROR_INCOMPATIBLE_DRIVER\n    at CheckVkSuccessImpl (../../third_party/dawn/src/dawn/native/vulkan/VulkanError.cpp:88)\n    at CreateVkInstance (../../third_party/dawn/src/dawn/native/vulkan/BackendVk.cpp:458)\n    at Initialize (../../third_party/dawn/src/dawn/native/vulkan/BackendVk.cpp:344)\n    at Create (../../third_party/dawn/src/dawn/native/vulkan/BackendVk.cpp:266)\n    at operator() (../../third_party/dawn/src/dawn/native/vulkan/BackendVk.cpp:521)\n\n\n\n^CisTempSketch: true. Input was /tmp/.arduinoIDE-unsaved20241127-911572-vfe16y.lc64g/sketch_dec27a\nIgnored marking workspace as a closed sketch. The sketch was detected as temporary. Workspace URI: file:///tmp/.arduinoIDE-unsaved20241127-911572-vfe16y.lc64g/sketch_dec27a.\nisTempSketch: true. Input was /tmp/.arduinoIDE-unsaved20241127-911572-vfe16y.lc64g/sketch_dec27a\nIgnored marking workspace as a closed sketch. The sketch was detected as temporary. Workspace URI: file:///tmp/.arduinoIDE-unsaved20241127-911572-vfe16y.lc64g/sketch_dec27a.\nClosing channel on service path '/services/electron-window'.\nClosing channel on service path '/services/ide-updater'.\nStored workspaces roots: \nNo sketches were scheduled for deletion.\n</code></pre> <p>Searching around for that specific error, all results are about issues for non-NVidia GPUs and thus not applicable. There is a hint of a specific library being problematic in the output of  <code>vulkaninfo --summary</code> but that seems to be a red herring.</p> <code>$ vulkaninfo --summary</code> <pre><code>$ vulkaninfo --summary\nWARNING: [Loader Message] Code 0 : terminator_CreateInstance: Received return code -3 from call to vkCreateInstance in ICD /usr/lib/x86_64-linux-gnu/libvulkan_virtio.so. Skipping this driver.\n==========\nVULKANINFO\n==========\n\nVulkan Instance Version: 1.3.275\n\n\nInstance Extensions: count = 23\n-------------------------------\nVK_EXT_acquire_drm_display             : extension revision 1\nVK_EXT_acquire_xlib_display            : extension revision 1\nVK_EXT_debug_report                    : extension revision 10\nVK_EXT_debug_utils                     : extension revision 2\nVK_EXT_direct_mode_display             : extension revision 1\nVK_EXT_display_surface_counter         : extension revision 1\nVK_EXT_surface_maintenance1            : extension revision 1\nVK_EXT_swapchain_colorspace            : extension revision 4\nVK_KHR_device_group_creation           : extension revision 1\nVK_KHR_display                         : extension revision 23\nVK_KHR_external_fence_capabilities     : extension revision 1\nVK_KHR_external_memory_capabilities    : extension revision 1\nVK_KHR_external_semaphore_capabilities : extension revision 1\nVK_KHR_get_display_properties2         : extension revision 1\nVK_KHR_get_physical_device_properties2 : extension revision 2\nVK_KHR_get_surface_capabilities2       : extension revision 1\nVK_KHR_portability_enumeration         : extension revision 1\nVK_KHR_surface                         : extension revision 25\nVK_KHR_surface_protected_capabilities  : extension revision 1\nVK_KHR_wayland_surface                 : extension revision 6\nVK_KHR_xcb_surface                     : extension revision 6\nVK_KHR_xlib_surface                    : extension revision 6\nVK_LUNARG_direct_driver_loading        : extension revision 1\n\nInstance Layers: count = 8\n--------------------------\nVK_LAYER_INTEL_nullhw             INTEL NULL HW                1.1.73   version 1\nVK_LAYER_MESA_device_select       Linux device selection layer 1.3.211  version 1\nVK_LAYER_MESA_overlay             Mesa Overlay layer           1.3.211  version 1\nVK_LAYER_NV_optimus               NVIDIA Optimus layer         1.3.277  version 1\nVK_LAYER_VALVE_steam_fossilize_32 Steam Pipeline Caching Layer 1.3.207  version 1\nVK_LAYER_VALVE_steam_fossilize_64 Steam Pipeline Caching Layer 1.3.207  version 1\nVK_LAYER_VALVE_steam_overlay_32   Steam Overlay Layer          1.3.207  version 1\nVK_LAYER_VALVE_steam_overlay_64   Steam Overlay Layer          1.3.207  version 1\n\nDevices:\n========\nGPU0:\n        apiVersion         = 1.3.277\n        driverVersion      = 550.120.0.0\n        vendorID           = 0x10de\n        deviceID           = 0x2182\n        deviceType         = PHYSICAL_DEVICE_TYPE_DISCRETE_GPU\n        deviceName         = NVIDIA GeForce GTX 1660 Ti\n        driverID           = DRIVER_ID_NVIDIA_PROPRIETARY\n        driverName         = NVIDIA\n        driverInfo         = 550.120\n        conformanceVersion = 1.3.7.2\n        deviceUUID         = f77cc413-629c-e437-1b69-3c3edb0915b8\n        driverUUID         = b0238158-7345-5c56-8ef8-c3afdfd4908a\nGPU1:\n        apiVersion         = 1.3.274\n        driverVersion      = 0.0.1\n        vendorID           = 0x10005\n        deviceID           = 0x0000\n        deviceType         = PHYSICAL_DEVICE_TYPE_CPU\n        deviceName         = llvmpipe (LLVM 17.0.6, 256 bits)\n        driverID           = DRIVER_ID_MESA_LLVMPIPE\n        driverName         = llvmpipe\n        driverInfo         = Mesa 24.0.9-0ubuntu0.3 (LLVM 17.0.6)\n        conformanceVersion = 1.3.1.1\n        deviceUUID         = 6d657361-3234-2e30-2e39-2d3075627500\n        driverUUID         = 6c6c766d-7069-7065-5555-494400000000\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#visual-studio-code","title":"Visual Studio Code","text":"<p>Since the young artist wanted to use Visual Studio Code to comfortably edit websites, I got around to try it out and rather like how it uses more of the screen than the self-hosted Visual Studio Code Server, and the screen on this PC is significantly smaller so this should come in handy.</p> <p>Installation is fairly simple, so much a single <code>.deb</code> file can be installed directly, but the apt repository can also be installed manually with the following script:</p> <pre><code># apt install apt-transport-https gpg wget -y\n# wget -qO- https://packages.microsoft.com/keys/microsoft.asc \\\n  | gpg --dearmor &gt; packages.microsoft.gpg\n# install -D -o root -g root -m 644 packages.microsoft.gpg \\\n  /etc/apt/keyrings/packages.microsoft.gpg\n# echo \"deb [arch=amd64,arm64,armhf signed-by=/etc/apt/keyrings/packages.microsoft.gpg] https://packages.microsoft.com/repos/code stable main\" \\\n  &gt; /etc/apt/sources.list.d/vscode.list\n# rm -f packages.microsoft.gpg\n# apt update\n# apt install code -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  code\n0 upgraded, 1 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 105 MB of archives.\nAfter this operation, 424 MB of additional disk space will be used.\nGet:1 https://packages.microsoft.com/repos/code stable/main amd64 code amd64 1.96.2-1734607745 [105 MB]\nFetched 105 MB in 4s (29.7 MB/s)\nPreconfiguring packages ...\nSelecting previously unselected package code.\n(Reading database ... 426277 files and directories currently installed.)\nPreparing to unpack .../code_1.96.2-1734607745_amd64.deb ...\nUnpacking code (1.96.2-1734607745) ...\nSetting up code (1.96.2-1734607745) ...\nWarning in file \"/usr/share/applications/displaycal-vrml-to-x3d-converter.desktop\": usage of MIME type \"x-world/x-vrml\" is discouraged (the use of \"x-world\" as media type is strongly discouraged in favor of a subtype of the \"application\" media type)\nWarning in file \"/usr/share/applications/displaycal-vrml-to-x3d-converter.desktop\": usage of MIME type \"x-world/x-vrml\" is discouraged (the use of \"x-world\" as media type is strongly discouraged in favor of a subtype of the \"application\" media type)\nProcessing triggers for shared-mime-info (2.4-4) ...\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#displaycal","title":"DisplayCal","text":"<p>DisplayCAL is no longer maintained, it was dropped from Ubuntu 20.04 because it would not work with Python 3, but was still possible to build with python2.7 packages. Later, that was no longer possible in Ubuntu 22.04, so a new port to Python 3 was started: the DisplayCAL Python 3 Project.</p> <p>Back in late 2022, the best method around was in (French) DisplayCAL en Python 3 and required only a few basic packages.</p> <p>As or late 2024, the new project has its own Installation Instructions (Linux) but in Ubuntu Studio 24.04 none of this is necessary; <code>apt install displaycal</code> will do.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#system-configuration","title":"System Configuration","text":"<p>The above having covered installing software, there are still system configurations that need to be tweaked.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#ubuntu-pro","title":"Ubuntu Pro","text":"<p>When updating the system with <code>apt full-upgrade -y</code> a notice comes up about additional security updates:</p> <pre><code># apt full-upgrade -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\nGet more security updates through Ubuntu Pro with 'esm-apps' enabled:\n  libdcmtk17t64 python3-waitress libcjson1 libavdevice60 ffmpeg libpostproc57\n  libavcodec60 libavutil58 libswscale7 libswresample4 libavformat60\n  libavfilter9\nLearn more about Ubuntu Pro at https://ubuntu.com/pro\nThe following upgrades have been deferred due to phasing:\n  python3-distupgrade ubuntu-release-upgrader-core ubuntu-release-upgrader-qt\n0 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.\n</code></pre> <p>This being a new system, indeed it's not attached to an Ubuntu Pro account (the old system was):</p> <pre><code># pro security-status\ninstalled:\n     1645 packages from Ubuntu Main/Restricted repository\n     1581 packages from Ubuntu Universe/Multiverse repository\n     3 packages from third parties\n\nTo get more information about the packages, run\n    pro security-status --help\nfor a list of available options.\n\nThis machine is receiving security patching for Ubuntu Main/Restricted\nrepository until 2029.\nThis machine is NOT attached to an Ubuntu Pro subscription.\n\nUbuntu Pro with 'esm-infra' enabled provides security updates for\nMain/Restricted packages until 2034.\n\nUbuntu Pro with 'esm-apps' enabled provides security updates for\nUniverse/Multiverse packages until 2034. There are 12 pending security updates.\n\nTry Ubuntu Pro with a free personal subscription on up to 5 machines.\nLearn more at https://ubuntu.com/pro\n</code></pre> <p>After creating an Ubuntu account a token is available to use with <code>pro attach</code>:</p> <pre><code># pro attach ...\nEnabling Ubuntu Pro: ESM Apps\nUbuntu Pro: ESM Apps enabled\nEnabling Ubuntu Pro: ESM Infra\nUbuntu Pro: ESM Infra enabled\nEnabling Livepatch\nLivepatch enabled\nThis machine is now attached to 'Ubuntu Pro - free personal subscription'\n\nSERVICE          ENTITLED  STATUS       DESCRIPTION\nanbox-cloud      yes       disabled     Scalable Android in the cloud\nesm-apps         yes       enabled      Expanded Security Maintenance for Applications\nesm-infra        yes       enabled      Expanded Security Maintenance for Infrastructure\nlandscape        yes       disabled     Management and administration tool for Ubuntu\nlivepatch        yes       warning      Current kernel is not covered by livepatch\nrealtime-kernel* yes       disabled     Ubuntu kernel with PREEMPT_RT patches integrated\n\n* Service has variants\n\nNOTICES\nOperation in progress: pro attach\nThe current kernel (6.8.0-50-lowlatency, x86_64) is not covered by livepatch.\nCovered kernels are listed here: https://ubuntu.com/security/livepatch/docs/kernels\nEither switch to a covered kernel or `sudo pro disable livepatch` to dismiss this warning.\n\nFor a list of all Ubuntu Pro services and variants, run 'pro status --all'\nEnable services with: pro enable &lt;service&gt;\n\n    Account: ponder.stibbons@uu.am\nSubscription: Ubuntu Pro - free personal subscription\n</code></pre> <p>Now the system can be updated again with <code>apt full-upgrade -y</code> to receive those additional security updates:</p> <code># apt full-upgrade -y</code> <pre><code># apt full-upgrade -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\nThe following upgrades have been deferred due to phasing:\npython3-distupgrade ubuntu-release-upgrader-core ubuntu-release-upgrader-qt\nThe following packages will be upgraded:\nffmpeg libavcodec60 libavdevice60 libavfilter9 libavformat60 libavutil58 libcjson1\nlibdcmtk17t64 libpostproc57 libswresample4 libswscale7 python3-waitress\n12 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.\n12 esm-apps security updates\nNeed to get 19.3 MB of archives.\nAfter this operation, 6,144 B of additional disk space will be used.\nPreparing to unpack .../00-libswscale7_7%3a6.1.1-3ubuntu5+esm2_amd64.deb ...\nUnpacking libswscale7:amd64 (7:6.1.1-3ubuntu5+esm2) over (7:6.1.1-3ubuntu5) ...\nPreparing to unpack .../01-libavdevice60_7%3a6.1.1-3ubuntu5+esm2_amd64.deb ...\nUnpacking libavdevice60:amd64 (7:6.1.1-3ubuntu5+esm2) over (7:6.1.1-3ubuntu5) ...\nPreparing to unpack .../02-libavformat60_7%3a6.1.1-3ubuntu5+esm2_amd64.deb ...\nUnpacking libavformat60:amd64 (7:6.1.1-3ubuntu5+esm2) over (7:6.1.1-3ubuntu5) ...\nPreparing to unpack .../03-libavfilter9_7%3a6.1.1-3ubuntu5+esm2_amd64.deb ...\nUnpacking libavfilter9:amd64 (7:6.1.1-3ubuntu5+esm2) over (7:6.1.1-3ubuntu5) ...\nPreparing to unpack .../04-libavcodec60_7%3a6.1.1-3ubuntu5+esm2_amd64.deb ...\nUnpacking libavcodec60:amd64 (7:6.1.1-3ubuntu5+esm2) over (7:6.1.1-3ubuntu5) ...\nPreparing to unpack .../05-libavutil58_7%3a6.1.1-3ubuntu5+esm2_amd64.deb ...\nUnpacking libavutil58:amd64 (7:6.1.1-3ubuntu5+esm2) over (7:6.1.1-3ubuntu5) ...\nPreparing to unpack .../06-libpostproc57_7%3a6.1.1-3ubuntu5+esm2_amd64.deb ...\nUnpacking libpostproc57:amd64 (7:6.1.1-3ubuntu5+esm2) over (7:6.1.1-3ubuntu5) ...\nPreparing to unpack .../07-libswresample4_7%3a6.1.1-3ubuntu5+esm2_amd64.deb ...\nUnpacking libswresample4:amd64 (7:6.1.1-3ubuntu5+esm2) over (7:6.1.1-3ubuntu5) ...\nPreparing to unpack .../08-ffmpeg_7%3a6.1.1-3ubuntu5+esm2_amd64.deb ...\nUnpacking ffmpeg (7:6.1.1-3ubuntu5+esm2) over (7:6.1.1-3ubuntu5) ...\nPreparing to unpack .../09-libcjson1_1.7.17-1ubuntu0.1~esm2_amd64.deb ...\nUnpacking libcjson1:amd64 (1.7.17-1ubuntu0.1~esm2) over (1.7.17-1) ...\nPreparing to unpack .../10-libdcmtk17t64_3.6.7-9.1ubuntu0.1~esm1_amd64.deb ...\nUnpacking libdcmtk17t64:amd64 (3.6.7-9.1ubuntu0.1~esm1) over (3.6.7-9.1build4) ...\nPreparing to unpack .../11-python3-waitress_2.1.2-2ubuntu0.1~esm1_all.deb ...\nUnpacking python3-waitress (2.1.2-2ubuntu0.1~esm1) over (2.1.2-2) ...\nSetting up python3-waitress (2.1.2-2ubuntu0.1~esm1) ...\nSetting up libdcmtk17t64:amd64 (3.6.7-9.1ubuntu0.1~esm1) ...\nSetting up libavutil58:amd64 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up libcjson1:amd64 (1.7.17-1ubuntu0.1~esm2) ...\nSetting up libswresample4:amd64 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up libavcodec60:amd64 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up libpostproc57:amd64 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up libswscale7:amd64 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up libavformat60:amd64 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up libavfilter9:amd64 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up libavdevice60:amd64 (7:6.1.1-3ubuntu5+esm2) ...\nSetting up ffmpeg (7:6.1.1-3ubuntu5+esm2) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.3) ...\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#make-dmesg-non-privileged","title":"Make <code>dmesg</code> non-privileged","text":"<p>Since Ubuntu 22.04, <code>dmesg</code> has become a privileged operation by default:</p> <pre><code>$ dmesg\ndmesg: read kernel buffer failed: Operation not permitted\n</code></pre> <p>This is controlled by </p> <pre><code># sysctl kernel.dmesg_restrict\nkernel.dmesg_restrict = 1\n</code></pre> <p>To revert this default, and make it permanent (source):</p> <pre><code># echo 'kernel.dmesg_restrict=0' | tee -a /etc/sysctl.d/99-sysctl.conf\n</code></pre> <p>This is effective only after a reboot, but the next section requires one too and is much more interesting.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#make-sddm-look-good","title":"Make SDDM Look Good","text":"<p>Ubuntu Studio 24.04 uses  Simple Desktop Display Manager (SDDM) (sddm/sddm in GitHub) which is quite good looking out of the box, but I like to customize this for each computer.</p> <p>For most computers my favorite SDDM theme is Breeze-Noir-Dark, which I like to install system-wide.</p> <pre><code># unzip -d /usr/share/sddm/themes Breeze-Noir-Dark.zip\n</code></pre> <p>Action icons won\u2019t render if the directory name is changed.</p> <p>If needed, change the directory name in the <code>iconSource</code> fields in <code>Main.qml</code> to match final directory name so icons show. This is not the only thing that breaks when changing the directory name.</p> <p>Other than installing this theme, all I really change in it is the background image to use one that goes with the computer's name. In this case, search for images of \"raven wallpaper 4k\" led me to find this good-looking Raven Computer Wallpaper [2560x1600], which needs to be cropped to fit the 2560x1080 screen resolution:</p> <p></p> <p>Once copied to <code>/usr/share/sddm/themes/Breeze-Noir-Dark/</code> the image needs to be set up in both <code>theme.conf</code> and <code>theme.conf.user</code>:</p> <pre><code># cp /jammy/usr/share/sddm/themes/Breeze-Noir-Dark/raven-2560-1080.jpg \\\n  /usr/share/sddm/themes/Breeze-Noir-Dark/\n# cd /usr/share/sddm/themes/Breeze-Noir-Dark/\n\n# vi theme.conf\n[General]\ntype=image\ncolor=#132e43\nbackground=/usr/share/sddm/themes/Breeze-Noir-Dark/raven-2560-1080.jpg\n\n# vi theme.conf.user\n[General]\ntype=image\nbackground=raven-2560-1080.jpg\n</code></pre> <p>Additionally, as this is new in Ubuntu 24.04, the theme has to be selected by adding a <code>[Theme]</code> section in the system config in <code>/usr/lib/sddm/sddm.conf.d/ubuntustudio.conf</code></p> /usr/lib/sddm/sddm.conf.d/ubuntustudio.conf<pre><code>[General]\nInputMethod=\n\n[Theme]\nCurrent=\"Breeze-Noir-Dark\"\nEnableAvatars=True\n</code></pre> <p>Reportedly, you have to create the <code>/etc/sddm.conf.d</code> directory to add the Local configuration file that allows setting the theme:</p> <pre><code># mkdir /etc/sddm.conf.d\n# vi /etc/sddm.conf.d/ubuntustudio.conf\n</code></pre> <p>Besides setting the theme, it is also good to limit the range of user ids so that only human users show up:</p> /etc/sddm.conf.d/ubuntustudio.conf<pre><code>[Theme]\nCurrent=Breeze-Noir-Dark\n\n[Users]\nMaximumUid=1003\nMinimumUid=1000\n</code></pre> <p>It seems no longer necessary to manually add Redshift to one's desktop session. Previously, it would be necessary to launch Autostart and Add Application\u2026 to add Redshift.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#weekly-btrfs-scrub","title":"Weekly btrfs scrub","text":"<p>To keep BTRFS file systems healthy, it is recommended to run a weekly scrub to check everything for consistency. For this, I run the script from crontab every Saturday night.</p> <pre><code># wget -O /usr/local/bin/btrfs-scrub-all \\\n  http://marc.merlins.org/linux/scripts/btrfs-scrub\n\n# crontab -e\n...\n# m h  dom mon dow   command\n0 23 * * 6 /usr/local/bin/btrfs-scrub-all\n</code></pre> <p>Marc MERLIN keeps the script updated, so each systme may benefit from a few modifications, e.g. 1. Remove tests for laptop battery status, when running on a PC. 2. Set the <code>BTRFS_SCRUB_SKIP</code> to filter out partitions to skip.</p> /usr/local/bin/btrfs-scrub-all<pre><code>#! /bin/bash\n\n# By Marc MERLIN &lt;marc_soft@merlins.org&gt; 2014/03/20\n# License: Apache-2.0\n# http://marc.merlins.org/perso/btrfs/post_2014-03-19_Btrfs-Tips_-Btrfs-Scrub-and-Btrfs-Filesystem-Repair.html\n\nwhich btrfs &gt;/dev/null || exit 0\nexport PATH=/usr/local/bin:/sbin:$PATH\n\nFILTER='(^Dumping|balancing, usage)'\nBTRFS_SCRUB_SKIP=\"noskip\"\nsource /etc/btrfs_config 2&gt;/dev/null\ntest -n \"$DEVS\" || DEVS=$(grep '\\&lt;btrfs\\&gt;' /proc/mounts | awk '{ print $1 }' | sort -u | grep -v $BTRFS_SCRUB_SKIP)\nfor btrfs in $DEVS\ndo\n    tail -n 0 -f /var/log/syslog | grep \"BTRFS\" | grep -Ev '(disk space caching is enabled|unlinked .* orphans|turning on discard|device label .* devid .* transid|enabling SSD mode|BTRFS: has skinny extents|BTRFS: device label|BTRFS info )' &amp;\n    mountpoint=\"$(grep \"$btrfs\" /proc/mounts | awk '{ print $2 }' | sort | head -1)\"\n    logger -s \"Quick Metadata and Data Balance of $mountpoint ($btrfs)\" &gt;&amp;2\n    # Even in 4.3 kernels, you can still get in places where balance\n    # won't work (no place left, until you run a -m0 one first)\n    # I'm told that proactively rebalancing metadata may not be a good idea.\n    #btrfs balance start -musage=20 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    # but a null rebalance should help corner cases:\n    sleep 10\n    btrfs balance start -musage=0 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    # After metadata, let's do data:\n    sleep 10\n    btrfs balance start -dusage=0 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    sleep 10\n    btrfs balance start -dusage=20 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    # And now we do scrub. Note that scrub can fail with \"no space left\n    # on device\" if you're very out of balance.\n    logger -s \"Starting scrub of $mountpoint\" &gt;&amp;2\n    echo btrfs scrub start -Bd $mountpoint\n    # -r is read only, but won't fix a redundant array.\n    #ionice -c 3 nice -10 btrfs scrub start -Bdr $mountpoint\n    time ionice -c 3 nice -10 btrfs scrub start -Bd $mountpoint\n    pkill -f 'tail -n 0 -f /var/log/syslog'\n    logger \"Ended scrub of $mountpoint\" &gt;&amp;2\ndone\n</code></pre> <p>The whole process takes about 8 minutes for the 2TB NVMe SSD, then something like 4.4 hours for each of the 2TB RAID 5 of HDDs:</p> <code># /usr/local/bin/btrfs-scrub-all</code> <pre><code># /usr/local/bin/btrfs-scrub-all\n&lt;13&gt;Dec 27 23:01:10 root: Quick Metadata and Data Balance of /home/raid (/dev/md0)\nDone, had to relocate 0 out of 1560 chunks\nDone, had to relocate 0 out of 1560 chunks\nDone, had to relocate 0 out of 1560 chunks\n&lt;13&gt;Dec 27 23:01:48 root: Starting scrub of /home/raid\nbtrfs scrub start -Bd /home/raid\nStarting scrub on devid 1\n\nScrub device /dev/md0 (id 1) done\nScrub started:    Fri Dec 27 23:01:48 2024\nStatus:           finished\nDuration:         4:24:16\nTotal to scrub:   1.51TiB\nRate:             100.22MiB/s\nError summary:    no errors found\n\nreal    264m15.768s\nuser    0m0.007s\nsys     4m55.957s\n&lt;13&gt;Dec 28 03:26:04 root: Quick Metadata and Data Balance of /home (/dev/nvme0n1p4)\nDone, had to relocate 0 out of 1214 chunks\nDone, had to relocate 0 out of 1214 chunks\nDone, had to relocate 132 out of 1214 chunks\n&lt;13&gt;Dec 28 03:27:31 root: Starting scrub of /home\nbtrfs scrub start -Bd /home\nStarting scrub on devid 1\n\nScrub device /dev/nvme0n1p4 (id 1) done\nScrub started:    Sat Dec 28 03:27:31 2024\nStatus:           finished\nDuration:         0:07:45\nTotal to scrub:   1.02TiB\nRate:             2.25GiB/s\nError summary:    no errors found\n\nreal    7m45.015s\nuser    0m0.000s\nsys     3m11.951s\n</code></pre> <p></p> <p>During the first scrub on Ubuntu Studio 24.04 there were ATA errors in the <code>dmesg</code> log concerning one of the disks (<code>sda</code>):</p> <pre><code>[33774.356941] BTRFS info (device md0): balance: start -musage=0 -susage=0\n[33774.359384] BTRFS info (device md0): balance: ended with status: 0\n[33786.804683] BTRFS info (device md0): balance: start -dusage=0\n[33786.808232] BTRFS info (device md0): balance: ended with status: 0\n[33799.400749] BTRFS info (device md0): balance: start -dusage=20\n[33799.403721] BTRFS info (device md0): balance: ended with status: 0\n[33800.445942] BTRFS info (device md0): scrub: started on devid 1\n...\n[42150.439716] ata1.00: exception Emask 0x0 SAct 0xc00040 SErr 0x0 action 0x0\n[42150.439727] ata1.00: irq_stat 0x40000008\n[42150.439731] ata1.00: failed command: READ FPDMA QUEUED\n[42150.439734] ata1.00: cmd 60/00:30:00:81:ae/03:00:35:00:00/40 tag 6 ncq dma 393216 in\n                        res 41/40:00:80:82:ae/00:00:35:00:00/40 Emask 0x409 (media error) &lt;F&gt;\n[42150.439744] ata1.00: status: { DRDY ERR }\n[42150.439748] ata1.00: error: { UNC }\n[42150.446005] ata1.00: LPM support broken, forcing max_power\n[42150.452351] ata1.00: LPM support broken, forcing max_power\n[42150.452418] ata1.00: configured for UDMA/133\n[42150.452459] sd 0:0:0:0: [sda] tag#6 FAILED Result: hostbyte=DID_OK driverbyte=DRIVER_OK cmd_age=13s\n[42150.452465] sd 0:0:0:0: [sda] tag#6 Sense Key : Medium Error [current] \n[42150.452469] sd 0:0:0:0: [sda] tag#6 Add. Sense: Unrecovered read error - auto reallocate failed\n[42150.452474] sd 0:0:0:0: [sda] tag#6 CDB: Read(10) 28 00 35 ae 81 00 00 03 00 00\n[42150.452476] I/O error, dev sda, sector 900629120 op 0x0:(READ) flags 0x0 phys_seg 15 prio class 3\n[42150.452643] ata1: EH complete\n</code></pre> <p>Also, the weekly Btrfs scrub doesn't seem to really need <code>inn</code> or any of its dependencies, so this installation step was skipped:</p> <pre><code># apt install inn -y\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#smart-monitoring","title":"S.M.A.R.T. Monitoring","text":"<p>Install Smartmontools to setup up S.M.A.R.T. monitoring:</p> <pre><code># apt install smartmontools gsmartcontrol libnotify-bin nvme-cli -y\n</code></pre> <p>Create <code>/usr/local/bin/smartdnotify</code> to notify when errors are found:</p> /usr/local/bin/smartdnotify<pre><code>#!/bin/sh\n\n# Ignore NVME \"error\" entries that are not errors.\nelog=/root/smart-latest-error-log-entry\nnvme error-log /dev/nvme0 | tail -16 &gt; $elog\ngrep -iq 'status_field[[:blank:]]*: 0.SUCCESS' $elog &amp;&amp; exit 0\n\n# Otherwise, log and notify the error.\ndata=/root/smart-latest-error\necho \"SMARTD_FAILTYPE=$SMARTD_FAILTYPE\" &gt;&gt; $data\necho \"SMARTD_MESSAGE=\u2019$SMARTD_MESSAGE\u2019\" &gt;&gt; $data\nsudo -u coder DISPLAY=:0 DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus notify-send \"S.M.A.R.T Error ($SMARTD_FAILTYPE)\" \"$SMARTD_MESSAGE\"  -i /usr/share/pixmaps/yoshimi.png\n</code></pre> <p>Configure <code>/etc/smartd.conf</code> to run this script:</p> <pre><code>DEVICESCAN -d removable -n standby -m root -M exec /usr/local/bin/smartdnotify\n</code></pre> <p>Restart the <code>smartd</code> service:</p> <pre><code># systemctl restart smartd.service\n\n# systemctl status smartd.service\n\u25cf smartmontools.service - Self Monitoring and Reporting Technology (SMART) Daemon\n     Loaded: loaded (/usr/lib/systemd/system/smartmontools.service; enabled; preset: enabled)\n     Active: active (running) since Fri 2024-12-27 23:47:21 CET; 3s ago\n       Docs: man:smartd(8)\n             man:smartd.conf(5)\n   Main PID: 1363551 (smartd)\n     Status: \"Next check of 4 devices will start at 00:17:21\"\n      Tasks: 1 (limit: 38353)\n     Memory: 1.9M (peak: 2.4M)\n        CPU: 40ms\n     CGroup: /system.slice/smartmontools.service\n             \u2514\u25001363551 /usr/sbin/smartd -n\n\nDec 27 23:47:20 raven smartd[1363551]: Device: /dev/nvme0, Samsung SSD 970 EVO Plus 2TB, S/N:S4J4NX0W427520B, FW:2B2QEXM7, 2.00 TB\nDec 27 23:47:20 raven smartd[1363551]: Device: /dev/nvme0, is SMART capable. Adding to \"monitor\" list.\nDec 27 23:47:20 raven smartd[1363551]: Device: /dev/nvme0, state read from /var/lib/smartmontools/smartd.Samsung_SSD_970_EVO_Plus_2TB-S4J4NX0W427520B.nvme.state\nDec 27 23:47:20 raven smartd[1363551]: Monitoring 3 ATA/SATA, 0 SCSI/SAS and 1 NVMe devices\nDec 27 23:47:20 raven smartd[1363551]: Device: /dev/sda [SAT], 3 Currently unreadable (pending) sectors\nDec 27 23:47:21 raven smartd[1363551]: Device: /dev/sda [SAT], state written to /var/lib/smartmontools/smartd.ST1000LM024_HN_M101MBB-S2R8J9DD902619.ata.state\nDec 27 23:47:21 raven smartd[1363551]: Device: /dev/sdb [SAT], state written to /var/lib/smartmontools/smartd.ST1000LM024_HN_M101MBB-S2R8J9DD902602.ata.state\nDec 27 23:47:21 raven smartd[1363551]: Device: /dev/sdc [SAT], state written to /var/lib/smartmontools/smartd.ST1000LM024_HN_M101MBB-S2R8J9DD902613.ata.state\nDec 27 23:47:21 raven smartd[1363551]: Device: /dev/nvme0, state written to /var/lib/smartmontools/smartd.Samsung_SSD_970_EVO_Plus_2TB-S4J4NX0W427520B.nvme.state\nDec 27 23:47:21 raven systemd[1]: Started smartmontools.service - Self Monitoring and Reporting Technology (SMART) Daemon.\n</code></pre> <p>Then add a scrip to re-notify: <code>/usr/local/bin/smartd-renotify</code></p> /usr/local/bin/smartd-renotify<pre><code>#!/bin/sh\n\n# Ignore NVME \"error\" entries that are not errors.\nelog=/root/smart-latest-error-log-entry\nnvme error-log /dev/nvme0 | tail -16 &gt; $elog\ngrep -iq 'status_field[[:blank:]]*: 0.SUCCESS' $elog &amp;&amp; exit 0\n\n# Otherwise, re-notify.\nlatest=/root/smart-latest-error\nif [ -f $latest ] ; then\n  . $latest\n  if [ -n \"$SMARTD_FAILTYPE\" ]\n  then\n    sudo -u coder DISPLAY=:0 DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus notify-send \"S.M.A.R.T Error ($SMARTD_FAILTYPE)\" \"$SMARTD_MESSAGE\"  -i /usr/share/pixmaps/yoshimi.png\n  fi\nfi\n</code></pre> <pre><code># chmod +x /usr/local/bin/smartdnotify /usr/local/bin/smartd-renotify\n# crontab -e\n*/5 * * * * /usr/local/bin/smartd-renotify\n</code></pre> <p>The above scripts have been updated to avoid spurious <code>ErrorCount</code> errors, as discussed in  ErrorCount in NVME.</p> <p>It is still always good to check that the disk is healthy:</p> <code># smartctl -a /dev/nvme0</code> <pre><code># smartctl -a /dev/nvme0\nsmartctl 7.4 2023-08-01 r5530 [x86_64-linux-6.8.0-50-lowlatency] (local build)\nCopyright (C) 2002-23, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF INFORMATION SECTION ===\nModel Number:                       Samsung SSD 970 EVO Plus 2TB\nSerial Number:                      S4J4NX0W427520B\nFirmware Version:                   2B2QEXM7\nPCI Vendor/Subsystem ID:            0x144d\nIEEE OUI Identifier:                0x002538\nTotal NVM Capacity:                 2,000,398,934,016 [2.00 TB]\nUnallocated NVM Capacity:           0\nController ID:                      4\nNVMe Version:                       1.3\nNumber of Namespaces:               1\nNamespace 1 Size/Capacity:          2,000,398,934,016 [2.00 TB]\nNamespace 1 Utilization:            1,304,029,368,320 [1.30 TB]\nNamespace 1 Formatted LBA Size:     512\nNamespace 1 IEEE EUI-64:            002538 5431b769e1\nLocal Time is:                      Fri Dec 27 23:50:16 2024 CET\nFirmware Updates (0x16):            3 Slots, no Reset required\nOptional Admin Commands (0x0017):   Security Format Frmw_DL Self_Test\nOptional NVM Commands (0x005f):     Comp Wr_Unc DS_Mngmt Wr_Zero Sav/Sel_Feat Timestmp\nLog Page Attributes (0x03):         S/H_per_NS Cmd_Eff_Lg\nMaximum Data Transfer Size:         512 Pages\nWarning  Comp. Temp. Threshold:     85 Celsius\nCritical Comp. Temp. Threshold:     85 Celsius\n\nSupported Power States\nSt Op     Max   Active     Idle   RL RT WL WT  Ent_Lat  Ex_Lat\n 0 +     7.50W       -        -    0  0  0  0        0       0\n 1 +     5.90W       -        -    1  1  1  1        0       0\n 2 +     3.60W       -        -    2  2  2  2        0       0\n 3 -   0.0700W       -        -    3  3  3  3      210    1200\n 4 -   0.0050W       -        -    4  4  4  4     2000    8000\n\nSupported LBA Sizes (NSID 0x1)\nId Fmt  Data  Metadt  Rel_Perf\n 0 +     512       0         0\n\n=== START OF SMART DATA SECTION ===\nSMART overall-health self-assessment test result: PASSED\n\nSMART/Health Information (NVMe Log 0x02)\nCritical Warning:                   0x00\nTemperature:                        54 Celsius\nAvailable Spare:                    100%\nAvailable Spare Threshold:          10%\nPercentage Used:                    0%\nData Units Read:                    18,227,693 [9.33 TB]\nData Units Written:                 16,663,786 [8.53 TB]\nHost Read Commands:                 420,175,269\nHost Write Commands:                178,467,400\nController Busy Time:               1,543\nPower Cycles:                       36\nPower On Hours:                     599\nUnsafe Shutdowns:                   3\nMedia and Data Integrity Errors:    0\nError Information Log Entries:      169\nWarning  Comp. Temperature Time:    0\nCritical Comp. Temperature Time:    0\nTemperature Sensor 1:               54 Celsius\nTemperature Sensor 2:               58 Celsius\n\nError Information (NVMe Log 0x01, 16 of 64 entries)\nNum   ErrCount  SQId   CmdId  Status  PELoc          LBA  NSID    VS  Message\n  0        169     0  0x0006  0x4004      -            0     0     -  Invalid Field in Command\n\nSelf-test Log (NVMe Log 0x06)\nSelf-test status: No self-test in progress\nNo Self-tests Logged\n</code></pre> <p>Most of the entries are the same: not an error at all:</p> <pre><code># nvme error-log /dev/nvme0 | grep status_field | sort | uniq -c\n     63 status_field    : 0(Successful Completion: The command completed without error)\n      1 status_field    : 0x2002(Invalid Field in Command: A reserved coded value or an unsupported value in a defined field)\n</code></pre>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#bluetooth-controller-and-devices","title":"Bluetooth controller and devices","text":"<p>The system board MSI B450I Gaming Plus AC supports Bluetooth\u00ae 4.2, 4.1, BLE, 4.0, 3.0, 2.1, 2.1+EDR but getting devices to pair, connect and work reliably is another story.</p> <p>The bluetooth controller is detected and shows up in <code>dmesg</code>:</p> <pre><code>[   12.847474] Bluetooth: Core ver 2.22\n[   12.847825] NET: Registered PF_BLUETOOTH protocol family\n[   12.847828] Bluetooth: HCI device and connection manager initialized\n[   12.847835] Bluetooth: HCI socket layer initialized\n[   12.847840] Bluetooth: L2CAP socket layer initialized\n[   12.847848] Bluetooth: SCO socket layer initialized\n...\n[   12.924043] usbcore: registered new interface driver btusb\n[   12.934914] Bluetooth: hci0: Legacy ROM 2.x revision 5.0 build 25 week 20 2015\n[   12.936183] Bluetooth: hci0: Intel Bluetooth firmware file: intel/ibt-hw-37.8.10-fw-22.50.19.14.f.bseq\n...\n[   14.440042] Bluetooth: hci0: Intel BT fw patch 0x43 completed &amp; activated\n...\n[   16.872207] Bluetooth: BNEP (Ethernet Emulation) ver 1.3\n[   16.872215] Bluetooth: BNEP filters: protocol multicast\n[   16.872223] Bluetooth: BNEP socket layer initialized\n[   16.876155] Bluetooth: MGMT ver 1.22\n...\n[   22.338512] Bluetooth: RFCOMM TTY layer initialized\n[   22.338525] Bluetooth: RFCOMM socket layer initialized\n[   22.338532] Bluetooth: RFCOMM ver 1.11\n</code></pre> <p>Given the above, it may be possible to use the PlayStation Dual Shock 4 controller.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#troubleshooting","title":"Troubleshooting","text":"<p>The following issue arose several months after the initial install.</p>"},{"location":"blog/2024/12/27/ubuntu-studio-2404-on-raven-gaming-pc-and-more/#active-state-power-management-on-pcie","title":"Active State Power Management on PCIe","text":"<p>On July 27, 2025 at 10:45, after the system had been in used for nearly a week, updated and rebooted multiple times, and was last powered off at 01:18, <code>dmesg</code> started showing the following message once or twice every 5-10 minutes:</p> <pre><code>[ 8633.714962] pcieport 0000:00:01.3: AER: Multiple Correctable error message received from 0000:26:00.0\n[ 8633.715004] pcieport 0000:20:05.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)\n[ 8633.715006] pcieport 0000:20:05.0:   device [1022:43c7] error status/mask=00000041/00002000\n[ 8633.715009] pcieport 0000:20:05.0:    [ 0] RxErr                  (First)\n[ 8633.715012] pcieport 0000:20:05.0:    [ 6] BadTLP                \n[ 8633.715020] iwlwifi 0000:26:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)\n[ 8633.715023] iwlwifi 0000:26:00.0:   device [8086:24fb] error status/mask=00000001/00002000\n[ 8633.715025] iwlwifi 0000:26:00.0:    [ 0] RxErr                  (First)\n[ 8633.715028] iwlwifi 0000:26:00.0: AER:   Error of this Agent is reported first\n</code></pre> <p>The same message had been logged a couple of times a few days earlier (on July 22), as found in <code>/var/log/kern.log.1</code></p> <pre><code>pcieport 0000:00:01.3: AER: Multiple Correctable error message received from 0000:26:00.0\npcieport 0000:20:05.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)\npcieport 0000:20:05.0:   device [1022:43c7] error status/mask=00000041/00002000\npcieport 0000:20:05.0:    [ 0] RxErr                  (First)\npcieport 0000:20:05.0:    [ 6] BadTLP                \niwlwifi 0000:26:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Receiver ID)\niwlwifi 0000:26:00.0:   device [8086:24fb] error status/mask=00000001/00002000\niwlwifi 0000:26:00.0:    [ 0] RxErr                  (First)\niwlwifi 0000:26:00.0: AER:   Error of this Agent is reported first\nwlp38s0: deauthenticated from 62:22:54:ed:bb:f2 (Reason: 6=CLASS2_FRAME_FROM_NONAUTH_STA)\nwlp38s0: authenticate with 62:22:54:ed:bb:f2 (local address=d4:3b:04:e3:ae:9a)\nwlp38s0: send auth to 62:22:54:ed:bb:f2 (try 1/3)\npcieport 0000:00:01.3: AER: Multiple Correctable error message received from 0000:26:00.0\niwlwifi 0000:26:00.0: PCIe Bus Error: severity=Correctable, type=Physical Layer, (Transmitter ID)\niwlwifi 0000:26:00.0:   device [8086:24fb] error status/mask=00001041/00002000\niwlwifi 0000:26:00.0:    [ 0] RxErr                  (First)\niwlwifi 0000:26:00.0:    [ 6] BadTLP                \niwlwifi 0000:26:00.0:    [12] Timeout               \nwlp38s0: deauthenticated from 62:22:54:ed:bb:f2 (Reason: 6=CLASS2_FRAME_FROM_NONAUTH_STA)\nwlp38s0: authenticate with 62:22:54:ed:bb:f2 (local address=d4:3b:04:e3:ae:9a)\nwlp38s0: send auth to 62:22:54:ed:bb:f2 (try 1/3)\nwlp38s0: authenticated\nwlp38s0: associate with 62:22:54:ed:bb:f2 (try 1/3)\nwlp38s0: RX AssocResp from 62:22:54:ed:bb:f2 (capab=0x131 status=0 aid=5)\n</code></pre> <p>PCIe device <code>0000:26</code> is the on-board WiFi controller, which would explain those de/auth events that follow immediately after the AER error messages:</p> <pre><code>$ lspci | grep '2[06]:0'\n20:00.0 PCI bridge: Advanced Micro Devices, Inc. [AMD] 400 Series Chipset PCIe Port (rev 01)\n20:01.0 PCI bridge: Advanced Micro Devices, Inc. [AMD] 400 Series Chipset PCIe Port (rev 01)\n20:04.0 PCI bridge: Advanced Micro Devices, Inc. [AMD] 400 Series Chipset PCIe Port (rev 01)\n20:05.0 PCI bridge: Advanced Micro Devices, Inc. [AMD] 400 Series Chipset PCIe Port (rev 01)\n20:06.0 PCI bridge: Advanced Micro Devices, Inc. [AMD] 400 Series Chipset PCIe Port (rev 01)\n20:07.0 PCI bridge: Advanced Micro Devices, Inc. [AMD] 400 Series Chipset PCIe Port (rev 01)\n26:00.0 Network controller: Intel Corporation Dual Band Wireless-AC 3168NGW [Stone Peak] (rev 10)\n</code></pre> <p>While there appears to be no (findable) recorded history of <code>AER: Multiple Correctable error message received</code> happening on an <code>Intel Corporation Dual Band Wireless-AC 3168NGW</code>, there was a very similar one in [SOLVED] AER: Correctable error message received forum thread from Oct 26, 2024. The solution proposed there did seem to work: update <code>/etc/default/grub</code> to add <code>pcie_aspm=off</code> to the <code>GRUB_CMDLINE_LINUX_DEFAULT</code> variable:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"noquiet nosplash pcie_aspm=off\"\n</code></pre> <p>After rebooting the system with <code>pcie_aspm=off</code> the above errors have not shown up again.</p>"},{"location":"blog/2024/12/28/continuous-monitoring-for-tp-link-tapo-devices/","title":"Continuous Monitoring for TP-Link Tapo devices","text":"<p>In an old house with aging electrical wiring and a limited power contract, keeping power consumption in check is quite necessary and can be a bit of a challenge. Some appliances are very power-hungry for short periods of time, at unpredictable times throughout the day, while others are running constantly and add up to a baseline that quietly takes a chunk of the power budget.</p> <p>A decent way to keep an eye on power consumption is offered by the TP-Link Tapo line of products, in particular their smart plugs with energy monitoring and temperature and humidity sensors. These devices are relatively easy to setup, reliable, discrete and not too expensive... although they do add up fast!</p> <p>For all the smart features in these devices and the companion app, there is no way to have a panoramic view of aggregated power consumption broken down by device, or to configure thresholds based on the aggregated power consumption from all appliances. Such panoramic view was not hard to implement by building on the Continuous Monitoring solution previously built for monigoring computing resources (already monitoring temperatures and power consumption).</p> <p></p>"},{"location":"blog/2024/12/28/continuous-monitoring-for-tp-link-tapo-devices/#monitoring-for-tapo-devices","title":"Monitoring for Tapo devices","text":"<p>Monitoring of these devices can be implemented, at least in Rust or Python, using the Unofficial Tapo API Client at github.com/mihai-dinculescu/tapo. This library has been tested with many devices, including all those already in use in this house and those that may be used in the future:</p> <ul> <li>H100 Smart Hub with Chime located in the office, where its powered by an UPS     for 24x7 availability and very conveniently close to me, so I can hear it chime. </li> <li>T100 Smart Motion Sensor hanging around the front door, to alert me when someone is     coming up the house, in case I don't hear the bell.</li> <li>T310 Smart Temperature &amp; Humidity Sensor, needed in most rooms around the house,     because in a house with multiple floors and different exposure to sun and wind,     temperature and humidity do vary greatly across rooms.</li> <li>P100 Mini Smart Wi-Fi Socket without Energy Monitoring, ordered by mistake     and not yet of great use; there are no low-power appliances wanting remote control.</li> <li>P110 Mini Smart Wi-Fi Socket, Energy Monitoring, slightly cheaper and bulkier than...</li> <li>P115 Mini Smart Wi-Fi Socket, Energy Monitoring, by far my favorite monitoring and     remote control device, slightly more expensive than the P110 but fits anywhere.</li> <li>P304M Smart Wi-Fi Power Strip, Energy Monitoring, apparently only available in the UK     as of the end of 2024 but hopefully available in more regions soon. This would make     a great replacement for P110/P115 devices in a few places where multiple appliances     require separate remote control.</li> <li>L900-5 Smart Wi-Fi Light Strip (5 m.), or maybe even</li> <li>L930-5 Smart Wi-Fi Light Strip, Multicolor may be the one to brighten up my office. </li> </ul> <p>Note</p> <p>The Python library under  <code>tapo-py</code> does not (yet) support a few interesting features of the H100 Smart Hub with Chime, namely those that would allow sounding (and stopping) alarms.</p> <p>The Python library can be installed via <code>pip</code>, along with a couple of other Python dependencies.</p>"},{"location":"blog/2024/12/28/continuous-monitoring-for-tp-link-tapo-devices/#smart-actions","title":"Smart Actions","text":"<p>The companion app supports creating Smart Actions to make these devices work together, such as making the hub sound an alarm when the motion sensor is triggered, or turning a plug on/off when the temperature and/or humidity in a room crosses a threshold.</p> <p>Although this is not connected directly with the monitoring of power consumption, it inspires me to think of smarter actions:</p> <ul> <li>Turn off the boilder if total power use exceeds a threshold, so the total power     consumption won't exceed the limited power contract.</li> <li>Sound a chime when power consumption from an appliance matches a specific pattern,     this can be used to detect when the washing machine has finished.</li> <li>Sound an alarm when total power consumption crosses a threshold, because this can     and does happen surprisingly often (a few times per day).</li> </ul> <p>These may lead me to go down the rabbit hole of Home Assistant, although support for Tapo devices in the TP-Link Smart Home integration seems to be missing all sensors, those are supported only by a separate integration at github.com/petretiandrea/home-assistant-tapo-p100.</p>"},{"location":"blog/2024/12/28/continuous-monitoring-for-tp-link-tapo-devices/#implementation","title":"Implementation","text":"<p>Tapo devices sample values every 2 seconds, although this is only clearly advertised for the temperature and humidity sensors. However, for a simple start, sampling once per minute will be enough and makes it possible to run the monitoring script simply from root's crontab:</p> <pre><code>root@raven:~# crontab -l\n# m h  dom mon dow   command\n  * * * * * /usr/local/bin/conmon-tapo.py\n</code></pre> <p>Thus the <code>conmon-tapo.py</code> script reports temperature, humidity and power use every minute, for those devices listed in the a configuration file like <code>tapo.yaml</code>.</p> <p>The code is based on several Python examples under <code>tapo-py/examples</code>, in particular:</p> <ul> <li><code>get_child_device_list()</code>     to get the list of devices paired with a hub, e.g. temperature and humidity sensors.</li> <li><code>get_temperature_humidity_records()</code>     to get values from temperature and humidity sensors.</li> <li><code>get_current_power()</code>     to get values from smart plugs with energy monitoring.</li> </ul> <p><code>conmon-tapo.py</code> runs through 4 steps each time:</p> <ol> <li>Load a list of devices (with IP addresses) and other settings from a configuration file.</li> <li>Poll the devices for their latest values. Skip those that are unreadable.</li> <li>Synthesize measurements for fake devices; as encoded in the configuration file.</li> <li>Post measurements to an InfluxDB server,     tagged with the hostname to     differentiate devices across sites.</li> </ol> <p>The tapo app allows creating multiple \"homes\" and \"rooms\" to track the location of each device, but these attributes are not included in the device information returned by <code>get_device_info()</code>.</p> Expand to see an example <code>device_info</code> result. <pre><code>Device info: {\n    \"at_low_battery\": false,\n    \"avatar\": \"balcony\",\n    \"bind_count\": 1,\n    \"category\": \"subg.trigger.temp-hmdt-sensor\",\n    \"current_humidity\": 81,\n    \"current_humidity_exception\": 21,\n    \"current_temp\": 13.199999809265137,\n    \"current_temp_exception\": -6.800000190734863,\n    \"device_id\": \"802EB916B90AEB800E30246DF68C303322310766\",\n    \"fw_ver\": \"1.5.0 Build 230105 Rel.150707\",\n    \"hw_id\": \"2AE1228C7CE042A310FC70EA70D7A788\",\n    \"hw_ver\": \"1.0\",\n    \"jamming_rssi\": -116,\n    \"jamming_signal_level\": 1,\n    \"lastOnboardingTimestamp\": 1723047115,\n    \"mac\": \"98254A51E05C\",\n    \"nickname\": \"Exterior\",\n    \"oem_id\": \"02F7CFFC203A7E622F6EA84BBB74C68F\",\n    \"parent_device_id\": \"802D84BECDA15417AA1BD7CF881AEE6622451CF8\",\n    \"region\": \"Europe/Madrid\",\n    \"report_interval\": 16,\n    \"rssi\": -86,\n    \"signal_level\": 1,\n    \"specs\": \"EU\",\n    \"status\": \"online\",\n    \"status_follow_edge\": false,\n    \"temp_unit\": \"celsius\",\n    \"type\": \"SMART.TAPOSENSOR\"\n}\n</code></pre>"},{"location":"blog/2024/12/28/continuous-monitoring-for-tp-link-tapo-devices/#failing-gracefully","title":"Failing gracefully","text":"<p>The Python library relies on client applications to handle any and all exceptions, which can sometimes lead to very crypt stack traces. See Device IP dependencies for those encountered so far, Trying to read a P115 like it was an H100 produced a particularly unique stack trace.</p> <p>Such exceptions can be triggered by power loss (devide not reachable) or their IP addresses changing (device refusing connection or causing other errors). Since all those are events that do happen and not entirely under control, the script has a very largey <code>try-expect-else</code> block to simply skip devices whenever fetching data fails. If the device was only temporarly unavailable, its values will be fetched next time.</p> <p>When devices have their IP address changed, it is necessary to update the configuration files to update the relevant <code>devices[].ip</code> values. There is a feature request to Support client.getDeviceList so that devices can be detected when their IP addresses change; in the mean time, this comment provides a few ideas to implement device discovery.</p>"},{"location":"blog/2024/12/28/continuous-monitoring-for-tp-link-tapo-devices/#always-on-appliances","title":"Always-on appliances","text":"<p>There are several appliances in the house that operate essentially 24x7 with a fairly constant power consumption. A common example in humid climates are dehumidifiers running constantly, with a power consumption typically between 150 and 300 W. Computers also have relativly constant power consumption, except during periods of high CPU or GPU load.</p> <p>Instead of having all these appliances actually monitored with a dedicated smart plug with energy monitoring (P110 or P115), they can be hard-coded in the configuration file under <code>always_on</code>.</p> <p>This can be used for appliances that go on and off at regular intervals, such as a refrigerator that runs the compressor for about 10 minutes every 20-30 minutes, consuming about 80 W each time. This is only the low-power cycle to maintain the internal temperature. When the refrigerator is not opened; the compressor will run for a longer time and/or draw more power (about 400 W) when the refrigerator is refilled with groceries that are not cold.</p>"},{"location":"blog/2024/12/28/continuous-monitoring-for-tp-link-tapo-devices/#rarely-on-appliances","title":"Rarely-on appliances","text":"<p>Smaller appliances that are used sporadically are not monitored at all, but they also tend to be significantly power-hungry (especially in the kitchen). This is where the available power metric comes in handy: substract the total current power consumption from the contracted power rate, then compare this difference with what the appliance will need.</p> <p>What what will appliance X need? This can be measured with one of the P110 or P115 plugs once or twice to get a reading of each of these appliance's typical power needs, then a simple paper note stuck on the appliance with the number should serve as a reminder to check whether there is currently enough power available.</p>"},{"location":"blog/2024/12/28/continuous-monitoring-for-tp-link-tapo-devices/#adjustable-appliances","title":"Adjustable appliances","text":"<p>Electric heaters, hair dryers and other power-hungry, adjustable appliances would need a slightly bigger paper note stuck on them to indicate what their power consumption is depending on each adjustable setting. Some appliances don't really need this though, for instance a small toaster will draw the same power (e.g. 750 W) while it's on, its adjustment changing only the duration.</p> <p>This should be practical enough thanks to how well the dashboard shows on mobile screens: </p> <p></p>"},{"location":"blog/2024/12/28/continuous-monitoring-for-tp-link-tapo-devices/#a-more-long-term-database","title":"A more long-term database","text":"<p>InfluxDB database for Continuous Monitoring was set up with a `30 days retention policy which would not be enough to capture long-term trends from sensors data. At times it was found also slightly too, a more generous 90 days should do nicely, after all the current database takes only 1.3 GB of disk space.</p> <p>For the monitoring of temperature, humidity and power consumption, create a new database (e.g. <code>home</code>) with a much larger retention policy of 5 years:</p> <pre><code>$ influx -host localhost -port 30086 -username admin -password xxxxxxxxxx\nConnected to http://localhost:30086 version 1.8.10\nInfluxDB shell version: 1.6.7~rc0\n\n&gt; CREATE DATABASE home\n&gt; USE home\nUsing database home\n&gt; CREATE RETENTION POLICY \"5_years\" ON \"home\" DURATION 2000d REPLICATION 1\n&gt; ALTER RETENTION POLICY \"5_years\" on \"home\" DURATION 2000d REPLICATION 1 DEFAULT\n&gt; SHOW RETENTION POLICIES ON home\nname    duration   shardGroupDuration replicaN default\n----    --------   ------------------ -------- -------\nautogen 0s         168h0m0s           1        false\n5_years 48000h0m0s 168h0m0s           1        true\n\n&gt; USE monitoring\nUsing database monitoring\n&gt; CREATE RETENTION POLICY \"90_days\" ON \"monitoring\" DURATION 90d REPLICATION 1\n&gt; ALTER RETENTION POLICY \"90_days\" on \"monitoring\" DURATION 90d REPLICATION 1 DEFAULT\n&gt; SHOW RETENTION POLICIES ON monitoring\nname    duration  shardGroupDuration replicaN default\n----    --------  ------------------ -------- -------\nautogen 0s        168h0m0s           1        false\n90_days 2160h0m0s 24h0m0s            1        true\n</code></pre> <p>Warning</p> <p>Do NOT <code>DROP RETENTION POLICY</code> before copying measurements over to the new one, otherwise <code>DROP RETENTION POLICY</code> will permanently delete all measurements and data stored in the retention policy.</p> <p>With the new database ready, all that is left to do is update the configuration file to set <code>influxdb.database: \"home\"</code> and the script will immediately start pushing measurements to the new database, because the configuration file is reloaded on every run.</p>"},{"location":"blog/2024/12/28/continuous-monitoring-for-tp-link-tapo-devices/#copy-measurements-over","title":"Copy measurements over","text":"<p>Old measurements are still in the <code>monitoring</code> database, leaving the graphs empty until new data fills them back in. In the mean time, it is possible to copy measurments across databases using the INTO clause:</p> <pre><code>$ influx -host localhost -port 30086 -username admin -password xxxxxxxxxx\nConnected to http://localhost:30086 version 1.8.10\nInfluxDB shell version: 1.6.7~rc0\n\n&gt; USE home\nUsing database home\n\n&gt; show MEASUREMENTS\nname: measurements\nname\n----\ninet_down\ninet_ping\ninet_up\ntapo_current_power\ntapo_humidity\ntapo_temperature\n\n&gt; SELECT * INTO \"home\".\"5_years\".\"tapo_current_power\" FROM \"monitoring\".\"90_days\".\"tapo_current_power\" GROUP BY *\nname: result\ntime written\n---- -------\n0    210\n\n&gt; SELECT * INTO \"home\".\"5_years\".\"tapo_humidity\" FROM \"monitoring\".\"90_days\".\"tapo_humidity\" GROUP BY *\nname: result\ntime written\n---- -------\n0    85\n\n&gt; SELECT * INTO \"home\".\"5_years\".\"tapo_temperature\" FROM \"monitoring\".\"90_days\".\"tapo_temperature\" GROUP BY *\nname: result\ntime written\n---- -------\n0    85\n</code></pre> <p>That should make the charts look less empty, having copied data back to 12/28 23:00:35.</p> <p>Note</p> <p>If that seems like a suspiciously small amount of data, it's because it is. See that warning up there about not dropping a retention policy? Ask me how I know.</p>"},{"location":"blog/2024/12/28/continuous-monitoring-for-tp-link-tapo-devices/#import-older-historical-data","title":"Import older historical data","text":"<p>It is possible to export history data from Tapo temperature &amp; humidity sensor and receive CSV files via e-mail. In terms of retention policy and granularity, data is stored for up to 2 calendar years but when the interval is set to 1 minute, only the data for the most recent month can be exported. Still good enough to see seasonal patterns.</p> <p>There is also an option to export energy monitoring data, but it is much more limited: \"all\" energy data that can be exported (via e-mail in <code>.xls</code> files) is limited to the last 24 hours in 15-minute intervals and the last 7 days in 1-hour intervals. </p> <p>Both options to export data send files that do not contain any reference as to which device the data is from. It is thus necessary to organize those files in some way that allows recovering those details later; each file needs to be mapped to the <code>model</code> and <code>nickname</code> of the relevant device. Since the <code>model</code> is only a single word, the files can be stored in directories named after the <code>model</code> and <code>nickname</code> separated by a blank space, e.g.</p> <pre><code>T315 Exterior/\n    1-min.csv  15-min.csv\nP115 PC/\n    'Energy Usage.xls'   Power.xls\n</code></pre> <p>This will require a new script to load data from these CSV and XLS files and push measurements to InfluxDB. For the time being, having exported all available data once the above setup is stable, this can be left for later as part of...</p>"},{"location":"blog/2024/12/28/continuous-monitoring-for-tp-link-tapo-devices/#future-improvements","title":"Future improvements","text":"<p>These are ideas to implement in the future, very roughly sorted by priority:</p> <ol> <li>Create script to react to the total power consumption going above a threshold by     turning off the less critical appliances (e.g. a boiler) to keep the more critical     appliances running.</li> <li>Add monitoring for devices with <code>at_low_battery: true</code> (poll daily).</li> <li>Ingest historical data from exported CSV and XLS files from the Tapo app.</li> <li>Add support daily periods on <code>always_on</code> (e.g. always on during the night).<ul> <li>P100 Mini Smart Wi-Fi Socket without Energy Monitoring devices may be useful     to make a couple of appliances go off/on at a fixed schedule.</li> </ul> </li> <li>Make the script adapt its reporting rate; but report only very 1 minute when     there are no changes, to skip repeated values within each sensible tolerances.<ul> <li>T310 Smart Temperature &amp; Humidity Sensor specifies Accuracy: \u00b10.3 \u00b0C, \u00b13% RH     but monitoring so far shows the sensors are precise enough to keep     measurements steady over time, so we can use the reporting accuracy of     \u00b10.1 \u00b0C, \u00b11% RH.</li> <li>P115 Mini Smart Wi-Fi Socket, Energy Monitoring does not specify the accuracy of     measurements, so again we can use the reporting accuracy of 1 W for values     under 1000 W and 10 W for values above 1000 W, of simply 10 W for all values.</li> </ul> </li> <li>Make the script adapt its polling rate; start polling every 2 seconds,     then adjust to each device's <code>report_interval</code>.</li> <li>Add discovery of new devices to cope with IP address changes.</li> <li>Create script to export minimal data over to Pi Pico or ESP32.</li> <li>Find a way to update the list of <code>always_on</code> devices on demand.</li> </ol>"},{"location":"blog/2024/12/31/migrating-unifi-controller-to-kubernetes/","title":"Migrating UniFi Controller to Kubernetes","text":"<p>The old UniFi Controller and its required Mongo DB have been a bit of a hassle to keep updated while running directly on the host OS in my little homelab server, so the time has come to migrate this to the new linuxserver.io/docker-unifi-network-application on my little Kubernetes cluster on my new Kubernetes cluster.</p> <p>Warning</p> <p>Beware of outdated documentation, most articles out there like Install Unifi Controller on Kubernetes, are based on the deprecated linuxserver/unifi-controller, while others like setting up the UniFi Network Controller using Docker are using jacobalberty/unifi-docker which was quite outdated until recently.</p>"},{"location":"blog/2024/12/31/migrating-unifi-controller-to-kubernetes/#deploy-the-unifi-network-application","title":"Deploy the Unifi Network Application","text":""},{"location":"blog/2024/12/31/migrating-unifi-controller-to-kubernetes/#system-requirements","title":"System requirements","text":"<p>The Unifi Network Application requires a Mongo DB backend and both will need writeable directories and a dedicated user:</p> <pre><code># groupadd unifi -g 119\n# useradd  unifi -u 119 -g 119 -s /usr/sbin/nologin\n# mkdir -p /home/k8s/unifi/config /home/k8s/unifi/mongodb\n# vi /home/k8s/unifi/init-mongo.sh\n# chown -R unifi:unifi /home/k8s/unifi\n# ls -lan /home/k8s/unifi\ntotal 4\ndrwxr-xr-x 1 119 119  52 Dec 31 16:06 .\ndrwxr-xr-x 1   0   0 264 Dec 31 16:05 ..\ndrwxr-xr-x 1 119 119   0 Dec 31 16:05 config\n-rw-r--r-- 1 119 119 425 Dec 31 16:06 init-mongo.sh\ndrwxr-xr-x 1 119 119   0 Dec 31 16:05 mongodb\n</code></pre> <p>Note the UID/GID (119) to be used later.</p> <p>Create the script <code>/home/k8s/unifi/init-mongo.sh</code> using the exact content from the  Setting Up Your External Database documentation of linuxserver/unifi-network-application:</p> /home/k8s/unifi/init-mongo.sh<pre><code>#!/bin/bash\n\nif which mongosh &gt; /dev/null 2&gt;&amp;1; then\n  mongo_init_bin='mongosh'\nelse\n  mongo_init_bin='mongo'\nfi\n\"${mongo_init_bin}\" &lt;&lt;EOF\nuse ${MONGO_AUTHSOURCE}\ndb.auth(\"${MONGO_INITDB_ROOT_USERNAME}\", \"${MONGO_INITDB_ROOT_PASSWORD}\")\ndb.createUser({\n  user: \"${MONGO_USER}\",\n  pwd: \"${MONGO_PASS}\",\n  roles: [\n    { db: \"${MONGO_DBNAME}\", role: \"dbOwner\" },\n    { db: \"${MONGO_DBNAME}_stat\", role: \"dbOwner\" }\n  ]\n})\nEOF\n</code></pre>"},{"location":"blog/2024/12/31/migrating-unifi-controller-to-kubernetes/#kubernetes-deployment","title":"Kubernetes deployment","text":"<p>There is no ready-to-use Kubernetes deployment in the documentation of linuxserver/unifi-network-application, or anywhere else I could find. The following deployment is based on the recommended docker-compose and parts of previous Kubernetes deployments:</p> <ul> <li>Plex Media Server     mounts multiple directories and exposes multiple TCP and UDP ports.</li> <li>InfluxDB and Grafana     has services that depend on conneting with others over HTTP.</li> <li>Audiobookshelf     has the <code>websocket</code> requirement.</li> <li>Kubernetes Dashboard     enables HTTPS in the backend and disables TLS validation.</li> </ul> <p>In addition to deploying the right set of objects, there are very specific requirements in terms of which version of MongoDB can be used depending on the version of the UniFi Network Application that is deployed. Check the correct pairs of versions under the Additional information section of the latest release of UniFi Network Application in the latest linuxserver/unifi-network-application release; e.g. 9.0.114 specifies that Version 9.0 and newer supports up to MongoDB 8.0 and those are the versions used here.</p> UniFi Network Application deployment. unifi-network-app.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: unifi\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: mongo-pv-data\n  namespace: unifi\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/unifi/mongodb\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: mongo-pv-init\n  namespace: unifi\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/unifi/init-mongo.sh\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mongo-pvc-data\n  namespace: unifi\nspec:\n  storageClassName: manual\n  volumeName: mongo-pv-data\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mongo-pvc-init\n  namespace: unifi\nspec:\n  storageClassName: manual\n  volumeName: mongo-pv-init\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  namespace: unifi\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      containers:\n      - image: docker.io/mongo:8.0.0\n        imagePullPolicy: IfNotPresent\n        name: mongo\n        env:\n        - name: \"MONGO_AUTHSOURCE\"\n          value: \"admin\"\n        - name: \"MONGO_DBNAME\"\n          value: \"unifi\"\n        - name: \"MONGO_INITDB_ROOT_USERNAME\"\n          value: \"root\"\n        - name: \"MONGO_INITDB_ROOT_PASSWORD\"\n          value: \"*************************\"\n        - name: \"MONGO_PASS\"\n          value: \"*************************\"\n        - name: \"MONGO_USER\"\n          value: \"unifi\"\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - mountPath: /data/db\n          name: mongo-data\n        - mountPath: /docker-entrypoint-initdb.d/init-mongo.sh\n          name: mongo-init\n      securityContext:\n        runAsUser: 119\n        runAsGroup: 119\n      volumes:\n      - name: mongo-data\n        persistentVolumeClaim:\n          claimName: mongo-pvc-data\n      - name: mongo-init\n        persistentVolumeClaim:\n          claimName: mongo-pvc-init\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: mongo\n  name: mongo-svc\n  namespace: unifi\nspec:\n  ports:\n  - port: 27017\n    protocol: TCP\n    targetPort: 27017\n    nodePort: 32717\n  selector:\n    app: mongo\n  type: NodePort\n\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: unifi-pv-config\n  namespace: unifi\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/unifi/config\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: unifi-pvc-config\n  namespace: unifi\nspec:\n  storageClassName: manual\n  volumeName: unifi-pv-config\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: unifi\n  namespace: unifi\nspec:\n  selector:\n    matchLabels:\n      app: unifi\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  template:\n    metadata:\n      labels:\n        app: unifi\n    spec:\n      containers:\n      - image: lscr.io/linuxserver/unifi-network-application:9.0.114\n        imagePullPolicy: IfNotPresent\n        name: unifi\n        env:\n        - name: \"PUID\"\n          value: \"119\"\n        - name: \"PGID\"\n          value: \"119\"\n        - name: TZ\n          value: Europe/Amsterdam\n        - name: \"MONGO_AUTHSOURCE\"\n          value: \"admin\"\n        - name: \"MONGO_DBNAME\"\n          value: \"unifi\"\n        - name: \"MONGO_PASS\"\n          value: \"*************************\"\n        - name: \"MONGO_USER\"\n          value: \"unifi\"\n        - name: \"MONGO_HOST\"\n          value: \"mongo-svc\"\n        - name: \"MONGO_PORT\"\n          value: \"27017\"\n        ports:\n        - containerPort: 6789\n          name: mob-speedtest\n          protocol: TCP\n        - containerPort: 8080\n          name: device-inform\n          protocol: TCP\n        - containerPort: 8443\n          name: web-ui\n          protocol: TCP\n        - containerPort: 1900\n          name: ssdp\n          protocol: UDP\n        - containerPort: 3478\n          name: stun\n          protocol: UDP\n        - containerPort: 10001\n          name: ap-discovery\n          protocol: UDP\n        volumeMounts:\n        - mountPath: /config\n          name: unifi-config\n      volumes:\n      - name: unifi-config\n        persistentVolumeClaim:\n          claimName: unifi-pvc-config\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: unifi-tcp\n  namespace: unifi\n  annotations:\n    metallb.universe.tf/allow-shared-ip: unifi\nspec:\n  type: LoadBalancer\n  loadBalancerIP: 192.168.0.173\n  ports:\n  - name: mob-speedtest\n    protocol: TCP\n    port: 6789\n    targetPort: 6789\n  - name: device-inform\n    protocol: TCP\n    port: 8080\n    targetPort: 8080\n  - name: web-ui\n    protocol: TCP\n    port: 8443\n    targetPort: 8443\n  selector:\n    app: unifi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: unifi-udp\n  namespace: unifi\n  annotations:\n    metallb.universe.tf/allow-shared-ip: unifi\nspec:\n  type: LoadBalancer\n  loadBalancerIP: 192.168.0.173\n  ports:\n    - name: stun\n      protocol: UDP\n      port: 3478\n      targetPort: 3478\n    - name: ap-discovery\n      protocol: UDP\n      port: 10001\n      targetPort: 10001\n    - name: ssdp\n      protocol: UDP\n      port: 1900\n      targetPort: 1900\n  selector:\n    app: unifi\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: unifi-ingress\n  namespace: unifi\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: unifi-tcp\n    nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"false\"\n    nginx.ingress.kubernetes.io/whitelist-source-range: 10.244.0.0/16\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: uni.ssl.uu.am\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: unifi-tcp\n                port:\n                  number: 8443\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - uni.ssl.uu.am\n</code></pre>"},{"location":"blog/2024/12/31/migrating-unifi-controller-to-kubernetes/#troubleshooting","title":"Troubleshooting","text":"<p>Putting this deployment together was a bit of a process, so here are some notes and warnings about the troubles that had to be sorted out along the way:</p> <p>Warning</p> <p>Double-check that the Unifi Application data directory is mounted exactly on <code>/config</code>; otherwise the application will create an ephemeral directory that will be discarded when restarting the pod. Once this data is lost, the application has to be setup again, and every access point adopted since the last backup has to be factory reset to be readopted.</p> This Unifi image does not support running rootless. <p>Attempting to set <code>securityContext</code> (as is done for the <code>mongodb</code> image) will result in fatal errors and crash-loop:</p> <pre><code>s6-overlay-suexec: warning: unable to gain root privileges (is the suid bit set?)\ns6-mkdir: warning: unable to mkdir /run/s6: Permission denied\ns6-mkdir: warning: unable to mkdir /run/service: Permission denied\ns6-overlay-suexec: fatal: child failed with exit code 111\n</code></pre> This image does not seem to use the <code>system.properties</code> files. <p>The <code>~MONGO_...~</code> strings in the <code>system.properties</code> files should be replaced with the values of the environment variables set in the deployment, but they are not:</p> <pre><code>$ kubectl -n unifi exec \\\n  $(kubectl get pods -n unifi | grep unifi | cut -f1 -d' ') \\\n  -- cat /defaults/system.properties | grep MONGO\ndb.mongo.uri=mongodb://~MONGO_USER~:~MONGO_PASS~@~MONGO_HOST~:~MONGO_PORT~/~MONGO_DBNAME~?tls=~MONGO_TLS~~MONGO_AUTHSOURCE~\nstatdb.mongo.uri=mongodb://~MONGO_USER~:~MONGO_PASS~@~MONGO_HOST~:~MONGO_PORT~/~MONGO_DBNAME~_stat?tls=~MONGO_TLS~~MONGO_AUTHSOURCE~\nunifi.db.name=~MONGO_DBNAME~\n\n$ kubectl -n unifi exec \\\n  $(kubectl get pods -n unifi | grep unifi | cut -f1 -d' ') \\\n  -- cat /config/data/system.properties\ncat: /config/data/system.properties: No such file or directory\ncommand terminated with exit code 1\n</code></pre> <p>Yet the environment variables are correctly set in the running pod:</p> <pre><code>$ kubectl -n unifi exec \\\n  $(kubectl get pods -n unifi | grep unifi | cut -f1 -d' ') \\\n  -- printenv | grep MONGO\nMONGO_PORT=27017\nMONGO_PASS=*************************\nMONGO_USER=unifi\nMONGO_HOST=mongo-svc\nMONGO_AUTHSOURCE=admin\nMONGO_DBNAME=unifi\nMONGO_SVC_SERVICE_PORT=27017\nMONGO_SVC_PORT_27017_TCP_ADDR=10.104.94.112\nMONGO_SVC_PORT=tcp://10.104.94.112:27017\nMONGO_SVC_PORT_27017_TCP_PROTO=tcp\nMONGO_SVC_SERVICE_HOST=10.104.94.112\nMONGO_SVC_PORT_27017_TCP=tcp://10.104.94.112:27017\nMONGO_SVC_PORT_27017_TCP_PORT=27017\n\n$ kubectl -n unifi exec \\\n  $(kubectl get pods -n unifi | grep unifi | cut -f1 -d' ') \\\n  -- cat /run/s6/container_environment/MONGO_PORT\n27017\n</code></pre> Pay close attention to how the pods are connected. <p>Misconfiguration in either the Mongo DB service port or the <code>MONGO_HOST</code> value in the Unify deployment can easily lead to the Unifi application failing to start because it's not able to connect to MondoDB:</p> <pre><code>$ kubectl -n unifi logs \\\n  $(kubectl get pods -n unifi | grep unifi | cut -f1 -d' ') -f\n[migrations] started\n[migrations] no migrations found\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n      \u2588\u2588\u2557     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557\n      \u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\n      \u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\n      \u2588\u2588\u2551     \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\n      \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n      \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d\n\n  Brought to you by linuxserver.io\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nTo support LSIO projects visit:\nhttps://www.linuxserver.io/donate/\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGID/UID\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nUser UID:    119\nUser GID:    119\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nLinuxserver.io version: 8.6.9-ls73\nBuild-date: 2024-12-24T17:37:56+00:00\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n*** Waiting for MONGO_HOST mongo-svc to be reachable. ***\n*** Defined MONGO_HOST mongo-svc is not reachable, cannot proceed. ***\n</code></pre> <p>The environment variables are correctly set in the running pod:</p> <pre><code>$ kubectl -n unifi exec \\\n  $(kubectl get pods -n unifi | grep unifi | cut -f1 -d' ') \\\n  -- nc -zv mongo-svc 27017\nnc: connect to mongo-svc (10.104.94.112) port 27017 (tcp) failed: Connection refused\ncommand terminated with exit code 1\n</code></pre> <p>The service is reachable instead of <code>mongo-svc:27017</code> only when the <code>mongo</code> deployment has <code>targetPort: 27017</code></p> <pre><code>$ kubectl -n unifi exec \\\n  $(kubectl get pods -n unifi | grep unifi | cut -f1 -d' ') \\\n  -- nc -zv mongo-svc.unifi 27017\nConnection to mongo-svc.unifi (10.104.94.112) 27017 port [tcp/*] succeeded!\n</code></pre> Clear the Mondo DB every time deployment fails. <p>Scripts in the <code>/docker-entrypoint-initdb.d</code> folder will be executed only if the database has never been initialized before. If the deployment fails at any point, delete the contents of <code>/home/k8s/unifi/mongodb</code> so right before reapplying the deployment.</p> <p>Otherwise, if the database has been previously incorrectly intialized, the user <code>unifi</code> will not be found and the <code>unifi</code> application will be constantly re-trying and logging authentication errors:</p> <pre><code>$ kubectl -n unifi logs \\\n  $(kubectl get pods -n unifi | grep unifi | cut -f1 -d' ') -f\n[migrations] started\n[migrations] no migrations found\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n      \u2588\u2588\u2557     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557\n      \u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\n      \u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\n      \u2588\u2588\u2551     \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\n      \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n      \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d\n\n  Brought to you by linuxserver.io\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nTo support LSIO projects visit:\nhttps://www.linuxserver.io/donate/\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGID/UID\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nUser UID:    119\nUser GID:    119\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nLinuxserver.io version: 8.6.9-ls73\nBuild-date: 2024-12-24T17:37:56+00:00\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n*** Waiting for MONGO_HOST mongo-svc.unifi to be reachable. ***\nGenerating 4,096 bit RSA key pair and self-signed certificate (SHA384withRSA) with a validity of 3,650 days\n        for: CN=unifi\n[custom-init] No custom files found, skipping...\nException in thread \"launcher\" java.lang.IllegalStateException: Tomcat failed to start up\nCaused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'mongoRuntimeService' defined in com.ubnt.service.db.CoreDatabaseSpringContext: Exception authenticating MongoCredential{mechanism=SCRAM-SHA-1, userName='unifi', source='admin', password=&lt;hidden&gt;, mechanismProperties=&lt;hidden&gt;}\nCaused by: com.mongodb.MongoSecurityException: Exception authenticating MongoCredential{mechanism=SCRAM-SHA-1, userName='unifi', source='admin', password=&lt;hidden&gt;, mechanismProperties=&lt;hidden&gt;}\nCaused by: com.mongodb.MongoCommandException: Command failed with error 18 (AuthenticationFailed): 'Authentication failed.' on server mongo-svc.unifi:27017. The full response is {\"ok\": 0.0, \"errmsg\": \"Authentication failed.\", \"code\": 18, \"codeName\": \"AuthenticationFailed\"}\n</code></pre> <p>The full response is in JSON format; it doesn't tell much:</p> <pre><code>{\n  \"ok\": 0,\n  \"errmsg\": \"Authentication failed.\",\n  \"code\": 18,\n  \"codeName\": \"AuthenticationFailed\"\n}\n</code></pre> <p>The reason behind the error is clearly stated in the Mondo DB logs, which are conveniently in JSON format so they can be piped to <code>jq</code>:</p> <p></p><pre><code>$ kubectl -n unifi logs \\\n  $(kubectl get pods -n unifi | grep mongo | cut -f1 -d' ') \\\n  | grep ACCESS | head -2 | jq\n</code></pre> <pre><code>{\n  \"t\": {\n    \"$date\": \"2024-12-31T20:18:09.557+00:00\"\n  },\n  \"s\": \"I\",\n  \"c\": \"ACCESS\",\n  \"id\": 20251,\n  \"ctx\": \"conn3\",\n  \"msg\": \"Supported SASL mechanisms requested for unknown user\",\n  \"attr\": {\n    \"user\": \"unifi@admin\"\n  }\n}\n{\n  \"t\": {\n    \"$date\": \"2024-12-31T20:18:09.558+00:00\"\n  },\n  \"s\": \"I\",\n  \"c\": \"ACCESS\",\n  \"id\": 20249,\n  \"ctx\": \"conn3\",\n  \"msg\": \"Authentication failed\",\n  \"attr\": {\n    \"mechanism\": \"SCRAM-SHA-256\",\n    \"speculative\": true,\n    \"principalName\": \"unifi\",\n    \"authenticationDatabase\": \"admin\",\n    \"remote\": \"10.244.0.85:53330\",\n    \"extraInfo\": {},\n    \"error\": \"UserNotFound: Could not find user \\\"unifi\\\" for db \\\"admin\\\"\"\n  }\n}\n</code></pre><p></p> <p>The user <code>unifi</code> was not found because the database was not initialized correctly, because it failed to initialize in a previous iteration, because the <code>mongo-init</code> volume containing the script was not mounted in the <code>mongo</code> container.</p> Make sure to enable HTTPS backend protocol in Nginx. <p>Otherwise, Nginx will be unable to connect to unifi because it will reject plain HTTP requests on port 8443:</p> <pre><code>Bad Request\nThis combination of host and port requires TLS.\n</code></pre> <p>The (better) solution is to enable HTTPS as the backend protocol and instruct Nginx to skip TLS certification validation. This is what the Kubernetes Dashboard deployment does as well; in fact that's where I found those 3 lines.</p> Do not bother trying to enable plain HTTP UI on port 8880. <p>Since UniFi 8.2 (at least) it is no longer possible to disable the HTTP to HTTPS redirect.</p>"},{"location":"blog/2024/12/31/migrating-unifi-controller-to-kubernetes/#final-result","title":"Final result","text":"<p>Apply the deployment and wait a few minutes for services to start:</p> <pre><code>$ kubectl apply -f unifi-network-app.yaml\nnamespace/unifi created\npersistentvolume/mongo-pv-data created\npersistentvolume/mongo-pv-init created\npersistentvolumeclaim/mongo-pvc-data created\npersistentvolumeclaim/mongo-pvc-init created\ndeployment.apps/mongo created\nservice/mongo-svc created\npersistentvolume/unifi-pv-config created\npersistentvolumeclaim/unifi-pvc-config created\ndeployment.apps/unifi created\nservice/unifi-tcp created\nservice/unifi-udp created\ningress.networking.k8s.io/unifi-ingress created\n</code></pre> <pre><code>$ kubectl get all -n unifi\nNAME                            READY   STATUS    RESTARTS   AGE\npod/cm-acme-http-solver-w26rm   1/1     Running   0          36s\npod/mongo-564774d869-dfk7h      1/1     Running   0          36s\npod/unifi-584f4847c7-vpthl      1/1     Running   0          36s\n\nNAME                TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                                         AGE\nservice/mongo-svc   NodePort       10.103.150.110   &lt;none&gt;          27017:32717/TCP                                 37s\nservice/unifi-tcp   LoadBalancer   10.105.232.48    192.168.0.173   6789:31231/TCP,8080:32034/TCP,8443:30909/TCP    37s\nservice/unifi-udp   LoadBalancer   10.108.54.45     192.168.0.173   3478:31805/UDP,10001:32694/UDP,1900:30234/UDP   37s\n\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/mongo   1/1     1            1           37s\ndeployment.apps/unifi   1/1     1            1           37s\n\nNAME                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/mongo-564774d869   1         1         1       37s\nreplicaset.apps/unifi-584f4847c7   1         1         1       37s\n</code></pre> <p>If all goes well, there will be no errors in the logs and the web UI will be available at https://uni.ssl.uu.am/</p> <pre><code>[migrations] started\n[migrations] no migrations found\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n      \u2588\u2588\u2557     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557\n      \u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\n      \u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\n      \u2588\u2588\u2551     \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\n      \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\n      \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d\n\n  Brought to you by linuxserver.io\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nTo support LSIO projects visit:\nhttps://www.linuxserver.io/donate/\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGID/UID\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nUser UID:    119\nUser GID:    119\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nLinuxserver.io version: 8.6.9-ls73\nBuild-date: 2024-12-24T17:37:56+00:00\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n*** Waiting for MONGO_HOST mongo-svc to be reachable. ***\nGenerating 4,096 bit RSA key pair and self-signed certificate (SHA384withRSA) with a validity of 3,650 days\n        for: CN=unifi\n[custom-init] No custom files found, skipping...\n[ls.io-init] done.\n</code></pre> <p></p> <p>From this point on, follow the documentation for UniFi - Backups and Migration to migrate the current site from the old controller to the new UniFi Network Application.</p>"},{"location":"blog/2024/12/31/migrating-unifi-controller-to-kubernetes/#december-2025-update-to-v10","title":"December 2025 Update to v10","text":"<p>Updating ther UniFi Network application across minor versions was a simple operation, performed twice (from 9.0.114 to 9.3.45 and then to 9.4.19 only two days later) when Migration to new ISP, without it being deemed worth documenting the process.</p> <p>Updating the UniFi Network application from 9.4.19 to the v10.x branch is a much more significant platform shift, thus worth documenting in more detail. As of late December 2025, version 10.0.160 is the established stable release for this branch. A direct upgrade from version 9.4.19 to 10.0.162 is supported, no intermediate updates are required for the application itself. </p> <p>While v10.0.160 is considered stable for general release. The newer release v10.0.162 reached stable status on December 9 and is highly recommended as it contains critical bug fixes for WiFi blackout schedules and WAN monitoring. The v10.x branch fully supports the current MongoDB 8.0.0 and Java 17/21 setup, so those component need not be updated.</p> <p>What is recommended, when updating across major versions of the UniFi Network application, is to make full backups of the local storage of both Mongo and UniFi Network:</p> <ol> <li> <p>In the UniFi Network application, go to Settings &gt; System &gt; Backups and download     a Settings Only backup file (<code>.unf</code>).</p> <ul> <li>If this takes too long, just check that the newest automatic backup under     <code>/home/k8s/unifi/config/data/backup/autobackup</code> is not too old.</li> </ul> </li> <li> <p>Stop the UniFi Network deployment, then the Mondo deployment.</p> <pre><code>$ kubectl scale -n unifi deployment unifi --replicas=0\n$ kubectl scale -n unifi deployment mongo --replicas=0\n</code></pre> </li> <li> <p>Make a full backup of both deployments local storage (under <code>/home/k8s</code>)</p> <pre><code>$ cp -a /home/k8s/unifi /home/k8s/unifi.backup-2025-12-23.unify-9.4.19\n</code></pre> </li> <li> <p>Update the UniFi Network deployment manifest to version v10.0.162 and apply it.</p> </li> <li> <p>Restart the Mongo deployment.</p> <pre><code>$ kubectl scale -n unifi deployment mongo --replicas=1\n</code></pre> </li> <li> <p>After a minute, restart the UniFi Network deployment.</p> <pre><code>$ kubectl scale -n unifi deployment unifi --replicas=1\n</code></pre> </li> </ol> <p>If some of the access points show up as Offline, this may be incorrect. Check whether they are accessible via SSH, that their configuration (<code>cfg/mgmt</code>) points to the correct IP address (the <code>LoadBalancer</code> IP for the <code>unifi-svc</code> service) and that they report themselves as <code>Connected</code> when running <code>info</code> in their shell:</p> <pre><code>U7Lite:~# info\n\nModel:       U7-Lite\nVersion:     8.3.2.18064\nMAC Address: 84:78:48:86:4a:ac\nIP Address:  192.168.0.138\nHostname:    U7Lite\nUptime:      113831 seconds\nNTP:         Synchronized\n\nStatus:      Connected (http://192.168.0.173:8080/inform)\n</code></pre> <p>All access points should show up as Connected after a few minutes.</p>"},{"location":"blog/2025/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-lexicon/","title":"Upgrading single-node Kubernetes cluster on Ubuntu Studio 24.04 (lexicon)","text":"<p>Upgrading the single-node kubernetes cluster on rapture went smoothly, so it's time to repeat the process on <code>lexicon</code>, especially since the current version (1.26) will be End Of Life next month!</p>"},{"location":"blog/2025/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-lexicon/#upgrade-to-127","title":"Upgrade to 1.27","text":"<pre><code># kubeadm version\nkubeadm version: &amp;version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.15\", GitCommit:\"1649f592f1909b97aa3c2a0a8f968a3fd05a7b8b\", GitTreeState:\"clean\", BuildDate:\"2024-03-14T01:03:33Z\", GoVersion:\"go1.21.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\n# kubectl get nodes\nNAME      STATUS   ROLES           AGE    VERSION\nlexicon   Ready    control-plane   659d   v1.26.15\n</code></pre> <p>Again, the workloads are not critical, lets pretend they are and use <code>kubectl drain</code> to remove the node from service to ensure all pods are shut down gracefully:</p> <pre><code># kubectl drain --ignore-daemonsets lexicon\nnode/lexicon cordoned\nerror: unable to drain node \"lexicon\" due to error:cannot delete Pods with local storage (use --delete-emptydir-data to override): kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-p27cg, kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-vwxnt, metrics-server/metrics-server-74c749979-wd278, continuing command...\nThere are pending nodes to be drained:\n lexicon\ncannot delete Pods with local storage (use --delete-emptydir-data to override): kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-p27cg, kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-vwxnt, metrics-server/metrics-server-74c749979-wd278\n</code></pre> <p>This warning can be ignored safely, those are literally empty directories which at most have a little JSON file:</p> <pre><code># kubectl -n kubernetes-dashboard describe pod kubernetes-dashboard | grep containerd\n    Container ID:  containerd://32baf2a5a439f9623a08fc6127a3d605140b09b46c40c8d535685daf5127bbb0\n\n# find /home/lib/ -type d | grep 32baf2a5a439f9623a08fc6127a3d605140b09b46c40c8d535685daf5127bbb0\n/home/lib/containerd/io.containerd.runtime.v2.task/k8s.io/32baf2a5a439f9623a08fc6127a3d605140b09b46c40c8d535685daf5127bbb0\n/home/lib/containerd/io.containerd.grpc.v1.cri/containers/32baf2a5a439f9623a08fc6127a3d605140b09b46c40c8d535685daf5127bbb0\n\n# ls -l /home/lib/containerd/io.containerd.runtime.v2.task/k8s.io/26e0473de095cd0d4cf505eb6b7938764349c7c5257eaf8540136d5037e3d69e /home/lib/containerd/io.containerd.grpc.v1.cri/containers/26e0473de095cd0d4cf505eb6b7938764349c7c5257eaf8540136d5037e3d69e\n/home/lib/containerd/io.containerd.grpc.v1.cri/containers/26e0473de095cd0d4cf505eb6b7938764349c7c5257eaf8540136d5037e3d69e:\ntotal 4\n-rw------- 1 root root 225 Jan 10 06:08 status\n\n/home/lib/containerd/io.containerd.runtime.v2.task/k8s.io/26e0473de095cd0d4cf505eb6b7938764349c7c5257eaf8540136d5037e3d69e:\ntotal 0\n\n# cat /home/lib/containerd/io.containerd.grpc.v1.cri/containers/32baf2a5a439f9623a08fc6127a3d605140b09b46c40c8d535685daf5127bbb0/status | jq -r .\n{\n  \"Version\": \"v1\",\n  \"Pid\": 29784,\n  \"CreatedAt\": 1736485731044226300,\n  \"StartedAt\": 1736485731100578000,\n  \"FinishedAt\": 0,\n  \"ExitCode\": 0,\n  \"Reason\": \"\",\n  \"Message\": \"\",\n  \"Resources\": {\n    \"linux\": {\n      \"cpu_period\": 100000,\n      \"cpu_shares\": 2,\n      \"oom_score_adj\": 1000\n    }\n  }\n}\n</code></pre> <p>Thus, lets drain the node:</p> <code># kubectl drain --ignore-daemonsets --delete-emptydir-data lexicon</code> <pre><code># kubectl drain --ignore-daemonsets --delete-emptydir-data lexicon\nnode/lexicon already cordoned\nWarning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-nrrg6, kube-system/kube-proxy-qlt8c, metallb-system/speaker-t8f7k, monitoring/telegraf-ztg2t, node-feature-discovery/nfd-node-feature-discovery-worker-xmzwk\nevicting pod atuin-server/atuin-cdc5879b8-8kl24\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-jhhhc\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-cb9bh\nevicting pod cert-manager/cert-manager-64f9f45d6f-qx4hs\nevicting pod cert-manager/cert-manager-webhook-d4f4545d7-tf4l6\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-4gg7d\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-22z7l\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-2n2dz\nevicting pod code-server/code-server-5768c9d997-cr858\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-2xr55\nevicting pod default/inteldeviceplugins-controller-manager-7994555cdb-gjmfm\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-48vvc\nevicting pod firefly-iii/firefly-iii-5bd9bb7c4-vxv87\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-7qsnq\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-5v9tk\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-6jpm7\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-4lr52\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-6jvv4\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-4n488\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-6ldtx\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-4nr7s\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-m7gcs\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-jq85x\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-5st6w\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-4wphp\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-76rlb\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-hhv8z\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-m8q2t\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-lrqfc\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-8mrll\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-hlxqp\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-928zt\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-hpfx4\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-bbpm2\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-jf458\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-bncmb\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-jgk98\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-c68ws\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-c9khh\nevicting pod unifi/unifi-6678ccc684-7g9qb\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-728m7\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-7m8b6\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-mngbg\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-cdqq9\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-k7rlx\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-nntnl\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-kq25w\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-nptbm\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-ksqm2\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-ntrgw\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-p9gsv\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-pftg7\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-pnr9n\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-pvrt5\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-qck44\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-qfhzv\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-qxgfc\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-qzlfw\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-r7v2f\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-rltww\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-rpfnc\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-rx2kb\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-rx92b\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-s4ggm\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-sbhrt\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-sg6nw\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-sj4vv\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-sksd9\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-t2b58\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-t9fnn\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-tn62g\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-tvcdg\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-hd72r\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-jdvng\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-tzr6h\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-jmg44\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-vvl94\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-vlqdn\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-wv8qh\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-jmsfn\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-x6v5x\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-xf5qv\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-jtqtc\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-k8xvt\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-zcwd5\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-xh6s5\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-zp8fd\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-khdf4\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-zr662\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-kprpk\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-zrj82\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-lhffx\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-zttt5\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-lxxqk\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-mdvx4\nevicting pod firefly-iii/firefly-iii-mysql-68f59d48f-k5mpw\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-mp7k9\nevicting pod homebox/homebox-7fb9d44d48-wxj6q\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-mptfs\nevicting pod ingress-nginx/ingress-nginx-admission-create-7zfzs\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-nqnvc\nevicting pod ingress-nginx/ingress-nginx-admission-patch-4tvvk\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-pk8f4\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-244q5\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-qgb8v\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-24z6b\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-r2t6n\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-2brp6\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-r9kjw\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-2g7t8\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-2g2c2\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-2k6pl\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-rnz84\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-2mtgv\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-rt5lm\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-48gnx\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-s5w6f\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-4k4vc\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-s989f\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-4rktx\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-snhd6\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-smqsc\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-srhvn\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-4sldn\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-vdznd\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-4wcq2\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-vftsh\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-4wq8x\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-vsxpk\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-4ztpd\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-vxbpz\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-5m6w5\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-vz69m\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-5rnzx\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-wg556\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-66dpd\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-x8sw4\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-6gdb7\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-xg4bx\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-6tf9z\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-xmfkv\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-7h2qx\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-xndqz\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-87lgg\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-826lj\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-8p9kb\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-xtm2h\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-8rs7g\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-z5nr6\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-zp4tr\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-9lljr\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-zqd8d\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-9lmrg\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-zsfpv\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-9mfzf\nevicting pod ingress-nginx/ingress-nginx-controller-746f764ff4-x2xc2\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-9t8dd\nevicting pod kavita/kavita-766577cf45-dz9sl\nevicting pod kube-system/coredns-787d4945fb-67z8g\nevicting pod komga/komga-549b6dd9b-4zvb5\nevicting pod kube-system/coredns-787d4945fb-gsx6h\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-p27cg\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-bqk9h\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-vwxnt\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-bqw8m\nevicting pod local-path-storage/local-path-provisioner-8bc8875b-lbjh8\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-c77rw\nevicting pod metallb-system/controller-68bf958bf9-79mpk\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-c7dwq\nevicting pod metrics-server/metrics-server-74c749979-wd278\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-cmvt2\nevicting pod minecraft-server/minecraft-server-bd48db4bd-wjc22\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-ddf2h\nevicting pod monitoring/grafana-7647f97d64-k8h5m\nevicting pod monitoring/influxdb-57bddc4dc9-2z4v8\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-dksm6\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-dkjmb\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-f28xf\nevicting pod navidrome/navidrome-5f76b6ddf5-gcft4\nevicting pod node-feature-discovery/nfd-node-feature-discovery-master-5f56c75d-xjkb8\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-f2k7w\nevicting pod plexserver/plexserver-5dbdfc898b-9v2s8\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-f5pft\nevicting pod unifi/mongo-d55fbb7f5-pvdfr\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-ghjtz\nevicting pod ingress-nginx/ingress-nginx-controller-6b58ffdc97-gs2w6\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-ckx84\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-dgghp\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-dzlgj\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-f75kw\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-f8nm6\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-fphdc\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-fv6zx\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-fvfwr\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-fzq24\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-g28xp\nevicting pod firefly-iii/firefly-iii-7bfd57ff49-h97wd\nevicting pod cert-manager/cert-manager-cainjector-56bbdd5c47-ltjgx\nevicting pod audiobookshelf/audiobookshelf-987b955fb-64gxm\npod/firefly-iii-7bfd57ff49-ksqm2 evicted\npod/firefly-iii-7bfd57ff49-kq25w evicted\npod/firefly-iii-7bfd57ff49-nntnl evicted\npod/firefly-iii-7bfd57ff49-cdqq9 evicted\npod/firefly-iii-7bfd57ff49-728m7 evicted\npod/firefly-iii-7bfd57ff49-ntrgw evicted\npod/firefly-iii-7bfd57ff49-mngbg evicted\npod/firefly-iii-7bfd57ff49-nptbm evicted\npod/firefly-iii-7bfd57ff49-k7rlx evicted\npod/firefly-iii-7bfd57ff49-p9gsv evicted\npod/firefly-iii-7bfd57ff49-pftg7 evicted\npod/firefly-iii-7bfd57ff49-pnr9n evicted\npod/firefly-iii-7bfd57ff49-pvrt5 evicted\nI0110 20:09:23.926166 1095415 request.go:690] Waited for 1.000164008s due to client-side throttling, not priority and fairness, request: POST:https://10.0.0.6:6443/api/v1/namespaces/firefly-iii/pods/firefly-iii-7bfd57ff49-qck44/eviction\npod/firefly-iii-7bfd57ff49-qck44 evicted\npod/atuin-cdc5879b8-8kl24 evicted\npod/firefly-iii-7bfd57ff49-qfhzv evicted\npod/firefly-iii-7bfd57ff49-qxgfc evicted\npod/firefly-iii-7bfd57ff49-qzlfw evicted\npod/firefly-iii-7bfd57ff49-r7v2f evicted\npod/firefly-iii-7bfd57ff49-rltww evicted\npod/firefly-iii-7bfd57ff49-rpfnc evicted\npod/firefly-iii-7bfd57ff49-rx2kb evicted\npod/firefly-iii-7bfd57ff49-rx92b evicted\npod/firefly-iii-7bfd57ff49-s4ggm evicted\npod/firefly-iii-7bfd57ff49-sbhrt evicted\npod/firefly-iii-7bfd57ff49-sg6nw evicted\npod/firefly-iii-7bfd57ff49-sj4vv evicted\npod/firefly-iii-7bfd57ff49-sksd9 evicted\npod/firefly-iii-7bfd57ff49-t2b58 evicted\npod/firefly-iii-7bfd57ff49-t9fnn evicted\npod/firefly-iii-7bfd57ff49-cb9bh evicted\npod/firefly-iii-7bfd57ff49-tn62g evicted\npod/firefly-iii-7bfd57ff49-tvcdg evicted\npod/firefly-iii-7bfd57ff49-c9khh evicted\npod/firefly-iii-7bfd57ff49-7m8b6 evicted\npod/ingress-nginx-controller-6b58ffdc97-hd72r evicted\npod/ingress-nginx-controller-6b58ffdc97-jdvng evicted\npod/firefly-iii-7bfd57ff49-tzr6h evicted\npod/ingress-nginx-controller-6b58ffdc97-jmg44 evicted\npod/firefly-iii-7bfd57ff49-vvl94 evicted\npod/firefly-iii-7bfd57ff49-vlqdn evicted\npod/firefly-iii-7bfd57ff49-wv8qh evicted\npod/ingress-nginx-controller-6b58ffdc97-jmsfn evicted\npod/firefly-iii-7bfd57ff49-x6v5x evicted\npod/firefly-iii-7bfd57ff49-xf5qv evicted\npod/ingress-nginx-controller-6b58ffdc97-jtqtc evicted\npod/ingress-nginx-controller-6b58ffdc97-k8xvt evicted\npod/firefly-iii-7bfd57ff49-zcwd5 evicted\npod/firefly-iii-7bfd57ff49-xh6s5 evicted\npod/firefly-iii-7bfd57ff49-zp8fd evicted\npod/ingress-nginx-controller-6b58ffdc97-khdf4 evicted\npod/firefly-iii-7bfd57ff49-zr662 evicted\npod/unifi-6678ccc684-7g9qb evicted\npod/ingress-nginx-controller-6b58ffdc97-kprpk evicted\npod/firefly-iii-7bfd57ff49-zrj82 evicted\npod/ingress-nginx-controller-6b58ffdc97-lhffx evicted\npod/firefly-iii-7bfd57ff49-zttt5 evicted\npod/ingress-nginx-controller-6b58ffdc97-lxxqk evicted\npod/ingress-nginx-controller-6b58ffdc97-mdvx4 evicted\nI0110 20:09:34.067126 1095415 request.go:690] Waited for 1.124324732s due to client-side throttling, not priority and fairness, request: GET:https://10.0.0.6:6443/api/v1/namespaces/firefly-iii/pods/firefly-iii-mysql-68f59d48f-k5mpw\npod/ingress-nginx-controller-6b58ffdc97-mp7k9 evicted\npod/homebox-7fb9d44d48-wxj6q evicted\npod/ingress-nginx-controller-6b58ffdc97-mptfs evicted\npod/ingress-nginx-admission-create-7zfzs evicted\npod/ingress-nginx-controller-6b58ffdc97-nqnvc evicted\npod/ingress-nginx-admission-patch-4tvvk evicted\npod/ingress-nginx-controller-6b58ffdc97-pk8f4 evicted\npod/ingress-nginx-controller-6b58ffdc97-244q5 evicted\npod/ingress-nginx-controller-6b58ffdc97-qgb8v evicted\npod/ingress-nginx-controller-6b58ffdc97-24z6b evicted\npod/firefly-iii-mysql-68f59d48f-k5mpw evicted\npod/ingress-nginx-controller-6b58ffdc97-r2t6n evicted\npod/ingress-nginx-controller-6b58ffdc97-2brp6 evicted\npod/ingress-nginx-controller-6b58ffdc97-r9kjw evicted\npod/ingress-nginx-controller-6b58ffdc97-2g7t8 evicted\npod/ingress-nginx-controller-6b58ffdc97-2g2c2 evicted\npod/ingress-nginx-controller-6b58ffdc97-2k6pl evicted\npod/ingress-nginx-controller-6b58ffdc97-rnz84 evicted\npod/ingress-nginx-controller-6b58ffdc97-2mtgv evicted\npod/ingress-nginx-controller-6b58ffdc97-rt5lm evicted\npod/ingress-nginx-controller-6b58ffdc97-48gnx evicted\npod/ingress-nginx-controller-6b58ffdc97-s5w6f evicted\npod/ingress-nginx-controller-6b58ffdc97-4k4vc evicted\npod/ingress-nginx-controller-6b58ffdc97-s989f evicted\npod/ingress-nginx-controller-6b58ffdc97-snhd6 evicted\npod/ingress-nginx-controller-6b58ffdc97-4rktx evicted\npod/ingress-nginx-controller-6b58ffdc97-smqsc evicted\npod/ingress-nginx-controller-6b58ffdc97-4sldn evicted\npod/ingress-nginx-controller-6b58ffdc97-srhvn evicted\npod/ingress-nginx-controller-6b58ffdc97-vdznd evicted\npod/ingress-nginx-controller-6b58ffdc97-4wcq2 evicted\npod/ingress-nginx-controller-6b58ffdc97-vftsh evicted\npod/ingress-nginx-controller-6b58ffdc97-4wq8x evicted\npod/ingress-nginx-controller-6b58ffdc97-vsxpk evicted\npod/ingress-nginx-controller-6b58ffdc97-4ztpd evicted\npod/ingress-nginx-controller-6b58ffdc97-vxbpz evicted\npod/ingress-nginx-controller-6b58ffdc97-5m6w5 evicted\npod/ingress-nginx-controller-6b58ffdc97-vz69m evicted\npod/ingress-nginx-controller-6b58ffdc97-5rnzx evicted\npod/ingress-nginx-controller-6b58ffdc97-wg556 evicted\npod/ingress-nginx-controller-6b58ffdc97-66dpd evicted\npod/ingress-nginx-controller-6b58ffdc97-x8sw4 evicted\npod/ingress-nginx-controller-6b58ffdc97-6gdb7 evicted\npod/ingress-nginx-controller-6b58ffdc97-xg4bx evicted\npod/ingress-nginx-controller-6b58ffdc97-6tf9z evicted\npod/ingress-nginx-controller-6b58ffdc97-xmfkv evicted\npod/ingress-nginx-controller-6b58ffdc97-7h2qx evicted\npod/ingress-nginx-controller-6b58ffdc97-xndqz evicted\npod/ingress-nginx-controller-6b58ffdc97-87lgg evicted\npod/ingress-nginx-controller-6b58ffdc97-826lj evicted\nI0110 20:09:44.125105 1095415 request.go:690] Waited for 21.198402997s due to client-side throttling, not priority and fairness, request: POST:https://10.0.0.6:6443/api/v1/namespaces/ingress-nginx/pods/ingress-nginx-controller-6b58ffdc97-zqd8d/eviction\npod/ingress-nginx-controller-6b58ffdc97-8p9kb evicted\npod/ingress-nginx-controller-6b58ffdc97-xtm2h evicted\npod/ingress-nginx-controller-6b58ffdc97-8rs7g evicted\npod/ingress-nginx-controller-6b58ffdc97-z5nr6 evicted\npod/ingress-nginx-controller-6b58ffdc97-zp4tr evicted\npod/ingress-nginx-controller-6b58ffdc97-9lljr evicted\npod/ingress-nginx-controller-6b58ffdc97-zqd8d evicted\npod/ingress-nginx-controller-6b58ffdc97-9lmrg evicted\npod/ingress-nginx-controller-6b58ffdc97-zsfpv evicted\npod/ingress-nginx-controller-6b58ffdc97-9mfzf evicted\npod/ingress-nginx-controller-6b58ffdc97-9t8dd evicted\npod/ingress-nginx-controller-6b58ffdc97-bqk9h evicted\npod/ingress-nginx-controller-6b58ffdc97-bqw8m evicted\npod/ingress-nginx-controller-6b58ffdc97-c77rw evicted\npod/controller-68bf958bf9-79mpk evicted\npod/ingress-nginx-controller-6b58ffdc97-c7dwq evicted\npod/ingress-nginx-controller-6b58ffdc97-cmvt2 evicted\npod/dashboard-metrics-scraper-7bc864c59-p27cg evicted\npod/ingress-nginx-controller-6b58ffdc97-ddf2h evicted\npod/ingress-nginx-controller-6b58ffdc97-dksm6 evicted\npod/ingress-nginx-controller-6b58ffdc97-dkjmb evicted\npod/ingress-nginx-controller-6b58ffdc97-f28xf evicted\npod/nfd-node-feature-discovery-master-5f56c75d-xjkb8 evicted\npod/ingress-nginx-controller-6b58ffdc97-f2k7w evicted\npod/ingress-nginx-controller-6b58ffdc97-f5pft evicted\nI0110 20:09:54.125869 1095415 request.go:690] Waited for 31.198805794s due to client-side throttling, not priority and fairness, request: POST:https://10.0.0.6:6443/api/v1/namespaces/firefly-iii/pods/firefly-iii-7bfd57ff49-fzq24/eviction\npod/mongo-d55fbb7f5-pvdfr evicted\npod/ingress-nginx-controller-6b58ffdc97-ghjtz evicted\npod/ingress-nginx-controller-6b58ffdc97-gs2w6 evicted\npod/firefly-iii-7bfd57ff49-7qsnq evicted\npod/firefly-iii-7bfd57ff49-ckx84 evicted\npod/firefly-iii-7bfd57ff49-4wphp evicted\npod/firefly-iii-7bfd57ff49-dgghp evicted\npod/firefly-iii-7bfd57ff49-dzlgj evicted\npod/firefly-iii-7bfd57ff49-5v9tk evicted\npod/influxdb-57bddc4dc9-2z4v8 evicted\npod/firefly-iii-7bfd57ff49-f75kw evicted\npod/firefly-iii-7bfd57ff49-6jpm7 evicted\npod/firefly-iii-7bfd57ff49-f8nm6 evicted\npod/firefly-iii-7bfd57ff49-fphdc evicted\npod/firefly-iii-7bfd57ff49-4lr52 evicted\npod/firefly-iii-7bfd57ff49-fv6zx evicted\npod/firefly-iii-7bfd57ff49-6jvv4 evicted\npod/firefly-iii-7bfd57ff49-fvfwr evicted\npod/coredns-787d4945fb-67z8g evicted\npod/firefly-iii-7bfd57ff49-4n488 evicted\npod/komga-549b6dd9b-4zvb5 evicted\npod/firefly-iii-7bfd57ff49-fzq24 evicted\npod/firefly-iii-7bfd57ff49-6ldtx evicted\npod/metrics-server-74c749979-wd278 evicted\npod/firefly-iii-7bfd57ff49-4nr7s evicted\npod/firefly-iii-7bfd57ff49-g28xp evicted\npod/plexserver-5dbdfc898b-9v2s8 evicted\npod/firefly-iii-7bfd57ff49-jq85x evicted\npod/firefly-iii-7bfd57ff49-h97wd evicted\npod/firefly-iii-7bfd57ff49-m7gcs evicted\npod/firefly-iii-7bfd57ff49-5st6w evicted\npod/cert-manager-cainjector-56bbdd5c47-ltjgx evicted\npod/firefly-iii-7bfd57ff49-jhhhc evicted\npod/minecraft-server-bd48db4bd-wjc22 evicted\npod/coredns-787d4945fb-gsx6h evicted\npod/firefly-iii-7bfd57ff49-4gg7d evicted\npod/kubernetes-dashboard-6c7ccbcf87-vwxnt evicted\npod/grafana-7647f97d64-k8h5m evicted\nI0110 20:10:04.266451 1095415 request.go:690] Waited for 7.323749578s due to client-side throttling, not priority and fairness, request: GET:https://10.0.0.6:6443/api/v1/namespaces/firefly-iii/pods/firefly-iii-7bfd57ff49-22z7l\npod/firefly-iii-7bfd57ff49-22z7l evicted\npod/firefly-iii-7bfd57ff49-2xr55 evicted\npod/inteldeviceplugins-controller-manager-7994555cdb-gjmfm evicted\npod/firefly-iii-7bfd57ff49-2n2dz evicted\npod/firefly-iii-7bfd57ff49-48vvc evicted\npod/firefly-iii-5bd9bb7c4-vxv87 evicted\npod/code-server-5768c9d997-cr858 evicted\npod/firefly-iii-7bfd57ff49-928zt evicted\npod/firefly-iii-7bfd57ff49-lrqfc evicted\npod/firefly-iii-7bfd57ff49-8mrll evicted\npod/firefly-iii-7bfd57ff49-hhv8z evicted\npod/firefly-iii-7bfd57ff49-hlxqp evicted\npod/navidrome-5f76b6ddf5-gcft4 evicted\npod/firefly-iii-7bfd57ff49-m8q2t evicted\npod/firefly-iii-7bfd57ff49-jf458 evicted\npod/firefly-iii-7bfd57ff49-hpfx4 evicted\npod/firefly-iii-7bfd57ff49-jgk98 evicted\npod/firefly-iii-7bfd57ff49-bbpm2 evicted\npod/firefly-iii-7bfd57ff49-bncmb evicted\npod/firefly-iii-7bfd57ff49-c68ws evicted\npod/firefly-iii-7bfd57ff49-76rlb evicted\npod/cert-manager-64f9f45d6f-qx4hs evicted\npod/cert-manager-webhook-d4f4545d7-tf4l6 evicted\npod/audiobookshelf-987b955fb-64gxm evicted\npod/ingress-nginx-controller-746f764ff4-x2xc2 evicted\npod/kavita-766577cf45-dz9sl evicted\npod/local-path-provisioner-8bc8875b-lbjh8 evicted\n</code></pre> <p>Note</p> <p>Local storage defined as <code>PersistentVolume</code> is not deleted:</p> <pre><code># du -sh /home/k8s/*\n210M    /home/k8s/atuin-server\n986M    /home/k8s/audiobookshelf\n4.7G    /home/k8s/code-server\n197M    /home/k8s/firefly-iii\n3.4M    /home/k8s/grafana\n236K    /home/k8s/homebox\n2.9G    /home/k8s/influxdb\n56M     /home/k8s/kavita\n14M     /home/k8s/komga\n9.0M    /home/k8s/local-path-storage\n1.6G    /home/k8s/minecraft-server\n37G     /home/k8s/minecraft-server-backups\n804M    /home/k8s/minecraft-server-spigot\n126M    /home/k8s/navidrome\n6.4G    /home/k8s/plexmediaserver\n404M    /home/k8s/unifi\n</code></pre> <p>At this point the Kubernetes cluster is running everything but those pods that are not part of Kubernetes:</p> <code># kubectl get deployments -A; kubectl get pods -A</code> <pre><code># kubectl get deployments -A\nNAMESPACE                NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE\natuin-server             atuin                                   0/1     1            0           494d\naudiobookshelf           audiobookshelf                          0/1     1            0           315d\ncert-manager             cert-manager                            0/1     1            0           635d\ncert-manager             cert-manager-cainjector                 0/1     1            0           635d\ncert-manager             cert-manager-webhook                    0/1     1            0           635d\ncode-server              code-server                             0/1     1            0           631d\ndefault                  inteldeviceplugins-controller-manager   0/1     1            0           483d\nfirefly-iii              firefly-iii                             0/1     1            0           236d\nfirefly-iii              firefly-iii-mysql                       0/1     1            0           236d\nhomebox                  homebox                                 0/1     1            0           185d\ningress-nginx            ingress-nginx-controller                0/1     1            0           649d\nkavita                   kavita                                  0/1     1            0           235d\nkomga                    komga                                   0/1     1            0           233d\nkube-system              coredns                                 0/2     2            0           659d\nkubernetes-dashboard     dashboard-metrics-scraper               0/1     1            0           649d\nkubernetes-dashboard     kubernetes-dashboard                    0/1     1            0           649d\nlocal-path-storage       local-path-provisioner                  0/1     1            0           635d\nmetallb-system           controller                              0/1     1            0           649d\nmetrics-server           metrics-server                          0/1     1            0           649d\nminecraft-server         minecraft-server                        0/1     1            0           14h\nmonitoring               grafana                                 0/1     1            0           265d\nmonitoring               influxdb                                0/1     1            0           265d\nnavidrome                navidrome                               0/1     1            0           76d\nnode-feature-discovery   nfd-node-feature-discovery-master       0/1     1            0           483d\nplexserver               plexserver                              0/1     1            0           467d\nunifi                    mongo                                   0/1     1            0           75d\nunifi                    unifi                                   0/1     1            0           9d\n\n# kubectl get pods -A\nNAMESPACE                NAME                                                     READY   STATUS    RESTARTS        AGE\natuin-server             atuin-cdc5879b8-kcwh8                                    0/2     Pending   0               3m2s\naudiobookshelf           audiobookshelf-987b955fb-vsf4t                           0/1     Pending   0               2m29s\ncert-manager             cert-manager-64f9f45d6f-mqt2z                            0/1     Pending   0               2m29s\ncert-manager             cert-manager-cainjector-56bbdd5c47-hrmtb                 0/1     Pending   0               2m30s\ncert-manager             cert-manager-webhook-d4f4545d7-5mzt2                     0/1     Pending   0               2m29s\ncode-server              code-server-5768c9d997-2ntzw                             0/1     Pending   0               2m17s\ndefault                  inteldeviceplugins-controller-manager-7994555cdb-k7bfz   0/2     Pending   0               2m28s\nfirefly-iii              firefly-iii-5bd9bb7c4-ff4bs                              0/1     Pending   0               2m18s\nfirefly-iii              firefly-iii-mysql-68f59d48f-dqhkh                        0/1     Pending   0               2m53s\nhomebox                  homebox-7fb9d44d48-hg9jn                                 0/1     Pending   0               2m52s\ningress-nginx            ingress-nginx-controller-746f764ff4-zf2fz                0/1     Pending   0               2m41s\nkavita                   kavita-766577cf45-5x2b2                                  0/1     Pending   0               2m40s\nkomga                    komga-549b6dd9b-njkfc                                    0/1     Pending   0               2m40s\nkube-flannel             kube-flannel-ds-nrrg6                                    1/1     Running   131 (14h ago)   650d\nkube-system              coredns-787d4945fb-bqp8b                                 0/1     Pending   0               2m40s\nkube-system              coredns-787d4945fb-gmzww                                 0/1     Pending   0               2m40s\nkube-system              etcd-lexicon                                             1/1     Running   78 (14h ago)    293d\nkube-system              kube-apiserver-lexicon                                   1/1     Running   78 (14h ago)    293d\nkube-system              kube-controller-manager-lexicon                          1/1     Running   78 (14h ago)    293d\nkube-system              kube-proxy-qlt8c                                         1/1     Running   115 (14h ago)   659d\nkube-system              kube-scheduler-lexicon                                   1/1     Running   78 (14h ago)    293d\nkubernetes-dashboard     dashboard-metrics-scraper-7bc864c59-8zv47                0/1     Pending   0               2m39s\nkubernetes-dashboard     kubernetes-dashboard-6c7ccbcf87-78c58                    0/1     Pending   0               2m39s\nlocal-path-storage       local-path-provisioner-8bc8875b-n44d6                    0/1     Pending   0               2m39s\nmetallb-system           controller-68bf958bf9-9f7f9                              0/1     Pending   0               2m38s\nmetallb-system           speaker-t8f7k                                            1/1     Running   171 (14h ago)   649d\nmetrics-server           metrics-server-74c749979-vggv9                           0/1     Pending   0               2m38s\nminecraft-server         minecraft-server-bd48db4bd-k4w4x                         0/1     Pending   0               2m37s\nmonitoring               grafana-7647f97d64-ljdt9                                 0/1     Pending   0               2m37s\nmonitoring               influxdb-57bddc4dc9-kmbw5                                0/1     Pending   0               2m37s\nmonitoring               telegraf-ztg2t                                           1/1     Running   39 (14h ago)    87d\nnavidrome                navidrome-5f76b6ddf5-4447v                               0/1     Pending   0               2m36s\nnode-feature-discovery   nfd-node-feature-discovery-master-5f56c75d-88r54         0/1     Pending   0               2m36s\nnode-feature-discovery   nfd-node-feature-discovery-worker-xmzwk                  0/1     Error     495 (79s ago)   483d\nplexserver               plexserver-5dbdfc898b-ng44d                              0/1     Pending   0               2m35s\nunifi                    mongo-d55fbb7f5-lssjb                                    0/1     Pending   0               2m35s\nunifi                    unifi-6678ccc684-6n9h7                                   0/1     Pending   0               2m58s\n</code></pre> <p>Now, to next step is to determine which version to upgrade to, update the minor version in the repository configuration and then find the latest patch version:</p> <pre><code># vi /etc/apt/sources.list.d/kubernetes.list\ndeb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.27/deb/ /\n\n# apt update\n...\nE: The repository 'https://pkgs.k8s.io/core:/stable:/v1.27/deb  InRelease' is not signed.\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\nN: See apt-secure(8) manpage for repository creation and user configuration details.\n</code></pre> <p>The first time around <code>apt update</code> failed because the relese key in <code>/etc/apt/keyrings/kubernetes-apt-keyring.gpg</code> was obsolete; it had to be replaced with the latest one </p> <pre><code># curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key \\\n  | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n# apt update\n</code></pre> <pre><code># apt-cache madison kubeadm\n   kubeadm | 1.27.16-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.15-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.14-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.13-2.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.12-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.11-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.10-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.9-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.8-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.7-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.6-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.5-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.4-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.3-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n   kubeadm | 1.27.0-2.1 | https://pkgs.k8s.io/core:/stable:/v1.27/deb  Packages\n</code></pre> <p>The latest patch version is 1.27.16 and that is the one to upgrade control plane nodes to:</p> <code>apt-get update &amp;&amp; apt-get install -y kubeadm='1.27.16-*'</code> <pre><code># apt-mark unhold kubeadm &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubeadm='1.27.16-*' &amp;&amp; \\\napt-mark hold kubeadm\nkubeadm was already not on hold.\nGet:1 https://cli.github.com/packages stable InRelease [3,917 B]\nHit:2 http://ch.archive.ubuntu.com/ubuntu jammy InRelease                                      \nHit:3 https://apt.releases.hashicorp.com jammy InRelease                                       \nHit:4 http://ch.archive.ubuntu.com/ubuntu jammy-updates InRelease                              \nHit:5 https://download.docker.com/linux/ubuntu jammy InRelease                                 \nIgn:6 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 InRelease                     \nHit:7 http://ch.archive.ubuntu.com/ubuntu jammy-backports InRelease                            \nHit:8 http://ch.archive.ubuntu.com/ubuntu jammy-security InRelease                             \nHit:9 https://apt.grafana.com stable InRelease                                                 \nErr:1 https://cli.github.com/packages stable InRelease                                         \n    The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nHit:11 https://esm.ubuntu.com/apps/ubuntu jammy-apps-security InRelease\nHit:12 https://esm.ubuntu.com/apps/ubuntu jammy-apps-updates InRelease                         \nHit:13 https://esm.ubuntu.com/infra/ubuntu jammy-infra-security InRelease                      \nHit:10 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.27/deb  InRelease\nHit:14 https://esm.ubuntu.com/infra/ubuntu jammy-infra-updates InRelease                       \nHit:15 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release\nErr:10 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.27/deb  InRelease\n    The following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project &lt;isv:kubernetes@build.opensuse.org&gt;\nHit:16 https://dl.ui.com/unifi/debian unifi-7.3 InRelease\nErr:17 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release.gpg\n    The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://cli.github.com/packages stable InRelease: The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.27/deb  InRelease: The following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project &lt;isv:kubernetes@build.opensuse.org&gt;\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release: The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Failed to fetch https://cli.github.com/packages/dists/stable/InRelease  The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: Failed to fetch https://pkgs.k8s.io/core:/stable:/v1.27/deb/InRelease  The following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project &lt;isv:kubernetes@build.opensuse.org&gt;\nW: Failed to fetch https://repo.mongodb.org/apt/ubuntu/dists/xenial/mongodb-org/3.6/Release.gpg  The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.27.16-1.1' (isv:kubernetes:core:stable:v1.27:pkgs.k8s.io [amd64]) for 'kubeadm'\nThe following packages were automatically installed and are no longer required:\nlinux-headers-5.15.0-122 linux-headers-5.15.0-124 linux-headers-5.15.0-125\nUse 'apt autoremove' to remove them.\nThe following packages will be upgraded:\nkubeadm\n1 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\nNeed to get 10.0 MB of archives.\nAfter this operation, 848 kB of additional disk space will be used.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.27/deb  kubeadm 1.27.16-1.1 [10.0 MB]\nFetched 10.0 MB in 1s (11.9 MB/s)\n(Reading database ... 280984 files and directories currently installed.)\nPreparing to unpack .../kubeadm_1.27.16-1.1_amd64.deb ...\nUnpacking kubeadm (1.27.16-1.1) over (1.26.15-1.1) ...\nSetting up kubeadm (1.27.16-1.1) ...\nScanning processes...                                                                           \nScanning processor microcode...                                                                 \nScanning linux images...                                                                        \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubeadm set on hold.\n\n# kubeadm version\nkubeadm version: &amp;version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.16\", GitCommit:\"cbb86e0d7f4a049666fac0551e8b02ef3d6c3d9a\", GitTreeState:\"clean\", BuildDate:\"2024-07-17T01:52:04Z\", GoVersion:\"go1.22.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n</code></pre> <p>Verify and apply the upgrade plan:</p> <code># kubeadm upgrade plan &amp;&amp; kubeadm upgrade apply v1.27.16</code> <pre><code># kubeadm upgrade plan\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks.\n[upgrade] Running cluster health checks\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: v1.26.3\n[upgrade/versions] kubeadm version: v1.27.16\nI0110 20:23:20.288112 1323315 version.go:256] remote version is much newer: v1.32.0; falling back to: stable-1.27\n[upgrade/versions] Target version: v1.27.16\n[upgrade/versions] Latest version in the v1.26 series: v1.26.15\n\nUpgrade to the latest version in the v1.26 series:\n\nCOMPONENT                 CURRENT   TARGET\nkube-apiserver            v1.26.3   v1.26.15\nkube-controller-manager   v1.26.3   v1.26.15\nkube-scheduler            v1.26.3   v1.26.15\nkube-proxy                v1.26.3   v1.26.15\nCoreDNS                   v1.9.3    v1.10.1\netcd                      3.5.6-0   3.5.12-0\n\nYou can now apply the upgrade by executing the following command:\n\n        kubeadm upgrade apply v1.26.15\n\n_____________________________________________________________________\n\nComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\nCOMPONENT   CURRENT        TARGET\nkubelet     1 x v1.26.15   v1.27.16\n\nUpgrade to the latest stable version:\n\nCOMPONENT                 CURRENT   TARGET\nkube-apiserver            v1.26.3   v1.27.16\nkube-controller-manager   v1.26.3   v1.27.16\nkube-scheduler            v1.26.3   v1.27.16\nkube-proxy                v1.26.3   v1.27.16\nCoreDNS                   v1.9.3    v1.10.1\netcd                      3.5.6-0   3.5.12-0\n\nYou can now apply the upgrade by executing the following command:\n\n        kubeadm upgrade apply v1.27.16\n\n_____________________________________________________________________\n\n\nThe table below shows the current state of component configs as understood by this version of kubeadm.\nConfigs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or\nresetting to kubeadm defaults before a successful upgrade can be performed. The version to manually\nupgrade to is denoted in the \"PREFERRED VERSION\" column.\n\nAPI GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED\nkubeproxy.config.k8s.io   v1alpha1          v1alpha1            no\nkubelet.config.k8s.io     v1beta1           v1beta1             no\n_____________________________________________________________________\n\n\n# kubeadm upgrade apply v1.27.16\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks.\n[upgrade] Running cluster health checks\n[upgrade/version] You have chosen to change the cluster version to \"v1.27.16\"\n[upgrade/versions] Cluster version: v1.26.3\n[upgrade/versions] kubeadm version: v1.27.16\n[upgrade] Are you sure you want to proceed? [y/N]: y\n[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster\n[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection\n[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'\nW0110 20:25:52.171444 1330873 checks.go:835] detected that the sandbox image \"registry.k8s.io/pause:3.6\" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.\n[upgrade/apply] Upgrading your Static Pod-hosted control plane to version \"v1.27.16\" (timeout: 5m0s)...\n[upgrade/etcd] Upgrading to TLS for etcd\n[upgrade/staticpods] Preparing for \"etcd\" upgrade\n[upgrade/staticpods] Renewing etcd-server certificate\n[upgrade/staticpods] Renewing etcd-peer certificate\n[upgrade/staticpods] Renewing etcd-healthcheck-client certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/etcd.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-20-25-59/etcd.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=etcd\n[upgrade/staticpods] Component \"etcd\" upgraded successfully!\n[upgrade/etcd] Waiting for etcd to become available\n[upgrade/staticpods] Writing new Static Pod manifests to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests2962201605\"\n[upgrade/staticpods] Preparing for \"kube-apiserver\" upgrade\n[upgrade/staticpods] Renewing apiserver certificate\n[upgrade/staticpods] Renewing apiserver-kubelet-client certificate\n[upgrade/staticpods] Renewing front-proxy-client certificate\n[upgrade/staticpods] Renewing apiserver-etcd-client certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-20-25-59/kube-apiserver.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=kube-apiserver\n[upgrade/staticpods] Component \"kube-apiserver\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-controller-manager\" upgrade\n[upgrade/staticpods] Renewing controller-manager.conf certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-20-25-59/kube-controller-manager.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=kube-controller-manager\n[upgrade/staticpods] Component \"kube-controller-manager\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-scheduler\" upgrade\n[upgrade/staticpods] Renewing scheduler.conf certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-20-25-59/kube-scheduler.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=kube-scheduler\n[upgrade/staticpods] Component \"kube-scheduler\" upgraded successfully!\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config4258126724/config.yaml\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\n[upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.27.16\". Enjoy!\n\n[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.\n</code></pre> <p>Now that the control plane is updated, proceed to upgrade kubelet and kubectl</p> <code>apt-get update &amp;&amp; apt-get install -y kubelet='1.27.16-*' kubectl='1.27.16-*'</code> <pre><code># apt-mark unhold kubelet kubectl &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubelet='1.27.16-*' kubectl='1.27.16-*' &amp;&amp; \\\napt-mark hold kubelet kubectl\nkubelet was already not on hold.\nkubectl was already not on hold.\nHit:1 http://ch.archive.ubuntu.com/ubuntu jammy InRelease\nGet:2 http://ch.archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]                     \nHit:3 https://download.docker.com/linux/ubuntu jammy InRelease                                 \nGet:4 https://cli.github.com/packages stable InRelease [3,917 B]                               \nHit:5 http://ch.archive.ubuntu.com/ubuntu jammy-backports InRelease                            \nHit:6 https://apt.grafana.com stable InRelease                                                 \nHit:7 https://apt.releases.hashicorp.com jammy InRelease                                       \nGet:8 http://ch.archive.ubuntu.com/ubuntu jammy-security InRelease [129 kB]                    \nIgn:10 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 InRelease                    \nHit:11 https://esm.ubuntu.com/apps/ubuntu jammy-apps-security InRelease\nErr:4 https://cli.github.com/packages stable InRelease              \nThe following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nHit:13 https://esm.ubuntu.com/apps/ubuntu jammy-apps-updates InRelease\nHit:14 https://esm.ubuntu.com/infra/ubuntu jammy-infra-security InRelease\nHit:15 https://esm.ubuntu.com/infra/ubuntu jammy-infra-updates InRelease                       \nHit:9 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.27/deb  InRelease\nHit:16 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release                      \nHit:12 https://dl.ui.com/unifi/debian unifi-7.3 InRelease\nErr:9 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.27/deb  InRelease\nThe following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project &lt;isv:kubernetes@build.opensuse.org&gt;\nErr:17 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release.gpg\nThe following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nFetched 261 kB in 1s (274 kB/s)\nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://cli.github.com/packages stable InRelease: The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.27/deb  InRelease: The following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project &lt;isv:kubernetes@build.opensuse.org&gt;\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release: The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Failed to fetch https://cli.github.com/packages/dists/stable/InRelease  The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: Failed to fetch https://pkgs.k8s.io/core:/stable:/v1.27/deb/InRelease  The following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project &lt;isv:kubernetes@build.opensuse.org&gt;\nW: Failed to fetch https://repo.mongodb.org/apt/ubuntu/dists/xenial/mongodb-org/3.6/Release.gpg  The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.27.16-1.1' (isv:kubernetes:core:stable:v1.27:pkgs.k8s.io [amd64]) for 'kubelet'\nSelected version '1.27.16-1.1' (isv:kubernetes:core:stable:v1.27:pkgs.k8s.io [amd64]) for 'kubectl'\nThe following packages were automatically installed and are no longer required:\n    linux-headers-5.15.0-122 linux-headers-5.15.0-124 linux-headers-5.15.0-125\nUse 'apt autoremove' to remove them.\nThe following packages will be upgraded:\n    kubectl kubelet\n2 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\nNeed to get 29.5 MB of archives.\nAfter this operation, 15.3 MB disk space will be freed.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.27/deb  kubectl 1.27.16-1.1 [10.4 MB]\nGet:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.27/deb  kubelet 1.27.16-1.1 [19.2 MB]\nFetched 29.5 MB in 1s (21.5 MB/s) \n(Reading database ... 280984 files and directories currently installed.)\nPreparing to unpack .../kubectl_1.27.16-1.1_amd64.deb ...\nUnpacking kubectl (1.27.16-1.1) over (1.26.15-1.1) ...\nPreparing to unpack .../kubelet_1.27.16-1.1_amd64.deb ...\nUnpacking kubelet (1.27.16-1.1) over (1.26.15-1.1) ...\ndpkg: warning: unable to delete old directory '/etc/sysconfig': Directory not empty\nSetting up kubectl (1.27.16-1.1) ...\nSetting up kubelet (1.27.16-1.1) ...\nScanning processes...                                                                           \nScanning candidates...                                                                          \nScanning processor microcode...                                                                 \nScanning linux images...                                                                        \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nRestarting services...\nsystemctl restart kubelet.service\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubelet set on hold.\nkubectl set on hold.\n\n# systemctl daemon-reload\n# systemctl restart kubelet\n\n# kubectl  version --output=yaml\n    clientVersion:\n    buildDate: \"2024-07-17T01:53:56Z\"\n    compiler: gc\n    gitCommit: cbb86e0d7f4a049666fac0551e8b02ef3d6c3d9a\n    gitTreeState: clean\n    gitVersion: v1.27.16\n    goVersion: go1.22.5\n    major: \"1\"\n    minor: \"27\"\n    platform: linux/amd64\nkustomizeVersion: v5.0.1\nserverVersion:\n    buildDate: \"2024-07-17T01:44:26Z\"\n    compiler: gc\n    gitCommit: cbb86e0d7f4a049666fac0551e8b02ef3d6c3d9a\n    gitTreeState: clean\n    gitVersion: v1.27.16\n    goVersion: go1.22.5\n    major: \"1\"\n    minor: \"27\"\n    platform: linux/amd64\n</code></pre> <p>Finally, bring the node back online by marking it schedulable:</p> <code># kubectl uncordon lexicon</code> <pre><code># kubectl uncordon lexicon\nnode/lexicon uncordoned\n# kubectl get pods -A\nNAMESPACE                NAME                                                     READY   STATUS              RESTARTS          AGE\natuin-server             atuin-cdc5879b8-kcwh8                                    2/2     Running             0                 21m\naudiobookshelf           audiobookshelf-987b955fb-vsf4t                           0/1     ContainerCreating   0                 20m\ncert-manager             cert-manager-64f9f45d6f-mqt2z                            1/1     Running             0                 20m\ncert-manager             cert-manager-cainjector-56bbdd5c47-hrmtb                 1/1     Running             0                 20m\ncert-manager             cert-manager-webhook-d4f4545d7-5mzt2                     0/1     Running             0                 20m\ncode-server              code-server-5768c9d997-2ntzw                             0/1     ContainerCreating   0                 20m\ndefault                  inteldeviceplugins-controller-manager-7994555cdb-k7bfz   2/2     Running             0                 20m\nfirefly-iii              firefly-iii-5bd9bb7c4-ff4bs                              0/1     ContainerCreating   0                 20m\nfirefly-iii              firefly-iii-mysql-68f59d48f-dqhkh                        1/1     Running             0                 20m\nhomebox                  homebox-7fb9d44d48-hg9jn                                 1/1     Running             0                 20m\ningress-nginx            ingress-nginx-controller-746f764ff4-zf2fz                0/1     Running             0                 20m\nkavita                   kavita-766577cf45-5x2b2                                  0/1     ContainerCreating   0                 20m\nkomga                    komga-549b6dd9b-njkfc                                    0/1     ContainerCreating   0                 20m\nkube-flannel             kube-flannel-ds-nrrg6                                    1/1     Running             131 (14h ago)     650d\nkube-system              coredns-5d78c9869d-llpsv                                 1/1     Running             0                 102s\nkube-system              coredns-5d78c9869d-wn9xh                                 1/1     Running             0                 102s\nkube-system              coredns-787d4945fb-gmzww                                 1/1     Terminating         0                 20m\nkube-system              etcd-lexicon                                             1/1     Running             1 (47s ago)       43s\nkube-system              kube-apiserver-lexicon                                   1/1     Running             1 (46s ago)       43s\nkube-system              kube-controller-manager-lexicon                          1/1     Running             1 (47s ago)       47s\nkube-system              kube-proxy-pg4bx                                         1/1     Running             0                 102s\nkube-system              kube-scheduler-lexicon                                   1/1     Running             1 (47s ago)       47s\nkubernetes-dashboard     dashboard-metrics-scraper-7bc864c59-8zv47                1/1     Running             0                 20m\nkubernetes-dashboard     kubernetes-dashboard-6c7ccbcf87-78c58                    0/1     ContainerCreating   0                 20m\nlocal-path-storage       local-path-provisioner-8bc8875b-n44d6                    1/1     Running             0                 20m\nmetallb-system           controller-68bf958bf9-9f7f9                              0/1     Running             0                 20m\nmetallb-system           speaker-t8f7k                                            1/1     Running             171 (14h ago)     649d\nmetrics-server           metrics-server-74c749979-vggv9                           0/1     Running             0                 20m\nminecraft-server         minecraft-server-bd48db4bd-k4w4x                         0/1     ContainerCreating   0                 20m\nmonitoring               grafana-7647f97d64-ljdt9                                 1/1     Running             0                 20m\nmonitoring               influxdb-57bddc4dc9-kmbw5                                1/1     Running             0                 20m\nmonitoring               telegraf-ztg2t                                           1/1     Running             39 (14h ago)      87d\nnavidrome                navidrome-5f76b6ddf5-4447v                               0/1     ContainerCreating   0                 20m\nnode-feature-discovery   nfd-node-feature-discovery-master-5f56c75d-88r54         0/1     Running             0                 20m\nnode-feature-discovery   nfd-node-feature-discovery-worker-xmzwk                  1/1     Running             501 (2m48s ago)   483d\nplexserver               plexserver-5dbdfc898b-ng44d                              0/1     ContainerCreating   0                 20m\nunifi                    mongo-d55fbb7f5-lssjb                                    1/1     Running             0                 20m\nunifi                    unifi-6678ccc684-6n9h7                                   1/1     Running             0                 21m\n</code></pre> <p>After a minute or two all the services are back up and running and the Kubernetes dashboard shows all workloads green and OK :)</p> <p>Now just repeat the same process again to upgrade to 1.28, then 1.29, then 1.30, then 1.31, and finally 1.32. Easy!</p>"},{"location":"blog/2025/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-lexicon/#upgrade-to-128","title":"Upgrade to 1.28","text":"<p>Upgrade version 1.27.x to version 1.28.x now, again starting by determining which version to upgrade to and updating the minor version in the repository configuration and then find the latest patch version:</p> <pre><code># vi /etc/apt/sources.list.d/kubernetes.list\ndeb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /\n\n# curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key \\\n  | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# apt update\n# apt-cache madison kubeadm\n   kubeadm | 1.28.15-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.14-2.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.13-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.12-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.11-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.10-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.9-2.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.8-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.7-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.6-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.5-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.4-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.3-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n</code></pre> <p>Now, before updating <code>kubeadm</code>, drain the node again:</p> <code># kubectl drain --ignore-daemonsets --delete-emptydir-data lexicon</code> <pre><code># kubectl drain --ignore-daemonsets --delete-emptydir-data lexicon\nnode/lexicon cordoned\nWarning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-nrrg6, kube-system/kube-proxy-pg4bx, metallb-system/speaker-t8f7k, monitoring/telegraf-ztg2t, node-feature-discovery/nfd-node-feature-discovery-worker-xmzwk\nevicting pod atuin-server/atuin-cdc5879b8-kcwh8\nevicting pod cert-manager/cert-manager-64f9f45d6f-mqt2z\nevicting pod audiobookshelf/audiobookshelf-987b955fb-vsf4t\nevicting pod komga/komga-549b6dd9b-njkfc\nevicting pod cert-manager/cert-manager-cainjector-56bbdd5c47-hrmtb\nevicting pod cert-manager/cert-manager-webhook-d4f4545d7-5mzt2\nevicting pod kube-system/coredns-5d78c9869d-llpsv\nevicting pod code-server/code-server-5768c9d997-2ntzw\nevicting pod unifi/unifi-6678ccc684-6n9h7\nevicting pod default/inteldeviceplugins-controller-manager-7994555cdb-k7bfz\nevicting pod kube-system/coredns-5d78c9869d-wn9xh\nevicting pod firefly-iii/firefly-iii-5bd9bb7c4-ff4bs\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-8zv47\nevicting pod firefly-iii/firefly-iii-mysql-68f59d48f-dqhkh\nevicting pod local-path-storage/local-path-provisioner-8bc8875b-n44d6\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-78c58\nevicting pod metrics-server/metrics-server-74c749979-vggv9\nevicting pod metallb-system/controller-68bf958bf9-9f7f9\nevicting pod monitoring/grafana-7647f97d64-ljdt9\nevicting pod minecraft-server/minecraft-server-bd48db4bd-k4w4x\nevicting pod node-feature-discovery/nfd-node-feature-discovery-master-5f56c75d-88r54\nevicting pod homebox/homebox-7fb9d44d48-hg9jn\nevicting pod navidrome/navidrome-5f76b6ddf5-4447v\nevicting pod ingress-nginx/ingress-nginx-controller-746f764ff4-zf2fz\nevicting pod plexserver/plexserver-5dbdfc898b-ng44d\nevicting pod unifi/mongo-d55fbb7f5-lssjb\nevicting pod monitoring/influxdb-57bddc4dc9-kmbw5\nevicting pod kavita/kavita-766577cf45-5x2b2\nI0110 20:54:21.549407 1964930 request.go:696] Waited for 1.000157756s due to client-side throttling, not priority and fairness, request: POST:https://10.0.0.6:6443/api/v1/namespaces/cert-manager/pods/cert-manager-webhook-d4f4545d7-5mzt2/eviction\npod/navidrome-5f76b6ddf5-4447v evicted\npod/homebox-7fb9d44d48-hg9jn evicted\npod/cert-manager-64f9f45d6f-mqt2z evicted\npod/atuin-cdc5879b8-kcwh8 evicted\npod/metrics-server-74c749979-vggv9 evicted\npod/cert-manager-cainjector-56bbdd5c47-hrmtb evicted\npod/inteldeviceplugins-controller-manager-7994555cdb-k7bfz evicted\npod/influxdb-57bddc4dc9-kmbw5 evicted\npod/firefly-iii-5bd9bb7c4-ff4bs evicted\npod/dashboard-metrics-scraper-7bc864c59-8zv47 evicted\npod/komga-549b6dd9b-njkfc evicted\npod/plexserver-5dbdfc898b-ng44d evicted\npod/code-server-5768c9d997-2ntzw evicted\npod/firefly-iii-mysql-68f59d48f-dqhkh evicted\npod/cert-manager-webhook-d4f4545d7-5mzt2 evicted\npod/grafana-7647f97d64-ljdt9 evicted\npod/minecraft-server-bd48db4bd-k4w4x evicted\npod/kubernetes-dashboard-6c7ccbcf87-78c58 evicted\npod/audiobookshelf-987b955fb-vsf4t evicted\npod/mongo-d55fbb7f5-lssjb evicted\npod/controller-68bf958bf9-9f7f9 evicted\npod/nfd-node-feature-discovery-master-5f56c75d-88r54 evicted\npod/coredns-5d78c9869d-llpsv evicted\npod/coredns-5d78c9869d-wn9xh evicted\npod/unifi-6678ccc684-6n9h7 evicted\npod/ingress-nginx-controller-746f764ff4-zf2fz evicted\npod/local-path-provisioner-8bc8875b-n44d6 evicted\npod/kavita-766577cf45-5x2b2 evicted\nnode/lexicon drained\n</code></pre> <p>The latest patch version is 1.28.10 and that is the one to upgrade control plane nodes to:</p> <code>apt-get update &amp;&amp; apt-get install -y kubeadm='1.28.15-*'</code> <pre><code># apt-mark unhold kubeadm &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubeadm='1.28.15-*' &amp;&amp; \\\napt-mark hold kubeadm\nCanceled hold on kubeadm.\nHit:1 http://ch.archive.ubuntu.com/ubuntu jammy InRelease\nHit:2 http://ch.archive.ubuntu.com/ubuntu jammy-updates InRelease                              \nHit:3 http://ch.archive.ubuntu.com/ubuntu jammy-backports InRelease                            \nHit:4 https://download.docker.com/linux/ubuntu jammy InRelease                                 \nGet:5 https://cli.github.com/packages stable InRelease [3,917 B]                               \nIgn:6 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 InRelease                     \nHit:7 http://ch.archive.ubuntu.com/ubuntu jammy-security InRelease                             \nHit:8 https://apt.releases.hashicorp.com jammy InRelease                                       \nHit:9 https://apt.grafana.com stable InRelease                                                 \nHit:12 https://esm.ubuntu.com/apps/ubuntu jammy-apps-security InRelease                        \nHit:13 https://esm.ubuntu.com/apps/ubuntu jammy-apps-updates InRelease                 \nErr:5 https://cli.github.com/packages stable InRelease                                 \nThe following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nHit:14 https://esm.ubuntu.com/infra/ubuntu jammy-infra-security InRelease                      \nHit:15 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release                      \nHit:16 https://esm.ubuntu.com/infra/ubuntu jammy-infra-updates InRelease                       \nHit:10 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.28/deb  InRelease\nHit:11 https://dl.ui.com/unifi/debian unifi-7.3 InRelease         \nErr:17 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release.gpg\nThe following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nFetched 3,917 B in 1s (5,193 B/s)\nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://cli.github.com/packages stable InRelease: The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release: The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Failed to fetch https://cli.github.com/packages/dists/stable/InRelease  The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: Failed to fetch https://repo.mongodb.org/apt/ubuntu/dists/xenial/mongodb-org/3.6/Release.gpg  The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.28.15-1.1' (isv:kubernetes:core:stable:v1.28:pkgs.k8s.io [amd64]) for 'kubeadm'\nThe following packages were automatically installed and are no longer required:\nlinux-headers-5.15.0-122 linux-headers-5.15.0-124 linux-headers-5.15.0-125\nUse 'apt autoremove' to remove them.\nThe following packages will be upgraded:\nkubeadm\n1 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\nNeed to get 10.1 MB of archives.\nAfter this operation, 369 kB of additional disk space will be used.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.28/deb  kubeadm 1.28.15-1.1 [10.1 MB]\nFetched 10.1 MB in 1s (15.7 MB/s)\n(Reading database ... 280984 files and directories currently installed.)\nPreparing to unpack .../kubeadm_1.28.15-1.1_amd64.deb ...\nUnpacking kubeadm (1.28.15-1.1) over (1.28.10-1.1) ...\nSetting up kubeadm (1.28.15-1.1) ...\nScanning processes...                                                                           \nScanning processor microcode...                                                                 \nScanning linux images...                                                                        \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubeadm set on hold.\n</code></pre> <p>Verify and apply the upgrade plan:</p> <code># kubeadm upgrade plan &amp;&amp; kubeadm upgrade apply v1.28.15</code> <pre><code># kubeadm upgrade plan\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks.\n[upgrade] Running cluster health checks\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: v1.27.16\n[upgrade/versions] kubeadm version: v1.28.15\nI0110 21:01:58.619294 2087758 version.go:256] remote version is much newer: v1.32.0; falling back to: stable-1.28\n[upgrade/versions] Target version: v1.28.15\n[upgrade/versions] Latest version in the v1.27 series: v1.27.16\n\nComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\nCOMPONENT   CURRENT        TARGET\nkubelet     1 x v1.27.16   v1.28.15\n\nUpgrade to the latest stable version:\n\nCOMPONENT                 CURRENT    TARGET\nkube-apiserver            v1.27.16   v1.28.15\nkube-controller-manager   v1.27.16   v1.28.15\nkube-scheduler            v1.27.16   v1.28.15\nkube-proxy                v1.27.16   v1.28.15\nCoreDNS                   v1.10.1    v1.10.1\netcd                      3.5.12-0   3.5.15-0\n\nYou can now apply the upgrade by executing the following command:\n\n        kubeadm upgrade apply v1.28.15\n\n_____________________________________________________________________\n\n\nThe table below shows the current state of component configs as understood by this version of kubeadm.\nConfigs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or\nresetting to kubeadm defaults before a successful upgrade can be performed. The version to manually\nupgrade to is denoted in the \"PREFERRED VERSION\" column.\n\nAPI GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED\nkubeproxy.config.k8s.io   v1alpha1          v1alpha1            no\nkubelet.config.k8s.io     v1beta1           v1beta1             no\n_____________________________________________________________________\n\n\n# kubeadm upgrade apply v1.28.15\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks.\n[upgrade] Running cluster health checks\n[upgrade/version] You have chosen to change the cluster version to \"v1.28.15\"\n[upgrade/versions] Cluster version: v1.27.16\n[upgrade/versions] kubeadm version: v1.28.15\n[upgrade] Are you sure you want to proceed? [y/N]: y\n[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster\n[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection\n[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'\nW0110 21:03:05.773743 2099300 checks.go:835] detected that the sandbox image \"registry.k8s.io/pause:3.6\" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.\n[upgrade/apply] Upgrading your Static Pod-hosted control plane to version \"v1.28.15\" (timeout: 5m0s)...\n[upgrade/etcd] Upgrading to TLS for etcd\n[upgrade/staticpods] Preparing for \"etcd\" upgrade\n[upgrade/staticpods] Renewing etcd-server certificate\n[upgrade/staticpods] Renewing etcd-peer certificate\n[upgrade/staticpods] Renewing etcd-healthcheck-client certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/etcd.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-21-03-08/etcd.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=etcd\n[upgrade/staticpods] Component \"etcd\" upgraded successfully!\n[upgrade/etcd] Waiting for etcd to become available\n[upgrade/staticpods] Writing new Static Pod manifests to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests3220413090\"\n[upgrade/staticpods] Preparing for \"kube-apiserver\" upgrade\n[upgrade/staticpods] Renewing apiserver certificate\n[upgrade/staticpods] Renewing apiserver-kubelet-client certificate\n[upgrade/staticpods] Renewing front-proxy-client certificate\n[upgrade/staticpods] Renewing apiserver-etcd-client certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-21-03-08/kube-apiserver.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=kube-apiserver\n[upgrade/staticpods] Component \"kube-apiserver\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-controller-manager\" upgrade\n[upgrade/staticpods] Renewing controller-manager.conf certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-21-03-08/kube-controller-manager.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=kube-controller-manager\n[upgrade/staticpods] Component \"kube-controller-manager\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-scheduler\" upgrade\n[upgrade/staticpods] Renewing scheduler.conf certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-21-03-08/kube-scheduler.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=kube-scheduler\n[upgrade/staticpods] Component \"kube-scheduler\" upgraded successfully!\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config4047502448/config.yaml\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\n[upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.28.15\". Enjoy!\n\n[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.\n</code></pre> <p>Now that the control plane is updated, proceed to upgrade kubelet and kubectl</p> <code>apt-get update &amp;&amp; apt-get install -y kubelet='1.28.15-*' kubectl='1.28.15-*'</code> <pre><code># apt-mark unhold kubelet kubectl &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubelet='1.28.15-*' kubectl='1.28.15-*' &amp;&amp; \\\napt-mark hold kubelet kubectl\nCanceled hold on kubelet.\nCanceled hold on kubectl.\nHit:1 https://download.docker.com/linux/ubuntu jammy InRelease\nHit:2 http://ch.archive.ubuntu.com/ubuntu jammy InRelease                                      \nIgn:3 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 InRelease                     \nGet:4 https://cli.github.com/packages stable InRelease [3,917 B]                               \nHit:5 http://ch.archive.ubuntu.com/ubuntu jammy-updates InRelease                              \nHit:6 https://apt.releases.hashicorp.com jammy InRelease                                       \nHit:7 http://ch.archive.ubuntu.com/ubuntu jammy-backports InRelease                            \nHit:8 http://ch.archive.ubuntu.com/ubuntu jammy-security InRelease                             \nHit:9 https://apt.grafana.com stable InRelease                                                 \nErr:4 https://cli.github.com/packages stable InRelease                                         \nThe following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nHit:12 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release                  \nHit:10 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.28/deb  InRelease\nHit:11 https://dl.ui.com/unifi/debian unifi-7.3 InRelease                                      \nErr:13 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release.gpg\nThe following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nHit:14 https://esm.ubuntu.com/apps/ubuntu jammy-apps-security InRelease\nHit:15 https://esm.ubuntu.com/apps/ubuntu jammy-apps-updates InRelease\nHit:16 https://esm.ubuntu.com/infra/ubuntu jammy-infra-security InRelease\nHit:17 https://esm.ubuntu.com/infra/ubuntu jammy-infra-updates InRelease\nFetched 3,917 B in 1s (3,152 B/s)\nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://cli.github.com/packages stable InRelease: The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release: The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Failed to fetch https://cli.github.com/packages/dists/stable/InRelease  The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: Failed to fetch https://repo.mongodb.org/apt/ubuntu/dists/xenial/mongodb-org/3.6/Release.gpg  The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.28.15-1.1' (isv:kubernetes:core:stable:v1.28:pkgs.k8s.io [amd64]) for 'kubelet'\nSelected version '1.28.15-1.1' (isv:kubernetes:core:stable:v1.28:pkgs.k8s.io [amd64]) for 'kubectl'\nThe following packages were automatically installed and are no longer required:\nebtables linux-headers-5.15.0-122 linux-headers-5.15.0-124 linux-headers-5.15.0-125 socat\nUse 'apt autoremove' to remove them.\nThe following packages will be upgraded:\nkubectl kubelet\n2 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\nNeed to get 30.0 MB of archives.\nAfter this operation, 2,494 kB of additional disk space will be used.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.28/deb  kubectl 1.28.15-1.1 [10.4 MB]\nGet:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.28/deb  kubelet 1.28.15-1.1 [19.6 MB]\nFetched 30.0 MB in 1s (29.2 MB/s)\n(Reading database ... 280984 files and directories currently installed.)\nPreparing to unpack .../kubectl_1.28.15-1.1_amd64.deb ...\nUnpacking kubectl (1.28.15-1.1) over (1.27.16-1.1) ...\nPreparing to unpack .../kubelet_1.28.15-1.1_amd64.deb ...\nUnpacking kubelet (1.28.15-1.1) over (1.27.16-1.1) ...\nSetting up kubectl (1.28.15-1.1) ...\nSetting up kubelet (1.28.15-1.1) ...\nScanning processes...                                                                           \nScanning candidates...                                                                          \nScanning processor microcode...                                                                 \nScanning linux images...                                                                        \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nRestarting services...\nsystemctl restart kubelet.service\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubelet set on hold.\nkubectl set on hold.\n\n# systemctl daemon-reload\n# systemctl restart kubelet\n\n# kubectl  version --output=yaml\nclientVersion:\n    buildDate: \"2024-10-22T20:34:56Z\"\n    compiler: gc\n    gitCommit: 841856557ef0f6a399096c42635d114d6f2cf7f4\n    gitTreeState: clean\n    gitVersion: v1.28.15\n    goVersion: go1.22.8\n    major: \"1\"\n    minor: \"28\"\n    platform: linux/amd64\nkustomizeVersion: v5.0.4-0.20230601165947-6ce0bf390ce3\nserverVersion:\n    buildDate: \"2024-10-22T20:26:27Z\"\n    compiler: gc\n    gitCommit: 841856557ef0f6a399096c42635d114d6f2cf7f4\n    gitTreeState: clean\n    gitVersion: v1.28.15\n    goVersion: go1.22.8\n    major: \"1\"\n    minor: \"28\"\n    platform: linux/amd64\n</code></pre> <p>Finally, bring the node back online by marking it schedulable:</p> <code># kubectl uncordon lexicon</code> <pre><code># kubectl uncordon lexicon\nnode/lexicon uncordoned\n# kubectl get pods -A\nNAMESPACE                NAME                                                     READY   STATUS    RESTARTS          AGE\natuin-server             atuin-cdc5879b8-4ssbz                                    0/2     Pending   0                 15m\naudiobookshelf           audiobookshelf-987b955fb-2vkq7                           0/1     Pending   0                 15m\ncert-manager             cert-manager-64f9f45d6f-hjkd6                            0/1     Pending   0                 15m\ncert-manager             cert-manager-cainjector-56bbdd5c47-2nxfs                 0/1     Pending   0                 15m\ncert-manager             cert-manager-webhook-d4f4545d7-jjzpj                     0/1     Pending   0                 15m\ncode-server              code-server-5768c9d997-5mmz6                             0/1     Pending   0                 15m\ndefault                  inteldeviceplugins-controller-manager-7994555cdb-jvxtk   0/2     Pending   0                 14m\nfirefly-iii              firefly-iii-5bd9bb7c4-vh9qx                              0/1     Pending   0                 14m\nfirefly-iii              firefly-iii-mysql-68f59d48f-2zlb4                        0/1     Pending   0                 15m\nhomebox                  homebox-7fb9d44d48-ml6hp                                 0/1     Pending   0                 15m\ningress-nginx            ingress-nginx-controller-746f764ff4-hfl47                0/1     Pending   0                 15m\nkavita                   kavita-766577cf45-2hdrc                                  0/1     Pending   0                 15m\nkomga                    komga-549b6dd9b-fxbfr                                    0/1     Pending   0                 15m\nkube-flannel             kube-flannel-ds-nrrg6                                    1/1     Running   131 (15h ago)     650d\nkube-system              coredns-5d78c9869d-vmf8t                                 0/1     Pending   0                 15m\nkube-system              coredns-5d78c9869d-x5tpm                                 0/1     Pending   0                 15m\nkube-system              etcd-lexicon                                             1/1     Running   1 (20s ago)       16s\nkube-system              kube-apiserver-lexicon                                   1/1     Running   1 (20s ago)       16s\nkube-system              kube-controller-manager-lexicon                          1/1     Running   1 (20s ago)       16s\nkube-system              kube-proxy-7rfxg                                         1/1     Running   0                 3m36s\nkube-system              kube-scheduler-lexicon                                   1/1     Running   1 (20s ago)       16s\nkubernetes-dashboard     dashboard-metrics-scraper-7bc864c59-mpm5d                0/1     Pending   0                 15m\nkubernetes-dashboard     kubernetes-dashboard-6c7ccbcf87-z2798                    0/1     Pending   0                 15m\nlocal-path-storage       local-path-provisioner-8bc8875b-lwc8t                    0/1     Pending   0                 15m\nmetallb-system           controller-68bf958bf9-dblz8                              0/1     Pending   0                 15m\nmetallb-system           speaker-t8f7k                                            1/1     Running   171 (15h ago)     649d\nmetrics-server           metrics-server-74c749979-84jk5                           0/1     Pending   0                 15m\nminecraft-server         minecraft-server-bd48db4bd-hscp6                         0/1     Pending   0                 15m\nmonitoring               grafana-7647f97d64-k57tg                                 0/1     Pending   0                 15m\nmonitoring               influxdb-57bddc4dc9-562fw                                0/1     Pending   0                 15m\nmonitoring               telegraf-ztg2t                                           1/1     Running   39 (15h ago)      87d\nnavidrome                navidrome-5f76b6ddf5-92k2z                               0/1     Pending   0                 15m\nnode-feature-discovery   nfd-node-feature-discovery-master-5f56c75d-rkgrm         0/1     Pending   0                 15m\nnode-feature-discovery   nfd-node-feature-discovery-worker-xmzwk                  1/1     Running   509 (2m59s ago)   483d\nplexserver               plexserver-5dbdfc898b-nsmjw                              0/1     Pending   0                 15m\nunifi                    mongo-d55fbb7f5-td9zr                                    0/1     Pending   0                 15m\nunifi                    unifi-6678ccc684-2zsbg                                   0/1     Pending   0                 15m\n</code></pre> <p>After a minute or two all the services are back up and running and the Kubernetes dashboard shows all workloads green and OK again.</p>"},{"location":"blog/2025/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-lexicon/#upgrade-to-129","title":"Upgrade to 1.29","text":"<p>Upgrade version 1.28.x to version 1.29.x now, again starting by determining which version to upgrade to and updating the minor version in the repository configuration and then find the latest patch version:</p> <pre><code># curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key \\\n  | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# vi /etc/apt/sources.list.d/kubernetes.list\ndeb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /\n\n# apt update\n# apt-cache madison kubeadm\n   kubeadm | 1.29.12-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n   kubeadm | 1.29.11-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n   kubeadm | 1.29.10-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n   kubeadm | 1.29.9-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n   kubeadm | 1.29.8-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n   kubeadm | 1.29.7-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n   kubeadm | 1.29.6-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n   kubeadm | 1.29.5-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n   kubeadm | 1.29.4-2.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n   kubeadm | 1.29.3-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n   kubeadm | 1.29.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n   kubeadm | 1.29.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n   kubeadm | 1.29.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages\n</code></pre> <p>Now, before updating <code>kubeadm</code>, drain the node again:</p> <code># kubectl drain --ignore-daemonsets --delete-emptydir-data lexicon</code> <pre><code># kubectl drain --ignore-daemonsets --delete-emptydir-data lexicon\nnode/lexicon cordoned\nWarning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-nrrg6, kube-system/kube-proxy-7rfxg, metallb-system/speaker-t8f7k, monitoring/telegraf-ztg2t, node-feature-discovery/nfd-node-feature-discovery-worker-xmzwk\nevicting pod audiobookshelf/audiobookshelf-987b955fb-2vkq7\nevicting pod cert-manager/cert-manager-64f9f45d6f-hjkd6\nevicting pod navidrome/navidrome-5f76b6ddf5-92k2z\nevicting pod komga/komga-549b6dd9b-fxbfr\nevicting pod atuin-server/atuin-cdc5879b8-4ssbz\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-z2798\nevicting pod cert-manager/cert-manager-cainjector-56bbdd5c47-2nxfs\nevicting pod node-feature-discovery/nfd-node-feature-discovery-master-5f56c75d-rkgrm\nevicting pod local-path-storage/local-path-provisioner-8bc8875b-lwc8t\nevicting pod cert-manager/cert-manager-webhook-d4f4545d7-jjzpj\nevicting pod metallb-system/controller-68bf958bf9-dblz8\nevicting pod code-server/code-server-5768c9d997-5mmz6\nevicting pod kube-system/coredns-5d78c9869d-vmf8t\nevicting pod default/inteldeviceplugins-controller-manager-7994555cdb-jvxtk\nevicting pod minecraft-server/minecraft-server-bd48db4bd-hscp6\nevicting pod firefly-iii/firefly-iii-5bd9bb7c4-vh9qx\nevicting pod monitoring/grafana-7647f97d64-k57tg\nevicting pod firefly-iii/firefly-iii-mysql-68f59d48f-2zlb4\nevicting pod homebox/homebox-7fb9d44d48-ml6hp\nevicting pod monitoring/influxdb-57bddc4dc9-562fw\nevicting pod unifi/unifi-6678ccc684-2zsbg\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-mpm5d\nevicting pod metrics-server/metrics-server-74c749979-84jk5\nevicting pod plexserver/plexserver-5dbdfc898b-nsmjw\nevicting pod kube-system/coredns-5d78c9869d-x5tpm\nevicting pod ingress-nginx/ingress-nginx-controller-746f764ff4-hfl47\nevicting pod unifi/mongo-d55fbb7f5-td9zr\nevicting pod kavita/kavita-766577cf45-2hdrc\npod/controller-68bf958bf9-dblz8 evicted\nI0110 21:17:25.198339 2388267 request.go:697] Waited for 1.200180309s due to client-side throttling, not priority and fairness, request: POST:https://10.0.0.6:6443/api/v1/namespaces/cert-manager/pods/cert-manager-cainjector-56bbdd5c47-2nxfs/eviction\npod/cert-manager-64f9f45d6f-hjkd6 evicted\npod/homebox-7fb9d44d48-ml6hp evicted\npod/influxdb-57bddc4dc9-562fw evicted\npod/cert-manager-webhook-d4f4545d7-jjzpj evicted\npod/code-server-5768c9d997-5mmz6 evicted\npod/cert-manager-cainjector-56bbdd5c47-2nxfs evicted\npod/nfd-node-feature-discovery-master-5f56c75d-rkgrm evicted\npod/firefly-iii-5bd9bb7c4-vh9qx evicted\npod/navidrome-5f76b6ddf5-92k2z evicted\npod/kubernetes-dashboard-6c7ccbcf87-z2798 evicted\npod/atuin-cdc5879b8-4ssbz evicted\npod/grafana-7647f97d64-k57tg evicted\npod/inteldeviceplugins-controller-manager-7994555cdb-jvxtk evicted\npod/audiobookshelf-987b955fb-2vkq7 evicted\npod/unifi-6678ccc684-2zsbg evicted\npod/minecraft-server-bd48db4bd-hscp6 evicted\npod/firefly-iii-mysql-68f59d48f-2zlb4 evicted\npod/komga-549b6dd9b-fxbfr evicted\npod/dashboard-metrics-scraper-7bc864c59-mpm5d evicted\npod/metrics-server-74c749979-84jk5 evicted\npod/coredns-5d78c9869d-vmf8t evicted\npod/mongo-d55fbb7f5-td9zr evicted\npod/plexserver-5dbdfc898b-nsmjw evicted\npod/coredns-5d78c9869d-x5tpm evicted\npod/ingress-nginx-controller-746f764ff4-hfl47 evicted\npod/local-path-provisioner-8bc8875b-lwc8t evicted\npod/kavita-766577cf45-2hdrc evicted\nnode/lexicon drained\n</code></pre> <p>The latest patch version is 1.29.12 and that is the one to upgrade control plane nodes to:</p> <code>apt-get update &amp;&amp; apt-get install -y kubeadm='1.29.12-*'</code> <pre><code># apt-mark unhold kubeadm &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubeadm='1.29.12-*' &amp;&amp; \\\napt-mark hold kubeadm\nCanceled hold on kubeadm.\nHit:1 http://ch.archive.ubuntu.com/ubuntu jammy InRelease\nIgn:2 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 InRelease                     \nHit:3 http://ch.archive.ubuntu.com/ubuntu jammy-updates InRelease                              \nHit:4 http://ch.archive.ubuntu.com/ubuntu jammy-backports InRelease                            \nHit:5 http://ch.archive.ubuntu.com/ubuntu jammy-security InRelease                             \nGet:6 https://cli.github.com/packages stable InRelease [3,917 B]                               \nHit:7 https://download.docker.com/linux/ubuntu jammy InRelease                                 \nHit:8 https://apt.releases.hashicorp.com jammy InRelease                                       \nHit:10 https://apt.grafana.com stable InRelease                                                \nHit:11 https://esm.ubuntu.com/apps/ubuntu jammy-apps-security InRelease                   \nHit:13 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release                      \nHit:14 https://esm.ubuntu.com/apps/ubuntu jammy-apps-updates InRelease                         \nHit:9 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.29/deb  InRelease\nHit:15 https://esm.ubuntu.com/infra/ubuntu jammy-infra-security InRelease\nHit:16 https://esm.ubuntu.com/infra/ubuntu jammy-infra-updates InRelease\nErr:6 https://cli.github.com/packages stable InRelease\n    The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nHit:12 https://dl.ui.com/unifi/debian unifi-7.3 InRelease\nErr:17 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release.gpg\n    The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://cli.github.com/packages stable InRelease: The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release: The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Failed to fetch https://cli.github.com/packages/dists/stable/InRelease  The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: Failed to fetch https://repo.mongodb.org/apt/ubuntu/dists/xenial/mongodb-org/3.6/Release.gpg  The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.29.12-1.1' (isv:kubernetes:core:stable:v1.29:pkgs.k8s.io [amd64]) for 'kubeadm'\nThe following packages were automatically installed and are no longer required:\n    ebtables linux-headers-5.15.0-122 linux-headers-5.15.0-124 linux-headers-5.15.0-125 socat\nUse 'apt autoremove' to remove them.\nThe following packages will be upgraded:\n    kubeadm\n1 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\nNeed to get 10.2 MB of archives.\nAfter this operation, 81.9 kB of additional disk space will be used.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.29/deb  kubeadm 1.29.12-1.1 [10.2 MB]\nFetched 10.2 MB in 1s (12.2 MB/s)\n(Reading database ... 280984 files and directories currently installed.)\nPreparing to unpack .../kubeadm_1.29.12-1.1_amd64.deb ...\nUnpacking kubeadm (1.29.12-1.1) over (1.28.15-1.1) ...\nSetting up kubeadm (1.29.12-1.1) ...\nScanning processes...                                                                           \nScanning processor microcode...                                                                 \nScanning linux images...                                                                        \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubeadm set on hold.\n</code></pre> <p>Verify and apply the upgrade plan:</p> <code># kubeadm upgrade plan &amp;&amp; kubeadm upgrade apply v1.29.12</code> <pre><code># kubeadm upgrade plan\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks.\n[upgrade] Running cluster health checks\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: v1.28.15\n[upgrade/versions] kubeadm version: v1.29.12\nI0110 21:19:52.303946 2429342 version.go:256] remote version is much newer: v1.32.0; falling back to: stable-1.29\n[upgrade/versions] Target version: v1.29.12\n[upgrade/versions] Latest version in the v1.28 series: v1.28.15\n\nComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\nCOMPONENT   CURRENT        TARGET\nkubelet     1 x v1.28.15   v1.29.12\n\nUpgrade to the latest stable version:\n\nCOMPONENT                 CURRENT    TARGET\nkube-apiserver            v1.28.15   v1.29.12\nkube-controller-manager   v1.28.15   v1.29.12\nkube-scheduler            v1.28.15   v1.29.12\nkube-proxy                v1.28.15   v1.29.12\nCoreDNS                   v1.10.1    v1.11.1\netcd                      3.5.15-0   3.5.16-0\n\nYou can now apply the upgrade by executing the following command:\n\n        kubeadm upgrade apply v1.29.12\n\n_____________________________________________________________________\n\n\nThe table below shows the current state of component configs as understood by this version of kubeadm.\nConfigs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or\nresetting to kubeadm defaults before a successful upgrade can be performed. The version to manually\nupgrade to is denoted in the \"PREFERRED VERSION\" column.\n\nAPI GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED\nkubeproxy.config.k8s.io   v1alpha1          v1alpha1            no\nkubelet.config.k8s.io     v1beta1           v1beta1             no\n_____________________________________________________________________\n\n# kubeadm upgrade apply v1.29.12\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks.\n[upgrade] Running cluster health checks\n[upgrade/version] You have chosen to change the cluster version to \"v1.29.12\"\n[upgrade/versions] Cluster version: v1.28.15\n[upgrade/versions] kubeadm version: v1.29.12\n[upgrade] Are you sure you want to proceed? [y/N]: y\n[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster\n[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection\n[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'\nW0110 21:21:18.273461 2445192 checks.go:835] detected that the sandbox image \"registry.k8s.io/pause:3.6\" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.\n[upgrade/apply] Upgrading your Static Pod-hosted control plane to version \"v1.29.12\" (timeout: 5m0s)...\n[upgrade/etcd] Upgrading to TLS for etcd\n[upgrade/staticpods] Preparing for \"etcd\" upgrade\n[upgrade/staticpods] Renewing etcd-server certificate\n[upgrade/staticpods] Renewing etcd-peer certificate\n[upgrade/staticpods] Renewing etcd-healthcheck-client certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/etcd.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-21-21-21/etcd.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=etcd\n[upgrade/staticpods] Component \"etcd\" upgraded successfully!\n[upgrade/etcd] Waiting for etcd to become available\n[upgrade/staticpods] Writing new Static Pod manifests to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests797080025\"\n[upgrade/staticpods] Preparing for \"kube-apiserver\" upgrade\n[upgrade/staticpods] Renewing apiserver certificate\n[upgrade/staticpods] Renewing apiserver-kubelet-client certificate\n[upgrade/staticpods] Renewing front-proxy-client certificate\n[upgrade/staticpods] Renewing apiserver-etcd-client certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-21-21-21/kube-apiserver.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=kube-apiserver\n[upgrade/staticpods] Component \"kube-apiserver\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-controller-manager\" upgrade\n[upgrade/staticpods] Renewing controller-manager.conf certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-21-21-21/kube-controller-manager.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=kube-controller-manager\n[upgrade/staticpods] Component \"kube-controller-manager\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-scheduler\" upgrade\n[upgrade/staticpods] Renewing scheduler.conf certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-21-21-21/kube-scheduler.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)\n[apiclient] Found 1 Pods for label selector component=kube-scheduler\n[upgrade/staticpods] Component \"kube-scheduler\" upgraded successfully!\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config2507373492/config.yaml\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"super-admin.conf\" kubeconfig file\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\n[upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.29.12\". Enjoy!\n\n[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.\n</code></pre> <p>Now that the control plane is updated, proceed to upgrade kubelet and kubectl</p> <code>apt-get update &amp;&amp; apt-get install -y kubelet='1.29.12-*' kubectl='1.29.12-*'</code> <pre><code># apt-mark unhold kubelet kubectl &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubelet='1.29.12-*' kubectl='1.29.12-*' &amp;&amp; \\\napt-mark hold kubelet kubectl\nCanceled hold on kubelet.\nCanceled hold on kubectl.\nHit:1 http://ch.archive.ubuntu.com/ubuntu jammy InRelease\nHit:2 http://ch.archive.ubuntu.com/ubuntu jammy-updates InRelease                              \nHit:3 https://apt.releases.hashicorp.com jammy InRelease                                       \nGet:4 https://cli.github.com/packages stable InRelease [3,917 B]                               \nHit:5 http://ch.archive.ubuntu.com/ubuntu jammy-backports InRelease                            \nHit:6 https://download.docker.com/linux/ubuntu jammy InRelease                                 \nIgn:7 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 InRelease                     \nHit:8 http://ch.archive.ubuntu.com/ubuntu jammy-security InRelease                             \nHit:9 https://apt.grafana.com stable InRelease                                                 \nHit:10 https://esm.ubuntu.com/apps/ubuntu jammy-apps-security InRelease                        \nHit:12 https://esm.ubuntu.com/apps/ubuntu jammy-apps-updates InRelease                    \nHit:14 https://esm.ubuntu.com/infra/ubuntu jammy-infra-security InRelease              \nErr:4 https://cli.github.com/packages stable InRelease                                 \n    The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nHit:15 https://esm.ubuntu.com/infra/ubuntu jammy-infra-updates InRelease               \nHit:16 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release                      \nHit:11 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.29/deb  InRelease\nHit:13 https://dl.ui.com/unifi/debian unifi-7.3 InRelease                           \nErr:17 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release.gpg\n    The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://cli.github.com/packages stable InRelease: The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release: The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Failed to fetch https://cli.github.com/packages/dists/stable/InRelease  The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: Failed to fetch https://repo.mongodb.org/apt/ubuntu/dists/xenial/mongodb-org/3.6/Release.gpg  The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.29.12-1.1' (isv:kubernetes:core:stable:v1.29:pkgs.k8s.io [amd64]) for 'kubelet'\nSelected version '1.29.12-1.1' (isv:kubernetes:core:stable:v1.29:pkgs.k8s.io [amd64]) for 'kubectl'\nThe following packages were automatically installed and are no longer required:\n    ebtables linux-headers-5.15.0-122 linux-headers-5.15.0-124 linux-headers-5.15.0-125 socat\nUse 'apt autoremove' to remove them.\nThe following packages will be upgraded:\n    kubectl kubelet\n2 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\nNeed to get 30.5 MB of archives.\nAfter this operation, 2,556 kB of additional disk space will be used.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.29/deb  kubectl 1.29.12-1.1 [10.6 MB]\nGet:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.29/deb  kubelet 1.29.12-1.1 [19.9 MB]\nFetched 30.5 MB in 1s (27.2 MB/s) \n(Reading database ... 280984 files and directories currently installed.)\nPreparing to unpack .../kubectl_1.29.12-1.1_amd64.deb ...\nUnpacking kubectl (1.29.12-1.1) over (1.28.15-1.1) ...\nPreparing to unpack .../kubelet_1.29.12-1.1_amd64.deb ...\nUnpacking kubelet (1.29.12-1.1) over (1.28.15-1.1) ...\nSetting up kubectl (1.29.12-1.1) ...\nSetting up kubelet (1.29.12-1.1) ...\nScanning processes...                                                                           \nScanning candidates...                                                                          \nScanning processor microcode...                                                                 \nScanning linux images...                                                                        \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nRestarting services...\nsystemctl restart kubelet.service\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubelet set on hold.\nkubectl set on hold.\n\n# systemctl daemon-reload\n# systemctl restart kubelet\n\n# kubectl  version --output=yaml\nclientVersion:\n    buildDate: \"2024-12-10T11:36:13Z\"\n    compiler: gc\n    gitCommit: 9253c9bda3d8bd76848bb4a21b309c28c0aab2f7\n    gitTreeState: clean\n    gitVersion: v1.29.12\n    goVersion: go1.22.9\n    major: \"1\"\n    minor: \"29\"\n    platform: linux/amd64\nkustomizeVersion: v5.0.4-0.20230601165947-6ce0bf390ce3\nserverVersion:\n    buildDate: \"2024-12-10T11:27:08Z\"\n    compiler: gc\n    gitCommit: 9253c9bda3d8bd76848bb4a21b309c28c0aab2f7\n    gitTreeState: clean\n    gitVersion: v1.29.12\n    goVersion: go1.22.9\n    major: \"1\"\n    minor: \"29\"\n    platform: linux/amd64\n</code></pre> <p>Finally, bring the node back online by marking it schedulable:</p> <code># kubectl uncordon lexicon</code> <pre><code># kubectl uncordon lexicon\nnode/lexicon uncordoned\n# kubectl get pods -A\nNAMESPACE                NAME                                                     READY   STATUS              RESTARTS        AGE\natuin-server             atuin-cdc5879b8-vf6pn                                    2/2     Running             0               10m\naudiobookshelf           audiobookshelf-987b955fb-c9vrr                           0/1     ContainerCreating   0               10m\ncert-manager             cert-manager-64f9f45d6f-mxjxc                            0/1     ContainerCreating   0               10m\ncert-manager             cert-manager-cainjector-56bbdd5c47-bznk2                 1/1     Running             0               10m\ncert-manager             cert-manager-webhook-d4f4545d7-zbq67                     0/1     ContainerCreating   0               10m\ncode-server              code-server-5768c9d997-rhzmx                             0/1     ContainerCreating   0               10m\ndefault                  inteldeviceplugins-controller-manager-7994555cdb-g58dm   0/2     ContainerCreating   0               10m\nfirefly-iii              firefly-iii-5bd9bb7c4-wqrqb                              0/1     ContainerCreating   0               10m\nfirefly-iii              firefly-iii-mysql-68f59d48f-8n6ff                        0/1     ContainerCreating   0               10m\nhomebox                  homebox-7fb9d44d48-mb62g                                 0/1     ContainerCreating   0               10m\ningress-nginx            ingress-nginx-controller-746f764ff4-kjfjv                0/1     Running             0               10m\nkavita                   kavita-766577cf45-285x5                                  0/1     ContainerCreating   0               10m\nkomga                    komga-549b6dd9b-bfz6n                                    0/1     ContainerCreating   0               10m\nkube-flannel             kube-flannel-ds-nrrg6                                    1/1     Running             131 (15h ago)   650d\nkube-system              coredns-5d78c9869d-p96rz                                 0/1     Terminating         0               10m\nkube-system              coredns-76f75df574-4qtvd                                 1/1     Running             0               3m34s\nkube-system              coredns-76f75df574-9p9xg                                 0/1     ContainerCreating   0               3m34s\nkube-system              etcd-lexicon                                             1/1     Running             0               5m7s\nkube-system              kube-apiserver-lexicon                                   1/1     Running             0               4m25s\nkube-system              kube-controller-manager-lexicon                          1/1     Running             0               4m4s\nkube-system              kube-proxy-xv4gm                                         1/1     Running             0               3m34s\nkube-system              kube-scheduler-lexicon                                   1/1     Running             0               3m48s\nkubernetes-dashboard     dashboard-metrics-scraper-7bc864c59-5hnt8                1/1     Running             0               10m\nkubernetes-dashboard     kubernetes-dashboard-6c7ccbcf87-q6z7q                    0/1     ContainerCreating   0               10m\nlocal-path-storage       local-path-provisioner-8bc8875b-wlbt9                    0/1     ContainerCreating   0               10m\nmetallb-system           controller-68bf958bf9-2tztx                              0/1     ContainerCreating   0               10m\nmetallb-system           speaker-t8f7k                                            1/1     Running             171 (15h ago)   649d\nmetrics-server           metrics-server-74c749979-ngtp7                           0/1     ContainerCreating   0               10m\nminecraft-server         minecraft-server-bd48db4bd-nbzwd                         0/1     ContainerCreating   0               10m\nmonitoring               grafana-7647f97d64-6vgdh                                 0/1     ContainerCreating   0               10m\nmonitoring               influxdb-57bddc4dc9-gw2lx                                0/1     ContainerCreating   0               10m\nmonitoring               telegraf-ztg2t                                           1/1     Running             39 (15h ago)    87d\nnavidrome                navidrome-5f76b6ddf5-z9gmk                               0/1     ContainerCreating   0               10m\nnode-feature-discovery   nfd-node-feature-discovery-master-5f56c75d-fv894         0/1     Running             0               10m\nnode-feature-discovery   nfd-node-feature-discovery-worker-xmzwk                  1/1     Running             517 (29s ago)   483d\nplexserver               plexserver-5dbdfc898b-fmdb7                              0/1     ContainerCreating   0               10m\nunifi                    mongo-d55fbb7f5-478g7                                    0/1     ContainerCreating   0               10m\nunifi                    unifi-6678ccc684-lv6lz                                   0/1     ContainerCreating   0               10m\n</code></pre> <p>After a minute or two all the services are back up and running and the Kubernetes dashboard shows all workloads green and OK again.</p>"},{"location":"blog/2025/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-lexicon/#upgrade-to-130","title":"Upgrade to 1.30","text":"<p>Upgrade version 1.29.x to version 1.30.x now, again starting by determining which version to upgrade to and updating the minor version in the repository configuration and then find the latest patch version:</p> <pre><code># curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key \\\n  | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# vi /etc/apt/sources.list.d/kubernetes.list\ndeb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /\n\n# apt update\n# apt-cache madison kubeadm\n   kubeadm | 1.30.8-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages\n   kubeadm | 1.30.7-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages\n   kubeadm | 1.30.6-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages\n   kubeadm | 1.30.5-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages\n   kubeadm | 1.30.4-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages\n   kubeadm | 1.30.3-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages\n   kubeadm | 1.30.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages\n   kubeadm | 1.30.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages\n   kubeadm | 1.30.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages\n</code></pre> <p>Now, before updating <code>kubeadm</code>, drain the node again:</p> <code># kubectl drain --ignore-daemonsets --delete-emptydir-data lexicon</code> <pre><code># kubectl drain --ignore-daemonsets --delete-emptydir-data lexicon\nnode/lexicon cordoned\nWarning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-nrrg6, kube-system/kube-proxy-xv4gm, metallb-system/speaker-t8f7k, monitoring/telegraf-ztg2t, node-feature-discovery/nfd-node-feature-discovery-worker-xmzwk\nevicting pod audiobookshelf/audiobookshelf-987b955fb-c9vrr\nevicting pod atuin-server/atuin-cdc5879b8-vf6pn\nevicting pod unifi/unifi-6678ccc684-lv6lz\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-5hnt8\nevicting pod kube-system/coredns-76f75df574-9p9xg\nevicting pod monitoring/grafana-7647f97d64-6vgdh\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-q6z7q\nevicting pod local-path-storage/local-path-provisioner-8bc8875b-wlbt9\nevicting pod monitoring/influxdb-57bddc4dc9-gw2lx\nevicting pod metallb-system/controller-68bf958bf9-2tztx\nevicting pod cert-manager/cert-manager-64f9f45d6f-mxjxc\nevicting pod navidrome/navidrome-5f76b6ddf5-z9gmk\nevicting pod metrics-server/metrics-server-74c749979-ngtp7\nevicting pod cert-manager/cert-manager-cainjector-56bbdd5c47-bznk2\nevicting pod plexserver/plexserver-5dbdfc898b-fmdb7\nevicting pod minecraft-server/minecraft-server-bd48db4bd-nbzwd\nevicting pod unifi/mongo-d55fbb7f5-478g7\nevicting pod firefly-iii/firefly-iii-mysql-68f59d48f-8n6ff\nevicting pod kavita/kavita-766577cf45-285x5\nevicting pod komga/komga-549b6dd9b-bfz6n\nevicting pod cert-manager/cert-manager-webhook-d4f4545d7-zbq67\nevicting pod kube-system/coredns-76f75df574-4qtvd\nevicting pod node-feature-discovery/nfd-node-feature-discovery-master-5f56c75d-fv894\nevicting pod code-server/code-server-5768c9d997-rhzmx\nevicting pod homebox/homebox-7fb9d44d48-mb62g\nevicting pod firefly-iii/firefly-iii-5bd9bb7c4-wqrqb\nevicting pod default/inteldeviceplugins-controller-manager-7994555cdb-g58dm\nevicting pod ingress-nginx/ingress-nginx-controller-746f764ff4-kjfjv\nI0110 21:33:20.594704 2688967 request.go:697] Waited for 1.200299817s due to client-side throttling, not priority and fairness, request: POST:https://10.0.0.6:6443/api/v1/namespaces/node-feature-discovery/pods/nfd-node-feature-discovery-master-5f56c75d-fv894/eviction\npod/audiobookshelf-987b955fb-c9vrr evicted\npod/grafana-7647f97d64-6vgdh evicted\npod/kubernetes-dashboard-6c7ccbcf87-q6z7q evicted\npod/nfd-node-feature-discovery-master-5f56c75d-fv894 evicted\npod/atuin-cdc5879b8-vf6pn evicted\npod/controller-68bf958bf9-2tztx evicted\npod/komga-549b6dd9b-bfz6n evicted\npod/cert-manager-64f9f45d6f-mxjxc evicted\npod/firefly-iii-5bd9bb7c4-wqrqb evicted\npod/dashboard-metrics-scraper-7bc864c59-5hnt8 evicted\npod/code-server-5768c9d997-rhzmx evicted\npod/navidrome-5f76b6ddf5-z9gmk evicted\npod/influxdb-57bddc4dc9-gw2lx evicted\npod/cert-manager-webhook-d4f4545d7-zbq67 evicted\npod/metrics-server-74c749979-ngtp7 evicted\npod/inteldeviceplugins-controller-manager-7994555cdb-g58dm evicted\npod/cert-manager-cainjector-56bbdd5c47-bznk2 evicted\npod/firefly-iii-mysql-68f59d48f-8n6ff evicted\npod/minecraft-server-bd48db4bd-nbzwd evicted\npod/unifi-6678ccc684-lv6lz evicted\npod/plexserver-5dbdfc898b-fmdb7 evicted\npod/mongo-d55fbb7f5-478g7 evicted\npod/homebox-7fb9d44d48-mb62g evicted\npod/coredns-76f75df574-9p9xg evicted\npod/coredns-76f75df574-4qtvd evicted\npod/ingress-nginx-controller-746f764ff4-kjfjv evicted\npod/local-path-provisioner-8bc8875b-wlbt9 evicted\npod/kavita-766577cf45-285x5 evicted\nnode/lexicon drained\n</code></pre> <p>The latest patch version is 1.30.8 and that is the one to upgrade control plane nodes to:</p> <code>apt-get update &amp;&amp; apt-get install -y kubeadm='1.30.8-*'</code> <pre><code># apt-mark unhold kubeadm &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubeadm='1.30.8-*' &amp;&amp; \\\napt-mark hold kubeadm\nCanceled hold on kubeadm.\nHit:1 http://ch.archive.ubuntu.com/ubuntu jammy InRelease\nHit:2 http://ch.archive.ubuntu.com/ubuntu jammy-updates InRelease                              \nGet:3 https://cli.github.com/packages stable InRelease [3,917 B]                               \nHit:4 http://ch.archive.ubuntu.com/ubuntu jammy-backports InRelease                            \nHit:5 https://download.docker.com/linux/ubuntu jammy InRelease                                 \nHit:6 http://ch.archive.ubuntu.com/ubuntu jammy-security InRelease                             \nHit:7 https://apt.releases.hashicorp.com jammy InRelease                                       \nIgn:8 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 InRelease                     \nHit:9 https://apt.grafana.com stable InRelease                                                 \nErr:3 https://cli.github.com/packages stable InRelease                                         \n    The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nHit:12 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release                      \nHit:10 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb  InRelease\nHit:11 https://dl.ui.com/unifi/debian unifi-7.3 InRelease                              \nErr:13 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release.gpg\n    The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nHit:14 https://esm.ubuntu.com/apps/ubuntu jammy-apps-security InRelease\nHit:15 https://esm.ubuntu.com/apps/ubuntu jammy-apps-updates InRelease\nHit:16 https://esm.ubuntu.com/infra/ubuntu jammy-infra-security InRelease\nHit:17 https://esm.ubuntu.com/infra/ubuntu jammy-infra-updates InRelease\nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://cli.github.com/packages stable InRelease: The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release: The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Failed to fetch https://cli.github.com/packages/dists/stable/InRelease  The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: Failed to fetch https://repo.mongodb.org/apt/ubuntu/dists/xenial/mongodb-org/3.6/Release.gpg  The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.30.8-1.1' (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for 'kubeadm'\nThe following additional packages will be installed:\n    cri-tools\nThe following packages will be upgraded:\n    cri-tools kubeadm\n2 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\nNeed to get 31.7 MB of archives.\nAfter this operation, 4,985 kB of additional disk space will be used.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb  cri-tools 1.30.1-1.1 [21.3 MB]\nGet:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb  kubeadm 1.30.8-1.1 [10.4 MB]\nFetched 31.7 MB in 1s (30.8 MB/s)\n(Reading database ... 224773 files and directories currently installed.)\nPreparing to unpack .../cri-tools_1.30.1-1.1_amd64.deb ...\nUnpacking cri-tools (1.30.1-1.1) over (1.28.0-1.1) ...\nPreparing to unpack .../kubeadm_1.30.8-1.1_amd64.deb ...\nUnpacking kubeadm (1.30.8-1.1) over (1.29.12-1.1) ...\nSetting up cri-tools (1.30.1-1.1) ...\nSetting up kubeadm (1.30.8-1.1) ...\nScanning processes...                                                                           \nScanning processor microcode...                                                                 \nScanning linux images...                                                                        \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubeadm set on hold.\n</code></pre> <p>Verify and apply the upgrade plan:</p> <code># kubeadm upgrade plan &amp;&amp; kubeadm upgrade apply v1.30.8</code> <pre><code># kubeadm upgrade plan\n[preflight] Running pre-flight checks.\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[upgrade] Running cluster health checks\nW0110 21:35:27.210495 2728007 health.go:132] The preflight check \"CreateJob\" was skipped because there are no schedulable Nodes in the cluster.\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: 1.29.12\n[upgrade/versions] kubeadm version: v1.30.8\nI0110 21:35:27.800090 2728007 version.go:256] remote version is much newer: v1.32.0; falling back to: stable-1.30\n[upgrade/versions] Target version: v1.30.8\n[upgrade/versions] Latest version in the v1.29 series: v1.29.12\n\nComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\nCOMPONENT   NODE      CURRENT    TARGET\nkubelet     lexicon   v1.29.12   v1.30.8\n\nUpgrade to the latest stable version:\n\nCOMPONENT                 NODE      CURRENT    TARGET\nkube-apiserver            lexicon   v1.29.12   v1.30.8\nkube-controller-manager   lexicon   v1.29.12   v1.30.8\nkube-scheduler            lexicon   v1.29.12   v1.30.8\nkube-proxy                          1.29.12    v1.30.8\nCoreDNS                             v1.11.1    v1.11.3\netcd                      lexicon   3.5.16-0   3.5.15-0\n\nYou can now apply the upgrade by executing the following command:\n\n        kubeadm upgrade apply v1.30.8\n\n_____________________________________________________________________\n\n\nThe table below shows the current state of component configs as understood by this version of kubeadm.\nConfigs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or\nresetting to kubeadm defaults before a successful upgrade can be performed. The version to manually\nupgrade to is denoted in the \"PREFERRED VERSION\" column.\n\nAPI GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED\nkubeproxy.config.k8s.io   v1alpha1          v1alpha1            no\nkubelet.config.k8s.io     v1beta1           v1beta1             no\n_____________________________________________________________________\n\n\n# kubeadm upgrade apply v1.30.8\n[preflight] Running pre-flight checks.\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[upgrade] Running cluster health checks\nW0110 21:36:16.532765 2741120 health.go:132] The preflight check \"CreateJob\" was skipped because there are no schedulable Nodes in the cluster.\n[upgrade/version] You have chosen to change the cluster version to \"v1.30.8\"\n[upgrade/versions] Cluster version: v1.29.12\n[upgrade/versions] kubeadm version: v1.30.8\n[upgrade] Are you sure you want to proceed? [y/N]: y\n[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster\n[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection\n[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'\nW0110 21:36:19.260976 2741120 checks.go:844] detected that the sandbox image \"registry.k8s.io/pause:3.6\" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.\n[upgrade/apply] Upgrading your Static Pod-hosted control plane to version \"v1.30.8\" (timeout: 5m0s)...\n[upgrade/etcd] Upgrading to TLS for etcd\n[upgrade/etcd] Non fatal issue encountered during upgrade: the desired etcd version \"3.5.15-0\" is older than the currently installed \"3.5.16-0\". Skipping etcd upgrade\n[upgrade/staticpods] Writing new Static Pod manifests to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests2222122892\"\n[upgrade/staticpods] Preparing for \"kube-apiserver\" upgrade\n[upgrade/staticpods] Renewing apiserver certificate\n[upgrade/staticpods] Renewing apiserver-kubelet-client certificate\n[upgrade/staticpods] Renewing front-proxy-client certificate\n[upgrade/staticpods] Renewing apiserver-etcd-client certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-21-36-33/kube-apiserver.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=kube-apiserver\n[upgrade/staticpods] Component \"kube-apiserver\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-controller-manager\" upgrade\n[upgrade/staticpods] Renewing controller-manager.conf certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-21-36-33/kube-controller-manager.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=kube-controller-manager\n[upgrade/staticpods] Component \"kube-controller-manager\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-scheduler\" upgrade\n[upgrade/staticpods] Renewing scheduler.conf certificate\n[upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-21-36-33/kube-scheduler.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=kube-scheduler\n[upgrade/staticpods] Component \"kube-scheduler\" upgraded successfully!\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config995930464/config.yaml\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\n[upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.30.8\". Enjoy!\n\n[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.\n</code></pre> <p>Now that the control plane is updated, proceed to upgrade kubelet and kubectl</p> <code>apt-get update &amp;&amp; apt-get install -y kubelet='1.30.8-*' kubectl='1.30.8-*'</code> <pre><code># apt-mark unhold kubelet kubectl &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubelet='1.30.8-*' kubectl='1.30.8-*' &amp;&amp; \\\napt-mark hold kubelet kubectl\nCanceled hold on kubelet.\nCanceled hold on kubectl.\nHit:1 http://ch.archive.ubuntu.com/ubuntu jammy InRelease\nGet:2 https://cli.github.com/packages stable InRelease [3,917 B]                               \nHit:3 http://ch.archive.ubuntu.com/ubuntu jammy-updates InRelease                              \nHit:4 http://ch.archive.ubuntu.com/ubuntu jammy-backports InRelease                            \nHit:5 https://download.docker.com/linux/ubuntu jammy InRelease                                 \nIgn:6 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 InRelease                     \nHit:7 http://ch.archive.ubuntu.com/ubuntu jammy-security InRelease                             \nHit:8 https://apt.releases.hashicorp.com jammy InRelease                                       \nHit:9 https://apt.grafana.com stable InRelease                                                 \nErr:2 https://cli.github.com/packages stable InRelease                                         \n    The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nHit:12 https://esm.ubuntu.com/apps/ubuntu jammy-apps-security InRelease\nHit:13 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release\nHit:14 https://esm.ubuntu.com/apps/ubuntu jammy-apps-updates InRelease                         \nHit:15 https://esm.ubuntu.com/infra/ubuntu jammy-infra-security InRelease                      \nHit:10 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb  InRelease\nHit:11 https://dl.ui.com/unifi/debian unifi-7.3 InRelease           \nHit:16 https://esm.ubuntu.com/infra/ubuntu jammy-infra-updates InRelease\nErr:17 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release.gpg\n    The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://cli.github.com/packages stable InRelease: The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release: The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Failed to fetch https://cli.github.com/packages/dists/stable/InRelease  The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: Failed to fetch https://repo.mongodb.org/apt/ubuntu/dists/xenial/mongodb-org/3.6/Release.gpg  The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.30.8-1.1' (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for 'kubelet'\nSelected version '1.30.8-1.1' (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for 'kubectl'\nThe following packages will be upgraded:\n    kubectl kubelet\n2 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\nNeed to get 28.9 MB of archives.\nAfter this operation, 11.2 MB disk space will be freed.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb  kubectl 1.30.8-1.1 [10.8 MB]\nGet:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb  kubelet 1.30.8-1.1 [18.1 MB]\nFetched 28.9 MB in 1s (29.5 MB/s)\n(Reading database ... 224773 files and directories currently installed.)\nPreparing to unpack .../kubectl_1.30.8-1.1_amd64.deb ...\nUnpacking kubectl (1.30.8-1.1) over (1.29.12-1.1) ...\nPreparing to unpack .../kubelet_1.30.8-1.1_amd64.deb ...\nUnpacking kubelet (1.30.8-1.1) over (1.29.12-1.1) ...\nSetting up kubectl (1.30.8-1.1) ...\nSetting up kubelet (1.30.8-1.1) ...\nScanning processes...                                                                           \nScanning candidates...                                                                          \nScanning processor microcode...                                                                 \nScanning linux images...                                                                        \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nRestarting services...\nsystemctl restart kubelet.service\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubelet set on hold.\nkubectl set on hold.\n\n# systemctl daemon-reload\n# systemctl restart kubelet\n\n# kubectl  version --output=yaml\nclientVersion:\n    buildDate: \"2024-12-10T11:31:16Z\"\n    compiler: gc\n    gitCommit: 354eac776046f4268e9989b21f8d1bba06033379\n    gitTreeState: clean\n    gitVersion: v1.30.8\n    goVersion: go1.22.9\n    major: \"1\"\n    minor: \"30\"\n    platform: linux/amd64\nkustomizeVersion: v5.0.4-0.20230601165947-6ce0bf390ce3\nserverVersion:\n    buildDate: \"2024-12-10T11:25:06Z\"\n    compiler: gc\n    gitCommit: 354eac776046f4268e9989b21f8d1bba06033379\n    gitTreeState: clean\n    gitVersion: v1.30.8\n    goVersion: go1.22.9\n    major: \"1\"\n    minor: \"30\"\n    platform: linux/amd64\n</code></pre> <p>Finally, bring the node back online by marking it schedulable:</p> <code># kubectl uncordon lexicon</code> <pre><code># kubectl uncordon lexicon\nnode/lexicon uncordoned\n# kubectl get pods -A\nNAMESPACE                NAME                                                     READY   STATUS              RESTARTS        AGE\natuin-server             atuin-cdc5879b8-9zg5b                                    2/2     Running             0               7m48s\naudiobookshelf           audiobookshelf-987b955fb-2m7m2                           0/1     ContainerCreating   0               7m48s\ncert-manager             cert-manager-64f9f45d6f-vdwvx                            1/1     Running             0               7m46s\ncert-manager             cert-manager-cainjector-56bbdd5c47-mlr6n                 1/1     Running             0               7m45s\ncert-manager             cert-manager-webhook-d4f4545d7-2gt2b                     0/1     Running             0               7m45s\ncode-server              code-server-5768c9d997-6xs5t                             1/1     Running             0               7m47s\ndefault                  inteldeviceplugins-controller-manager-7994555cdb-8bf65   2/2     Running             0               7m35s\nfirefly-iii              firefly-iii-5bd9bb7c4-9f5c8                              0/1     ContainerCreating   0               7m46s\nfirefly-iii              firefly-iii-mysql-68f59d48f-5t9gs                        0/1     ContainerCreating   0               7m48s\nhomebox                  homebox-7fb9d44d48-xwznt                                 0/1     ContainerCreating   0               7m44s\ningress-nginx            ingress-nginx-controller-746f764ff4-mkkzh                0/1     Running             0               7m48s\nkavita                   kavita-766577cf45-l2jf6                                  0/1     ContainerCreating   0               7m47s\nkomga                    komga-549b6dd9b-z5bwv                                    0/1     ContainerCreating   0               7m48s\nkube-flannel             kube-flannel-ds-nrrg6                                    1/1     Running             131 (15h ago)   650d\nkube-system              coredns-55cb58b774-cmrlk                                 1/1     Running             0               3m1s\nkube-system              coredns-55cb58b774-rckht                                 1/1     Running             0               3m1s\nkube-system              coredns-76f75df574-vldnl                                 1/1     Terminating         0               7m47s\nkube-system              etcd-lexicon                                             1/1     Running             1 (105s ago)    104s\nkube-system              kube-apiserver-lexicon                                   1/1     Running             1 (104s ago)    104s\nkube-system              kube-controller-manager-lexicon                          1/1     Running             1 (104s ago)    100s\nkube-system              kube-proxy-4gpjg                                         1/1     Running             0               3m1s\nkube-system              kube-scheduler-lexicon                                   1/1     Running             1 (104s ago)    104s\nkubernetes-dashboard     dashboard-metrics-scraper-7bc864c59-vwznk                1/1     Running             0               7m48s\nkubernetes-dashboard     kubernetes-dashboard-6c7ccbcf87-j46fg                    0/1     ContainerCreating   0               7m48s\nlocal-path-storage       local-path-provisioner-8bc8875b-7pn9w                    1/1     Running             0               7m48s\nmetallb-system           controller-68bf958bf9-lhckf                              0/1     Running             0               7m46s\nmetallb-system           speaker-t8f7k                                            1/1     Running             171 (15h ago)   649d\nmetrics-server           metrics-server-74c749979-wx77h                           0/1     Running             0               7m35s\nminecraft-server         minecraft-server-bd48db4bd-jkdz4                         0/1     ContainerCreating   0               7m48s\nmonitoring               grafana-7647f97d64-rnz5j                                 1/1     Running             0               7m48s\nmonitoring               influxdb-57bddc4dc9-q2z99                                1/1     Running             0               7m47s\nmonitoring               telegraf-ztg2t                                           1/1     Running             39 (15h ago)    87d\nnavidrome                navidrome-5f76b6ddf5-lc47q                               0/1     ContainerCreating   0               7m45s\nnode-feature-discovery   nfd-node-feature-discovery-master-5f56c75d-dh6fc         0/1     Running             0               7m47s\nnode-feature-discovery   nfd-node-feature-discovery-worker-xmzwk                  1/1     Running             522 (44s ago)   483d\nplexserver               plexserver-5dbdfc898b-2lfsq                              0/1     ContainerCreating   0               7m45s\nunifi                    mongo-d55fbb7f5-wz592                                    1/1     Running             0               7m44s\nunifi                    unifi-6678ccc684-6vvgc                                   1/1     Running             0               7m48s\n</code></pre> <p>After a minute or two all the services are back up and running and the Kubernetes dashboard shows all workloads green and OK again.</p>"},{"location":"blog/2025/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-lexicon/#upgrade-to-131","title":"Upgrade to 1.31","text":"<p>Upgrade version 1.30.x to version 1.31.x now, again starting by determining which version to upgrade to and updating the minor version in the repository configuration and then find the latest patch version:</p> <pre><code># curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key \\\n  | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# vi /etc/apt/sources.list.d/kubernetes.list\ndeb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /\n\n# apt update\n# apt-cache madison kubeadm\n   kubeadm | 1.31.4-1.1 | https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages\n   kubeadm | 1.31.3-1.1 | https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages\n   kubeadm | 1.31.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages\n   kubeadm | 1.31.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages\n   kubeadm | 1.31.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages\n</code></pre> <p>Now, before updating <code>kubeadm</code>, drain the node again:</p> <code># kubectl drain --ignore-daemonsets --delete-emptydir-data lexicon</code> <pre><code># kubectl drain --ignore-daemonsets --delete-emptydir-data lexicon\nnode/lexicon cordoned\nWarning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-nrrg6, kube-system/kube-proxy-4gpjg, metallb-system/speaker-t8f7k, monitoring/telegraf-ztg2t, node-feature-discovery/nfd-node-feature-discovery-worker-xmzwk\nevicting pod unifi/unifi-6678ccc684-6vvgc\nevicting pod atuin-server/atuin-cdc5879b8-9zg5b\nevicting pod audiobookshelf/audiobookshelf-987b955fb-2m7m2\nevicting pod cert-manager/cert-manager-64f9f45d6f-vdwvx\nevicting pod cert-manager/cert-manager-cainjector-56bbdd5c47-mlr6n\nevicting pod cert-manager/cert-manager-webhook-d4f4545d7-2gt2b\nevicting pod kubernetes-dashboard/kubernetes-dashboard-6c7ccbcf87-j46fg\nevicting pod kubernetes-dashboard/dashboard-metrics-scraper-7bc864c59-vwznk\nevicting pod monitoring/influxdb-57bddc4dc9-q2z99\nevicting pod code-server/code-server-5768c9d997-6xs5t\nevicting pod navidrome/navidrome-5f76b6ddf5-lc47q\nevicting pod homebox/homebox-7fb9d44d48-xwznt\nevicting pod ingress-nginx/ingress-nginx-controller-746f764ff4-mkkzh\nevicting pod local-path-storage/local-path-provisioner-8bc8875b-7pn9w\nevicting pod kavita/kavita-766577cf45-l2jf6\nevicting pod default/inteldeviceplugins-controller-manager-7994555cdb-8bf65\nevicting pod node-feature-discovery/nfd-node-feature-discovery-master-5f56c75d-dh6fc\nevicting pod komga/komga-549b6dd9b-z5bwv\nevicting pod firefly-iii/firefly-iii-5bd9bb7c4-9f5c8\nevicting pod metrics-server/metrics-server-74c749979-wx77h\nevicting pod plexserver/plexserver-5dbdfc898b-2lfsq\nevicting pod kube-system/coredns-55cb58b774-rckht\nevicting pod metallb-system/controller-68bf958bf9-lhckf\nevicting pod kube-system/coredns-55cb58b774-cmrlk\nevicting pod unifi/mongo-d55fbb7f5-wz592\nevicting pod firefly-iii/firefly-iii-mysql-68f59d48f-5t9gs\nevicting pod monitoring/grafana-7647f97d64-rnz5j\nI0110 22:43:43.853741 4143116 request.go:697] Waited for 1.000055459s due to client-side throttling, not priority and fairness, request: POST:https://10.0.0.6:6443/api/v1/namespaces/default/pods/inteldeviceplugins-controller-manager-7994555cdb-8bf65/eviction\npod/navidrome-5f76b6ddf5-lc47q evicted\npod/audiobookshelf-987b955fb-2m7m2 evicted\npod/cert-manager-cainjector-56bbdd5c47-mlr6n evicted\npod/atuin-cdc5879b8-9zg5b evicted\npod/code-server-5768c9d997-6xs5t evicted\npod/cert-manager-64f9f45d6f-vdwvx evicted\npod/cert-manager-webhook-d4f4545d7-2gt2b evicted\npod/dashboard-metrics-scraper-7bc864c59-vwznk evicted\npod/nfd-node-feature-discovery-master-5f56c75d-dh6fc evicted\npod/influxdb-57bddc4dc9-q2z99 evicted\npod/homebox-7fb9d44d48-xwznt evicted\npod/firefly-iii-5bd9bb7c4-9f5c8 evicted\npod/metrics-server-74c749979-wx77h evicted\npod/kubernetes-dashboard-6c7ccbcf87-j46fg evicted\npod/inteldeviceplugins-controller-manager-7994555cdb-8bf65 evicted\npod/controller-68bf958bf9-lhckf evicted\npod/firefly-iii-mysql-68f59d48f-5t9gs evicted\npod/mongo-d55fbb7f5-wz592 evicted\npod/grafana-7647f97d64-rnz5j evicted\npod/unifi-6678ccc684-6vvgc evicted\npod/komga-549b6dd9b-z5bwv evicted\npod/plexserver-5dbdfc898b-2lfsq evicted\npod/coredns-55cb58b774-cmrlk evicted\npod/coredns-55cb58b774-rckht evicted\npod/ingress-nginx-controller-746f764ff4-mkkzh evicted\npod/kavita-766577cf45-l2jf6 evicted\npod/local-path-provisioner-8bc8875b-7pn9w evicted\nnode/lexicon drained\n</code></pre> <p>The latest patch version is 1.31.4 and that is the one to upgrade control plane nodes to:</p> <code>apt-get update &amp;&amp; apt-get install -y kubeadm='1.31.4-*'</code> <pre><code># apt-mark unhold kubeadm &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubeadm='1.31.4-*' &amp;&amp; \\\napt-mark hold kubeadm\nCanceled hold on kubeadm.\nHit:1 http://ch.archive.ubuntu.com/ubuntu jammy InRelease\nHit:2 https://download.docker.com/linux/ubuntu jammy InRelease                                 \nHit:3 http://ch.archive.ubuntu.com/ubuntu jammy-updates InRelease                              \nHit:4 https://apt.releases.hashicorp.com jammy InRelease                                       \nIgn:5 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 InRelease                     \nHit:6 http://ch.archive.ubuntu.com/ubuntu jammy-backports InRelease                            \nHit:7 https://apt.grafana.com stable InRelease                                                 \nHit:8 http://ch.archive.ubuntu.com/ubuntu jammy-security InRelease                             \nGet:9 https://cli.github.com/packages stable InRelease [3,917 B]                               \nGet:12 https://esm.ubuntu.com/apps/ubuntu jammy-apps-security InRelease [7,565 B]              \nGet:13 https://esm.ubuntu.com/apps/ubuntu jammy-apps-updates InRelease [7,456 B]\nHit:14 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release             \nGet:15 https://esm.ubuntu.com/infra/ubuntu jammy-infra-security InRelease [7,450 B]            \nGet:16 https://esm.ubuntu.com/infra/ubuntu jammy-infra-updates InRelease [7,449 B]             \nHit:10 https://dl.ui.com/unifi/debian unifi-7.3 InRelease                                      \nHit:11 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  InRelease\nErr:9 https://cli.github.com/packages stable InRelease\n    The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nErr:17 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release.gpg\n    The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nFetched 29.9 kB in 1s (29.8 kB/s)\nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://cli.github.com/packages stable InRelease: The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release: The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Failed to fetch https://cli.github.com/packages/dists/stable/InRelease  The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: Failed to fetch https://repo.mongodb.org/apt/ubuntu/dists/xenial/mongodb-org/3.6/Release.gpg  The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.31.4-1.1' (isv:kubernetes:core:stable:v1.31:pkgs.k8s.io [amd64]) for 'kubeadm'\nThe following packages will be upgraded:\n    kubeadm\n1 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\nNeed to get 11.4 MB of archives.\nAfter this operation, 8,032 kB of additional disk space will be used.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubeadm 1.31.4-1.1 [11.4 MB]\nFetched 11.4 MB in 1s (14.8 MB/s)\n(Reading database ... 224773 files and directories currently installed.)\nPreparing to unpack .../kubeadm_1.31.4-1.1_amd64.deb ...\nUnpacking kubeadm (1.31.4-1.1) over (1.30.8-1.1) ...\nSetting up kubeadm (1.31.4-1.1) ...\nScanning processes...                                                                           \nScanning processor microcode...                                                                 \nScanning linux images...                                                                        \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubeadm set on hold.\n</code></pre> <p>Verify and apply the upgrade plan:</p> <code># kubeadm upgrade plan &amp;&amp; kubeadm upgrade apply v1.31.4</code> <pre><code># kubeadm upgrade plan\n[preflight] Running pre-flight checks.\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[upgrade] Running cluster health checks\nW0110 22:45:52.938665 4182951 health.go:132] The preflight check \"CreateJob\" was skipped because there are no schedulable Nodes in the cluster.\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: 1.30.8\n[upgrade/versions] kubeadm version: v1.31.4\nI0110 22:45:53.506618 4182951 version.go:261] remote version is much newer: v1.32.0; falling back to: stable-1.31\n[upgrade/versions] Target version: v1.31.4\n[upgrade/versions] Latest version in the v1.30 series: v1.30.8\n\nComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\nCOMPONENT   NODE      CURRENT   TARGET\nkubelet     lexicon   v1.30.8   v1.31.4\n\nUpgrade to the latest stable version:\n\nCOMPONENT                 NODE      CURRENT    TARGET\nkube-apiserver            lexicon   v1.30.8    v1.31.4\nkube-controller-manager   lexicon   v1.30.8    v1.31.4\nkube-scheduler            lexicon   v1.30.8    v1.31.4\nkube-proxy                          1.30.8     v1.31.4\nCoreDNS                             v1.11.3    v1.11.3\netcd                      lexicon   3.5.16-0   3.5.15-0\n\nYou can now apply the upgrade by executing the following command:\n\n        kubeadm upgrade apply v1.31.4\n\n_____________________________________________________________________\n\n\nThe table below shows the current state of component configs as understood by this version of kubeadm.\nConfigs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or\nresetting to kubeadm defaults before a successful upgrade can be performed. The version to manually\nupgrade to is denoted in the \"PREFERRED VERSION\" column.\n\nAPI GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED\nkubeproxy.config.k8s.io   v1alpha1          v1alpha1            no\nkubelet.config.k8s.io     v1beta1           v1beta1             no\n_____________________________________________________________________\n\n\n# kubeadm upgrade apply v1.31.4\n[preflight] Running pre-flight checks.\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[upgrade] Running cluster health checks\nW0110 22:45:59.615950 4184761 health.go:132] The preflight check \"CreateJob\" was skipped because there are no schedulable Nodes in the cluster.\n[upgrade/version] You have chosen to change the cluster version to \"v1.31.4\"\n[upgrade/versions] Cluster version: v1.30.8\n[upgrade/versions] kubeadm version: v1.31.4\n[upgrade] Are you sure you want to proceed? [y/N]: y\n[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster\n[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection\n[upgrade/prepull] You can also perform this action beforehand using 'kubeadm config images pull'\nW0110 22:46:02.831598 4184761 checks.go:846] detected that the sandbox image \"registry.k8s.io/pause:3.6\" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use \"registry.k8s.io/pause:3.10\" as the CRI sandbox image.\n[upgrade/apply] Upgrading your Static Pod-hosted control plane to version \"v1.31.4\" (timeout: 5m0s)...\n[upgrade/staticpods] Writing new Static Pod manifests to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests542531162\"\n[upgrade/etcd] Non fatal issue encountered during upgrade: the desired etcd version \"3.5.15-0\" is older than the currently installed \"3.5.16-0\". Skipping etcd upgrade\n[upgrade/staticpods] Preparing for \"kube-apiserver\" upgrade\n[upgrade/staticpods] Renewing apiserver certificate\n[upgrade/staticpods] Renewing apiserver-kubelet-client certificate\n[upgrade/staticpods] Renewing front-proxy-client certificate\n[upgrade/staticpods] Renewing apiserver-etcd-client certificate\n[upgrade/staticpods] Moving new manifest to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" and backing up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-22-46-13/kube-apiserver.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=kube-apiserver\n[upgrade/staticpods] Component \"kube-apiserver\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-controller-manager\" upgrade\n[upgrade/staticpods] Renewing controller-manager.conf certificate\n[upgrade/staticpods] Moving new manifest to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" and backing up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-22-46-13/kube-controller-manager.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=kube-controller-manager\n[upgrade/staticpods] Component \"kube-controller-manager\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-scheduler\" upgrade\n[upgrade/staticpods] Renewing scheduler.conf certificate\n[upgrade/staticpods] Moving new manifest to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" and backing up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2025-01-10-22-46-13/kube-scheduler.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=kube-scheduler\n[upgrade/staticpods] Component \"kube-scheduler\" upgraded successfully!\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config1002620026/config.yaml\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\n[upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.31.4\". Enjoy!\n\n[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.\n</code></pre> <p>Now that the control plane is updated, proceed to upgrade kubelet and kubectl</p> <code>apt-get update &amp;&amp; apt-get install -y kubelet='1.31.4-*' kubectl='1.31.4-*'</code> <pre><code># apt-mark unhold kubelet kubectl &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubelet='1.31.4-*' kubectl='1.31.4-*' &amp;&amp; \\\napt-mark hold kubelet kubectl\nCanceled hold on kubelet.\nCanceled hold on kubectl.\nIgn:1 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 InRelease\nHit:2 https://download.docker.com/linux/ubuntu jammy InRelease                                 \nGet:3 https://cli.github.com/packages stable InRelease [3,917 B]                               \nHit:4 http://ch.archive.ubuntu.com/ubuntu jammy InRelease                                      \nHit:5 http://ch.archive.ubuntu.com/ubuntu jammy-updates InRelease                              \nHit:6 https://apt.releases.hashicorp.com jammy InRelease                                       \nHit:7 https://apt.grafana.com stable InRelease                                                 \nHit:9 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release                       \nErr:3 https://cli.github.com/packages stable InRelease                                         \n    The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nHit:8 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  InRelease\nHit:11 http://ch.archive.ubuntu.com/ubuntu jammy-backports InRelease                           \nHit:10 https://dl.ui.com/unifi/debian unifi-7.3 InRelease           \nHit:12 http://ch.archive.ubuntu.com/ubuntu jammy-security InRelease \nErr:13 https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release.gpg\n    The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nHit:14 https://esm.ubuntu.com/apps/ubuntu jammy-apps-security InRelease\nHit:15 https://esm.ubuntu.com/apps/ubuntu jammy-apps-updates InRelease\nHit:16 https://esm.ubuntu.com/infra/ubuntu jammy-infra-security InRelease\nHit:17 https://esm.ubuntu.com/infra/ubuntu jammy-infra-updates InRelease\nReading package lists... Done\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://cli.github.com/packages stable InRelease: The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.6 Release: The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Failed to fetch https://cli.github.com/packages/dists/stable/InRelease  The following signatures were invalid: EXPKEYSIG 23F3D4EA75716059 GitHub CLI &lt;opensource+cli@github.com&gt;\nW: Failed to fetch https://repo.mongodb.org/apt/ubuntu/dists/xenial/mongodb-org/3.6/Release.gpg  The following signatures were invalid: EXPKEYSIG 58712A2291FA4AD5 MongoDB 3.6 Release Signing Key &lt;packaging@mongodb.com&gt;\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.31.4-1.1' (isv:kubernetes:core:stable:v1.31:pkgs.k8s.io [amd64]) for 'kubelet'\nSelected version '1.31.4-1.1' (isv:kubernetes:core:stable:v1.31:pkgs.k8s.io [amd64]) for 'kubectl'\nThe following packages will be upgraded:\n    kubectl kubelet\n2 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\nNeed to get 26.4 MB of archives.\nAfter this operation, 18.3 MB disk space will be freed.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubectl 1.31.4-1.1 [11.2 MB]\nGet:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.31/deb  kubelet 1.31.4-1.1 [15.2 MB]\nFetched 26.4 MB in 1s (36.9 MB/s)\n(Reading database ... 224773 files and directories currently installed.)\nPreparing to unpack .../kubectl_1.31.4-1.1_amd64.deb ...\nUnpacking kubectl (1.31.4-1.1) over (1.30.8-1.1) ...\nPreparing to unpack .../kubelet_1.31.4-1.1_amd64.deb ...\nUnpacking kubelet (1.31.4-1.1) over (1.30.8-1.1) ...\nSetting up kubectl (1.31.4-1.1) ...\nSetting up kubelet (1.31.4-1.1) ...\nScanning processes...                                                                           \nScanning candidates...                                                                          \nScanning processor microcode...                                                                 \nScanning linux images...                                                                        \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nRestarting services...\nsystemctl restart kubelet.service\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubelet set on hold.\nkubectl set on hold.\n\n# systemctl daemon-reload\n# systemctl restart kubelet\n\n# kubectl  version --output=yaml\nclientVersion:\n    buildDate: \"2024-12-10T11:43:21Z\"\n    compiler: gc\n    gitCommit: a78aa47129b8539636eb86a9d00e31b2720fe06b\n    gitTreeState: clean\n    gitVersion: v1.31.4\n    goVersion: go1.22.9\n    major: \"1\"\n    minor: \"31\"\n    platform: linux/amd64\nkustomizeVersion: v5.4.2\nserverVersion:\n    buildDate: \"2024-12-10T11:37:27Z\"\n    compiler: gc\n    gitCommit: a78aa47129b8539636eb86a9d00e31b2720fe06b\n    gitTreeState: clean\n    gitVersion: v1.31.4\n    goVersion: go1.22.9\n    major: \"1\"\n    minor: \"31\"\n    platform: linux/amd64\n</code></pre> <p>Finally, bring the node back online by marking it schedulable:</p> <code># kubectl uncordon lexicon</code> <pre><code># kubectl uncordon lexicon\nnode/lexicon uncordoned\n# kubectl get pods -A\nNAMESPACE                NAME                                                     READY   STATUS              RESTARTS        AGE\natuin-server             atuin-cdc5879b8-xkv7l                                    0/2     ContainerCreating   0               5m4s\naudiobookshelf           audiobookshelf-987b955fb-b7dsq                           0/1     ContainerCreating   0               5m4s\ncert-manager             cert-manager-64f9f45d6f-6zrtz                            0/1     ContainerCreating   0               5m4s\ncert-manager             cert-manager-cainjector-56bbdd5c47-h4w7c                 0/1     ContainerCreating   0               5m4s\ncert-manager             cert-manager-webhook-d4f4545d7-zvwr7                     0/1     ContainerCreating   0               5m4s\ncode-server              code-server-5768c9d997-4mk8g                             0/1     ContainerCreating   0               5m4s\ndefault                  inteldeviceplugins-controller-manager-7994555cdb-87rsx   0/2     ContainerCreating   0               5m3s\nfirefly-iii              firefly-iii-5bd9bb7c4-ps87g                              0/1     ContainerCreating   0               4m53s\nfirefly-iii              firefly-iii-mysql-68f59d48f-lc9n5                        0/1     ContainerCreating   0               4m53s\nhomebox                  homebox-7fb9d44d48-78s7v                                 0/1     ContainerCreating   0               5m4s\ningress-nginx            ingress-nginx-controller-746f764ff4-lpvss                0/1     Running             0               5m4s\nkavita                   kavita-766577cf45-hr848                                  0/1     ContainerCreating   0               5m4s\nkomga                    komga-549b6dd9b-xktm8                                    0/1     ContainerCreating   0               4m53s\nkube-flannel             kube-flannel-ds-nrrg6                                    1/1     Running             131 (16h ago)   650d\nkube-system              coredns-55cb58b774-dzhbn                                 0/1     ContainerCreating   0               4m53s\nkube-system              coredns-55cb58b774-n4fdw                                 0/1     ContainerCreating   0               4m53s\nkube-system              etcd-lexicon                                             1/1     Running             1 (27s ago)     23s\nkube-system              kube-apiserver-lexicon                                   1/1     Running             1 (27s ago)     23s\nkube-system              kube-controller-manager-lexicon                          1/1     Running             1 (27s ago)     23s\nkube-system              kube-proxy-59lmn                                         1/1     Running             1 (27s ago)     53s\nkube-system              kube-scheduler-lexicon                                   1/1     Running             1 (27s ago)     27s\nkubernetes-dashboard     dashboard-metrics-scraper-7bc864c59-khpwq                0/1     ContainerCreating   0               5m4s\nkubernetes-dashboard     kubernetes-dashboard-6c7ccbcf87-nzssm                    0/1     ContainerCreating   0               5m4s\nlocal-path-storage       local-path-provisioner-8bc8875b-f6s75                    0/1     ContainerCreating   0               4m53s\nmetallb-system           controller-68bf958bf9-5rfgl                              0/1     ContainerCreating   0               4m53s\nmetallb-system           speaker-t8f7k                                            1/1     Running             172 (27s ago)   649d\nmetrics-server           metrics-server-74c749979-dpq84                           0/1     Running             0               4m53s\nmonitoring               grafana-7647f97d64-wzgsp                                 0/1     ContainerCreating   0               4m53s\nmonitoring               influxdb-57bddc4dc9-pqf8l                                1/1     Running             0               5m4s\nmonitoring               telegraf-ztg2t                                           1/1     Running             40 (19s ago)    87d\nnavidrome                navidrome-5f76b6ddf5-27sjc                               0/1     ContainerCreating   0               5m4s\nnode-feature-discovery   nfd-node-feature-discovery-master-5f56c75d-d7xs5         0/1     ContainerCreating   0               4m53s\nnode-feature-discovery   nfd-node-feature-discovery-worker-xmzwk                  1/1     Running             527 (27s ago)   483d\nplexserver               plexserver-5dbdfc898b-h7lm4                              0/1     ContainerCreating   0               4m53s\nunifi                    mongo-d55fbb7f5-p79nc                                    0/1     ContainerCreating   0               4m53s\nunifi                    unifi-6678ccc684-85kcr                                   0/1     ContainerCreating   0               5m4s\n</code></pre> <p>After a minute or two all the services are back up and running and the Kubernetes dashboard shows all workloads green and OK again.</p>"},{"location":"blog/2025/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-lexicon/#kubernetes-certificate-check","title":"Kubernetes Certificate Check","text":"<p>Kubernetes Certificate Expired less than a year ago, so it's about time to check whether the certificates have been automatically renewed since then:</p> <pre><code># ls -l /var/lib/kubelet/pki\ntotal 32\n-rw------- 1 root root 2826 Mar 22  2023 kubelet-client-2023-03-22-22-37-39.pem\n-rw------- 1 root root 1110 Jan 15  2024 kubelet-client-2024-01-15-16-44-55.pem\n-rw------- 1 root root 1110 Oct 11 06:08 kubelet-client-2024-10-11-06-08-33.pem\nlrwxrwxrwx 1 root root   59 Oct 11 06:08 kubelet-client-current.pem -&gt; /var/lib/kubelet/pki/kubelet-client-2024-10-11-06-08-33.pem\n-rw-r--r-- 1 root root 2246 Mar 22  2023 kubelet.crt\n-rw------- 1 root root 1675 Mar 22  2023 kubelet.key\n-rw------- 1 root root 1147 Mar 23  2024 kubelet-server-2024-03-23-13-17-35.pem\nlrwxrwxrwx 1 root root   59 Mar 23  2024 kubelet-server-current.pem -&gt; /var/lib/kubelet/pki/kubelet-server-2024-03-23-13-17-35.pem\n\n# openssl x509 -in /var/lib/kubelet/pki/kubelet.crt -text -noout  | grep -A 2 Validity\n        Validity\n            Not Before: Mar 22 20:37:39 2023 GMT\n            Not After : Mar 21 20:37:39 2024 GMT\n</code></pre> <p>Nope, the server certificate has not been renewed since then, at least not under <code>/var/lib/kubelet/pki</code>, but at the same time <code>kubeadm certs check-expiration</code> tells a different story: that certificates were renewed today, as expectd, when upgrading:</p> <pre><code># kubeadm certs check-expiration\n[check-expiration] Reading configuration from the cluster...\n[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n\nCERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED\nadmin.conf                 Jan 10, 2026 21:45 UTC   364d            ca                      no      \napiserver                  Jan 10, 2026 21:45 UTC   364d            ca                      no      \napiserver-etcd-client      Jan 10, 2026 21:45 UTC   364d            etcd-ca                 no      \napiserver-kubelet-client   Jan 10, 2026 21:45 UTC   364d            ca                      no      \ncontroller-manager.conf    Jan 10, 2026 21:45 UTC   364d            ca                      no      \netcd-healthcheck-client    Jan 10, 2026 20:21 UTC   364d            etcd-ca                 no      \netcd-peer                  Jan 10, 2026 20:21 UTC   364d            etcd-ca                 no      \netcd-server                Jan 10, 2026 20:21 UTC   364d            etcd-ca                 no      \nfront-proxy-client         Jan 10, 2026 21:45 UTC   364d            front-proxy-ca          no      \nscheduler.conf             Jan 10, 2026 21:45 UTC   364d            ca                      no      \nsuper-admin.conf           Jan 10, 2026 21:45 UTC   364d            ca                      no      \n\nCERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED\nca                      Mar 19, 2033 21:37 UTC   8y              no      \netcd-ca                 Mar 19, 2033 21:37 UTC   8y              no      \nfront-proxy-ca          Mar 19, 2033 21:37 UTC   8y              no      \n\n# kubectl -n kube-system get cm kubeadm-config -o yaml\napiVersion: v1\ndata:\n  ClusterConfiguration: |\n    apiServer:\n      extraArgs:\n      - name: authorization-mode\n        value: Node,RBAC\n    apiVersion: kubeadm.k8s.io/v1beta4\n    caCertificateValidityPeriod: 87600h0m0s\n    certificateValidityPeriod: 8760h0m0s\n    certificatesDir: /etc/kubernetes/pki\n    clusterName: kubernetes\n    controllerManager: {}\n    dns: {}\n    encryptionAlgorithm: RSA-2048\n    etcd:\n      local:\n        dataDir: /var/lib/etcd\n    imageRepository: registry.k8s.io\n    kind: ClusterConfiguration\n    kubernetesVersion: v1.31.4\n    networking:\n      dnsDomain: cluster.local\n      podSubnet: 10.244.0.0/16\n      serviceSubnet: 10.96.0.0/12\n    proxy: {}\n    scheduler: {}\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2023-03-22T21:37:45Z\"\n  name: kubeadm-config\n  namespace: kube-system\n  resourceVersion: \"90388726\"\n  uid: 0128b0c9-1380-442e-a8b1-4284cf1255bf\n</code></pre> <p>In the cluster configuration <code>certificatesDir: /etc/kubernetes/pki</code>, along with where certificates are stored, suggest that those are the relevant ceritifcats, not necessarily those under <code>/var/lib/kubelet/pki/</code>, and indeed the former directory has renwed certs:</p> <pre><code># ls -l /etc/kubernetes/pki/*\n-rw-r--r-- 1 root root 1281 Jan 10 22:46 /etc/kubernetes/pki/apiserver.crt\n-rw-r--r-- 1 root root 1123 Jan 10 22:46 /etc/kubernetes/pki/apiserver-etcd-client.crt\n-rw------- 1 root root 1675 Jan 10 22:46 /etc/kubernetes/pki/apiserver-etcd-client.key\n-rw------- 1 root root 1679 Jan 10 22:46 /etc/kubernetes/pki/apiserver.key\n-rw-r--r-- 1 root root 1176 Jan 10 22:46 /etc/kubernetes/pki/apiserver-kubelet-client.crt\n-rw------- 1 root root 1679 Jan 10 22:46 /etc/kubernetes/pki/apiserver-kubelet-client.key\n-rw-r--r-- 1 root root 1099 Mar 22  2023 /etc/kubernetes/pki/ca.crt\n-rw------- 1 root root 1679 Mar 22  2023 /etc/kubernetes/pki/ca.key\n-rw-r--r-- 1 root root 1115 Mar 22  2023 /etc/kubernetes/pki/front-proxy-ca.crt\n-rw------- 1 root root 1675 Mar 22  2023 /etc/kubernetes/pki/front-proxy-ca.key\n-rw-r--r-- 1 root root 1119 Jan 10 22:46 /etc/kubernetes/pki/front-proxy-client.crt\n-rw------- 1 root root 1679 Jan 10 22:46 /etc/kubernetes/pki/front-proxy-client.key\n-rw------- 1 root root 1679 Mar 22  2023 /etc/kubernetes/pki/sa.key\n-rw------- 1 root root  451 Mar 22  2023 /etc/kubernetes/pki/sa.pub\n\n/etc/kubernetes/pki/etcd:\ntotal 32\n-rw-r--r-- 1 root root 1086 Mar 22  2023 ca.crt\n-rw------- 1 root root 1675 Mar 22  2023 ca.key\n-rw-r--r-- 1 root root 1123 Jan 10 21:21 healthcheck-client.crt\n-rw------- 1 root root 1679 Jan 10 21:21 healthcheck-client.key\n-rw-r--r-- 1 root root 1196 Jan 10 21:21 peer.crt\n-rw------- 1 root root 1679 Jan 10 21:21 peer.key\n-rw-r--r-- 1 root root 1196 Jan 10 21:21 server.crt\n-rw------- 1 root root 1675 Jan 10 21:21 server.key\n</code></pre> <p>So it seems rotation of server certificates is working correctly.</p> <p>This should lead to server certificates being rotated every time the cluster patch version is upgraded, which should happen definitely more than once or twice per year.</p>"},{"location":"blog/2025/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-lexicon/#upgrade-to-132","title":"Upgrade to 1.32","text":"<p>Upgrade version 1.31.x to version 1.32.x now, again starting by determining which version to upgrade to and updating the minor version in the repository configuration and then find the latest patch version:</p> <pre><code># curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key \\\n  | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# vi /etc/apt/sources.list.d/kubernetes.list\ndeb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /\n\n# apt update\n# apt-cache madison kubeadm\n   kubeadm | 1.32.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.32/deb  Packages\n</code></pre> <p>The latest patch version is 1.32.0; there is only one patch version.</p> <p>Mathew Duggan's Upgrading Kubernetes - A Practical Guide makes a strong recommendation to not upgrade the minor version until it hits patch .2 at least.</p>"},{"location":"blog/2025/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-lexicon/#for-later-1323","title":"For later (1.32.3)","text":"<p>Now, before updating <code>kubeadm</code>, drain the node again:</p> <code># kubectl drain --ignore-daemonsets --delete-emptydir-data lexicon</code> <pre><code># kubectl drain --ignore-daemonsets --delete-emptydir-data lexicon\n</code></pre> <p>The latest patch version is 1.32.3 and that is the one to upgrade control plane nodes to:</p> <code>apt-get update &amp;&amp; apt-get install -y kubeadm='1.32.3-*'</code> <pre><code># apt-mark unhold kubeadm &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubeadm='1.32.3-*' &amp;&amp; \\\napt-mark hold kubeadm\n</code></pre> <p>Verify and apply the upgrade plan:</p> <code># kubeadm upgrade plan &amp;&amp; kubeadm upgrade apply v1.32.3</code> <pre><code># kubeadm upgrade plan\n\n# kubeadm upgrade apply v1.32.3\n</code></pre> <p>Now that the control plane is updated, proceed to upgrade kubelet and kubectl</p> <code>apt-get update &amp;&amp; apt-get install -y kubelet='1.32.0-*' kubectl='1.32.0-*'</code> <pre><code># apt-mark unhold kubelet kubectl &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubelet='1.32.3-*' kubectl='1.32.3-*' &amp;&amp; \\\napt-mark hold kubelet kubectl\n\n# systemctl daemon-reload\n# systemctl restart kubelet\n\n# kubectl  version --output=yaml\n</code></pre> <p>Finally, bring the node back online by marking it schedulable:</p> <code># kubectl uncordon lexicon</code> <pre><code># kubectl uncordon lexicon\nnode/lexicon uncordoned\n# kubectl get pods -A\n</code></pre> <p>After a minute or two all the services are back up and running and the Kubernetes dashboard shows all workloads green and OK again.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/","title":"Home Assistant on Kubernetes on Raspberry Pi 5 (Alfred)","text":"<p>That old house with aging electrical wiring, where last winter we needed Continuous Monitoring for TP-Link Tapo devices to keep power consumption in check at all times, could do with a more versatile and capable setup, to at least partially automate the juggling involved in keeping power consumption within the contracted capacity.</p> <p>Home Assistant should be a good way to scale this up, but what that old house needs in the first place is a 24x7 system, so here we go again to setup a brand new Raspberry Pi... enter Alfred, the new housekeeper.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#hardware","title":"Hardware","text":"<p>Although it may be overkill for the initial few purposes, the hardware chosen for this system is a rather generous setup to leave plenty of room for growth:</p> <ul> <li>Raspberry Pi 5 8GB</li> <li>Samsung 970 EVO Plus (2000 GB, M.2 2280)</li> <li>Argon ONE V3 M.2 NVME Raspberry Pi 5 Case</li> </ul> <p>This Argon ONE case is a bit tricky to assemble, it pays to follow the instructions carefully and make sure the PCIe cable is connected the right way and fully connected.</p> How this NVMe enclosure shows up in <code>dmesg</code>. <pre><code>[103602.068970] sd 10:0:0:0: [sdf] tag#11 uas_zap_pending 0 uas-tag 1 inflight: CMD \n[103602.068975] sd 10:0:0:0: [sdf] tag#11 CDB: Test Unit Ready 00 00 00 00 00 00\n[103602.069000] sd 10:0:0:0: [sdf] Test Unit Ready failed: Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK\n[103602.069013] sd 10:0:0:0: [sdf] Read Capacity(16) failed: Result: hostbyte=DID_ERROR driverbyte=DRIVER_OK\n[103602.069015] sd 10:0:0:0: [sdf] Sense not available.\n[103602.069022] sd 10:0:0:0: [sdf] Read Capacity(10) failed: Result: hostbyte=DID_ERROR driverbyte=DRIVER_OK\n[103602.069024] sd 10:0:0:0: [sdf] Sense not available.\n[103602.069029] sd 10:0:0:0: [sdf] 0 512-byte logical blocks: (0 B/0 B)\n[103602.069032] sd 10:0:0:0: [sdf] 0-byte physical blocks\n[103602.069035] sd 10:0:0:0: [sdf] Write Protect is off\n[103602.069038] sd 10:0:0:0: [sdf] Mode Sense: 00 00 00 00\n[103602.069042] sd 10:0:0:0: [sdf] Asking for cache data failed\n[103602.069046] sd 10:0:0:0: [sdf] Assuming drive cache: write through\n[103602.069049] sd 10:0:0:0: [sdf] Preferred minimum I/O size 4096 bytes not a multiple of physical block size (0 bytes)\n[103602.069052] sd 10:0:0:0: [sdf] Optimal transfer size 33553920 bytes not a multiple of physical block size (0 bytes)\n[103602.069483] sd 10:0:0:0: [sdf] Attached SCSI disk\n[103603.389453] usb 4-1: new SuperSpeed USB device number 3 using xhci_hcd\n[103603.402010] usb 4-1: New USB device found, idVendor=152d, idProduct=0583, bcdDevice=31.08\n[103603.402017] usb 4-1: New USB device strings: Mfr=1, Product=2, SerialNumber=3\n[103603.402019] usb 4-1: Product: USB Storage Device\n[103603.402021] usb 4-1: Manufacturer: JMicron\n[103603.402023] usb 4-1: SerialNumber: DD564198838E3\n[103603.403724] scsi host10: uas\n[103603.404249] scsi 10:0:0:0: Direct-Access     Samsung  SSD 970 EVO      3108 PQ: 0 ANSI: 6\n[103603.406054] sd 10:0:0:0: Attached scsi generic sg5 type 0\n[103603.406383] sd 10:0:0:0: [sdf] 3907029168 512-byte logical blocks: (2.00 TB/1.82 TiB)\n[103603.406386] sd 10:0:0:0: [sdf] 4096-byte physical blocks\n[103603.406504] sd 10:0:0:0: [sdf] Write Protect is off\n[103603.406507] sd 10:0:0:0: [sdf] Mode Sense: 5f 00 00 08\n[103603.406703] sd 10:0:0:0: [sdf] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\n[103603.406706] sd 10:0:0:0: [sdf] Preferred minimum I/O size 4096 bytes\n[103603.406708] sd 10:0:0:0: [sdf] Optimal transfer size 33553920 bytes not a multiple of preferred minimum block size (4096 bytes)\n[103603.410179]  sdf: sdf1 sdf2 sdf3 sdf4 sdf5 sdf6\n[103603.410501] sd 10:0:0:0: [sdf] Attached SCSI disk\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#install-to-nvme-ssd","title":"Install to NVMe SSD","text":"<p>To install Raspberry Pi Os directly on the NVMe drive, it goes first into an ICY BOX IB-1817M-C31 USB 3.1 NVMe enclosure to then use <code>rpi-imager</code> to install the latest 64-bit Raspberry Pi Os Lite on it. This will replace original partitions with just two (<code>bootfs</code>, <code>rootfs</code>):</p> <pre><code>[104897.486810]  sdf: sdf1 sdf2\n</code></pre> <pre><code># parted /dev/sdf print\nModel: Samsung SSD 970 EVO (scsi)\nDisk /dev/sdf: 2000GB\nSector size (logical/physical): 512B/4096B\nPartition Table: msdos\nDisk Flags: \n\nNumber  Start   End     Size    Type     File system  Flags\n 1      4194kB  541MB   537MB   primary  fat32        lba\n 2      541MB   2756MB  2215MB  primary  ext4\n</code></pre> GPT would be necessary only if the disk is larger than 2TB. <p>Booting Pi from NVME greater than 2TB (GPT as opposed to MBR) includes manual instructions to install Raspberry Pi OS on a 4TB NVME using a GPT table. If you image a disk which is larger than 2TB with the Raspberry Pi tools or images, your disk will be limited to 2TB because they use MBR (Master Boot Record) instead of GPT (GUID partition table).</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#system-configuration","title":"System Configuration","text":""},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#pcie-gen-3","title":"PCIe Gen 3","text":"<p>NVMe SSD boot with the Raspberry Pi 5, at least with this Argon ONE case, turned out to be easy. When using a HAT+-compliant NVMe adapter, there is no need to enable the external PCIe port, it will be enabled automatically, but it is useful to force PCIe Gen 3 speeds using these options in <code>/boot/firmware/config.txt</code></p> <pre><code>[all]\ndtparam=pciex1\ndtparam=pciex1_gen=3\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#wifi-reconfiguration","title":"WiFi (re)configuration","text":"<p>The system boots just fine on the first try. Somehow the WiFi connection was not established, but it did work well after setting up by running <code>raspi-config</code> and going through the menus.</p> <code>$ ip a</code> <pre><code>$ ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n    valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n    valid_lft forever preferred_lft forever\n2: eth0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000\n    link/ether 2c:cf:67:83:6f:3b brd ff:ff:ff:ff:ff:ff\n3: wlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 2c:cf:67:83:6f:3c brd ff:ff:ff:ff:ff:ff\n    inet 192.168.0.124/24 brd 192.168.0.255 scope global dynamic noprefixroute wlan0\n    valid_lft 85830sec preferred_lft 85830sec\n    inet6 fe80::17df:d960:1aa4:36ff/64 scope link noprefixroute \n    valid_lft forever preferred_lft forever\n</code></pre> About IP address conflicts with <code>lexicon</code>. <p>Initically the Raspberry Pi had the <code>.122</code> IP address assigned to it, which caused a conflict with the <code>metallb</code> configuration in the Kubernetes cluster in <code>lexicon</code>, where this IP had been assigned to the <code>ingress-nginx</code> service, so as soon as the Raspberry Pi was assigned that IP address, all services behind Nginx became unreachable. The workaround was to configure the UPC router to assign those IP addresses used by the <code>ingress-nginx</code> service to made-up MAC addresses, so they won't be assigned to physical devices.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#additional-wifi-networks","title":"Additional WiFi networks","text":"<p>How to add a second WiFi network to your Raspberry Pi (updated for OS Bookworm) explains how the wpa_supplicant.conf file is no longer used to configure Wi-Fi connections and, instead, the current Raspberry Pi OS (based on Bookworm) uses NetworkManager to manage network connections, including Wi-Fi.</p> <p>There is a separate configuration file for each WiFi connection under <code>/etc/NetworkManager/system-connections</code>, all it takes to configure additional WiFi connections is to</p> <ol> <li>Copy a working file with a new name (e.g. the SSID name).</li> <li>Replace the SSID and password.</li> <li>Replace the <code>id</code> with a new unique value of choice.</li> <li>Replace the <code>uuid</code> with a new unique value from running <code>uuid</code>.</li> <li>Set the permissions to <code>600</code></li> </ol> <p>This can be done in advance of replacing WiFi access points, etc.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#retrospective-on-ip-address-assignment","title":"Retrospective on IP address assignment","text":"<p>Warning</p> <p>When setting up a Kubernetes cluster to be relocated to a different LAN, make sure to assign a static IP address that will be available in the destination LAN. Otherwise, the cluster will become non-operational because the required certificates are signed for the initial IP address and are not trivial to replace without losing deployments and data.</p> Ask me how I know... <p>After relocating <code>alfred</code> to a its destination LAN a having a new IP address assigned from the local DCHP server (ISP router), the <code>kubelet</code> service is no longer accessible at the old (or new) IP address.</p> <p>On the old IP address, nothing is listening:</p> <pre><code>$ kubectl get all -A\nE0723 20:15:48.259515   23554 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"http://localhost:8080/api?timeout=32s\\\": dial tcp [::1]:8080: connect: connection refused\"\nE0723 20:15:48.261327   23554 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"http://localhost:8080/api?timeout=32s\\\": dial tcp [::1]:8080: connect: connection refused\"\nE0723 20:15:48.263000   23554 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"http://localhost:8080/api?timeout=32s\\\": dial tcp [::1]:8080: connect: connection refused\"\nE0723 20:15:48.264590   23554 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"http://localhost:8080/api?timeout=32s\\\": dial tcp [::1]:8080: connect: connection refused\"\nE0723 20:15:48.266104   23554 memcache.go:265] \"Unhandled Error\" err=\"couldn't get current server API group list: Get \\\"http://localhost:8080/api?timeout=32s\\\": dial tcp [::1]:8080: connect: connection refused\"\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n</code></pre> <p>The server is unable to connect to <code>etcd</code> on <code>192.168.0.124:6443</code></p> <pre><code>$ systemctl status kubelet.service\n...\n1323 pod_workers.go:1301] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"kube-apiserver\\\" with CrashLoopBackOff: \\\"back-off 5m0s restarting failed container=kube-apiserver pod=kube-apiserver-alfred_kube-system(b254715d65c91745b90&gt;\n1323 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"alfred\\\" not found\" node=\"alfred\"\n1323 scope.go:117] \"RemoveContainer\" containerID=\"7a63e157dfff240e6122e2677a690bcfbc25828b9e70b15f3183c0b5bd37b5d4\"\n1323 pod_workers.go:1301] \"Error syncing pod, skipping\" err=\"failed to \\\"StartContainer\\\" for \\\"etcd\\\" with CrashLoopBackOff: \\\"back-off 5m0s restarting failed container=etcd pod=etcd-alfred_kube-system(bd8b1f1f7ec248fb91965a18bb2161c3)\\\"\" pod=\"kube-sy&gt;\n1323 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https://192.168.0.124:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/alfred?timeout=10s\\\": dial tcp 192.168.0.124:6443: connect: no route to host\" interv&gt;\n1323 kubelet_node_status.go:107] \"Unable to register node with API server\" err=\"Post \\\"https://192.168.0.124:6443/api/v1/nodes\\\": dial tcp 192.168.0.124:6443: connect: no route to host\" node=\"alfred\"\n1323 event.go:368] \"Unable to write event (may retry after sleeping)\" err=\"Patch \\\"https://192.168.0.124:6443/api/v1/namespaces/default/events/alfred.1854f3b436893b1b\\\": dial tcp 192.168.0.124:6443: connect: no route to host\" event=\"&amp;Event{ObjectMeta:{&gt;\n1323 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"alfred\\\" not found\" node=\"alfred\"\n</code></pre> <p>Because the service is now listening on the new IP address:</p> <pre><code>$ netstat -na | grep 6443\ntcp        0      1 192.168.0.50:54216      192.168.0.124:6443      SYN_SENT \n\n$ ip a\n...\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 2c:cf:67:83:6f:3b brd ff:ff:ff:ff:ff:ff\n    inet 192.168.0.50/24 brd 192.168.0.255 scope global dynamic noprefixroute eth0\n      valid_lft 602389sec preferred_lft 602389sec\n    inet6 fe80::5269:183f:fa1a:d1e5/64 scope link noprefixroute \n      valid_lft forever preferred_lft forever\n</code></pre> <p>The old IP address is promptly found in several files that would need to be updated:</p> <pre><code>/etc/hosts\n/etc/kubernetes/kubelet.conf\n/etc/kubernetes/admin.conf\n~/.kube/config\n</code></pre> <p>But there are quite a few more files that would need to be updated (in multiple lines):</p> <pre><code>$ sudo grep -rin 124 /etc/kubernetes/*\n/etc/kubernetes/controller-manager.conf:5:    server: https://192.168.0.124:6443\n/etc/kubernetes/manifests/etcd.yaml:5:    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.0.124:2379\n/etc/kubernetes/manifests/etcd.yaml:16:    - --advertise-client-urls=https://192.168.0.124:2379\n/etc/kubernetes/manifests/etcd.yaml:22:    - --initial-advertise-peer-urls=https://192.168.0.124:2380\n/etc/kubernetes/manifests/etcd.yaml:23:    - --initial-cluster=alfred=https://192.168.0.124:2380\n/etc/kubernetes/manifests/etcd.yaml:25:    - --listen-client-urls=https://127.0.0.1:2379,https://192.168.0.124:2379\n/etc/kubernetes/manifests/etcd.yaml:27:    - --listen-peer-urls=https://192.168.0.124:2380\n/etc/kubernetes/manifests/kube-apiserver.yaml:5:    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.168.0.124:6443\n/etc/kubernetes/manifests/kube-apiserver.yaml:16:    - --advertise-address=192.168.0.124\n/etc/kubernetes/manifests/kube-apiserver.yaml:48:        host: 192.168.0.124\n/etc/kubernetes/manifests/kube-apiserver.yaml:59:        host: 192.168.0.124\n/etc/kubernetes/manifests/kube-apiserver.yaml:71:        host: 192.168.0.124\n/etc/kubernetes/scheduler.conf:5:    server: https://192.168.0.124:6443\n/etc/kubernetes/super-admin.conf:5:    server: https://192.168.0.124:6443\n</code></pre> <p>It would be best if the kubelet service updated all these from a single, centralized parameter. Based on https://stackoverflow.com/a/68391387, although <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> does not exist, this should be possible by setting <code>KUBELET_EXTRA_ARGS=--node-ip 192.168.0.50</code> in <code>/etc/default/kubelet</code> (then reloading and restarting the service):</p> <pre><code># systemctl daemon-reload\n# systemctl restart kubelet.service\n# systemctl status kubelet.service\n142316 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get \"https://192.168.0.50:6443/api/v1/nodes?fieldSelector=metadata.name%3Dalfred&amp;limit=500&amp;resourceVersion=0\": net/http: TLS handshake timeout\n142316 reflector.go:166] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \\\"https://192.168.0.50:6443/api/v1/nodes?fieldSelector=metadata.name%3Dalfred&amp;limit=500&amp;resourceVersion=0\\&gt;\n</code></pre> <p>Now the issue is that the TLS handshake fails, because certificates were created for the IP address 192.168.0.124 but not 192.168.0.50; this has been reported as <code>kudeamd</code> issue #338: Changing master IP address and the workaround is quite involved.    </p> <p>So the solution is, for now, to add 124 as a Static IP Address Using the Network Manager CLI.    </p> <p>However, 124 is in the middle of the router's DHCP pool and there is no option to assign IPs to MACs, so just in case add also .5 and perhaps later rebuild the cluster on that address.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#fix-locales","title":"Fix locales","text":"<p>The above looks like rpi-imager took my PC\u2019s locale settings:</p> <pre><code>-bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n</code></pre> <p>The solution is to use <code>raspi-config</code> to generate <code>en_US.UTF-8</code> and set it as the system default.</p> <pre><code>$ sudo raspi-config\n...\nGenerating locales (this might take a while)...\n  en_GB.UTF-8... done\n  en_US.UTF-8... done\nGeneration complete.\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#disable-swap","title":"Disable swap","text":"<p>With 8 GB of RAM there should be no reason to need disk swap, never mind that it would be rather fast in this system.</p> <pre><code>$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           7.9Gi       205Mi       7.2Gi       5.1Mi       590Mi       7.7Gi\nSwap:          511Mi          0B       511Mi\n\n$ sudo dphys-swapfile swapoff\n$ sudo dphys-swapfile uninstall\n$ sudo systemctl disable dphys-swapfile\nSynchronizing state of dphys-swapfile.service with SysV service script with /lib/systemd/systemd-sysv-install.\nExecuting: /lib/systemd/systemd-sysv-install disable dphys-swapfile\nRemoved \"/etc/systemd/system/multi-user.target.wants/dphys-swapfile.service\".\n\n$ free -h\n               total        used        free      shared  buff/cache   available\nMem:           7.9Gi       210Mi       7.2Gi       5.1Mi       591Mi       7.7Gi\nSwap:             0B          0B          0\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#ssh-server","title":"SSH Server","text":"<p>As a general good practice, even if this system won't have its SSH port open to the Internet, add the relevant public keys to <code>/home/pi/.ssh/authorized_keys</code> and disable password authentication by setting <code>PasswordAuthentication no</code> in <code>/etc/ssh/sshd_config</code>.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#disable-power-save-for-wifi","title":"Disable power save for WiFi","text":"<p>Raspberry Pi enables power save on WiFi by default:</p> <pre><code>[    5.598286] brcmfmac: F1 signature read @0x18000000=0x15264345\n[    5.600446] brcmfmac: brcmf_fw_alloc_request: using brcm/brcmfmac43455-sdio for chip BCM4345/6\n[    5.600754] usbcore: registered new interface driver brcmfmac\n[    5.790052] brcmfmac: brcmf_c_process_txcap_blob: no txcap_blob available (err=-2)\n[    5.790325] brcmfmac: brcmf_c_preinit_dcmds: Firmware: BCM4345/6 wl0: Aug 29 2023 01:47:08 version 7.45.265 (28bca26 CY) FWID 01-b677b91b\n[    6.423812] brcmfmac: brcmf_cfg80211_set_power_mgmt: power save enabled\n</code></pre> <p>This can lead to losing connectivity in certain conditions, so disable power save of WiFi to avoid having to deal with such problems like Reconnect on WiFi drop.</p> <pre><code>$ sudo /sbin/iw dev wlan0 set power_save off\n</code></pre> <pre><code>[ 1767.061929] brcmfmac: brcmf_cfg80211_set_power_mgmt: power save disabled\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#argon-case-scripts","title":"Argon case scripts","text":"<p>The instructions in page 13 of the Argon ONE V3 NVMe case manual to install the scripts for FAN control seem simple enough:</p> <p></p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#failed-argon-installation","title":"Failed Argon installation","text":"<p>However, these scripts were problematic for users with this hardware setup as of early 2024 and also in this case something didn't go well at first:</p> <pre><code>$ curl https://download.argon40.com/argon-eeprom.sh | bash\n*************\n Argon Setup  \n*************\nHit:1 http://archive.raspberrypi.com/debian bookworm InRelease\nHit:2 http://deb.debian.org/debian bookworm InRelease\nHit:3 http://deb.debian.org/debian-security bookworm-security InRelease\nGet:4 http://deb.debian.org/debian bookworm-updates InRelease [55.4 kB]\nFetched 55.4 kB in 1s (101 kB/s)    \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n*** UPDATE AVAILABLE ***\n\nRun \"sudo rpi-eeprom-update -a\" to install this update now.\n\nTo configure the bootloader update policy run \"sudo raspi-config\"\n\nBOOTLOADER: update available\n   CURRENT: Mon Sep 23 01:02:56 PM UTC 2024 (1727096576)\n    LATEST: Wed Feb 12 10:51:52 AM UTC 2025 (1739357512)\n   RELEASE: default (/usr/lib/firmware/raspberrypi/bootloader-2712/default)\n            Use raspi-config to change the release.\nUpdating bootloader EEPROM\n image: /usr/lib/firmware/raspberrypi/bootloader-2712/default/pieeprom-2025-02-12.bin\nconfig_src: blconfig device\nconfig: /tmp/tmpgeb40cic/boot.conf\n################################################################################\n[all]\nWAKE_ON_GPIO=0\nPCIE_PROBE=1\nBOOT_UART=1\nPOWER_OFF_ON_HALT=1\nBOOT_ORDER=0xf416\n\n################################################################################\n\n*** To cancel this update run 'sudo rpi-eeprom-update -r' ***\n\nERROR: rpi-eeprom-update -d -i -f /tmp/tmpgeb40cic/pieeprom.upd timeout\n</code></pre> <p>This EEPROM update failure appears to have been discussed absolutely nowhere on the Internet (what does this timeout mean?), leading to extreme caution moving forward. The active EEPROM config now seems like a subset of the above:</p> <pre><code>$ sudo rpi-eeprom-config \n[all]\nBOOT_UART=1\nPOWER_OFF_ON_HALT=0\nBOOT_ORDER=0xf461\n</code></pre> <p>Since this appears to be the recommendation from the above script output, lets cancel the pending update for now:</p> <pre><code>$ sudo rpi-eeprom-update -r\nRemoving temporary files from previous EEPROM update\n</code></pre> <p>There are a couple of newer versions available, the last one being very recent:</p> <pre><code>$ sudo rpi-eeprom-update -l\n/usr/lib/firmware/raspberrypi/bootloader-2712/default/pieeprom-2025-02-12.bin\n$ ls -l /usr/lib/firmware/raspberrypi/bootloader-2712/default/\ntotal 6248\n-rw-r--r-- 1 root root 2097152 Feb 18 10:42 pieeprom-2024-11-12.bin\n-rw-r--r-- 1 root root 2097152 Feb 18 10:42 pieeprom-2025-01-22.bin\n-rw-r--r-- 1 root root 2097152 Feb 18 10:42 pieeprom-2025-02-12.bin\n-rw-r--r-- 1 root root  102992 Feb 18 10:42 recovery.bin\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#update-eeprom","title":"Update EEPROM","text":"<p>Using raspi-config to update the bootloader and set it to <code>Latest</code> (without resetting config) worked just fine. After the required reboot, the <code>argon-eeprom.sh</code> script also run fine:</p> <code>curl https://download.argon40.com/argon-eeprom.sh | bash</code> <pre><code>$ curl https://download.argon40.com/argon-eeprom.sh | bash\n*************\nArgon Setup  \n*************\nHit:1 http://deb.debian.org/debian bookworm InRelease\nHit:2 http://archive.raspberrypi.com/debian bookworm InRelease\nHit:3 http://deb.debian.org/debian-security bookworm-security InRelease\nHit:4 http://deb.debian.org/debian bookworm-updates InRelease\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nBOOTLOADER: up to date\n  CURRENT: Wed Feb 12 10:51:52 AM UTC 2025 (1739357512)\n    LATEST: Wed Feb 12 10:51:52 AM UTC 2025 (1739357512)\n  RELEASE: latest (/usr/lib/firmware/raspberrypi/bootloader-2712/latest)\n            Use raspi-config to change the release.\nEEPROM settings up to date\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#successful-argon-installation","title":"Successful Argon installation","text":"<p>Then after another required reboot, the <code>argon1.sh</code> script worked fine too:</p> <code>curl https://download.argon40.com/argon1.sh | bash</code> <pre><code>$ curl https://download.argon40.com/argon1.sh | bash\n*************\n Argon Setup  \n*************\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\npython3-libgpiod is already the newest version (1.6.3-1+b3).\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  i2c-tools libi2c0 read-edid\nSuggested packages:\n  libi2c-dev\nThe following NEW packages will be installed:\n  i2c-tools libi2c0 python3-smbus read-edid\n0 upgraded, 4 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 116 kB of archives.\nAfter this operation, 773 kB of additional disk space will be used.\nGet:1 http://deb.debian.org/debian bookworm/main arm64 libi2c0 arm64 4.3-2+b3 [9,456 B]\nGet:2 http://deb.debian.org/debian bookworm/main arm64 i2c-tools arm64 4.3-2+b3 [78.6 kB]\nGet:3 http://deb.debian.org/debian bookworm/main arm64 python3-smbus arm64 4.3-2+b3 [11.8 kB]\nGet:4 http://deb.debian.org/debian bookworm/main arm64 read-edid arm64 3.0.2-1.1 [16.1 kB]\nFetched 116 kB in 0s (487 kB/s)      \nSelecting previously unselected package libi2c0:arm64.\n(Reading database ... 81258 files and directories currently installed.)\nPreparing to unpack .../libi2c0_4.3-2+b3_arm64.deb ...\nUnpacking libi2c0:arm64 (4.3-2+b3) ...\nSelecting previously unselected package i2c-tools.\nPreparing to unpack .../i2c-tools_4.3-2+b3_arm64.deb ...\nUnpacking i2c-tools (4.3-2+b3) ...\nSelecting previously unselected package python3-smbus:arm64.\nPreparing to unpack .../python3-smbus_4.3-2+b3_arm64.deb ...\nUnpacking python3-smbus:arm64 (4.3-2+b3) ...\nSelecting previously unselected package read-edid.\nPreparing to unpack .../read-edid_3.0.2-1.1_arm64.deb ...\nUnpacking read-edid (3.0.2-1.1) ...\nSetting up libi2c0:arm64 (4.3-2+b3) ...\nSetting up read-edid (3.0.2-1.1) ...\nSetting up i2c-tools (4.3-2+b3) ...\nSetting up python3-smbus:arm64 (4.3-2+b3) ...\nProcessing triggers for man-db (2.11.2-2) ...\nProcessing triggers for libc-bin (2.36-9+rpt2+deb12u9) ...\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ni2c-tools is already the newest version (4.3-2+b3).\ni2c-tools set to manually installed.\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nCreated symlink /etc/systemd/system/multi-user.target.wants/argononed.service \u2192 /lib/systemd/system/argononed.service.\nHit:1 http://deb.debian.org/debian bookworm InRelease\nHit:2 http://deb.debian.org/debian-security bookworm-security InRelease\nHit:3 http://deb.debian.org/debian bookworm-updates InRelease   \nHit:4 http://archive.raspberrypi.com/debian bookworm InRelease  \nReading package lists... Done                             \nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nBOOTLOADER: up to date\n   CURRENT: Wed Feb 12 10:51:52 AM UTC 2025 (1739357512)\n    LATEST: Wed Feb 12 10:51:52 AM UTC 2025 (1739357512)\n   RELEASE: latest (/usr/lib/firmware/raspberrypi/bootloader-2712/latest)\n            Use raspi-config to change the release.\nUpdating bootloader EEPROM\n image: /usr/lib/firmware/raspberrypi/bootloader-2712/latest/pieeprom-2025-02-12.bin\nconfig_src: blconfig device\nconfig: /tmp/tmpsnnegdcc/boot.conf\n################################################################################\n[all]\nPSU_MAX_CURRENT=5000\nWAKE_ON_GPIO=0\nPCIE_PROBE=1\nBOOT_UART=1\nPOWER_OFF_ON_HALT=1\nBOOT_ORDER=0xf416\n\n################################################################################\n\n*** To cancel this update run 'sudo rpi-eeprom-update -r' ***\n\n*** CREATED UPDATE /tmp/tmpsnnegdcc/pieeprom.upd  ***\n\n   CURRENT: Wed Feb 12 10:51:52 AM UTC 2025 (1739357512)\n    UPDATE: Wed Feb 12 10:51:52 AM UTC 2025 (1739357512)\n    BOOTFS: /boot/firmware\n'/tmp/tmp.U1Yfb9oJp3' -&gt; '/boot/firmware/pieeprom.upd'\n\nUPDATING bootloader. This could take up to a minute. Please wait\n\n*** Do not disconnect the power until the update is complete ***\n\nIf a problem occurs then the Raspberry Pi Imager may be used to create\na bootloader rescue SD card image which restores the default bootloader image.\n\nflashrom -p linux_spi:dev=/dev/spidev10.0,spispeed=16000 -w /boot/firmware/pieeprom.upd\nVerifying update\nVERIFY: SUCCESS\nUPDATE SUCCESSFUL\n*********************\n  Setup Completed \n*********************\nVersion 2502002\n\nWe acknowledge the valuable feedback of the following:\nghalfacree, NHHiker\n\nFeel free to join the discussions at https://forum.argon40.com\n\n\nUse 'argon-config' to configure device\n</code></pre> <p>And finally, after yet another reboot, the <code>argon-config</code> works:</p> <code>sudo argon-config</code> <pre><code>$ sudo argon-config\n--------------------------\nArgon Configuration Tool\nVersion 2502002\n--------------------------\n\nChoose Option:\n  1. Configure Fan\n  2. Configure IR\n  3. Argon Industria UPS\n  4. Configure BLSTR DAC (v3/v5 only)\n  5. Configure Units\n  6. System Information\n  7. Uninstall\n\n  0. Exit\nEnter Number (0-7):6\n--------------------------\n Argon System Information\n--------------------------\n\n  1. Temperatures\n  2. CPU Utilization\n  3. Storage Capacity\n  4. RAM\n  5. IP Address\n  6. Fan Speed\n\n  0. Back\nEnter Number (0-6):1\n--------------------------\nTEMPERATURE INFORMATION:\n   CPU: 40.2\u00b0C\n--------------------------\n\n  1. Temperatures\n  2. CPU Utilization\n  3. Storage Capacity\n  4. RAM\n  5. IP Address\n  6. Fan Speed\n\n  0. Back\nEnter Number (0-6):2\n--------------------------\nCPU USAGE INFORMATION:\n   cpu0: 0%\n   cpu1: 0%\n   cpu2: 0%\n   cpu3: 0%\n--------------------------\n\n  1. Temperatures\n  2. CPU Utilization\n  3. Storage Capacity\n  4. RAM\n  5. IP Address\n  6. Fan Speed\n\n  0. Back\nEnter Number (0-6):3\n--------------------------\nSTORAGE INFORMATION:\n   nvme0n1 0% used of 2TB\n--------------------------\n\n  1. Temperatures\n  2. CPU Utilization\n  3. Storage Capacity\n  4. RAM\n  5. IP Address\n  6. Fan Speed\n\n  0. Back\nEnter Number (0-6):4\n--------------------------\nRAM INFORMATION:\n   97% of 8GB\n--------------------------\n\n  1. Temperatures\n  2. CPU Utilization\n  3. Storage Capacity\n  4. RAM\n  5. IP Address\n  6. Fan Speed\n\n  0. Back\nEnter Number (0-6):5\n--------------------------\nIP INFORMATION:\n   192.168.0.124\n--------------------------\n\n  1. Temperatures\n  2. CPU Utilization\n  3. Storage Capacity\n  4. RAM\n  5. IP Address\n  6. Fan Speed\n\n  0. Back\nEnter Number (0-6):6\n--------------------------\nTEMPERATURE INFORMATION:\n   CPU: 40.2\u00b0C\nFAN CONFIGURATION INFORMATION:\n        65.0=100\n        60.0=55\n        55.0=30\nFAN SPEED INFORMATION:\n   Fan Speed 0\n--------------------------\n\n  1. Temperatures\n  2. CPU Utilization\n  3. Storage Capacity\n  4. RAM\n  5. IP Address\n  6. Fan Speed\n\n  0. Back\nEnter Number (0-6):0\n\nChoose Option:\n  1. Configure Fan\n  2. Configure IR\n  3. Argon Industria UPS\n  4. Configure BLSTR DAC (v3/v5 only)\n  5. Configure Units\n  6. System Information\n  7. Uninstall\n\n  0. Exit\nEnter Number (0-7):0\nThank you.\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#essential-software","title":"Essential Software","text":""},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#basic-packages","title":"Basic packages","text":"<p>Before moving forward, I like to install a few basics that make work easier, or on which subsequent packages depend:</p> <pre><code>$ sudo apt update\n$ sudo apt full-upgrade -y\n$ sudo apt install -y bc git iotop-c netcat-openbsd rename speedtest-cli \\\n  sysstat vim python3-pip python3-influxdb python3-numpy python3-absl \\\n  python3-unidecode\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#continuous-monitoring","title":"Continuous Monitoring","text":"<p>Continuous Monitoring on this system needs only the <code>conmon-st</code> script to gather metrics, since it will be reporting metrics to the InfluxDB and Grafana on Kubernetes already setup on <code>lexicon</code>.</p> <p>After installing the service file, it is also necessary to add <code>User=pi</code> in the <code>[Service]</code> section.</p> <pre><code>$ sudo systemctl enable conmon.service\n$ sudo systemctl daemon-reload\n$ sudo systemctl start conmon.service\n$ sudo systemctl status conmon.service\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#fail2ban","title":"Fail2Ban","text":"<p>Fail2Ban scans log files and bans IP addresses conducting too many failed login attempts. It is a rather basic protection mechanism, and this system is not intended to have its SSH port open to the Internet, but it is so easy to install and enable that there is no excuse not to.</p> <pre><code>$ sudo apt-get install iptables fail2ban -y\n$ sudo systemctl enable --now fail2ban\n</code></pre> <p>The configuration in <code>/etc/fail2ban/jail.conf</code> can be spiced up to make a little more trigger-happy:</p> <pre><code># \"bantime\" is the number of seconds that a host is banned.\nbantime  = 12h\n# A host is banned if it has generated \"maxretry\" during the last \"findtime\"\n# seconds.\nfindtime  = 90m\n# \"maxretry\" is the number of failures before a host get banned.\nmaxretry = 3\n# \"bantime.increment\" allows to use database for searching of previously banned ip's to increase a \n# default ban time using special formula, default it is banTime * 1, 2, 4, 8, 16, 32...\nbantime.increment = true\n</code></pre> <p>Then restart the service to pick the changes up:</p> <pre><code>$ sudo systemctl restart fail2ban\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#kubernetes","title":"Kubernetes","text":"<p>Raspberry Pi 5 - a Kubernetes cluster looks like installing Kubernetes on a Raspberry Pi is not any different than installing it on Ubuntu Server, so we can essentially combine the latest Single-node Kubernetes cluster on Ubuntu Studio desktop (rapture) with the original, more detailed Single-node Kubernetes cluster on Ubuntu Server (lexicon), which should mean mostly following kubernetes.io/docs along with the combined learnings from having done this already twice.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#github-repository","title":"GitHub Repository","text":"<p>Kubernetes deployments for all servers are best kept in a revision controlled GitHub repo, already in use since the last cluster was setup. Generate a new SSH key, add it to the GitHub account as an Authentication Key and, instead of keeping separate directories for each Kubernetes version, which seems to have been unnecessary, simply create a directory per server and keep common files at the root:</p> <pre><code>$ git clone git@github.com:xxxx/kubernetes-deployments.git\n$ cd kubernetes-deployments/\n$ mkdir alfred\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#install-kubernetes","title":"Install Kubernetes","text":"<p>The current stable release is now v1.32.2 which is already the 3rd patch in 1.32, so we use this version to Installing kubeadm, kubelet and kubectl from Debian packages:</p> <pre><code>$ curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key \\\n  | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n$ sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n$ echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' \\\n  | sudo tee /etc/apt/sources.list.d/kubernetes.list\n$ sudo apt-get update\n</code></pre> <p>Note</p> <p>The \"<code>/etc/apt/keyrings</code> already existed and the required packages were already installed: <code>ca-certificates</code>, <code>curl</code>, <code>gnupg</code> (and <code>apt-transport-https</code> is a dummy package).</p> <p>Once the APT repository is ready, install the packages:</p> <code>sudo apt-get install -y kubelet kubeadm kubectl</code> <pre><code>$ sudo apt-get install -y kubelet kubeadm kubectl\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  conntrack cri-tools kubernetes-cni\nThe following NEW packages will be installed:\n  conntrack cri-tools kubeadm kubectl kubelet kubernetes-cni\n0 upgraded, 6 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 83.4 MB of archives.\nAfter this operation, 330 MB of additional disk space will be used.\nFetched 83.4 MB in 9s (9,047 kB/s)                                                                   \nSelecting previously unselected package conntrack.\n(Reading database ... 83134 files and directories currently installed.)\nPreparing to unpack .../0-conntrack_1%3a1.4.7-1+b2_arm64.deb ...\nUnpacking conntrack (1:1.4.7-1+b2) ...\nSelecting previously unselected package cri-tools.\nPreparing to unpack .../1-cri-tools_1.32.0-1.1_arm64.deb ...\nUnpacking cri-tools (1.32.0-1.1) ...\nSelecting previously unselected package kubeadm.\nPreparing to unpack .../2-kubeadm_1.32.2-1.1_arm64.deb ...\nUnpacking kubeadm (1.32.2-1.1) ...\nSelecting previously unselected package kubectl.\nPreparing to unpack .../3-kubectl_1.32.2-1.1_arm64.deb ...\nUnpacking kubectl (1.32.2-1.1) ...\nSelecting previously unselected package kubernetes-cni.\nPreparing to unpack .../4-kubernetes-cni_1.6.0-1.1_arm64.deb ...\nUnpacking kubernetes-cni (1.6.0-1.1) ...\nSelecting previously unselected package kubelet.\nPreparing to unpack .../5-kubelet_1.32.2-1.1_arm64.deb ...\nUnpacking kubelet (1.32.2-1.1) ...\nSetting up conntrack (1:1.4.7-1+b2) ...\nSetting up kubectl (1.32.2-1.1) ...\nSetting up cri-tools (1.32.0-1.1) ...\nSetting up kubernetes-cni (1.6.0-1.1) ...\nSetting up kubeadm (1.32.2-1.1) ...\nSetting up kubelet (1.32.2-1.1) ...\nProcessing triggers for man-db (2.11.2-2) ...\n</code></pre> <p>And then, because updating Kubernetes is a rather involved process, hold them:</p> <pre><code>$ sudo apt-mark hold kubelet kubeadm kubectl\n</code></pre> <p>Enabling shell autocompletion for <code>kubectl</code> is very easy, since <code>bash-completion</code> is already installed:</p> <pre><code>$ kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl &gt; /dev/null\n$ sudo chmod a+r /etc/bash_completion.d/kubectl\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#enable-the-kubelet-service","title":"Enable the kubelet service","text":"<p>This step is only really necessary later, before bootstrapping the cluster with <code>kubeadm</code>, but it can be done any time; the service will just be waiting:</p> <pre><code>$ sudo systemctl enable --now kubelet\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#install-container-runtime","title":"Install container runtime","text":""},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#networking-setup","title":"Networking setup","text":"<p>Enabling IPv4 packet forwarding appears to be the only required network setup for Kubernetes v1.32, and this is already enabled in Rasberry Pi OS:</p> <pre><code>$ sudo sysctl net.ipv4.ip_forward\nnet.ipv4.ip_forward = 1\n</code></pre> <p>Even so, better to have this enabled explicitly now than to later have Kubernetes cluster DNS issues.</p> <pre><code>$ cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nEOF\n\n$ sudo sysctl --system\n</code></pre> <p>However, this alone is not enough for the successful deployment of the Network plugin required after bootstraping the cluster.</p> <p>Ommitting the additional steps to let iptables see bridged traffic, which seems to have been required only up to v1.29, would later result in the <code>kube-flannel</code> deployment failing to start up (stuck in a crash loop):</p> <pre><code>$ sudo modprobe overlay\n$ sudo modprobe br_netfilter\n$ sudo tee /etc/modules-load.d/k8s.conf&lt;&lt;EOF\nbr_netfilter\noverlay\nEOF\n\n$ sudo tee /etc/sysctl.d/k8s.conf&lt;&lt;EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n$ sudo sysctl --system\n</code></pre> <p>Reboot the system to make sure that the changes are permanent:</p> <pre><code>$ sudo sysctl -a | egrep 'net.ipv4.ip_forward |net.bridge.bridge-nf-call-ip'\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\n\n$ lsmod | egrep 'overlay|bridge'\nbridge                294912  1 br_netfilter\nstp                    49152  1 bridge\nllc                    49152  2 bridge,stp\noverlay               163840  11\nipv6                  589824  58 bridge,br_netfilter\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#install-containerd","title":"Install containerd","text":"<p>Installing a container runtime comes next, with <code>containerd</code> being the runtime of choice. Install using the <code>apt</code> repository:</p> <pre><code>$ sudo curl -fsSL https://download.docker.com/linux/debian/gpg \\\n  -o /etc/apt/keyrings/docker.asc\n$ sudo chmod a+r /etc/apt/keyrings/docker.asc\n$ echo \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n$ sudo apt-get update\n</code></pre> <p>Once the APT repository is ready, install the packages:</p> <code>sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</code> <pre><code>$ sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  docker-ce-rootless-extras libltdl7 libslirp0 pigz slirp4netns\nSuggested packages:\n  cgroupfs-mount | cgroup-lite\nThe following NEW packages will be installed:\n  containerd.io docker-buildx-plugin docker-ce docker-ce-cli docker-ce-rootless-extras\n  docker-compose-plugin libltdl7 libslirp0 pigz slirp4netns\n0 upgraded, 10 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 104 MB of archives.\nAfter this operation, 405 MB of additional disk space will be used.\nFetched 104 MB in 9s (11.8 MB/)\nSelecting previously unselected package pigz.\n(Reading database ... 83193 files and directories currently installed.)\nPreparing to unpack .../0-pigz_2.6-1_arm64.deb ...\nUnpacking pigz (2.6-1) ...\nSelecting previously unselected package containerd.io.\nPreparing to unpack .../1-containerd.io_1.7.25-1_arm64.deb ...\nUnpacking containerd.io (1.7.25-1) ...\nSelecting previously unselected package docker-buildx-plugin.\nPreparing to unpack .../2-docker-buildx-plugin_0.21.0-1~debian.12~bookworm_arm64.deb ...\nUnpacking docker-buildx-plugin (0.21.0-1~debian.12~bookworm) ...\nSelecting previously unselected package docker-ce-cli.\nPreparing to unpack .../3-docker-ce-cli_5%3a28.0.0-1~debian.12~bookworm_arm64.deb ...\nUnpacking docker-ce-cli (5:28.0.0-1~debian.12~bookworm) ...\nSelecting previously unselected package docker-ce.\nPreparing to unpack .../4-docker-ce_5%3a28.0.0-1~debian.12~bookworm_arm64.deb ...\nUnpacking docker-ce (5:28.0.0-1~debian.12~bookworm) ...\nSelecting previously unselected package docker-ce-rootless-extras.\nPreparing to unpack .../5-docker-ce-rootless-extras_5%3a28.0.0-1~debian.12~bookworm_arm64.deb ...\nUnpacking docker-ce-rootless-extras (5:28.0.0-1~debian.12~bookworm) ...\nSelecting previously unselected package docker-compose-plugin.\nPreparing to unpack .../6-docker-compose-plugin_2.33.0-1~debian.12~bookworm_arm64.deb ...\nUnpacking docker-compose-plugin (2.33.0-1~debian.12~bookworm) ...\nSelecting previously unselected package libltdl7:arm64.\nPreparing to unpack .../7-libltdl7_2.4.7-7~deb12u1_arm64.deb ...\nUnpacking libltdl7:arm64 (2.4.7-7~deb12u1) ...\nSelecting previously unselected package libslirp0:arm64.\nPreparing to unpack .../8-libslirp0_4.7.0-1_arm64.deb ...\nUnpacking libslirp0:arm64 (4.7.0-1) ...\nSelecting previously unselected package slirp4netns.\nPreparing to unpack .../9-slirp4netns_1.2.0-1_arm64.deb ...\nUnpacking slirp4netns (1.2.0-1) ...\nSetting up docker-buildx-plugin (0.21.0-1~debian.12~bookworm) ...\nSetting up containerd.io (1.7.25-1) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/containerd.service \u2192 /lib/systemd/system/containerd.service.\nSetting up docker-compose-plugin (2.33.0-1~debian.12~bookworm) ...\nSetting up libltdl7:arm64 (2.4.7-7~deb12u1) ...\nSetting up docker-ce-cli (5:28.0.0-1~debian.12~bookworm) ...\nSetting up libslirp0:arm64 (4.7.0-1) ...\nSetting up pigz (2.6-1) ...\nSetting up docker-ce-rootless-extras (5:28.0.0-1~debian.12~bookworm) ...\nSetting up slirp4netns (1.2.0-1) ...\nSetting up docker-ce (5:28.0.0-1~debian.12~bookworm) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/docker.service \u2192 /lib/systemd/system/docker.service.\nCreated symlink /etc/systemd/system/sockets.target.wants/docker.socket \u2192 /lib/systemd/system/docker.socket.\nProcessing triggers for man-db (2.11.2-2) ...\nProcessing triggers for libc-bin (2.36-9+rpt2+deb12u9) ...\n</code></pre> <p>In just a few moments <code>docker</code> is already running:</p> <code>systemctl status docker</code> <pre><code>$ systemctl status docker\n\u25cf docker.service - Docker Application Container Engine\n    Loaded: loaded (/lib/systemd/system/docker.service; enabled; preset: enabled)\n    Active: active (running) since Sat 2025-02-22 14:45:10 CET; 1min 58s ago\nTriggeredBy: \u25cf docker.socket\n      Docs: https://docs.docker.com\n  Main PID: 272049 (dockerd)\n      Tasks: 10\n        CPU: 179ms\n    CGroup: /system.slice/docker.service\n            \u2514\u2500272049 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n\nFeb 22 14:45:10 alfred dockerd[272049]: time=\"2025-02-22T14:45:10.298075305+01:00\" level=info msg=\"Loading containers: start.\"\nFeb 22 14:45:10 alfred dockerd[272049]: time=\"2025-02-22T14:45:10.482322273+01:00\" level=info msg=\"Loading containers: done.\"\nFeb 22 14:45:10 alfred dockerd[272049]: time=\"2025-02-22T14:45:10.493322483+01:00\" level=warning msg=\"WARNING: No memory limit support\"\nFeb 22 14:45:10 alfred dockerd[272049]: time=\"2025-02-22T14:45:10.493351206+01:00\" level=warning msg=\"WARNING: No swap limit support\"\nFeb 22 14:45:10 alfred dockerd[272049]: time=\"2025-02-22T14:45:10.493375243+01:00\" level=info msg=\"Docker daemon\" commit=af898ab containerd-snapshotter=false storage-driver=overlay2 version=28.0.0\nFeb 22 14:45:10 alfred dockerd[272049]: time=\"2025-02-22T14:45:10.493479613+01:00\" level=info msg=\"Initializing buildkit\"\nFeb 22 14:45:10 alfred dockerd[272049]: time=\"2025-02-22T14:45:10.538199044+01:00\" level=info msg=\"Completed buildkit initialization\"\nFeb 22 14:45:10 alfred dockerd[272049]: time=\"2025-02-22T14:45:10.543738454+01:00\" level=info msg=\"Daemon has completed initialization\"\nFeb 22 14:45:10 alfred systemd[1]: Started docker.service - Docker Application Container Engine.\nFeb 22 14:45:10 alfred dockerd[272049]: time=\"2025-02-22T14:45:10.543810435+01:00\" level=info msg=\"API listen on /run/docker.sock\"\n</code></pre> <p>The server is indeed reachable and its version can be checked:</p> <code>sudo docker version</code> <pre><code>$ sudo docker version\nClient: Docker Engine - Community\nVersion:           28.0.0\nAPI version:       1.48\nGo version:        go1.23.6\nGit commit:        f9ced58\nBuilt:             Wed Feb 19 22:10:37 2025\nOS/Arch:           linux/arm64\nContext:           default\n\nServer: Docker Engine - Community\nEngine:\n  Version:          28.0.0\n  API version:      1.48 (minimum version 1.24)\n  Go version:       go1.23.6\n  Git commit:       af898ab\n  Built:            Wed Feb 19 22:10:37 2025\n  OS/Arch:          linux/arm64\n  Experimental:     false\ncontainerd:\n  Version:          1.7.25\n  GitCommit:        bcc810d6b9066471b0b6fa75f557a15a1cbf31bb\nrunc:\n  Version:          1.2.4\n  GitCommit:        v1.2.4-0-g6c52b3f\ndocker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre> <p>And the basic <code>hello-world</code> example just works:</p> <code>sudo docker run hello-world</code> <pre><code>$ sudo docker run hello-world\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\nc9c5fd25a1bd: Pull complete \nDigest: sha256:e0b569a5163a5e6be84e210a2587e7d447e08f87a0e90798363fa44a0464a1e8\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (arm64v8)\n3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n$ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\nhttps://hub.docker.com/\n\nFor more examples and ideas, visit:\nhttps://docs.docker.com/get-started/\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#configure-containerd-for-kubernetes","title":"Configure <code>containerd</code> for Kubernetes","text":"<p>The default configutation that comes with <code>containerd</code>, at least when installed from the APT repository, needs two adjustments to work with Kubernetes</p> <ol> <li>Enable the use of <code>systemd</code> cgroup driver,    because Debian 12 (bookworm) uses both <code>systemd</code> and    cgroup v2.</li> <li>Enable CRI integration, which is disabled in <code>/etc/containerd/config.toml</code>,    but is needed to use <code>containerd</code> with Kubernetes.</li> </ol> <p>The safest method to set these configurations is to do it based off of the default configuration:</p> <pre><code>$ containerd config default \\\n | sed 's/disabled_plugins.*/disabled_plugins = []/' \\\n | sed 's/SystemdCgroup = false/SystemdCgroup = true/' \\\n | sudo tee /etc/containerd/config.toml &gt; /dev/null\n\n$ sudo systemctl restart containerd\n</code></pre> <p>Note</p> <p>There is no need to manually configure the cgroup driver for <code>kubelet</code> because, already since v1.22, <code>kubeadm</code> defaults it to <code>systemd</code>.</p> <p>Installing Raspberry Pi OS uses most of the disk for the root partition, so there is plenty of space under <code>/var</code> (unlike in my PCs).</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#bootstrap-with-kubeadm","title":"Bootstrap with <code>kubeadm</code>","text":"<p>Creating a cluster with kubeadm is the next big step towards creating the Kubernetes cluster.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#initialize-control-plane","title":"Initialize control-plane","text":"<p>Having reviewed the requirements and installed all the components already, initialize the control-plane node with the following flags:</p> <ul> <li><code>--cri-socket=unix:/run/containerd/containerd.sock</code> to make sure Kubernetes    uses the containerd runtime.</li> <li><code>--pod-network-cidr=10.244.0.0/16</code> as     required by flannel,    which is the network plugin to be installed later.</li> </ul> <code>sudo kubeadm init</code> <pre><code>$ sudo kubeadm init \\\n  --cri-socket=unix:/run/containerd/containerd.sock \\\n  --pod-network-cidr=10.244.0.0/16\n[init] Using Kubernetes version: v1.32.2\n[preflight] Running pre-flight checks\n        [WARNING SystemVerification]: missing optional cgroups: hugetlb\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action beforehand using 'kubeadm config images pull'\nW0222 18:06:17.311306   23707 checks.go:846] detected that the sandbox image \"registry.k8s.io/pause:3.8\" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use \"registry.k8s.io/pause:3.10\" as the CRI sandbox image.\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [alfred kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.0.124]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [alfred localhost] and IPs [192.168.0.124 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [alfred localhost] and IPs [192.168.0.124 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"super-admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\"\n[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s\n[kubelet-check] The kubelet is healthy after 1.500708247s\n[api-check] Waiting for a healthy API server. This can take up to 4m0s\n[api-check] The API server is healthy after 6.502008927s\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Skipping phase. Please see --upload-certs\n[mark-control-plane] Marking the node alfred as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n[mark-control-plane] Marking the node alfred as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]\n[bootstrap-token] Using token: wsa12a.382ftng8m2y3id9x\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.0.124:6443 --token wsa12a.382ftng8m2y3id9x \\\n        --discovery-token-ca-cert-hash sha256:33d013e353b67503bf547c210adc44fe52617626b6f310f801c87c1200269daf\n</code></pre> <p>Now <code>kubelet</code> is running and reporting pod startups:</p> <code>systemctl status kubelet</code> <pre><code>$ systemctl status kubelet\n\u25cf kubelet.service - kubelet: The Kubernetes Node Agent\n    Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; preset: enabled)\n    Drop-In: /usr/lib/systemd/system/kubelet.service.d\n            \u2514\u250010-kubeadm.conf\n    Active: active (running) since Sat 2025-02-22 18:07:07 CET; 59s ago\n      Docs: https://kubernetes.io/docs/\n  Main PID: 26454 (kubelet)\n      Tasks: 13 (limit: 9586)\n    Memory: 33.7M\n        CPU: 1.841s\n    CGroup: /system.slice/kubelet.service\n            \u2514\u250026454 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10\n\nFeb 22 18:07:08 alfred kubelet[26454]: I0222 18:07:08.449253   26454 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"kube-system/kube-apiserver-alfred\" podStartSLOduration=1.449228049 podStartE2EDuration=\"1.449228049s\" podCreationTimestamp=\"2025-02-22 18:07:07 +0100 CET\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-&gt;\nFeb 22 18:07:08 alfred kubelet[26454]: I0222 18:07:08.465459   26454 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"kube-system/kube-controller-manager-alfred\" podStartSLOduration=1.4654371689999999 podStartE2EDuration=\"1.465437169s\" podCreationTimestamp=\"2025-02-22 18:07:07 +0100 CET\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedP&gt;\nFeb 22 18:07:08 alfred kubelet[26454]: I0222 18:07:08.477805   26454 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"kube-system/kube-scheduler-alfred\" podStartSLOduration=1.47777902 podStartE2EDuration=\"1.47777902s\" podCreationTimestamp=\"2025-02-22 18:07:07 +0100 CET\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01&gt;\nFeb 22 18:07:11 alfred kubelet[26454]: I0222 18:07:11.830256   26454 kuberuntime_manager.go:1702] \"Updating runtime config through cri with podcidr\" CIDR=\"10.244.0.0/24\"\nFeb 22 18:07:11 alfred kubelet[26454]: I0222 18:07:11.831026   26454 kubelet_network.go:61] \"Updating Pod CIDR\" originalPodCIDR=\"\" newPodCIDR=\"10.244.0.0/24\"\nFeb 22 18:07:12 alfred kubelet[26454]: I0222 18:07:12.664117   26454 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kube-proxy\\\" (UniqueName: \\\"kubernetes.io/configmap/d82bc4b9-39fd-4616-9d9d-919b5dbf12f8-kube-proxy\\\") pod \\\"kube-proxy-m9rb6\\\" (UID: \\\"d82bc4b9-39fd-4616-9d9d-919b5dbf12f8\\\") \" pod=\"kube-system/kube-proxy-m9rb6\"\nFeb 22 18:07:12 alfred kubelet[26454]: I0222 18:07:12.664164   26454 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"xtables-lock\\\" (UniqueName: \\\"kubernetes.io/host-path/d82bc4b9-39fd-4616-9d9d-919b5dbf12f8-xtables-lock\\\") pod \\\"kube-proxy-m9rb6\\\" (UID: \\\"d82bc4b9-39fd-4616-9d9d-919b5dbf12f8\\\") \" pod=\"kube-system/kube-proxy-m9&gt;\nFeb 22 18:07:12 alfred kubelet[26454]: I0222 18:07:12.664191   26454 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"lib-modules\\\" (UniqueName: \\\"kubernetes.io/host-path/d82bc4b9-39fd-4616-9d9d-919b5dbf12f8-lib-modules\\\") pod \\\"kube-proxy-m9rb6\\\" (UID: \\\"d82bc4b9-39fd-4616-9d9d-919b5dbf12f8\\\") \" pod=\"kube-system/kube-proxy-m9rb&gt;\nFeb 22 18:07:12 alfred kubelet[26454]: I0222 18:07:12.664222   26454 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kube-api-access-kvm6p\\\" (UniqueName: \\\"kubernetes.io/projected/d82bc4b9-39fd-4616-9d9d-919b5dbf12f8-kube-api-access-kvm6p\\\") pod \\\"kube-proxy-m9rb6\\\" (UID: \\\"d82bc4b9-39fd-4616-9d9d-919b5dbf12f8\\\") \" pod=\"kube-sy&gt;\nFeb 22 18:07:13 alfred kubelet[26454]: I0222 18:07:13.955329   26454 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"kube-system/kube-proxy-m9rb6\" podStartSLOduration=1.955298193 podStartE2EDuration=\"1.955298193s\" podCreationTimestamp=\"2025-02-22 18:07:12 +0100 CET\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00&gt;\n</code></pre> <p>We can see all the containers already running:</p> <code>sudo crictl ps -a</code> <pre><code>$ sudo crictl ps -a\nWARN[0000] Config \"/etc/crictl.yaml\" does not exist, trying next: \"/usr/bin/crictl.yaml\" \nCONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD                              NAMESPACE\n96bfa773295db       e5aac5df76d9b       3 minutes ago       Running             kube-proxy                0                   913830cf3993f       kube-proxy-m9rb6                 kube-system\na6b39c9d1cb7b       3c9285acfd2ff       3 minutes ago       Running             kube-controller-manager   0                   a79090d0033d6       kube-controller-manager-alfred   kube-system\ndaabcca3c5ff7       6417e1437b6d9       3 minutes ago       Running             kube-apiserver            0                   eb82d6f92e0a6       kube-apiserver-alfred            kube-system\na3265cea76a4e       82dfa03f692fb       3 minutes ago       Running             kube-scheduler            0                   358bbf99544f9       kube-scheduler-alfred            kube-system\n58c64ed06ad12       7fc9d4aa817aa       3 minutes ago       Running             etcd                      0                   a176e6da07c76       etcd-alfred                      kube-system\n</code></pre> <p>The cluster is configured correctly to use registry.k8s.io/pause:3.10:</p> <pre><code>$ grep container-runtime /var/lib/kubelet/kubeadm-flags.env \nKUBELET_KUBEADM_ARGS=\"--container-runtime-endpoint=unix:/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10\"\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#troubleshooting-bootstrap","title":"Troubleshooting Bootstrap","text":"<p>The first attempt failed for a few reasons:</p> <code>sudo kubeadm init</code> <pre><code>$ sudo kubeadm init \\\n  --cri-socket=unix:/run/containerd/containerd.sock \\\n  --pod-network-cidr=10.244.0.0/16\n\n[init] Using Kubernetes version: v1.32.2\n[preflight] Running pre-flight checks\nW0222 16:05:57.929808  443810 checks.go:1077] [preflight] WARNING: Couldn't create the interface used for talking to the container runtime: failed to create new CRI runtime service: validate service connection: validate CRI v1 runtime API for endpoint \"unix:/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\n[preflight] The system verification failed. Printing the output from the verification:\nKERNEL_VERSION: 6.6.74+rpt-rpi-2712\nCONFIG_NAMESPACES: enabled\nCONFIG_NET_NS: enabled\nCONFIG_PID_NS: enabled\nCONFIG_IPC_NS: enabled\nCONFIG_UTS_NS: enabled\nCONFIG_CGROUPS: enabled\nCONFIG_CGROUP_BPF: enabled\nCONFIG_CGROUP_CPUACCT: enabled\nCONFIG_CGROUP_DEVICE: enabled\nCONFIG_CGROUP_FREEZER: enabled\nCONFIG_CGROUP_PIDS: enabled\nCONFIG_CGROUP_SCHED: enabled\nCONFIG_CPUSETS: enabled\nCONFIG_MEMCG: enabled\nCONFIG_INET: enabled\nCONFIG_EXT4_FS: enabled\nCONFIG_PROC_FS: enabled\nCONFIG_NETFILTER_XT_TARGET_REDIRECT: enabled (as module)\nCONFIG_NETFILTER_XT_MATCH_COMMENT: enabled (as module)\nCONFIG_FAIR_GROUP_SCHED: enabled\nCONFIG_OVERLAY_FS: enabled (as module)\nCONFIG_AUFS_FS: not set - Required for aufs.\nCONFIG_BLK_DEV_DM: enabled (as module)\nCONFIG_CFS_BANDWIDTH: enabled\nCONFIG_CGROUP_HUGETLB: not set - Required for hugetlb cgroup.\nCONFIG_SECCOMP: enabled\nCONFIG_SECCOMP_FILTER: enabled\nOS: Linux\nCGROUPS_CPU: enabled\nCGROUPS_CPUSET: enabled\nCGROUPS_DEVICES: enabled\nCGROUPS_FREEZER: enabled\nCGROUPS_MEMORY: missing\nCGROUPS_PIDS: enabled\nCGROUPS_HUGETLB: missing\nCGROUPS_IO: enabled\n        [WARNING SystemVerification]: missing optional cgroups: hugetlb\nerror execution phase preflight: [preflight] Some fatal errors occurred:\n        [ERROR SystemVerification]: missing required cgroups: memory\n[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`\nTo see the stack trace of this error execute with --v=5 or higher\n</code></pre> <p>There are 2 warnings and 1 error, although at least one of the warnings looks like it may be a fatal error preventing the creation of the cluster:</p> <ol> <li>WARNING: Couldn't create the interface used for talking to the     container runtime: failed to create new CRI runtime service: validate     service connection: validate CRI v1 runtime API for endpoint     <code>\"unix:/run/containerd/containerd.sock\"</code>: rpc error:     <code>code = Unimplemented desc = unknown service runtime.v1.RuntimeService</code><ul> <li>Solution: <code>sudo systemctl restart containerd</code></li> <li>Explanation: this was a subtle mistake; after replacing the    configuration in <code>/etc/containerd/config.toml</code> the service was not    restarted, because the command was <code>sudo systemctl start containerd</code>    (<code>start</code> instead of <code>restart</code>) which had no effect because the    service was already running. Later, this was suspected based on the    notes taken along the way (i.e. this article's draft) and     confirmed by the last modified time in the config file being 1 hour    later than the start time given by <code>sudo status containerd</code>.</li> </ul> </li> <li>WARNING: missing optional cgroups: hugetlb<ul> <li>This appears safe to ignore, since     HugeTLB Pages    is neither supported by the kernel nor necessary for this system.</li> </ul> </li> <li>ERROR: missing required cgroups: memory<ul> <li>Raspberry Pi OS bug #5933    causes this; the memory cgroup in the kernel command line: <pre><code>[    0.000000] Kernel command line: ... cgroup_disable=memory ...\n[    0.000000] cgroup: Disabling memory control group subsystem\n</code></pre>    This is not caused by the command line parameters defined in    <code>/boot/firmware/cmdline.txt</code> (documented in    Kernel command line (cmdline.txt)):    <code>console=serial0,115200 console=tty1 root=PARTUUID=7c5a31a3-02 rootfstype=ext4 fsck.repair=yes rootwait cfg80211.ieee80211_regdom=CH</code></li> <li>Solution: add the required cgroups    (<code>cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory</code>) to the    command line parameters defined in <code>/boot/firmware/cmdline.txt</code>:    <pre><code>$ sudo sed -i \\\n  's/^/cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory/ ' \\\n  /boot/firmware/cmdline.txt\n</code></pre>    After rebooting, <code>/proc/cmdline</code> shows both <code>cgroup_disable=memory</code>    and the added parameters, but the memory cgroup is now enabled:    <pre><code>[    0.000000] Kernel command line: ... cgroup_disable=memory ...\n    cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory ...\n[    0.000000] cgroup: Disabling memory control group subsystem\n[    0.000000] cgroup: Enabling cpuset control group subsystem\n[    0.000000] cgroup: Enabling memory control group subsystem\n</code></pre>    The <code>cgroup_memory=1</code> parameter is unknown to the kernel and passed on    to <code>/init</code>.</li> </ul> </li> </ol>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#setup-kubectl-access","title":"Setup <code>kubectl</code> access","text":"<p>To run <code>kubectl</code> as the non-root user (<code>pi</code>), copy the Kubernetes config file under the <code>~/.kube</code> directory and set the right permissions to it:</p> <pre><code>$ mkdir $HOME/.kube\n$ sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config\n$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n$ ls -l $HOME/.kube/config\n-rw------- 1 pi pi 5653 Feb 22 18:20 /home/pi/.kube/config\n$ kubectl cluster-info\nKubernetes control plane is running at https://192.168.0.124:6443\nCoreDNS is running at https://192.168.0.124:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#network-plugin","title":"Network plugin","text":"<p>Installing a Pod network add-on is the next required step and, once again, in lack of other suggestions, deploying flannel manually like in previous clusters seems the way to go:</p> <pre><code>$ wget \\\n  https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n\n$ kubectl apply -f kube-flannel.yml\nnamespace/kube-flannel created\nserviceaccount/flannel created\nclusterrole.rbac.authorization.k8s.io/flannel created\nclusterrolebinding.rbac.authorization.k8s.io/flannel created\nconfigmap/kube-flannel-cfg created\ndaemonset.apps/kube-flannel-ds created\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#troubleshooting-flannel","title":"Troubleshooting Flannel","text":"<p>Unlike in previous cluster setups, the <code>kube-flannel-ds</code> deployment fails and the pod stays in a <code>CrashLoopBackOff</code> cycle:</p> <pre><code>$ kubectl get pods -n kube-flannel \nNAME                    READY   STATUS             RESTARTS       AGE\nkube-flannel-ds-l6446   0/1     CrashLoopBackOff   10 (85s ago)   27m\n\n$ kubectl get all -n kube-flannel \nNAME                        READY   STATUS   RESTARTS      AGE\npod/kube-flannel-ds-l6446   0/1     Error    5 (88s ago)   3m18s\n\nNAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/kube-flannel-ds   1         1         0       1            0           &lt;none&gt;          3m18s\n</code></pre> <p>Inspecting the logs of this pod, in particular the error regarding <code>br_netfilter</code> suggests that the networking setup previously required by <code>containerd</code> was now, if not required by <code>containerd</code>, at least required by Flannel:</p> <pre><code>$ kubectl logs -n kube-flannel \\\n  $(kubectl get pods -n kube-flannel | grep kube-flannel | cut -f1 -d' ')\nDefaulted container \"kube-flannel\" out of: kube-flannel, install-cni-plugin (init), install-cni (init)\nI0222 18:58:21.486712       1 main.go:211] CLI flags config: {etcdEndpoints:http://127.0.0.1:4001,http://127.0.0.1:2379 etcdPrefix:/coreos.com/network etcdKeyfile: etcdCertfile: etcdCAFile: etcdUsername: etcdPassword: version:false kubeSubnetMgr:true kubeApiUrl: kubeAnnotationPrefix:flannel.alpha.coreos.com kubeConfigFile: iface:[] ifaceRegex:[] ipMasq:true ifaceCanReach: subnetFile:/run/flannel/subnet.env publicIP: publicIPv6: subnetLeaseRenewMargin:60 healthzIP:0.0.0.0 healthzPort:0 iptablesResyncSeconds:5 iptablesForwardRules:true netConfPath:/etc/kube-flannel/net-conf.json setNodeNetworkUnavailable:true}\nW0222 18:58:21.486829       1 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\nI0222 18:58:21.501277       1 kube.go:139] Waiting 10m0s for node controller to sync\nI0222 18:58:21.501371       1 kube.go:469] Starting kube subnet manager\nI0222 18:58:22.502220       1 kube.go:146] Node controller sync successful\nI0222 18:58:22.502244       1 main.go:231] Created subnet manager: Kubernetes Subnet Manager - alfred\nI0222 18:58:22.502249       1 main.go:234] Installing signal handlers\nI0222 18:58:22.502404       1 main.go:468] Found network config - Backend type: vxlan\nE0222 18:58:22.502451       1 main.go:268] Failed to check br_netfilter: stat /proc/sys/net/bridge/bridge-nf-call-iptables: no such file or directory\n</code></pre> <p>Having initially omitted the steps to let iptables see bridged traffic, these were performed after this failure and, afer restarting the daemonset (<code>ds</code>), proved to be the missing ingredient for success:</p> <pre><code>$ kubectl rollout restart ds kube-flannel-ds -n kube-flannel\ndaemonset.apps/kube-flannel-ds restarted\n\n$ kubectl get all -n kube-flannel\nNAME                        READY   STATUS    RESTARTS   AGE\npod/kube-flannel-ds-gq6b4   1/1     Running   0          4s\n\nNAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/kube-flannel-ds   1         1         1       1            1           &lt;none&gt;          43m\n\n$ kubectl logs -n kube-flannel \\\n  $(kubectl get pods -n kube-flannel | grep kube-flannel | cut -f1 -d' ')\nDefaulted container \"kube-flannel\" out of: kube-flannel, install-cni-plugin (init), install-cni (init)\nI0222 19:10:27.963740       1 main.go:211] CLI flags config: {etcdEndpoints:http://127.0.0.1:4001,http://127.0.0.1:2379 etcdPrefix:/coreos.com/network etcdKeyfile: etcdCertfile: etcdCAFile: etcdUsername: etcdPassword: version:false kubeSubnetMgr:true kubeApiUrl: kubeAnnotationPrefix:flannel.alpha.coreos.com kubeConfigFile: iface:[] ifaceRegex:[] ipMasq:true ifaceCanReach: subnetFile:/run/flannel/subnet.env publicIP: publicIPv6: subnetLeaseRenewMargin:60 healthzIP:0.0.0.0 healthzPort:0 iptablesResyncSeconds:5 iptablesForwardRules:true netConfPath:/etc/kube-flannel/net-conf.json setNodeNetworkUnavailable:true}\nW0222 19:10:27.963882       1 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\nI0222 19:10:27.981819       1 kube.go:139] Waiting 10m0s for node controller to sync\nI0222 19:10:27.981886       1 kube.go:469] Starting kube subnet manager\nI0222 19:10:27.985297       1 kube.go:490] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.244.0.0/24]\nI0222 19:10:28.981965       1 kube.go:146] Node controller sync successful\nI0222 19:10:28.981987       1 main.go:231] Created subnet manager: Kubernetes Subnet Manager - alfred\nI0222 19:10:28.981992       1 main.go:234] Installing signal handlers\nI0222 19:10:28.982155       1 main.go:468] Found network config - Backend type: vxlan\nI0222 19:10:28.984567       1 kube.go:669] List of node(alfred) annotations: map[string]string{\"flannel.alpha.coreos.com/backend-data\":\"{\\\"VNI\\\":1,\\\"VtepMAC\\\":\\\"8a:31:5f:70:6e:3f\\\"}\", \"flannel.alpha.coreos.com/backend-type\":\"vxlan\", \"flannel.alpha.coreos.com/kube-subnet-manager\":\"true\", \"flannel.alpha.coreos.com/public-ip\":\"192.168.0.124\", \"kubeadm.alpha.kubernetes.io/cri-socket\":\"unix:/run/containerd/containerd.sock\", \"node.alpha.kubernetes.io/ttl\":\"0\", \"volumes.kubernetes.io/controller-managed-attach-detach\":\"true\"}\nI0222 19:10:28.984614       1 match.go:211] Determining IP address of default interface\nI0222 19:10:28.984933       1 match.go:264] Using interface with name wlan0 and address 192.168.0.124\nI0222 19:10:28.984953       1 match.go:286] Defaulting external address to interface address (192.168.0.124)\nI0222 19:10:28.985003       1 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false\nI0222 19:10:28.987228       1 kube.go:636] List of node(alfred) annotations: map[string]string{\"flannel.alpha.coreos.com/backend-data\":\"{\\\"VNI\\\":1,\\\"VtepMAC\\\":\\\"8a:31:5f:70:6e:3f\\\"}\", \"flannel.alpha.coreos.com/backend-type\":\"vxlan\", \"flannel.alpha.coreos.com/kube-subnet-manager\":\"true\", \"flannel.alpha.coreos.com/public-ip\":\"192.168.0.124\", \"kubeadm.alpha.kubernetes.io/cri-socket\":\"unix:/run/containerd/containerd.sock\", \"node.alpha.kubernetes.io/ttl\":\"0\", \"volumes.kubernetes.io/controller-managed-attach-detach\":\"true\"}\nI0222 19:10:28.987278       1 vxlan.go:155] Interface flannel.1 mac address set to: 8a:31:5f:70:6e:3f\nI0222 19:10:28.987762       1 iptables.go:51] Starting flannel in iptables mode...\nW0222 19:10:28.987895       1 main.go:557] no subnet found for key: FLANNEL_IPV6_NETWORK in file: /run/flannel/subnet.env\nW0222 19:10:28.987918       1 main.go:557] no subnet found for key: FLANNEL_IPV6_SUBNET in file: /run/flannel/subnet.env\nI0222 19:10:28.987925       1 iptables.go:125] Setting up masking rules\nI0222 19:10:28.993678       1 iptables.go:226] Changing default FORWARD chain policy to ACCEPT\nI0222 19:10:28.995928       1 main.go:412] Wrote subnet file to /run/flannel/subnet.env\nI0222 19:10:28.995946       1 main.go:416] Running backend.\nI0222 19:10:28.996090       1 vxlan_network.go:65] watching for new subnet leases\nI0222 19:10:29.005453       1 iptables.go:372] bootstrap done\nI0222 19:10:29.009834       1 main.go:437] Waiting for all goroutines to exit\nI0222 19:10:29.014751       1 iptables.go:372] bootstrap done\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#enable-single-node-cluster","title":"Enable single-node cluster","text":"<p>Control plane node isolation is required for a single-node cluster, because otherwise a cluster will not schedule Pods on the control plane nodes for security reasons. This is reflected in the <code>Taints</code> found in the node details:</p> <pre><code>$ kubectl get nodes -o wide\nNAME     STATUS   ROLES           AGE    VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION        CONTAINER-RUNTIME\nalfred   Ready    control-plane   161m   v1.32.2   192.168.0.124   &lt;none&gt;        Debian GNU/Linux 12 (bookworm)   6.6.74+rpt-rpi-2712   containerd://1.7.25\n\n$ kubectl describe node alfred\nName:               alfred\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=arm64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=arm64\n                    kubernetes.io/hostname=alfred\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"8a:31:5f:70:6e:3f\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.0.124\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 22 Feb 2025 18:07:04 +0100\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  alfred\n  AcquireTime:     &lt;unset&gt;\n  RenewTime:       Sat, 22 Feb 2025 20:50:44 +0100\n</code></pre> <p>Remove this taint to allow other pods to be scheduled:</p> <pre><code>$ kubectl taint nodes --all node-role.kubernetes.io/control-plane-\nnode/alfred untainted\n\n$ kubectl describe node alfred | grep -i taint\nTaints:             &lt;none&gt;\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#test-pod-scheduling","title":"Test pod scheduling","text":"<pre><code>$ kubectl apply -f https://k8s.io/examples/pods/commands.yaml\npod/command-demo created\n\n$ kubectl get all\nNAME               READY   STATUS              RESTARTS   AGE\npod/command-demo   0/1     ContainerCreating   0          7s\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   172m\n\n$ kubectl get all\nNAME               READY   STATUS      RESTARTS   AGE\npod/command-demo   0/1     Completed   0          14s\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   172m\n\n$ kubectl events pods\nLAST SEEN           TYPE      REASON                    OBJECT             MESSAGE\n...\n12m                 Normal    Starting                  Node/alfred        \n12m                 Normal    RegisteredNode            Node/alfred        Node alfred event: Registered Node alfred in Controller\n19s                 Normal    Pulling                   Pod/command-demo   Pulling image \"debian\"\n19s                 Normal    Scheduled                 Pod/command-demo   Successfully assigned default/command-demo to alfred\n8s                  Normal    Pulled                    Pod/command-demo   Successfully pulled image \"debian\" in 11.231s (11.231s including waiting). Image size: 48316582 bytes.\n8s                  Normal    Created                   Pod/command-demo   Created container: command-demo-container\n7s                  Normal    Started                   Pod/command-demo   Started container command-demo-container\n</code></pre> <p>With the cluster now ready to run pods and services, move on to installing more components that will be used by the actual services: MetalLB Load Balancer, Kubernets Dashboard, Ingress Controller, HTTPS certificates with Let\u2019s Encrypt, including automatically renovated monthly.</p> <p>LocalPath PV provisioner for simple persistent storage in local file systems may be setup later, but experience with previous clusters so far has proven this unnecessary.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#metallb-load-balancer","title":"MetalLB Load Balancer","text":"<p>A Load Balancer is going to be necessary for the  Dashboard and other services, to expose individual services via open ports on the server (<code>NodePort</code>) or virtual IP addresses. Installation By Manifest is as simple as applying the provided manifest:</p> <pre><code>$ wget \\\n  https://raw.githubusercontent.com/metallb/metallb/v0.14.9/config/manifests/metallb-native.yaml\n\n$ kubectl apply -f metallb/metallb-native.yaml\nnamespace/metallb-system created\ncustomresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/communities.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/servicel2statuses.metallb.io created\nserviceaccount/controller created\nserviceaccount/speaker created\nrole.rbac.authorization.k8s.io/controller created\nrole.rbac.authorization.k8s.io/pod-lister created\nclusterrole.rbac.authorization.k8s.io/metallb-system:controller created\nclusterrole.rbac.authorization.k8s.io/metallb-system:speaker created\nrolebinding.rbac.authorization.k8s.io/controller created\nrolebinding.rbac.authorization.k8s.io/pod-lister created\nclusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created\nclusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created\nconfigmap/metallb-excludel2 created\nsecret/metallb-webhook-cert created\nservice/metallb-webhook-service created\ndeployment.apps/controller created\ndaemonset.apps/speaker created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/metallb-webhook-configuration created\n</code></pre> <p>Soon enough the deployment should have the controller and speaker running:</p> <pre><code>$ kubectl get all -n metallb-system\nNAME                             READY   STATUS    RESTARTS   AGE\npod/controller-bb5f47665-hgctc   1/1     Running   0          3m47s\npod/speaker-974wc                1/1     Running   0          3m47s\n\nNAME                              TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\nservice/metallb-webhook-service   ClusterIP   10.97.34.85   &lt;none&gt;        443/TCP   3m47s\n\nNAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\ndaemonset.apps/speaker   1         1         1       1            1           kubernetes.io/os=linux   3m47s\n\nNAME                         READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/controller   1/1     1            1           3m47s\n\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/controller-bb5f47665   1         1         1       3m47s\n</code></pre> <p>MetalLB remains idle until configured, which is done by deploying resources into its namespace (<code>metallb-system</code>). In this PC, a small range of IP addresses is advertised via  Layer 2 Configuration, which does not not require the IPs to be bound to the network interfaces:</p> metallb/ipaddress-pool-alfred.yaml<pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: production\n  namespace: metallb-system\nspec:\n  addresses:\n    - 192.168.0.151-192.168.0.160\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: l2-advert\n  namespace: metallb-system\n</code></pre> <p>The range is based on the local DHCP server configuration and which IPs are  currently in use; this range has just not been leased so far. The reason to use IPs from the leased range is that the router only allows adding port forwarding rules for those. This range is intentionally on the same network range and subnet as the DHCP server so that no routing is needed to reach MetalLB IPs.</p> <pre><code>$ kubectl apply -f ipaddress-pool-alfred.yaml\nipaddresspool.metallb.io/production created\nl2advertisement.metallb.io/l2-advert created\n\n$ kubectl get ipaddresspool.metallb.io -n metallb-system \nNAME         AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES\nproduction   true          false             [\"192.168.0.151-192.168.0.160\"]\n\n$ kubectl get l2advertisement.metallb.io -n metallb-system \nNAME        IPADDRESSPOOLS   IPADDRESSPOOL SELECTORS   INTERFACES\nl2-advert                                              \n\n$ kubectl describe ipaddresspool.metallb.io production -n metallb-system \nName:         production\nNamespace:    metallb-system\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  metallb.io/v1beta1\nKind:         IPAddressPool\nMetadata:\n  Creation Timestamp:  2025-02-22T21:45:44Z\n  Generation:          1\n  Resource Version:    22690\n  UID:                 ca4ffb14-4063-4a63-8ab4-75995db0347c\nSpec:\n  Addresses:\n    192.168.0.151-192.168.0.160\n  Auto Assign:       true\n  Avoid Buggy I Ps:  false\nEvents:              &lt;none&gt;\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#2026-upgrade-to-helm-chart","title":"2026 Upgrade to Helm chart","text":"<p>Upgrading Kubernetes eventually reached a point where MetalLB needed to be upgraded too; upgrading to Kubernetes 1.35 requires MetalLB 0.15 or later.</p> <p>MetalLb was last using the <code>metallb-native.yaml</code> instead of its Helm chart, and there is a <code>IPAddressPool</code> defined in a separate manifest, so now in order to migrate to the Helm chart without losing IP assignments or causing a collision, the migration needs to perform a \"takeover\". Kubernetes allows Helm to manage existing resources if they are correctly labeled and annotated. The <code>IPAddressPool</code> and <code>L2Advertisement</code> are Custom Resources (CRD). Helm will not delete these during the upgrade if the namespace is handled correctly, but they should be backed up first:</p> <pre><code>$ kubectl get ipaddresspools.metallb.io,l2advertisements.metallb.io -A -o yaml \\\n  &gt; metallb-config-backup.yaml\n</code></pre> <p>Install the MetalLB Helm chart:</p> <pre><code>$ helm repo add metallb https://metallb.github.io/metallb\n\"metallb\" has been added to your repositories\n\n$ helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"metallb\" chart repository\n...\nUpdate Complete. \u2388Happy Helming!\u2388\n</code></pre> <p>To minimize downtime during the takeover, pre-pull the Docker images so that the Helm installation happens in seconds rather than minutes:</p> <pre><code># docker pull quay.io/metallb/speaker:v0.15.3\nv0.15.3: Pulling from metallb/speaker\nfd4aa3667332: Pull complete \n...\n07003b9acb06: Pull complete \nDigest: sha256:c6a5b25b2e1fba610a57b2db4bb8141d7c133569d561a8cc29e38ca5113efbc4\nStatus: Downloaded newer image for quay.io/metallb/speaker:v0.15.3\nquay.io/metallb/speaker:v0.15.3\n\n# docker pull quay.io/metallb/controller:v0.15.3\nv0.15.3: Pulling from metallb/controller\nfd4aa3667332: Already exists \n...\nddf74a63f7d8: Already exists \n6dff88f058c1: Pull complete \ndb83dd67de88: Pull complete \nDigest: sha256:6698ccc54c380913816ed1fd0758637ec87dd79da419c4ab170a2c26c158ab89\nStatus: Downloaded newer image for quay.io/metallb/controller:v0.15.3\nquay.io/metallb/controller:v0.15.3\n</code></pre> <p>Helm requires specific metadata to \"adopt\" existing resources. Without this, Helm will fail with an \"already exists\" error. To avoid this, annotate and label the namespace:</p> <pre><code>$ kubectl describe namespaces metallb-system \nName:         metallb-system\nLabels:       kubernetes.io/metadata.name=metallb-system\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/enforce=privileged\n              pod-security.kubernetes.io/warn=privileged\nAnnotations:  &lt;none&gt;\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n\n$ kubectl annotate namespace metallb-system meta.helm.sh/release-name=metallb\nnamespace/metallb-system annotated\n\n$ kubectl annotate namespace metallb-system meta.helm.sh/release-namespace=metallb-system\nnamespace/metallb-system annotated\n\n$ kubectl label namespace metallb-system app.kubernetes.io/managed-by=Helm\nnamespace/metallb-system labeled\n\n$ kubectl describe namespaces metallb-system\nName:         metallb-system\nLabels:       app.kubernetes.io/managed-by=Helm\n              kubernetes.io/metadata.name=metallb-system\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/enforce=privileged\n              pod-security.kubernetes.io/warn=privileged\nAnnotations:  meta.helm.sh/release-name: metallb\n              meta.helm.sh/release-namespace: metallb-system\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n</code></pre> <p>Then, to avoid conflicts, delete the existing deployment/daemonset while keeping the CRDs and Namespace. It is critical not to delete the namespace, as doing so would trigger a cascading deletion of <code>IPAddressPools</code> and potentially <code>Services</code> using them.</p> <p>Do not run <code>kubectl delete -f</code> on the <code>metallb-native.yaml</code> manifest. Instead, use the manifest to identify and delete only the functional components (Deployments, DaemonSets, and RBAC) while leaving the Namespace and CRDs intact. To delete everything except the namespace and the CRDs, use the <code>--selector</code> to target MetalLB's internal components:</p> <pre><code>$ kubectl delete -n metallb-system -l app=metallb \\\n  deployment,daemonset,service,serviceaccount,role,rolebinding \ndeployment.apps \"controller\" deleted from metallb-system namespace\ndaemonset.apps \"speaker\" deleted from metallb-system namespace\nserviceaccount \"controller\" deleted from metallb-system namespace\nserviceaccount \"speaker\" deleted from metallb-system namespace\nrole.rbac.authorization.k8s.io \"controller\" deleted from metallb-system namespace\nrole.rbac.authorization.k8s.io \"pod-lister\" deleted from metallb-system namespace\nrolebinding.rbac.authorization.k8s.io \"controller\" deleted from metallb-system namespace\nrolebinding.rbac.authorization.k8s.io \"pod-lister\" deleted from metallb-system namespace\n\n$ kubectl delete -l app=metallb clusterrole,clusterrolebinding\nclusterrole.rbac.authorization.k8s.io \"metallb-system:controller\" deleted\nclusterrole.rbac.authorization.k8s.io \"metallb-system:speaker\" deleted\nclusterrolebinding.rbac.authorization.k8s.io \"metallb-system:controller\" deleted\nclusterrolebinding.rbac.authorization.k8s.io \"metallb-system:speaker\" deleted\n\n$ kubectl delete validatingwebhookconfiguration metallb-webhook-configuration\nvalidatingwebhookconfiguration.admissionregistration.k8s.io \"metallb-webhook-configuration\" deleted\n</code></pre> <p>The CRDs (<code>IPAddressPool</code> and <code>L2Advertisement</code>) in the 0.14.9 manifest don't have the <code>app=metallb</code> label, so the commands above will not touch them and thus the IP Pool configuration is safe.</p> <p>However, the above deletions are not enough, attempting to install the Helm chart fails with <code>INSTALLATION FAILED</code> errors due to many other pre-existing <code>CustomResourceDefinition</code> objects such as <code>bfdprofiles.metallb.io</code>:</p> <pre><code>$ helm install metallb metallb/metallb \\\n  --namespace metallb-system \\\n  --version 0.15.3\nError: INSTALLATION FAILED: Unable to continue with install: CustomResourceDefinition\n\"bfdprofiles.metallb.io\" in namespace \"\" exists and cannot be imported into the\ncurrent release: invalid ownership metadata; label validation error: missing key\n\"app.kubernetes.io/managed-by\": must be set to \"Helm\"; annotation validation error:\nmissing key \"meta.helm.sh/release-name\": must be set to \"metallb\"; annotation\nvalidation error: missing key \"meta.helm.sh/release-namespace\": must be set to\n\"metallb-system\"\n</code></pre> <p>So there are two options: <code>delete</code> or <code>annotate</code>; a few need to be deleted:</p> <pre><code>$ kubectl delete secret metallb-webhook-cert -n metallb-system\nsecret \"metallb-webhook-cert\" deleted from metallb-system namespace\n\n$ kubectl delete configmap metallb-excludel2 -n metallb-system\nconfigmap \"metallb-excludel2\" deleted from metallb-system namespace\n\n$ kubectl delete service metallb-webhook-service -n metallb-system\nservice \"metallb-webhook-service\" deleted from metallb-system namespace\n</code></pre> <p>The rest need to be annotated (to preserve them):</p> <pre><code>$ CRDS=\"bfdprofiles.metallb.io bgpadvertisements.metallb.io bgppeers.metallb.io communities.metallb.io ipaddresspools.metallb.io l2advertisements.metallb.io\"\n$ for crd in $CRDS; do\n  kubectl annotate crd $crd meta.helm.sh/release-name=metallb --overwrite\n  kubectl annotate crd $crd meta.helm.sh/release-namespace=metallb-system --overwrite\n  kubectl label crd $crd app.kubernetes.io/managed-by=Helm --overwrite\ndone\ncustomresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io labeled\ncustomresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io labeled\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io labeled\ncustomresourcedefinition.apiextensions.k8s.io/communities.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/communities.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/communities.metallb.io labeled\ncustomresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io labeled\ncustomresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io labeled\n\n$ kubectl annotate crd servicel2statuses.metallb.io meta.helm.sh/release-name=metallb --overwrite\ncustomresourcedefinition.apiextensions.k8s.io/servicel2statuses.metallb.io annotated\n\n$ kubectl annotate crd servicel2statuses.metallb.io meta.helm.sh/release-namespace=metallb-system --overwrite\ncustomresourcedefinition.apiextensions.k8s.io/servicel2statuses.metallb.io annotated\n\n$ kubectl label crd servicel2statuses.metallb.io app.kubernetes.io/managed-by=Helm --overwrite\ncustomresourcedefinition.apiextensions.k8s.io/servicel2statuses.metallb.io labeled\n</code></pre> <p>Once the old <code>controller</code> and <code>speaker</code> are gone, Helm can install 0.15.3 into the existing namespace:</p> <pre><code>$ helm install metallb metallb/metallb \\\n  --namespace metallb-system \\\n  --version 0.15.3\nNAME: metallb\nLAST DEPLOYED: Fri Jan 30 23:33:07 2026\nNAMESPACE: metallb-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nMetalLB is now running in the cluster.\n\nNow you can configure it via its CRs. Please refer to the metallb official docs\non how to use the CRs.\n</code></pre> <p>After the Helm install, the existing <code>IPAddressPool</code> is immediately active:</p> <pre><code>$ kubectl -n metallb-system get ipaddresspools\nNAME         AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES\nproduction   true          false             [\"192.168.0.151-192.168.0.160\"]\n\n$ kubectl -n metallb-system describe ipaddresspools\nName:         production\nNamespace:    metallb-system\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  metallb.io/v1beta1\nKind:         IPAddressPool\nMetadata:\n  Creation Timestamp:  2025-02-22T21:45:44Z\n  Generation:          1\n  Resource Version:    22690\n  UID:                 ca4ffb14-4063-4a63-8ab4-75995db0347c\nSpec:\n  Addresses:\n    192.168.0.151-192.168.0.160\n  Auto Assign:       true\n  Avoid Buggy I Ps:  false\nEvents:              &lt;none&gt;\n</code></pre> <p>Had this not worked, the Speaker pod logs should be inspected. When the pools are active, the speaker should be announcing services like this:</p> <pre><code>$ kubectl logs -n metallb-system metallb-speaker-54w68 | grep -i pool\n...\n{\"level\":\"info\",\"ts\":\"2026-01-30T23:27:20Z\",\"msg\":\"Starting EventSource\",\"controller\":\"bgppeer\",\"controllerGroup\":\"metallb.io\",\"controllerKind\":\"BGPPeer\",\"source\":\"kind source: *v1beta1.IPAddressPool\",\"stacktrace\":\"sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).startEventSourcesAndQueueLocked.func1.2.1\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.22.3/pkg/internal/controller/controller.go:353\"}\n{\"caller\":\"main.go:444\",\"event\":\"serviceAnnounced\",\"ips\":[\"192.168.0.154\"],\"level\":\"info\",\"msg\":\"service has IP, announcing\",\"pool\":\"production\",\"protocol\":\"layer2\",\"ts\":\"2026-01-30T23:27:20Z\"}\n{\"caller\":\"main.go:444\",\"event\":\"serviceAnnounced\",\"ips\":[\"192.168.0.154\"],\"level\":\"info\",\"msg\":\"service has IP, announcing\",\"pool\":\"production\",\"protocol\":\"layer2\",\"ts\":\"2026-01-30T23:27:20Z\"}\n</code></pre> <p>In the event of the logs showing errors about <code>\"no pools found\"</code>, re-apply the specific <code>IPAddressPool</code> manifest:</p> <pre><code>$ kubectl apply -f metallb/ipaddress-pool-alfred.yaml\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#kubernetes-dashboard","title":"Kubernetes Dashboard","text":"<p>As of v1.29, Kubernetes Dashboard supports only Helm-based  installation, which at this point means we have to Install helm from APT:</p> <pre><code>$ curl https://baltocdn.com/helm/signing.asc \\\n  | sudo gpg --dearmor -o /etc/apt/keyrings/helm.gpg\n$ echo \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" \\\n  | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\n$ sudo apt-get update\n$ sudo apt-get install -y helm\n</code></pre> <p>Then install the Helm repository for the Kubernetes dashboard:</p> <pre><code>$ helm repo add \\\n  kubernetes-dashboard \\\n  https://kubernetes.github.io/dashboard/\n\"kubernetes-dashboard\" has been added to your repositories\n</code></pre> <p>And install the Kubernetes dashboard (without any customization):</p> <pre><code>$ helm upgrade \\\n  --install kubernetes-dashboard \\\n    kubernetes-dashboard/kubernetes-dashboard \\\n  --create-namespace \\\n  --namespace kubernetes-dashboard\n\nRelease \"kubernetes-dashboard\" does not exist. Installing it now.\nNAME: kubernetes-dashboard\nLAST DEPLOYED: Sun Feb 23 17:08:33 2025\nNAMESPACE: kubernetes-dashboard\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\n*************************************************************************************************\n*** PLEASE BE PATIENT: Kubernetes Dashboard may need a few minutes to get up and become ready ***\n*************************************************************************************************\n\nCongratulations! You have just installed Kubernetes Dashboard in your cluster.\n\nTo access Dashboard run:\n  kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443\n\nNOTE: In case port-forward command does not work, make sure that kong service name is correct.\n      Check the services in Kubernetes Dashboard namespace using:\n        kubectl -n kubernetes-dashboard get svc\n\nDashboard will be available at:\n  https://localhost:8443\n\n$ kubectl get all -n kubernetes-dashboard \nNAME                                                        READY   STATUS    RESTARTS   AGE\npod/kubernetes-dashboard-api-5b7599777f-ltvmn               1/1     Running   0          17m\npod/kubernetes-dashboard-auth-9c9bf4947-gzvbw               1/1     Running   0          17m\npod/kubernetes-dashboard-kong-79867c9c48-9mx5z              1/1     Running   0          17m\npod/kubernetes-dashboard-metrics-scraper-55cc88cbcb-k7t4k   1/1     Running   0          17m\npod/kubernetes-dashboard-web-8f95766b5-z7cj7                1/1     Running   0          17m\n\nNAME                                           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/kubernetes-dashboard-api               ClusterIP   10.111.251.2     &lt;none&gt;        8000/TCP   17m\nservice/kubernetes-dashboard-auth              ClusterIP   10.96.196.222    &lt;none&gt;        8000/TCP   17m\nservice/kubernetes-dashboard-kong-proxy        ClusterIP   10.100.147.211   &lt;none&gt;        443/TCP    17m\nservice/kubernetes-dashboard-metrics-scraper   ClusterIP   10.106.137.217   &lt;none&gt;        8000/TCP   17m\nservice/kubernetes-dashboard-web               ClusterIP   10.106.239.53    &lt;none&gt;        8000/TCP   17m\n\nNAME                                                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kubernetes-dashboard-api               1/1     1            1           17m\ndeployment.apps/kubernetes-dashboard-auth              1/1     1            1           17m\ndeployment.apps/kubernetes-dashboard-kong              1/1     1            1           17m\ndeployment.apps/kubernetes-dashboard-metrics-scraper   1/1     1            1           17m\ndeployment.apps/kubernetes-dashboard-web               1/1     1            1           17m\n\nNAME                                                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/kubernetes-dashboard-api-5b7599777f               1         1         1       17m\nreplicaset.apps/kubernetes-dashboard-auth-9c9bf4947               1         1         1       17m\nreplicaset.apps/kubernetes-dashboard-kong-79867c9c48              1         1         1       17m\nreplicaset.apps/kubernetes-dashboard-metrics-scraper-55cc88cbcb   1         1         1       17m\nreplicaset.apps/kubernetes-dashboard-web-8f95766b5                1         1         1       17m\n</code></pre> <p>The dashboard is now behind the <code>kubernetes-dashboard-kong-proxy</code> service, and the suggested port forward can be used to map port 8443 to it:</p> <pre><code>$ kubectl -n kubernetes-dashboard port-forward \\\n  svc/kubernetes-dashboard-kong-proxy 8443:443 \\\n  --address 0.0.0.0\nForwarding from 0.0.0.0:8443 -&gt; 8443\n^C\n</code></pre> <p>The dasboard is then available at https://alfred:8443/:</p> <p></p> <p>Accessing the Dashboard UI requires creating a sample user; the setup in the tutorial creates an example admin user with all privileges, good enough for now:</p> dashboard/admin-sa-rbac.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  namespace: kubernetes-dashboard\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/service-account.name: \"admin-user\"   \ntype: kubernetes.io/service-account-token\n</code></pre> <pre><code>$ kubectl apply -f dashboard/admin-sa-rbac.yaml\nserviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\nsecret/admin-user created\n\n$ kubectl -n kubernetes-dashboard create token admin-user\neyJhbGciOiJSUzI1NiIsImtpZCI6IkFEVlNfMlN2SVJUbEpvMGNVeGRjako1LTlXakxFdXRzQy1kZjJIdHpCVUEifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzQwMzM5OTgwLCJpYXQiOjE3NDAzMzYzODAsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiMGMyODJkZjMtNjcwOC00N2NjLTk1NzItMTFmYzRkOGU3ZDEzIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiNjJiMmY5NzMtMWQ2MC00NzQ2LTk2ZTYtNmZiNDJhMGE1NmUzIn19LCJuYmYiOjE3NDAzMzYzODAsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbi11c2VyIn0.AgvTqQ3-cEuAQyKgaV0NYApq4cMjEd8rf4taOjwBDwtwBAeqIcGrv5nHv4RMTOK3CyxjBHevBOsOzZ_ycskngQ6aiq5lgrhGNzO8bKKMhFbZqvcO6soQfccNQFqCVcP01oBa6Ue7vx1YctWe1L5YJB201oQt8Fhvee72Xy5cyZ9BONKTp7lJTL48EXgGj6mSgwaHPKF6xfmYWgG8V3WXabN_MExVEEVrQMI0rJeS08iJ6S0yQAGi2VU4qpJ4m-1dwkoUtbqYmtNJnj8itP5tmuIdIYRJusg8ptoGHSBtV538U_QXEO4dLDTGgcWT1Y3wIwDvVaP969yJtXh5KaAdVg\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#troubleshooting-dashboard","title":"Troubleshooting Dashboard","text":"<p>Without the <code>--address 0.0.0.0</code> flag it will only accept connections from <code>localhost</code> and no other clients. This flag is necessary because this forward is the way to access the proxy over HTTPS and this is necessary for the access token to work:</p> <ul> <li>Bug #8794: Bearer Token Authentication not responding</li> <li>Bug #8795: Bearer token not working</li> </ul> <p>This issue makes impossible to access the dashboard bypassing the proxy, which could have been another way to access the dashboard from remote clients, using <code>kubectl proxy</code> with the following flags:</p> <pre><code>$ kubectl proxy \\\n  --address='0.0.0.0' \\\n  --disable-filter=true \\\n  --port=8001\nW0223 19:09:06.922315  850945 proxy.go:177] Request filter disabled, your proxy is vulnerable to XSRF attacks, please be cautious\nStarting to serve on [::]:8001\n</code></pre> <p>The dashboard is thus available on a really long URL, via its API: http://alfred:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard-kong-proxy:443/proxy/#/login but, due to the issues above, the token won't work:</p> <pre><code>$ kubectl logs -n kubernetes-dashboard \\\n  $(kubectl get pods -n kubernetes-dashboard | grep kubernetes-dashboard-web | cut -f1 -d' ') -f\n...\nE0223 18:46:31.743145       1 handler.go:33] \"Could not get user\" err=\"MSG_LOGIN_UNAUTHORIZED_ERROR\"\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#custom-helm-values","title":"Custom Helm Values","text":"<p>To customize the Kubernetes dashboard installation, use  helm chart values to enable the <code>ingress</code> and set services as <code>LoadBalancer</code> so they get each a dedicated virtual IP address:</p> dashboard/values.yaml<pre><code>app:\n  ingress:\n    enabled: true\n    hosts:\n      - alfred\n      - alfred-kong\nweb:\n  service:\n    type: LoadBalancer\nkong:\n  ingressController:\n    enabled: true\n  proxy:\n    type: LoadBalancer\n</code></pre> <p>Update the dashboard with the custom values:</p> <pre><code>$ helm upgrade \\\n  kubernetes-dashboard \\\n    kubernetes-dashboard/kubernetes-dashboard \\\n  --create-namespace \\\n  --namespace kubernetes-dashboard \\\n  --values=dashboard/values.yaml\nRelease \"kubernetes-dashboard\" has been upgraded. Happy Helming!\nNAME: kubernetes-dashboard\nLAST DEPLOYED: Sun Feb 23 17:59:31 2025\nNAMESPACE: kubernetes-dashboard\nSTATUS: deployed\nREVISION: 5\nTEST SUITE: None\nNOTES:\n*************************************************************************************************\n*** PLEASE BE PATIENT: Kubernetes Dashboard may need a few minutes to get up and become ready ***\n*************************************************************************************************\n\nCongratulations! You have just installed Kubernetes Dashboard in your cluster.\n\nTo access Dashboard run:\n  kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443\n\nNOTE: In case port-forward command does not work, make sure that kong service name is correct.\n      Check the services in Kubernetes Dashboard namespace using:\n        kubectl -n kubernetes-dashboard get svc\n\nDashboard will be available at:\n  https://localhost:8443\n\n\n\nLooks like you are deploying Kubernetes Dashboard on a custom domain(s).\nPlease make sure that the ingress configuration is valid.\nDashboard should be accessible on your configured domain(s) soon:\n  - https://alfred\n</code></pre> <p>The web UI is available on http://192.168.0.151:8000/ but only from the local host (e.g. via <code>curl</code>), it still refuses connections from other clients.</p> <p>On the contrary, https://alfred/ does not accept connections even locally, because it is only running as <code>ClusterIP</code> so it is only reachable on https://10.100.147.211/</p> <pre><code>$ kubectl get svc -n kubernetes-dashboard\nNAME                                           TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)               AGE\nkubernetes-dashboard-api                       ClusterIP      10.111.251.2     &lt;none&gt;          8000/TCP              72m\nkubernetes-dashboard-auth                      ClusterIP      10.96.196.222    &lt;none&gt;          8000/TCP              72m\nkubernetes-dashboard-kong-metrics              ClusterIP      10.99.128.207    &lt;none&gt;          10255/TCP,10254/TCP   30s\nkubernetes-dashboard-kong-proxy                LoadBalancer   10.100.147.211   192.168.0.152   443:32353/TCP         72m\nkubernetes-dashboard-kong-validation-webhook   ClusterIP      10.104.184.144   &lt;none&gt;          443/TCP               30s\nkubernetes-dashboard-metrics-scraper           ClusterIP      10.106.137.217   &lt;none&gt;          8000/TCP              72m\nkubernetes-dashboard-web                       LoadBalancer   10.106.239.53    192.168.0.151   8000:31544/TCP        72m\n</code></pre> <p>However, both services refurse connections from clients other than localhost, which makes the use of <code>LoadBalancer</code> rather pointless.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#alternatives-considered","title":"Alternatives considered","text":"<p>For a brief moment, considered falling back to the old deployment-based recommended.yaml used in previous clusters, but this may not work since  kubernetes-dashboard-7.10.3 is the first version compaible with Kubernetes v1.32.</p> <p>Other resources briefly considered were:</p> <ul> <li>Kubernetes Dashboard: Tutorial, Best Practices &amp; Alternatives</li> <li>Using Kong to access Kubernetes services, using a Gateway resource with no cloud provided LoadBalancer</li> <li>Setting trusted_ips in Kong.</li> </ul>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#new-in-2026-headlamp","title":"New in 2026: Headlamp","text":"<p>Kubernetes Dashboard was deprecated and archived in January 2026, and is no longer maintained due to lack of active maintainers and contributors. The suggested replacement is Headlamp, which can be deployed in-cluster with a (Pomerium) ingress.</p> <p>The easiest way to install Headlamp is to use their helm chart:</p> <pre><code>$ helm repo add headlamp https://kubernetes-sigs.github.io/headlamp/\n\"headlamp\" has been added to your repositories\n\n$ helm repo update\n\n$ helm install headlamp headlamp/headlamp \\\n  --namespace kube-system \\\n  -f headlamp-values.yaml\nNAME: headlamp\nLAST DEPLOYED: Sun Feb  1 00:27:44 2026\nNAMESPACE: kube-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace kube-system -l \"app.kubernetes.io/name=headlamp,app.kubernetes.io/instance=headlamp\" -o jsonpath=\"{.items[0].metadata.name}\")\n  export CONTAINER_PORT=$(kubectl get pod --namespace kube-system $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl --namespace kube-system port-forward $POD_NAME 8080:$CONTAINER_PORT\n2. Get the token using\n  kubectl create token headlamp --namespace kube-system\n</code></pre> <p><code>headlamp-values.yaml</code></p> <pre><code>config:\n  watchPlugins: true\npluginsManager:\n  enabled: true\n  configContent: |\n    plugins:\n      - name: cert-manager\n        source: https://artifacthub.io/packages/headlamp/headlamp-plugins/headlamp_cert-manager\n        version: 0.1.0\n      - name: flux\n        source: https://artifacthub.io/packages/headlamp/headlamp-plugins/headlamp_flux\n        version: 0.5.0\n      - name: trivy\n        source: https://artifacthub.io/packages/headlamp/headlamp-trivy/headlamp_trivy\n        version: 0.3.1\n    installOptions:\n      parallel: true\n      maxConcurrent: 2\n  baseImage: node:lts-alpine\n  version: latest\n</code></pre> <p>To make Headlamp available via Pomerium ingress on external URL, add an <code>Ingress</code> manifest under <code>pomerium/pomerium-ingress/</code> (and load it from <code>kustomization.yaml</code>):</p> <p><code>pomerium/pomerium-ingress/headlamp.yaml</code></p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: headlamp-pomerium-ingress\n  namespace: kube-system\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.pomerium.io/pass_identity_headers: true\nspec:\n  ingressClassName: pomerium\n  rules:\n    - host: batlamp.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: headlamp\n                port:\n                  number: 80\n  tls:\n    - secretName: tls-secret-cloudflare\n      hosts:\n        - batlamp.very-very-dark-gray.top\n</code></pre> <p>Finally, to log into the Headlamp dashboard, request a token for <code>headlamp</code>:</p> <pre><code>$ kubectl create token headlamp --namespace kube-system\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#token-bypass","title":"Token bypass","text":"<p>Having Headlamp accessible only through a Cloudflare Tunnel and Pomerium, both of which restrict access greatly, having to SSH in to create a token each team seems excessive and unnecessarily inconvenient. To bypass that, create a legacy long-lived token:</p> <p><code>headlamp-token.yaml</code></p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: headlamp-static-token\n  namespace: kube-system\n  annotations:\n    kubernetes.io/service-account.name: \"headlamp\"\ntype: kubernetes.io/service-account-token\n</code></pre> <p>Create this <code>Secret</code> and extract the token:</p> <pre><code>$ kubectl apply -f headlamp-token.yaml \nsecret/headlamp-static-token created\n\n$ kubectl get secret headlamp-static-token \\\n  -n kube-system -o jsonpath='{.data.token}' | base64 --decode\ney...\n</code></pre> <p>Get the base64-encoded Certificate Authority (<code>CA</code>) data from the cluster:</p> <pre><code>$ kubekubectl config view --raw \\\n  -o jsonpath='{.clusters[0].cluster.certificate-authority-data}'\nLS...\n</code></pre> <p>Then create a <code>Config</code> to hold that token plus the <code>CA</code> data from:</p> <p><code>headlamp-kubeconfig.yaml</code></p> <pre><code>    apiVersion: v1\n    kind: Config\n    clusters:\n    - cluster:\n        certificate-authority-data: LS...&lt;CLUSTER_CA_DATA&gt;\n        server: https://kubernetes.default.svc\n      name: kubernetes\n    contexts:\n    - context:\n        cluster: kubernetes\n        user: headlamp-user\n      name: headlamp-context\n    current-context: headlamp-context\n    users:\n    - name: headlamp-user\n      user:\n        token: ey...&lt;GENERATED_TOKEN&gt;\n</code></pre> <p>Create the <code>Secret</code> using the <code>create secret</code> command to wrap your file into an <code>Opaque</code> secret: </p> <pre><code>$ kubectl create secret generic headlamp-kubeconfig \\\n  --from-file=config=./headlamp-kubeconfig.yaml -n kube-system\nsecret/headlamp-kubeconfig created\n</code></pre> <p>Add the <code>kubeconfig</code> as a volume along with the <code>-dev</code> flag for less strict security:</p> <p><code>headlamp-values.yaml</code></p> <pre><code>config:\n  inCluster: false\n  extraArgs:\n    - \"-dev\"\n    - \"-kubeconfig=/home/headlamp/.config/Headlamp/kubeconfigs/config\"\n  watchPlugins: true\nenv:\n  - name: KUBECONFIG\n    value: /home/headlamp/.config/Headlamp/kubeconfigs/config\nvolumeMounts:\n  - name: kubeconfig-volume\n    mountPath: /home/headlamp/.config/Headlamp/kubeconfigs/\n    readOnly: true\nvolumes:\n  - name: kubeconfig-volume\n    secret:\n      secretName: headlamp-kubeconfig\npluginsManager:\n  ...\n</code></pre> <p>Finally, upgrade the deployment with the new Helm values:</p> <pre><code>$ helm upgrade headlamp headlamp/headlamp \\\n  --namespace kube-system \\\n  -f headlamp-values.yaml\nNAME: headlamp\nLAST DEPLOYED: Sun Feb  1 00:27:44 2026\nNAMESPACE: kube-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace kube-system -l \"app.kubernetes.io/name=headlamp,app.kubernetes.io/instance=headlamp\" -o jsonpath=\"{.items[0].metadata.name}\")\n  export CONTAINER_PORT=$(kubectl get pod --namespace kube-system $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl --namespace kube-system port-forward $POD_NAME 8080:$CONTAINER_PORT\n2. Get the token using\n  kubectl create token headlamp --namespace kube-system\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#ingress-controller","title":"Ingress Controller","text":"<p>Warning</p> <p>The Nginx Ingress Controller was deprecated in 2025 and replaced with Pomerium.</p> <p>An Nginx Ingress Controller will be used to redirect HTTPS requests to different services depending on the <code>Host</code> header, while all those requests will be hitting the same IP address.</p> <p>The current Nginx Installation Guide essentially suggests two methods to install Nginx:</p> <ul> <li>The Helm chart.</li> <li>The default v1.12 deployment   based on the Helm chart.</li> <li>A <code>NodePort</code> deployment for   bare metal clusters.</li> </ul> <p>Supported Versions table indicates Nginx v1.12 is the first version compaible with Kubernetes v1.32 and, most conviniently, the updated version is nearly the only difference between the default deployment based on the Helm template and the deployment used in previous clusters.</p> <pre><code>$ helm repo add \\\n  ingress-nginx \\\n  https://kubernetes.github.io/ingress-nginx\n\"ingress-nginx\" has been added to your repositories\n\n$ helm upgrade \\\n  --install ingress-nginx \\\n  ingress-nginx/ingress-nginx \\\n  --create-namespace \\\n  --namespace ingress-nginx\nRelease \"ingress-nginx\" does not exist. Installing it now.\nNAME: ingress-nginx\nLAST DEPLOYED: Sun Feb 23 21:26:13 2025\nNAMESPACE: ingress-nginx\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThe ingress-nginx controller has been installed.\nIt may take a few minutes for the load balancer IP to be available.\nYou can watch the status by running 'kubectl get service --namespace ingress-nginx ingress-nginx-controller --output wide --watch'\n\nAn example Ingress that makes use of the controller:\n  apiVersion: networking.k8s.io/v1\n  kind: Ingress\n  metadata:\n    name: example\n    namespace: foo\n  spec:\n    ingressClassName: nginx\n    rules:\n      - host: www.example.com\n        http:\n          paths:\n            - pathType: Prefix\n              backend:\n                service:\n                  name: exampleService\n                  port:\n                    number: 80\n              path: /\n    # This section is only required if TLS is to be enabled for the Ingress\n    tls:\n      - hosts:\n        - www.example.com\n        secretName: example-tls\n\nIf TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:\n\n  apiVersion: v1\n  kind: Secret\n  metadata:\n    name: example-tls\n    namespace: foo\n  data:\n    tls.crt: &lt;base64 encoded cert&gt;\n    tls.key: &lt;base64 encoded key&gt;\n  type: kubernetes.io/tls\n</code></pre> <p>The first virtual IP address is assigned to the <code>ingress-nginx-controller</code> service and there is NGinx happily returning 404 Not found:</p> <pre><code>$ kubectl get svc -n ingress-nginx\nNAME                                 TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                      AGE\ningress-nginx-controller             LoadBalancer   10.111.164.62   192.168.0.151   80:31235/TCP,443:32391/TCP   70s\ningress-nginx-controller-admission   ClusterIP      10.100.225.28   &lt;none&gt;          443/TCP                      70s\n\n$ curl -k https://192.168.0.151/\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#troubleshooting-ingress","title":"Troubleshooting Ingress","text":"<p>This is still not available from other hosts, connections to the <code>LoadBalancer</code> IP address times out or is refused, on both ports. This Stack Overflow answer suggests the reason is likely to be that the pod itself is not accepting connections from external hosts, but the Helm chart seems to default to accepting them:</p> <pre><code>$ helm upgrade \\\n  --install ingress-nginx \\\n  ingress-nginx/ingress-nginx \\\n  --create-namespace \\\n  --namespace ingress-nginx \\\n  --dry-run --debug\n...\n  service:\n    annotations: {}\n    appProtocol: true\n    clusterIP: \"\"\n    enableHttp: true\n    enableHttps: true\n    enabled: true\n    external:\n      enabled: true\n    type: LoadBalancer\n</code></pre> <p>Lets try uninstalling the Helm chart and installing instead the deployment for bare metal clusters:</p> <pre><code>$ helm uninstall ingress-nginx \\\n  --namespace ingress-nginx   \nrelease \"ingress-nginx\" uninstalled\n\n$ kubectl get all -n ingress-nginx\nNAME                                           READY   STATUS        RESTARTS       AGE\npod/ingress-nginx-controller-cd9d6bbd7-c4cbp   1/1     Terminating   1 (167m ago)   5d20h\n\n$ kubectl delete namespace ingress-nginx\nnamespace \"ingress-nginx\" deleted\n</code></pre> <pre><code>$ wget -O nginx-baremetal.yaml \\\n  https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.12.0/deploy/static/provider/baremetal/deploy.yaml\n\n$ kubectl apply -f nginx-baremetal.yaml\nnamespace/ingress-nginx created\nserviceaccount/ingress-nginx created\nserviceaccount/ingress-nginx-admission created\nrole.rbac.authorization.k8s.io/ingress-nginx created\nrole.rbac.authorization.k8s.io/ingress-nginx-admission created\nclusterrole.rbac.authorization.k8s.io/ingress-nginx created\nclusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created\nrolebinding.rbac.authorization.k8s.io/ingress-nginx created\nrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\nconfigmap/ingress-nginx-controller created\nservice/ingress-nginx-controller created\nservice/ingress-nginx-controller-admission created\ndeployment.apps/ingress-nginx-controller created\njob.batch/ingress-nginx-admission-create created\njob.batch/ingress-nginx-admission-patch created\ningressclass.networking.k8s.io/nginx created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created\n\n$ kubectl get all -n ingress-nginx\nNAME                                            READY   STATUS      RESTARTS   AGE\npod/ingress-nginx-admission-create-55p4r        0/1     Completed   0          37s\npod/ingress-nginx-admission-patch-fhhgq         0/1     Completed   1          37s\npod/ingress-nginx-controller-6d59d95796-5lggl   1/1     Running     0          37s\n\nNAME                                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nservice/ingress-nginx-controller             NodePort    10.110.176.19   &lt;none&gt;        80:31438/TCP,443:32035/TCP   37s\nservice/ingress-nginx-controller-admission   ClusterIP   10.103.89.225   &lt;none&gt;        443/TCP                      37s\n\nNAME                                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/ingress-nginx-controller   1/1     1            1           37s\n\nNAME                                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/ingress-nginx-controller-6d59d95796   1         1         1       37s\n\nNAME                                       STATUS     COMPLETIONS   DURATION   AGE\njob.batch/ingress-nginx-admission-create   Complete   1/1           3s         37s\njob.batch/ingress-nginx-admission-patch    Complete   1/1           4s         37s\n</code></pre> <p>Finally, at least those <code>NodePort</code> are reachable from other hosts:</p> <pre><code>$ curl http://192.168.0.124:31438/\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n$ curl -k https://192.168.0.124:32035/\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>But still, switching the <code>ingress-nginx-controller</code> services back to <code>LoadBalancer</code> makes the service unreachable; even when connecting to the Ethernet network, in case the problem would be related to which NIC Flannel is using, but neither the cable nor adding <code>--iface=wlan0</code> to the <code>args</code> for <code>flanneld</code> in <code>kube-flannel.yml</code> made any difference.</p> <p>Morever, the assigned <code>LoadBalancer</code> IP address does not respond to <code>ping</code> and there is no ARP resolution for it after trying:</p> <pre><code>$ ping 192.168.0.151 -c 1\nPING 192.168.0.151 (192.168.0.151) 56(84) bytes of data.\nFrom 192.168.0.2 icmp_seq=1 Destination Host Unreachable\n\n--- 192.168.0.151 ping statistics ---\n1 packets transmitted, 0 received, +1 errors, 100% packet loss, time 0ms\n\n$ ping 192.168.0.121 -c 1\nPING 192.168.0.121 (192.168.0.121) 56(84) bytes of data.\n64 bytes from 192.168.0.121: icmp_seq=1 ttl=64 time=0.183 ms\n\n--- 192.168.0.121 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.183/0.183/0.183/0.000 ms\n\n$ ping 192.168.0.124 -c 1\nPING 192.168.0.124 (192.168.0.124) 56(84) bytes of data.\n64 bytes from 192.168.0.124: icmp_seq=1 ttl=64 time=92.9 ms\n\n--- 192.168.0.124 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 92.896/92.896/92.896/0.000 ms\n\n$ nc -v 192.168.0.151 80\nnc: connect to 192.168.0.151 port 80 (tcp) failed: No route to host\n\n$ nc -v 192.168.0.151 443\nnc: connect to 192.168.0.151 port 443 (tcp) failed: No route to host\n\n$ arp -a\n? (192.168.0.121) at 2c:cf:67:83:6f:3b [ether] on enp5s0\nalfred (192.168.0.124) at 2c:cf:67:83:6f:3c [ether] on enp5s0\n? (192.168.0.151) at &lt;incomplete&gt; on enp5s0\n</code></pre> <p>The LoadbalancerIP or the External SVC IP will never be pingable suggest this last test may have been futile, but either way neither <code>ping</code> nor <code>nc</code> nor <code>tcptraceroute</code> nor <code>mtr</code> can connect to HTTP/S ports on that IP.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#fix-metallb-speaker","title":"Fix MetalLB Speaker","text":"<p>The problem turned out to be that MetalLB is not advertising my service from my control-plane nodes or from my single node cluster and the solutions are to either remove the node label, or make MetalLB circumvent it.</p> <p>This label is present by default in control panel nodes, so it is found here now:</p> <pre><code>$ kubectl describe node alfred | grep exclude\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n</code></pre> <p>While this label is in effect, MetalLB virtual IPs are not found even by <code>arping</code>:</p> <pre><code># arping 192.168.0.124\nARPING 192.168.0.124\n60 bytes from 2c:cf:67:83:6f:3b (192.168.0.124): index=0 time=161.545 usec\n60 bytes from 2c:cf:67:83:6f:3c (192.168.0.124): index=1 time=176.240 msec\n60 bytes from 2c:cf:67:83:6f:3c (192.168.0.124): index=2 time=177.685 msec\n60 bytes from 2c:cf:67:83:6f:3b (192.168.0.124): index=3 time=153.981 usec\n^C\n--- 192.168.0.124 statistics ---\n2 packets transmitted, 4 packets received,   0% unanswered (2 extra)\nrtt min/avg/max/std-dev = 0.154/88.560/177.685/88.404 ms\n\n# arping 192.168.0.151\nARPING 192.168.0.151\nTimeout\nTimeout\n^C\n--- 192.168.0.151 statistics ---\n3 packets transmitted, 0 packets received, 100% unanswered (0 extra)\n</code></pre> <p>IP address <code>.124</code> is the node's (non-virtual) address, <code>.154</code> is a <code>LoalBalancder</code> virtual address issued by MetalLB. </p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#remove-node-label-nodekubernetesioexclude-from-external-load-balancers","title":"Remove node label <code>node.kubernetes.io/exclude-from-external-load-balancers</code>","text":"<p>The recommended solution is to make sure your nodes are not labeled with the <code>node.kubernetes.io/exclude-from-external-load-balancers</code> label. This means removing the label (setting the label explicitly to <code>false</code> is not enough):</p> <pre><code>$ kubectl label nodes alfred \\\n  node.kubernetes.io/exclude-from-external-load-balancers-\nnode/alfred unlabeled\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#make-metallb-circumvent-the-node-label","title":"Make MetalLB circumvent the node label","text":"<p>An alternative method is provided in the troubleshooting guide as well: one way to circumvent the issue is to provide the speakers with the <code>--ignore-exclude-lb</code>. In this case, the flag can be added under the <code>args</code> for the Speaker's <code>DaemonSet</code> in the manifest:</p> metallb/metallb-native.yaml<pre><code>---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app: metallb\n    component: speaker\n  name: speaker\n  namespace: metallb-system\nspec:\n  selector:\n    matchLabels:\n      app: metallb\n      component: speaker\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: \"7472\"\n        prometheus.io/scrape: \"true\"\n      labels:\n        app: metallb\n        component: speaker\n    spec:\n      containers:\n      - args:\n        - --port=7472\n        - --log-level=info\n        - --ignore-exclude-lb=true\n</code></pre> <p>After applying either of the above changes, and reverting the Ingress Controller service to the <code>LoadBalancer</code> type, NGinx is finally reachable from other hosts:</p> <pre><code># arping 192.168.0.151\nARPING 192.168.0.151\n60 bytes from 2c:cf:67:83:6f:3b (192.168.0.151): index=0 time=224.584 usec\n60 bytes from 2c:cf:67:83:6f:3b (192.168.0.151): index=1 time=216.128 usec\n^C\n--- 192.168.0.151 statistics ---\n2 packets transmitted, 2 packets received,   0% unanswered (0 extra)\nrtt min/avg/max/std-dev = 0.216/0.220/0.225/0.004 ms\n\n$ curl 2&gt;/dev/null \\\n  -H \"Host: kubernetes-alfred.very-very-dark-gray.top\" \\\n  -k https://192.168.0.151/ | head\n&lt;!--\nCopyright 2017 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\n</code></pre> <p>As a nice bonus, NGinx still replies to the old <code>NodePort</code> port <code>32035</code>.</p> <code>kubectl apply -f metallb/metallb-native.yaml</code> <pre><code>$ kubectl apply -f metallb/metallb-native.yaml\nnamespace/metallb-system unchanged\ncustomresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io configured\ncustomresourcedefinition.apiextensions.k8s.io/communities.metallb.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/servicel2statuses.metallb.io unchanged\nserviceaccount/controller unchanged\nserviceaccount/speaker unchanged\nrole.rbac.authorization.k8s.io/controller unchanged\nrole.rbac.authorization.k8s.io/pod-lister unchanged\nclusterrole.rbac.authorization.k8s.io/metallb-system:controller unchanged\nclusterrole.rbac.authorization.k8s.io/metallb-system:speaker unchanged\nrolebinding.rbac.authorization.k8s.io/controller unchanged\nrolebinding.rbac.authorization.k8s.io/pod-lister unchanged\nclusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller unchanged\nclusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker unchanged\nconfigmap/metallb-excludel2 unchanged\nsecret/metallb-webhook-cert unchanged\nservice/metallb-webhook-service unchanged\ndeployment.apps/controller unchanged\ndaemonset.apps/speaker configured\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/metallb-webhook-configuration configured\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#allow-snippet-directives","title":"Allow snippet directives","text":"<p>To enable the use of snippets annotations in the next section, it is necessary to override <code>allow-snippet-annotations</code> which is set <code>to false</code> by default to mitigate known vulnerability CVE-2021-25742. This vulnerability only applies in clusters where multiple users have Kubernetes namespace administrator privileges, which is not the case here.</p> <p>To enable this in the deployment for bare metal clusters, replace the <code>data: null</code> in line 323 with the following 3-line section:</p> nginx-baremetal.yaml<pre><code>---\napiVersion: v1\ndata:\n  allow-snippet-annotations: \"true\"\n  annotations-risk-level: \"Critical\"\nkind: ConfigMap\n</code></pre> <p>Re-applying will only configure the <code>configmap</code> and webhook validation rules:</p> <pre><code>$ kubectl apply -f nginx-baremetal.yaml\nnamespace/ingress-nginx unchanged\nserviceaccount/ingress-nginx unchanged\nserviceaccount/ingress-nginx-admission unchanged\nrole.rbac.authorization.k8s.io/ingress-nginx unchanged\nrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged\nclusterrole.rbac.authorization.k8s.io/ingress-nginx unchanged\nclusterrole.rbac.authorization.k8s.io/ingress-nginx-admission unchanged\nrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged\nrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx unchanged\nclusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission unchanged\nconfigmap/ingress-nginx-controller configured\nservice/ingress-nginx-controller unchanged\nservice/ingress-nginx-controller-admission unchanged\ndeployment.apps/ingress-nginx-controller configured\njob.batch/ingress-nginx-admission-create unchanged\njob.batch/ingress-nginx-admission-patch unchanged\ningressclass.networking.k8s.io/nginx unchanged\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission configured\n</code></pre> Exactly why enable <code>allow-snippet-annotations</code> and raise the <code>annotations-risk-level</code>? <p>Without enabling <code>allow-snippet-annotations</code>, deploying the <code>Ingress</code> in the next section fails because the <code>configuration-snippet</code> used to hide server headers makes the deployment fail:</p> <pre><code>$ kubectl apply -f dashboard/nginx-ingress.yaml\n{\"metadata\":{\"annotations\":{\"cert-manager.io/issuer\":\"letsencrypt-staging\",\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"networking.k8s.io/v1\\\",\\\"kind\\\":\\\"Ingress\\\",\\\"metadata\\\":{\\\"annotations\\\":{\\\"cert-manager.io/issuer\\\":\\\"letsencrypt-staging\\\",\\\"nginx.ingress.kubernetes.io/auth-tls-verify-client\\\":\\\"false\\\",\\\"nginx.ingress.kubernetes.io/backend-protocol\\\":\\\"HTTPS\\\",\\\"nginx.ingress.kubernetes.io/configuration-snippet\\\":\\\"more_set_headers \\\\\\\"server: hide\\\\\\\";\\\\n\\\",\\\"nginx.ingress.kubernetes.io/whitelist-source-range\\\":\\\"10.244.0.0/16\\\"},\\\"name\\\":\\\"kubernetes-dashboard-ingress\\\",\\\"namespace\\\":\\\"kubernetes-dashboard\\\"},\\\"spec\\\":{\\\"ingressClassName\\\":\\\"nginx\\\",\\\"rules\\\":[{\\\"host\\\":\\\"k8s.alfred.uu.am\\\",\\\"http\\\":{\\\"paths\\\":[{\\\"backend\\\":{\\\"service\\\":{\\\"name\\\":\\\"kubernetes-dashboard-kong-proxy\\\",\\\"port\\\":{\\\"number\\\":443}}},\\\"path\\\":\\\"/\\\",\\\"pathType\\\":\\\"Prefix\\\"}]}}],\\\"tls\\\":[{\\\"hosts\\\":[\\\"k8s.alfred.uu.am\\\"],\\\"secretName\\\":\\\"tls-secret\\\"}]}}\\n\",\"nginx.ingress.kubernetes.io/configuration-snippet\":\"more_set_headers \\\"server: hide\\\";\\n\"}},\"spec\":{\"tls\":[{\"hosts\":[\"k8s.alfred.uu.am\"],\"secretName\":\"tls-secret\"}]}}\nto:\nResource: \"networking.k8s.io/v1, Resource=ingresses\", GroupVersionKind: \"networking.k8s.io/v1, Kind=Ingress\"\nName: \"kubernetes-dashboard-ingress\", Namespace: \"kubernetes-dashboard\"\nfor: \"dashboard/nginx-ingress.yaml\": error when patching \"dashboard/nginx-ingress.yaml\": admission webhook \"validate.nginx.ingress.kubernetes.io\" denied the request: nginx.ingress.kubernetes.io/configuration-snippet annotation cannot be used. Snippet directives are disabled by the Ingress administrator\n</code></pre> <p>The <code>Snippet directives are disabled by the Ingress administrator</code> and the end of the error message is the hint that <code>allow-snippet-annotations</code> needs to be enabled.</p> <p>Without raisig the <code>annotations-risk-level</code> to <code>Critical</code>, the deployment fails because, since controller 1.12, the annotation used to hide server headers (<code>nginx.ingress.kubernetes.io/configuration-snippet</code>) is rated <code>Critical</code>.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#kubernetes-dashboard-ingress","title":"Kubernetes Dashboard Ingress","text":"<p>With both Ngnix and the Kubernetes dashboard up and running, it is now possible to make the dashboard more conveniently accessible via Nginx. This <code>Ingress</code> is a slightly more complete one based on the example above:</p> dashboard/nginx-ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard-ingress\n  namespace: kubernetes-dashboard\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"false\"\n    nginx.ingress.kubernetes.io/whitelist-source-range: 10.244.0.0/16\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      more_set_headers \"server: hide\";\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: k8s.alfred\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard-kong-proxy\n                port:\n                  number: 443\n</code></pre> <p>This will point to the <code>kubernetes-dashboard-kong-proxy</code> which is the one listening on standard HTTPS port 443. This is the one targeted by the <code>kubectl port-forward</code> command above, which then exposes it on the node's port 8443.</p> <pre><code>$ kubectl apply -f dashboard/nginx-ingress.yaml\ningress.networking.k8s.io/kubernetes-dashboard-ingress created\n</code></pre> <p>A very quick way to test that the new <code>Ingress</code> is working correctly, add the <code>Host</code> header to the above <code>curl</code> command:</p> <pre><code>$ curl 2&gt;/dev/null \\\n  -H \"Host: k8s.alfred\" \\\n  -k https://192.168.0.151/ \\\n| head\n&lt;!--\nCopyright 2017 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Once this works in the local network, it can be made accessible externally by forwarding a port (443 if available, any other one otherwise) to the Nginx <code>LoadBalancer</code> IP (or <code>NodePort</code> on the node's local IP address; this should work just as well as if Nginx had a <code>LoadBalancer</code> IP address). It is also necessary to update (or clone) the ingress rule to set <code>host</code> to the FQDN that points to the router's exterenal IP address, e.g. <code>k8s.alfred.uu.am</code>.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#more-flannel-troubleshooting","title":"More Flannel Troubleshooting","text":"<p>It looks like no <code>LoadBalancer</code> or <code>NodePort</code> is reachable from other hosts and it looks like it may be a consequence of having initially omitted the steps to let iptables see bridged traffic, thus leading to Flannel problems. Comparing the output from <code>iptables</code> with that in <code>lexicon</code>, many rules with source and destination <code>br-*****</code> are missing:</p> <code>root@lexicon ~ # iptables -nvL</code> <pre><code>root@lexicon ~ # iptables -nvL\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\npkts bytes target     prot opt in     out     source               destination         \n530K   34M KUBE-PROXY-FIREWALL  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes load balancer firewall */\n2528K 4151M KUBE-FORWARD  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */\n468K   31M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes service portals */\n468K   31M KUBE-EXTERNAL-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes externally-visible service portals */\n468K   31M DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0           \n  33 16623 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            match-set docker-ext-bridges-v4 dst ctstate RELATED,ESTABLISHED\n468K   31M DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0           \n  26  1560 DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            match-set docker-ext-bridges-v4 dst\n    0     0 ACCEPT     all  --  wg0    *       0.0.0.0/0            0.0.0.0/0           \n    0     0 ACCEPT     all  --  br-1352dfaa6e69 *       0.0.0.0/0            0.0.0.0/0           \n  139 10023 ACCEPT     all  --  br-26c076fa82f8 *       0.0.0.0/0            0.0.0.0/0           \n    0     0 ACCEPT     all  --  br-f461a8ffa6e0 *       0.0.0.0/0            0.0.0.0/0           \n    0     0 ACCEPT     all  --  br-f9783559f0e8 *       0.0.0.0/0            0.0.0.0/0           \n    0     0 ACCEPT     all  --  docker0 *       0.0.0.0/0            0.0.0.0/0           \n468K   31M FLANNEL-FWD  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* flanneld forward */\n\nChain DOCKER (1 references)\npkts bytes target     prot opt in     out     source               destination         \n    0     0 ACCEPT     tcp  --  !br-26c076fa82f8 br-26c076fa82f8  0.0.0.0/0            172.19.0.4           tcp dpt:3000\n    0     0 ACCEPT     tcp  --  !br-26c076fa82f8 br-26c076fa82f8  0.0.0.0/0            172.19.0.3           tcp dpt:8086\n    0     0 ACCEPT     tcp  --  !br-26c076fa82f8 br-26c076fa82f8  0.0.0.0/0            172.19.0.2           tcp dpt:8125\n    0     0 DROP       all  --  !br-1352dfaa6e69 br-1352dfaa6e69  0.0.0.0/0            0.0.0.0/0           \n  24  1440 DROP       all  --  !br-26c076fa82f8 br-26c076fa82f8  0.0.0.0/0            0.0.0.0/0           \n    0     0 DROP       all  --  !br-f461a8ffa6e0 br-f461a8ffa6e0  0.0.0.0/0            0.0.0.0/0           \n    0     0 DROP       all  --  !br-f9783559f0e8 br-f9783559f0e8  0.0.0.0/0            0.0.0.0/0           \n    0     0 DROP       all  --  !docker0 docker0  0.0.0.0/0            0.0.0.0/0           \n\nChain DOCKER-ISOLATION-STAGE-1 (1 references)\npkts bytes target     prot opt in     out     source               destination         \n    0     0 DOCKER-ISOLATION-STAGE-2  all  --  br-1352dfaa6e69 !br-1352dfaa6e69  0.0.0.0/0            0.0.0.0/0           \n  137  9903 DOCKER-ISOLATION-STAGE-2  all  --  br-26c076fa82f8 !br-26c076fa82f8  0.0.0.0/0            0.0.0.0/0           \n    0     0 DOCKER-ISOLATION-STAGE-2  all  --  br-f461a8ffa6e0 !br-f461a8ffa6e0  0.0.0.0/0            0.0.0.0/0           \n    0     0 DOCKER-ISOLATION-STAGE-2  all  --  br-f9783559f0e8 !br-f9783559f0e8  0.0.0.0/0            0.0.0.0/0           \n    0     0 DOCKER-ISOLATION-STAGE-2  all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0           \n\nChain DOCKER-ISOLATION-STAGE-2 (5 references)\npkts bytes target     prot opt in     out     source               destination         \n    0     0 DROP       all  --  *      docker0  0.0.0.0/0            0.0.0.0/0           \n    0     0 DROP       all  --  *      br-f9783559f0e8  0.0.0.0/0            0.0.0.0/0           \n    0     0 DROP       all  --  *      br-f461a8ffa6e0  0.0.0.0/0            0.0.0.0/0           \n    0     0 DROP       all  --  *      br-26c076fa82f8  0.0.0.0/0            0.0.0.0/0           \n    0     0 DROP       all  --  *      br-1352dfaa6e69  0.0.0.0/0            0.0.0.0/0           \n\nChain DOCKER-USER (1 references)\npkts bytes target     prot opt in     out     source               destination         \n468K   31M RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0           \n\nChain FLANNEL-FWD (1 references)\npkts bytes target     prot opt in     out     source               destination         \n468K   31M ACCEPT     all  --  *      *       10.244.0.0/16        0.0.0.0/0            /* flanneld forward */\n  16   960 ACCEPT     all  --  *      *       0.0.0.0/0            10.244.0.0/16        /* flanneld forward */\n\nChain KUBE-FIREWALL (2 references)\npkts bytes target     prot opt in     out     source               destination         \n    0     0 DROP       all  --  *      *      !127.0.0.0/8          127.0.0.0/8          /* block incoming localnet connections */ ! ctstate RELATED,ESTABLISHED,DNAT\n\nChain KUBE-FORWARD (1 references)\npkts bytes target     prot opt in     out     source               destination         \n    0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate INVALID nfacct-name  ct_state_invalid_dropped_pkts\n3206  189K ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */\n448K 1979M ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding conntrack rule */ ctstate RELATED,ESTABLISHED\n</code></pre> <code>root@lexicon ~ # iptables --list-rules</code> <pre><code>root@lexicon ~ # iptables --list-rules\n-P INPUT ACCEPT\n-P FORWARD ACCEPT\n-P OUTPUT ACCEPT\n-N DOCKER\n-N DOCKER-ISOLATION-STAGE-1\n-N DOCKER-ISOLATION-STAGE-2\n-N DOCKER-USER\n-N FLANNEL-FWD\n-N KUBE-EXTERNAL-SERVICES\n-N KUBE-FIREWALL\n-N KUBE-FORWARD\n-N KUBE-KUBELET-CANARY\n-N KUBE-NODEPORTS\n-N KUBE-PROXY-CANARY\n-N KUBE-PROXY-FIREWALL\n-N KUBE-SERVICES\n-A INPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes load balancer firewall\" -j KUBE-PROXY-FIREWALL\n-A INPUT -m comment --comment \"kubernetes health check service ports\" -j KUBE-NODEPORTS\n-A INPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes externally-visible service portals\" -j KUBE-EXTERNAL-SERVICES\n-A INPUT -j KUBE-FIREWALL\n-A FORWARD -m conntrack --ctstate NEW -m comment --comment \"kubernetes load balancer firewall\" -j KUBE-PROXY-FIREWALL\n-A FORWARD -m comment --comment \"kubernetes forwarding rules\" -j KUBE-FORWARD\n-A FORWARD -m conntrack --ctstate NEW -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A FORWARD -m conntrack --ctstate NEW -m comment --comment \"kubernetes externally-visible service portals\" -j KUBE-EXTERNAL-SERVICES\n-A FORWARD -j DOCKER-USER\n-A FORWARD -m set --match-set docker-ext-bridges-v4 dst -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n-A FORWARD -j DOCKER-ISOLATION-STAGE-1\n-A FORWARD -m set --match-set docker-ext-bridges-v4 dst -j DOCKER\n-A FORWARD -i wg0 -j ACCEPT\n-A FORWARD -i br-1352dfaa6e69 -j ACCEPT\n-A FORWARD -i br-26c076fa82f8 -j ACCEPT\n-A FORWARD -i br-f461a8ffa6e0 -j ACCEPT\n-A FORWARD -i br-f9783559f0e8 -j ACCEPT\n-A FORWARD -i docker0 -j ACCEPT\n-A FORWARD -m comment --comment \"flanneld forward\" -j FLANNEL-FWD\n-A OUTPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes load balancer firewall\" -j KUBE-PROXY-FIREWALL\n-A OUTPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A OUTPUT -j KUBE-FIREWALL\n-A DOCKER -d 172.19.0.4/32 ! -i br-26c076fa82f8 -o br-26c076fa82f8 -p tcp -m tcp --dport 3000 -j ACCEPT\n-A DOCKER -d 172.19.0.3/32 ! -i br-26c076fa82f8 -o br-26c076fa82f8 -p tcp -m tcp --dport 8086 -j ACCEPT\n-A DOCKER -d 172.19.0.2/32 ! -i br-26c076fa82f8 -o br-26c076fa82f8 -p tcp -m tcp --dport 8125 -j ACCEPT\n-A DOCKER ! -i br-1352dfaa6e69 -o br-1352dfaa6e69 -j DROP\n-A DOCKER ! -i br-26c076fa82f8 -o br-26c076fa82f8 -j DROP\n-A DOCKER ! -i br-f461a8ffa6e0 -o br-f461a8ffa6e0 -j DROP\n-A DOCKER ! -i br-f9783559f0e8 -o br-f9783559f0e8 -j DROP\n-A DOCKER ! -i docker0 -o docker0 -j DROP\n-A DOCKER-ISOLATION-STAGE-1 -i br-1352dfaa6e69 ! -o br-1352dfaa6e69 -j DOCKER-ISOLATION-STAGE-2\n-A DOCKER-ISOLATION-STAGE-1 -i br-26c076fa82f8 ! -o br-26c076fa82f8 -j DOCKER-ISOLATION-STAGE-2\n-A DOCKER-ISOLATION-STAGE-1 -i br-f461a8ffa6e0 ! -o br-f461a8ffa6e0 -j DOCKER-ISOLATION-STAGE-2\n-A DOCKER-ISOLATION-STAGE-1 -i br-f9783559f0e8 ! -o br-f9783559f0e8 -j DOCKER-ISOLATION-STAGE-2\n-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2\n-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP\n-A DOCKER-ISOLATION-STAGE-2 -o br-f9783559f0e8 -j DROP\n-A DOCKER-ISOLATION-STAGE-2 -o br-f461a8ffa6e0 -j DROP\n-A DOCKER-ISOLATION-STAGE-2 -o br-26c076fa82f8 -j DROP\n-A DOCKER-ISOLATION-STAGE-2 -o br-1352dfaa6e69 -j DROP\n-A DOCKER-USER -j RETURN\n-A FLANNEL-FWD -s 10.244.0.0/16 -m comment --comment \"flanneld forward\" -j ACCEPT\n-A FLANNEL-FWD -d 10.244.0.0/16 -m comment --comment \"flanneld forward\" -j ACCEPT\n-A KUBE-FIREWALL ! -s 127.0.0.0/8 -d 127.0.0.0/8 -m comment --comment \"block incoming localnet connections\" -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP\n-A KUBE-FORWARD -m conntrack --ctstate INVALID -m nfacct --nfacct-name  ct_state_invalid_dropped_pkts -j DROP\n-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding rules\" -j ACCEPT\n-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n</code></pre> <code>pi@alfred:~ $ sudo iptables -nvL</code> <pre><code>pi@alfred:~ $ sudo iptables -nvL\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\npkts bytes target     prot opt in     out     source               destination         \n  904 68334 KUBE-PROXY-FIREWALL  0    --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes load balancer firewall */\n7786   25M KUBE-FORWARD  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */\n  904 68334 KUBE-SERVICES  0    --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes service portals */\n  904 68334 KUBE-EXTERNAL-SERVICES  0    --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes externally-visible service portals */\n  904 68334 DOCKER-USER  0    --  *      *       0.0.0.0/0            0.0.0.0/0           \n    0     0 ACCEPT     0    --  *      *       0.0.0.0/0            0.0.0.0/0            match-set docker-ext-bridges-v4 dst ctstate RELATED,ESTABLISHED\n  904 68334 DOCKER-ISOLATION-STAGE-1  0    --  *      *       0.0.0.0/0            0.0.0.0/0           \n    0     0 DOCKER     0    --  *      *       0.0.0.0/0            0.0.0.0/0            match-set docker-ext-bridges-v4 dst\n    0     0 ACCEPT     0    --  docker0 *       0.0.0.0/0            0.0.0.0/0           \n    0     0 FLANNEL-FWD  0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* flanneld forward */\n\nChain DOCKER (1 references)\npkts bytes target     prot opt in     out     source               destination         \n    0     0 DROP       0    --  !docker0 docker0  0.0.0.0/0            0.0.0.0/0           \n\nChain DOCKER-ISOLATION-STAGE-1 (1 references)\npkts bytes target     prot opt in     out     source               destination         \n    0     0 DOCKER-ISOLATION-STAGE-2  0    --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0           \n\nChain DOCKER-ISOLATION-STAGE-2 (1 references)\npkts bytes target     prot opt in     out     source               destination         \n    0     0 DROP       0    --  *      docker0  0.0.0.0/0            0.0.0.0/0           \n\nChain FLANNEL-FWD (1 references)\npkts bytes target     prot opt in     out     source               destination         \n    0     0 ACCEPT     0    --  *      *       10.244.0.0/16        0.0.0.0/0            /* flanneld forward */\n    0     0 ACCEPT     0    --  *      *       0.0.0.0/0            10.244.0.0/16        /* flanneld forward */\n\nChain KUBE-EXTERNAL-SERVICES (2 references)\npkts bytes target     prot opt in     out     source               destination         \n\nChain KUBE-FIREWALL (2 references)\npkts bytes target     prot opt in     out     source               destination         \n    0     0 DROP       0    --  *      *      !127.0.0.0/8          127.0.0.0/8          /* block incoming localnet connections */ ! ctstate RELATED,ESTABLISHED,DNAT\n\nChain KUBE-FORWARD (1 references)\npkts bytes target     prot opt in     out     source               destination         \n    0     0 DROP       0    --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate INVALID nfacct-name  ct_state_invalid_dropped_pkts\n    0     0 ACCEPT     0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */ mark match 0x4000/0x4000\n    0     0 ACCEPT     0    --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding conntrack rule */ ctstate RELATED,ESTABLISHED\n</code></pre> <code>pi@alfred:~ $ sudo iptables --list-rules</code> <pre><code>pi@alfred:~ $ sudo iptables --list-rules\n-P INPUT ACCEPT\n-P FORWARD ACCEPT\n-P OUTPUT ACCEPT\n-N DOCKER\n-N DOCKER-ISOLATION-STAGE-1\n-N DOCKER-ISOLATION-STAGE-2\n-N DOCKER-USER\n-N FLANNEL-FWD\n-N KUBE-EXTERNAL-SERVICES\n-N KUBE-FIREWALL\n-N KUBE-FORWARD\n-N KUBE-KUBELET-CANARY\n-N KUBE-NODEPORTS\n-N KUBE-PROXY-CANARY\n-N KUBE-PROXY-FIREWALL\n-N KUBE-SERVICES\n-A INPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes load balancer firewall\" -j KUBE-PROXY-FIREWALL\n-A INPUT -m comment --comment \"kubernetes health check service ports\" -j KUBE-NODEPORTS\n-A INPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes externally-visible service portals\" -j KUBE-EXTERNAL-SERVICES\n-A INPUT -j KUBE-FIREWALL\n-A FORWARD -m conntrack --ctstate NEW -m comment --comment \"kubernetes load balancer firewall\" -j KUBE-PROXY-FIREWALL\n-A FORWARD -m comment --comment \"kubernetes forwarding rules\" -j KUBE-FORWARD\n-A FORWARD -m conntrack --ctstate NEW -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A FORWARD -m conntrack --ctstate NEW -m comment --comment \"kubernetes externally-visible service portals\" -j KUBE-EXTERNAL-SERVICES\n-A FORWARD -j DOCKER-USER\n-A FORWARD -m set --match-set docker-ext-bridges-v4 dst -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n-A FORWARD -j DOCKER-ISOLATION-STAGE-1\n-A FORWARD -m set --match-set docker-ext-bridges-v4 dst -j DOCKER\n-A FORWARD -i docker0 -j ACCEPT\n-A FORWARD -m comment --comment \"flanneld forward\" -j FLANNEL-FWD\n-A OUTPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes load balancer firewall\" -j KUBE-PROXY-FIREWALL\n-A OUTPUT -m conntrack --ctstate NEW -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A OUTPUT -j KUBE-FIREWALL\n-A DOCKER ! -i docker0 -o docker0 -j DROP\n-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2\n-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP\n-A DOCKER-USER -j RETURN\n-A FLANNEL-FWD -s 10.244.0.0/16 -m comment --comment \"flanneld forward\" -j ACCEPT\n-A FLANNEL-FWD -d 10.244.0.0/16 -m comment --comment \"flanneld forward\" -j ACCEPT\n-A KUBE-FIREWALL ! -s 127.0.0.0/8 -d 127.0.0.0/8 -m comment --comment \"block incoming localnet connections\" -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP\n-A KUBE-FORWARD -m conntrack --ctstate INVALID -m nfacct --nfacct-name  ct_state_invalid_dropped_pkts -j DROP\n-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding rules\" -m mark --mark 0x4000/0x4000 -j ACCEPT\n-A KUBE-FORWARD -m comment --comment \"kubernetes forwarding conntrack rule\" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n</code></pre> <p>Removing those <code>DROP</code> rules did not help:</p> <ul> <li><code>sudo iptables -D KUBE-FIREWALL 1</code></li> <li><code>sudo iptables -D KUBE-FORWARD 1</code></li> </ul>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#https-certificates","title":"HTTPS certificates","text":"<p>The Kubernetes dashboard uses a self-signed certificate, and so does Nginx by default, which works in so far as encrypting traffic, but provides no guarantee that the traffic is coming from the actual servers and is just very annoying when browsers complain every time accessing the service. It is now time to get properly signed HTTPS certificates. This is the way.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#install-cert-manager","title":"Install <code>cert-manager</code>","text":"<p>cert-manager is the native Kubernetes certificate management controller of choice to issue certificates from Let's Encrypt to secure NGINX-ingress.</p> <p>Having already installed Helm (3.17), deployed the NGINX Ingress Controller, assigned <code>alfred.uu.am</code> to the router's external IP address, and deployed the Kubernetes dashboard service, the system is ready for Step 5 - Deploy cert-manager, installing with Helm:</p> <pre><code>$ helm repo add jetstack https://charts.jetstack.io --force-update\n\"jetstack\" has been added to your repositories\n\n$ helm install \\\n    cert-manager jetstack/cert-manager \\\n    --namespace cert-manager \\\n    --create-namespace \\\n    --version v1.17.0 \\\n    --set crds.enabled=true\nNAME: cert-manager\nLAST DEPLOYED: Sun Mar 23 17:19:57 2025\nNAMESPACE: cert-manager\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\ncert-manager v1.17.0 has been deployed successfully!\n\nIn order to begin issuing certificates, you will need to set up a ClusterIssuer\nor Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).\n\nMore information on the different types of issuers and how to configure them\ncan be found in our documentation:\n\nhttps://cert-manager.io/docs/configuration/\n\nFor information on how to configure cert-manager to automatically provision\nCertificates for Ingress resources, take a look at the `ingress-shim`\ndocumentation:\n\nhttps://cert-manager.io/docs/usage/ingress/\n</code></pre> <p>The <code>helm install</code> command takes several seconds to come back with the above output, at which point the pods and services are all up and running:</p> <pre><code>$ kubectl get all -n cert-manager\nNAME                                           READY   STATUS    RESTARTS   AGE\npod/cert-manager-665948465f-2574k              1/1     Running   0          73s\npod/cert-manager-cainjector-7c8f7984fb-6phv2   1/1     Running   0          73s\npod/cert-manager-webhook-7594bcdb99-w4tr9      1/1     Running   0          73s\n\nNAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)            AGE\nservice/cert-manager              ClusterIP   10.109.245.11    &lt;none&gt;        9402/TCP           73s\nservice/cert-manager-cainjector   ClusterIP   10.104.227.129   &lt;none&gt;        9402/TCP           73s\nservice/cert-manager-webhook      ClusterIP   10.104.66.87     &lt;none&gt;        443/TCP,9402/TCP   73s\n\nNAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/cert-manager              1/1     1            1           73s\ndeployment.apps/cert-manager-cainjector   1/1     1            1           73s\ndeployment.apps/cert-manager-webhook      1/1     1            1           73s\n\nNAME                                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/cert-manager-665948465f              1         1         1       73s\nreplicaset.apps/cert-manager-cainjector-7c8f7984fb   1         1         1       73s\nreplicaset.apps/cert-manager-webhook-7594bcdb99      1         1         1       73s\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#test-cert-manager","title":"Test <code>cert-manager</code>","text":"<p>Verify the installation by creating a simple self-signed certificate:</p> Test certificate: <code>test-resources.yaml</code> test-resources.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: cert-manager-test\n---\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: test-selfsigned\n  namespace: cert-manager-test\nspec:\n  selfSigned: {}\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: selfsigned-cert\n  namespace: cert-manager-test\nspec:\n  dnsNames:\n    - example.com\n  secretName: selfsigned-cert-tls\n  issuerRef:\n    name: test-selfsigned\n</code></pre> <p>Deploying this succeeds withing seconds, after which it can be cleaned up:</p> <code>kubectl apply -f test-resources.yaml</code> <pre><code>$ kubectl apply -f test-resources.yaml\nnamespace/cert-manager-test created\nissuer.cert-manager.io/test-selfsigned created\ncertificate.cert-manager.io/selfsigned-cert created\n\n$ kubectl describe certificate -n cert-manager-test\nName:         selfsigned-cert\nNamespace:    cert-manager-test\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         Certificate\nMetadata:\n  Creation Timestamp:  2025-03-23T16:32:58Z\n  Generation:          1\n  Resource Version:    2931074\n  UID:                 f0646ae2-8344-45a0-a453-f46ba16400b6\nSpec:\n  Dns Names:\n    example.com\n  Issuer Ref:\n    Name:       test-selfsigned\n  Secret Name:  selfsigned-cert-tls\nStatus:\n  Conditions:\n    Last Transition Time:  2025-03-23T16:32:59Z\n    Message:               Certificate is up to date and has not expired\n    Observed Generation:   1\n    Reason:                Ready\n    Status:                True\n    Type:                  Ready\n  Not After:               2025-06-21T16:32:59Z\n  Not Before:              2025-03-23T16:32:59Z\n  Renewal Time:            2025-05-22T16:32:59Z\n  Revision:                1\nEvents:\n  Type    Reason     Age   From                                       Message\n  ----    ------     ----  ----                                       -------\n  Normal  Issuing    6s    cert-manager-certificates-trigger          Issuing certificate as Secret does not exist\n  Normal  Generated  5s    cert-manager-certificates-key-manager      Stored new private key in temporary Secret resource \"selfsigned-cert-g9cv2\"\n  Normal  Requested  5s    cert-manager-certificates-request-manager  Created new CertificateRequest resource \"selfsigned-cert-1\"\n  Normal  Issuing    5s    cert-manager-certificates-issuing          The certificate has been successfully issued\n\n$ kubectl delete -f test-resources.yaml\nnamespace \"cert-manager-test\" deleted\nissuer.cert-manager.io \"test-selfsigned\" deleted\ncertificate.cert-manager.io \"selfsigned-cert\" deleted\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#configure-lets-encrypt","title":"Configure Let's Encrypt","text":"<p>Step 6 - Configure a Let's Encrypt Issuer shows how to create an <code>Issuer</code>, but in this system with multiple services running in different namespaces, a <code>ClusterIssuer</code> is needed instead:</p> cert-manager-issuer.yaml<pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: stibbons@uu.am\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n      - http01:\n          ingress:\n            class: nginx\n</code></pre> <pre><code>$ kubectl create -f cert-manager-issuer.yaml\nclusterissuer.cert-manager.io/letsencrypt-prod created\n\n$ kubectl describe clusterissuer.cert-manager.io/letsencrypt-prod\nName:         letsencrypt-prod\nNamespace:    \nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         ClusterIssuer\nMetadata:\n  Creation Timestamp:  2025-03-23T19:11:39Z\n  Generation:          1\n  Resource Version:    2945998\n  UID:                 f75d8154-b858-4c3b-ae70-c1cb632f7d7c\nSpec:\n  Acme:\n    Email:  stibbons@uu.am\n    Private Key Secret Ref:\n      Name:  letsencrypt-prod\n    Server:  https://acme-v02.api.letsencrypt.org/directory\n    Solvers:\n      http01:\n        Ingress:\n          Class:  nginx\nStatus:\n  Acme:\n    Last Private Key Hash:  M7S/jp7wvlxuzt4QTkelCzWNe5SqfhFZpqfAcNSOZL0=\n    Last Registered Email:  stibbons@uu.am\n    Uri:                    https://acme-v02.api.letsencrypt.org/acme/acct/2298686296\n  Conditions:\n    Last Transition Time:  2025-03-23T19:11:40Z\n    Message:               The ACME account was registered with the ACME server\n    Observed Generation:   1\n    Reason:                ACMEAccountRegistered\n    Status:                True\n    Type:                  Ready\nEvents:                    &lt;none&gt;\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#secure-kubernetes-dashboard-ingress","title":"Secure Kubernetes Dashboard Ingress","text":"<p>Step 7 - Deploy a TLS Ingress Resource is the last step left to actually make pratical use of all the above. Based on the provided example, apply the equivalent changes to <code>dashboard/nginx-ingress.yaml</code>, using <code>cert-manager.io/cluster-issuer</code> instead of <code>cert-manager.io/issuer</code> and adding the <code>tls</code> section with just the relevant FQDN under <code>hosts</code>:</p> dashboard/nginx-ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard-ingress\n  namespace: kubernetes-dashboard\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"false\"\n    nginx.ingress.kubernetes.io/whitelist-source-range: 10.244.0.0/16\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      more_set_headers \"server: hide\";\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: k8s.alfred.uu.am\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard-kong-proxy\n                port:\n                  number: 443\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - k8s.alfred.uu.am\n</code></pre> <p>Re-apply this deployment to trigger the request for a certificate signed by Let's Encrypt:</p> <pre><code>$ kubectl apply -f dashboard/nginx-ingress.yaml\ningress.networking.k8s.io/kubernetes-dashboard-ingress configured\n</code></pre> <p>Now there will be a pending order and challenge for a new certificate, so it is time to forward the router's external port 80 to the <code>NodePort</code> of the ACME solver:</p> <pre><code>$ kubectl get svc -A | grep acme\nkubernetes-dashboard   cm-acme-http-solver-b9ckt              NodePort    10.96.223.114    &lt;none&gt;        8089:31453/TCP               13s\n</code></pre> <p>The pod behind the service is listening and the logs can be monitored in real time:</p> <pre><code>$ kubectl -n kubernetes-dashboard logs \\\n  $(kubectl get pods -n kubernetes-dashboard | grep acme | cut -f1 -d' ') -f\n\nI0323 20:44:32.293925       1 solver.go:52] \"starting listener\" logger=\"cert-manager.acmesolver\" expected_domain=\"k8s.alfred.uu.am\" expected_token=\"Ib9C-k31rDjp2A5SH7jmELo4F0VNBvOfWzXg6Mu6YWc\" expected_key=\"Ib9C-k31rDjp2A5SH7jmELo4F0VNBvOfWzXg6Mu6YWc.AiIi2mdeMBlCng6As7epqFyP8bmjSGQqxIdEbnstHEo\" listen_port=8089\n</code></pre> <p>Now, instead of forwarding directly to the <code>NodePort</code> of this ACME resolver, it is more convinient to update the router's forwarding rules only once, to forward port the external port 80 to the node's port 32080, and then <code>patch</code> the ACME resolve service to change the service's NodePort.</p> <p>The <code>patch</code> command is directed at the specific ACME resolver service in each namespace:</p> <pre><code>$ kubectl -n kubernetes-dashboard patch \\\n    service cm-acme-http-solver-b9ckt \\\n     -p '{\"spec\":{\"ports\": [{\"port\": 8089, \"nodePort\": 32080}]}}'\nservice/cm-acme-http-solver-b9ckt patched\n\n$ kubectl get svc -A | grep acme\nkubernetes-dashboard   cm-acme-http-solver-b9ckt              NodePort    10.96.223.114    &lt;none&gt;        8089:32080/TCP               5m24s\n</code></pre> <p>Once the service is patched, the external connection reaches the resolver pod and the renewal is completed very shortly, which then deletes the pod and service. At that point, the command above to monitor the pod's logs will terminate when the pods finishes:</p> <pre><code>I0323 20:49:58.055831       1 solver.go:104] \"comparing token\" logger=\"cert-manager.acmesolver\" host=\"k8s.alfred.uu.am\" path=\"/.well-known/acme-challenge/Ib9C-k31rDjp2A5SH7jmELo4F0VNBvOfWzXg6Mu6YWc\" base_path=\"/.well-known/acme-challenge\" token=\"Ib9C-k31rDjp2A5SH7jmELo4F0VNBvOfWzXg6Mu6YWc\" headers={\"Accept\":[\"*/*\"],\"Accept-Encoding\":[\"gzip\"],\"Connection\":[\"close\"],\"User-Agent\":[\"Mozilla/5.0 (compatible; Let's Encrypt validation server; +https://www.letsencrypt.org)\"]} expected_token=\"Ib9C-k31rDjp2A5SH7jmELo4F0VNBvOfWzXg6Mu6YWc\"\nI0323 20:49:58.055843       1 solver.go:112] \"got successful challenge request, writing key\" logger=\"cert-manager.acmesolver\" host=\"k8s.alfred.uu.am\" path=\"/.well-known/acme-challenge/Ib9C-k31rDjp2A5SH7jmELo4F0VNBvOfWzXg6Mu6YWc\" base_path=\"/.well-known/acme-challenge\" token=\"Ib9C-k31rDjp2A5SH7jmELo4F0VNBvOfWzXg6Mu6YWc\" headers={\"Accept\":[\"*/*\"],\"Accept-Encoding\":[\"gzip\"],\"Connection\":[\"close\"],\"User-Agent\":[\"Mozilla/5.0 (compatible; Let's Encrypt validation server; +https://www.letsencrypt.org)\"]}\nE0323 20:49:59.002384       1 main.go:42] \"error executing command\" err=\"http: Server closed\" logger=\"cert-manager\"\n</code></pre> <p>From now on, accessing the Kubernetes dashboard at https://k8s.alfred.uu.am/ will no longer result in browsers complaining about the connection not being secure.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#automatic-renovation","title":"Automatic renovation","text":"<p>The above technique to <code>patch</code> each ACME resolver service can be extended to automatically patch each service when it is found running:</p> cert-renewal-port-fwd.sh<pre><code>#!/bin/bash\n#\n# Patch the nodePort of running cert-manager renewal challenge, to listen\n# on port 32080 which is the one the router is forwarding port 80 to.\n\n# Check if there is a LetsEncrypt challenge resolver (acme) running.\nexport KUBECONFIG=/etc/kubernetes/admin.conf\nnamespace=$(kubectl get svc -A | grep acme | awk '{print $1}' | head -1)\nservice=$(kubectl get svc -A | grep acme | awk '{print $2}' | head -1)\n\n# Patch the service to listen on port 32080 (set up in router).\nif [ -n \"${namespace}\" ] &amp;&amp; [ -n \"${service}\" ]; then\n    kubectl -n \"${namespace}\" patch service \"${service}\" -p '{\"spec\":{\"ports\": [{\"port\": 8089, \"nodePort\":32080}]}}'\nfi\n</code></pre> <p>Install this script in a convenient location and setup <code>crontab</code> to run it hourly:</p> <pre><code># Hourly patch montly cert renewal solvers.\n30 * * * * /home/pi/cert-renewal-port-fwd.sh\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#cloudflare-tunnel","title":"Cloudflare Tunnel","text":"<p>Although during the installation process this system was behind a router that supported port forwarding, so that ports 80 and 443 could be made available externally, the final destination may not provide this facility. To cope with an enviromeent without port forwarding capabilities, and/or a static IPv4 address, or possibly even CGNAT in the future, a Cloudflare tunnel will be used.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#install-cloudflared","title":"Install <code>cloudflared</code>","text":"<p>Install the latest <code>cloudflared</code> using the instructions provided for Any Debian Based Distribution:</p> <pre><code># curl -fsSL https://pkg.cloudflare.com/cloudflare-main.gpg \\\n  | tee /usr/share/keyrings/cloudflare-main.gpg &gt;/dev/null\n\n# echo 'deb [signed-by=/usr/share/keyrings/cloudflare-main.gpg] https://pkg.cloudflare.com/cloudflared any main' \\\n  | tee /etc/apt/sources.list.d/cloudflared.list\n\n# install cloudflared\nsudo apt-get update &amp;&amp; sudo apt-get install cloudflared\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#create-a-tunnel","title":"Create a tunnel","text":"Cloudflare requires adding a site before creating a tunnel. <p>Before creating a tunnel, Cloudflare requires  adding a site, which in turn requires updating DNS settins in a domain, as well as on Cloudflare, all of which can take hours to take effect.</p> <code>very-very-dark-gray.top</code> is an example cheap domain. <p><code>very-very-dark-gray.top</code> is an example cheap domain, probably not great for public or customer-facing applications, but should be fine for purely self-hosted personal applications.</p> <p>Create a tunnel named <code>alfred</code> using <code>cloudflared</code> as the connector. Cloudflare will provide the commands to run, including installating the connector (already done) and connecting the tunnel:</p> <pre><code>$ sudo cloudflared service install eyJhIjoiMD...\n2025-03-29T10:07:08Z INF Using Systemd\n2025-03-29T10:07:10Z INF Linux service for cloudflared installed successfully\n</code></pre> <p>The token in the command is much longer that Cloudflare shows.</p> <p>A few seconds after running the commmand, the Cloudflare console will show the tunnel as Connected; otherwise something is blocking it.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#public-hostnames","title":"Public hostnames","text":"<p>Once the tunnel is working, access to the Kubernetes dashboard ingress can be enabled by adding a public hostname to make <code>localhost:32035</code> (the <code>NodePort</code> of Nginx, using HTTPS), or the <code>LoadBalancer</code> IP, available behind https://kubernetes-alfred.very-very-dark-gray.top/.</p> <p>Make sure to enable No TLS Verify under Additional application settings &gt; TLS because Nginx will not be using the certificate from Let's Encrypt.</p> <p>Before Cloudflare can reach the Kubernetes dashboard through that URL, an additional <code>host</code> rule must be added to the Kubernetes Dashboard Ingress, so that Nginx knows to route requests with that <code>Host</code> value to the right service. Otherwise, Nginx will not find anything matching that URL:</p> <pre><code>$ curl 2&gt;/dev/null \\\n  -H \"Host: kubernetes-alfred.very-very-dark-gray.top\" \\\n  -k https://192.168.0.151/ \\\n| head\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n$ curl 2&gt;/dev/null \\\n  -H \"Host: kubernetes-alfred.very-very-dark-gray.top\" \\\n  -k https://192.168.0.124:32035/ \\\n| head\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>A rule for <code>host: kubernetes-alfred.very-very-dark-gray.top</code> must be added:</p> dashboard/nginx-ingress.yaml<pre><code>spec:\n  ingressClassName: nginx\n  rules:\n    - host: kubernetes-alfred.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard-kong-proxy\n                port:\n                  number: 443\n    - host: k8s.alfred.uu.am\n</code></pre> <p>After applying this change, the <code>kubernetes-dashboard-kong-proxy</code> service is available behind <code>kubernetes-alfred.very-very-dark-gray.top</code>:</p> <pre><code>$ kubectl apply -f dashboard/nginx-ingress.yaml\ningress.networking.k8s.io/kubernetes-dashboard-ingress created\n\n$ curl 2&gt;/dev/null \\\n  -H \"Host: kubernetes-alfred.very-very-dark-gray.top\" \\\n  -k https://192.168.0.151/ \\\n| head\n&lt;!--\nCopyright 2017 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>https://kubernetes-alfred.very-very-dark-gray.top/ will now lead to the Kubernetes dashboard, through its own HTTPS tunnel, provided all the configurations have been applied correctly.</p> <pre><code>$ curl 2&gt;/dev/null \\\n  -k https://kubernetes-alfred.very-very-dark-gray.top/ \\\n| head\n&lt;!--\nCopyright 2017 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#troubleshooting-public-hostnames","title":"Troubleshooting public hostnames","text":"Do not try using multi-level domains. <p>If the same hostname is added as <code>kubernetes.alfred.very-very-dark-gray.top</code> that's a 2nd-level domain and connections will fail, e.g. in Firefox with <code>SSL_ERROR_NO_CYPHER_OVERLAP</code> (and <code>curl</code> is also unable to connect)</p> <p>The reason for this to fail is that the Cloudflare  Universal SSL certificates only cover apex domain and one level of subdomain; see Multi-level subdomains.</p> Don't forget to enable No TLS Verify. <p>If No TLS Verify is not enabled, the 502 Bad Gateway page will continue to show, because Cloudflare won't accept connecting to Nginx with the default self-signed certificate. This can be checked by viewing <code>cloudflared</code> logs in the CLI to get detailed error messages:</p> <pre><code>$ cloudflared tail 3923ef0b-986b-43cd-9066-d48035a77f89\n2025-03-29T17:50:56Z ERR Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable originCertPath=\n2025-03-29T17:50:56Z ERR unable to construct management request URL error=\"unable to acquire management token for requested tunnel id: Error locating origin cert: client didn't specify origincert path\"\n</code></pre> <p>Here is the error message in a more readable format:</p> <p>Cannot determine default origin certificate path. No file cert.pem in <code>[~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp</code> <code>/etc/cloudflared /usr/local/etc/cloudflared]</code>. You need to specify the origin certificate path by specifying the <code>origincert</code> option in the configuration file, or set <code>TUNNEL_ORIGIN_CERT</code> environment variable <code>originCertPath=</code> 2025-03-29T17:50:56Z ERR unable to construct management request URL <code>error=\"unable to acquire management token for requested tunnel id:</code> <code>Error locating origin cert: client didn't specify origincert path\"</code></p> HTTP Host Header must match the hostname exactly. <p>Overriding HTTP Settings &gt; HTTP Host Header in the tunnel should not be necessary; by default it will send the correct <code>Host</code> header. Otherwise, if the override header is not exactly the same as the tunnel's hostname, e.g. <code>kubernetes.alfred.very-very-dark-gray.top</code> instead of <code>kubernetes-alfred.very-very-dark-gray.top</code>, requests will not match any rules in Nginx and recieve a 404 Not Found response:</p> <pre><code>$ curl 2&gt;/dev/null \\\n  -k https://kubernetes-alfred.very-very-dark-gray.top/ \\\n| head\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>To confirm the problem is in Nginx and not in the <code>cloudflared</code> connector, temporarily run the <code>port-forward</code> to make the Kubernetes Dashboard accessible directly without going through Nginx:</p> <pre><code>$ kubectl -n kubernetes-dashboard port-forward \\\n  svc/kubernetes-dashboard-kong-proxy 8443:443 \\\n  --address 0.0.0.0\nForwarding from 0.0.0.0:8443 -&gt; 8443\n^C\n</code></pre> <p>Edit the <code>kubernetes-alfred.very-very-dark-gray.top</code> tunnel to point to <code>https://localhost:8443</code> and the dashboard becomes immediately available.</p> <p>To debug the problem in Nginx, increase logging level to the maximum (<code>debug</code>):</p> nginx-baremetal.yaml<pre><code>---\napiVersion: v1\ndata:\n  allow-snippet-annotations: \"true\"\n  annotations-risk-level: \"Critical\"\n  error-log-level: debug\nkind: ConfigMap\n</code></pre> <p>After applying this change, the logs of the <code>ingress-nginx-controller</code> will included lots of details and will stream fast; be ready to grep for the relevant hostname to find the log entries for the request:</p> <pre><code>kubectl logs -n ingress-nginx \\\n  $(kubectl get pods -n ingress-nginx | grep ingress-nginx-controller | cut -f1 -d' ') -f\n...\n\"GET / HTTP/1.1\nHost: kubernetes.alfred.very-very-dark-gray.top\nX-Request-ID: 3a67e281e598abf5d21dda0679f0be59\nX-Real-IP: 10.244.0.1\nX-Forwarded-For: 10.244.0.1\nX-Forwarded-Host: kubernetes.alfred.very-very-dark-gray.top\nX-Forwarded-Port: 443\nX-Forwarded-Proto: https\nX-Forwarded-Scheme: https\nX-Scheme: https\nX-Original-Forwarded-For: 217.162.57.64\nUser-Agent: curl/8.5.0\nAccept: */*\nAccept-Encoding: gzip, br\nCdn-Loop: cloudflare; loops=1\nCf-Connecting-Ip: 217.162.57.64\nCf-Ipcountry: CH\nCf-Ray: 92815e1e1c1a039c-ZRH\nCf-Visitor: {\"scheme\":\"https\"}\nCf-Warp-Tag-Id: 3923ef0b-986b-43cd-9066-d48035a77f89\n...\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 event timer del: 22: 689556672\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 generic phase: 0\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 rewrite phase: 1\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 http script value: \"internal\"\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 http script set $proxy_upstream_name\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 test location: \"/\"\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 using configuration \"/\"\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 http cl:-1 max:1048576\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 rewrite phase: 3\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 http finalize request: 404, \"/?\" a:1, c:1\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 http special response: 404, \"/?\"\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 http set discard body\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 lua header filter for user lua code, uri \"/\"\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 lua capture header filter, uri \"/\"\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 headers more header filter, uri \"/\"\n2025/03/29 18:18:03 [debug] 1392#1392: *8405048 HTTP/1.1 404 Not Found\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#lets-encrypt-via-tunnel","title":"Let's Encrypt via tunnel","text":"<p>To use Let's Encrypt certificates behind a Cloudflare tunnel, an additional Public hostname must be added to the tunnel, to route requests for paths under <code>/.well-known/</code> to port 32080 over plain HTTP. Combined with the patching of ACME solvers from Automatic renovation, this makes solvers for <code>HTTP01</code> challenges reachable, so certificates can be issued and renewed.</p> <p>The Public hostname to route requests under <code>/.well-known/</code> must be the first one (these can be dragged to rearrange their order):</p> <p></p> <p>To avoid hitting Let's Encrypt rate limits, use a different secret name for each host:</p> dashboard/nginx-ingress.yaml<pre><code>  tls:\n    - secretName: tls-secret\n      hosts:\n        - k8s.alfred.uu.am\n    - secretName: tls-secret-cloudflare\n      hosts:\n        - kubernetes-alfred.very-very-dark-gray.top\n</code></pre> <p>After applying this update to the ingress, the sequence of events to Secure Kubernetes Dashboard Ingress is repeated, with a few extra steps involving the Cloudflare tunnel:</p> <ol> <li>A new solver starts, listening on a <code>NodePort</code>; this can be found in the list of     services:     <pre><code>$ kubectl get svc -A | grep acme\nkubernetes-dashboard   cm-acme-http-solver-chp9k              NodePort    10.103.151.109   &lt;none&gt;        8089:30956/TCP               3s\n</code></pre></li> <li>The logs indicate the solver is listening:     <pre><code>$ kubectl -n kubernetes-dashboard logs \\\n  $(kubectl get pods -n kubernetes-dashboard | grep acme | cut -f1 -d' ') -f\nI0330 12:04:55.423524       1 solver.go:52] \"starting listener\" logger=\"cert-manager.acmesolver\" expected_domain=\"kubernetes-alfred.very-very-dark-gray.top\" expected_token=\"aqO1GODyRAWRg4nnh6keja5mdYVSJg0zcA01eL0CHtI\" expected_key=\"aqO1GODyRAWRg4nnh6keja5mdYVSJg0zcA01eL0CHtI.AiIi2mdeMBlCng6As7epqFyP8bmjSGQqxIdEbnstHEo\" listen_port=8089\n</code></pre></li> <li>Streaming logs for the Cloudflare tunnel connector shows Request failed about     once a minute, with the URL for the challenge     (<code>/.well-known/acme-challenge/aqO1GODyRAWRg4nnh6keja5mdYVSJg0zcA01eL0CHtI</code>).     Error details show it cannot connect to port 32080:     <code>\"dial tcp [::1]:32080: connect: connection refused\"</code>.</li> <li>After <code>cert-renewal-port-fwd.sh</code> runs and patches the solver, it's listening on port     32080:     <pre><code>$ kubectl get svc -A | grep acme\nkubernetes-dashboard   cm-acme-http-solver-chp9k              NodePort    10.103.151.109   &lt;none&gt;        8089:32080/TCP               2m35s\n</code></pre></li> <li>Streaming logs for the tunnel connector now shows OK for subsequent requests.</li> <li>The solver receives the requests and finishes successfully.</li> </ol> <p>At this point it is no longer necessary to have No TLS Verify enable under Additional application settings &gt; TLS because Nginx is now using a certificate signed by Let's Encrypt. However, if No TLS Verify is to be disabled, then it is necessary to set Origin Server Name (<code>kubernetes-alfred.very-very-dark-gray.top</code>) to the FQDN so that Cloudflare accepts the certificate.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#tailscale","title":"Tailscale","text":"<p>Set up Tailscale, which involved mostly setting this up on the Tailscale admin console, and several changes in <code>alfred</code>:</p> <ol> <li>Install <code>tailscale</code> and connect to a new tailnet when creating the first one.</li> <li>Install the Tailscale Kubernetes operator     and add a Tailscale <code>Ingress</code> to the Kubernetes Dashboard Ingress.</li> <li>As a test, enabled Public access through Funnel     to check how services can be made accessible from outside the tailnet.<ul> <li>Not really necessary for the Kubernetes Dashboard, because SSH access is     necessary to obtain a token and Tailnet Funnel does not offer anything like     Cloudflare Access     that could possibly take over authentication.</li> </ul> </li> </ol>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#influxdb-and-grafana","title":"InfluxDB and Grafana","text":"<p>With all the above in place, this system is now ready to run InfluxDB and Grafana to collect metrics from Continuous Monitoring, Home Assistant and any other systems in its final location.</p> <p>Taking the combined deployment in <code>octavo</code> for InfluxDB and Grafana as the starting point, the deployment for <code>alfred</code> changes only a few details:</p> <ul> <li>Remove the <code>influxdb-ingress</code> (will not be receiving metrics remotely).</li> <li>Set a different FQDN for the <code>grafana-ingress</code> to access Grafana remotely.</li> <li>Add <code>grafana-ingress-tailscale</code> for access through Tailscale.</li> </ul> Combined deployment for InfluxDB and Grafana alfred/monitoring.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: monitoring\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: influxdb-pv\n  labels:\n    type: local\n  namespace: monitoring\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 30Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/k8s/influxdb\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: influxdb-pv-claim\n  namespace: monitoring\nspec:\n  storageClassName: manual\n  volumeName: influxdb-pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 30Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  labels:\n    app: influxdb\n  name: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      hostname: influxdb\n      containers:\n      - image: docker.io/influxdb:1.11.8\n        env:\n        - name: \"INFLUXDB_HTTP_AUTH_ENABLED\"\n          value: \"true\"\n        name: influxdb\n        volumeMounts:\n        - mountPath: /var/lib/influxdb\n          name: influxdb-data\n      securityContext:\n        runAsUser: 114\n        runAsGroup: 114\n      volumes:\n      - name: influxdb-data\n        persistentVolumeClaim:\n          claimName: influxdb-pv-claim\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: influxdb\n  name: influxdb-svc\n  namespace: monitoring\nspec:\n  ports:\n  - port: 18086\n    protocol: TCP\n    targetPort: 8086\n    nodePort: 30086\n  selector:\n    app: influxdb\n  type: NodePort\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: grafana-pv\n  labels:\n    type: local\n  namespace: monitoring\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 3Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/k8s/grafana\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: grafana-pv-claim\n  namespace: monitoring\nspec:\n  storageClassName: manual\n  volumeName: grafana-pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  labels:\n    app: grafana\n  name: grafana\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - image: docker.io/grafana/grafana:11.6.1\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: \"GF_AUTH_ANONYMOUS_ENABLED\"\n          value: \"true\"\n        - name: \"GF_SECURITY_ADMIN_USER\"\n          value: \"admin\"\n        - name: \"GF_SECURITY_ADMIN_PASSWORD\"\n          value: \"__________________________\"\n        name: grafana\n        volumeMounts:\n          - name: grafana-data\n            mountPath: /var/lib/grafana\n      securityContext:\n        runAsUser: 115\n        runAsGroup: 115\n        fsGroup: 115\n      volumes:\n      - name: grafana-data\n        persistentVolumeClaim:\n          claimName: grafana-pv-claim\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: grafana\n  name: grafana-svc\n  namespace: monitoring\nspec:\n  ports:\n  - port: 13000\n    protocol: TCP\n    targetPort: 3000\n    nodePort: 30300\n  selector:\n    app: grafana\n  type: NodePort\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana-ingress\n  namespace: monitoring\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: grafana-alfred.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: grafana-svc\n                port:\n                  number: 3000\n  tls:\n    - secretName: tls-secret-grafana\n      hosts:\n        - grafana-alfred.very-very-dark-gray.top\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana-ingress-tailscale\n  namespace: monitoring\nspec:\n  ingressClassName: tailscale\n  defaultBackend:\n    service:\n      name: grafana-svc\n      port:\n        number: 3000\n  tls:\n    - hosts:\n        - grafana-alfred\n</code></pre> <p>Before deploying the above, dedicated users and home directories must be created for <code>influxdb</code> and <code>grafana</code>:</p> <pre><code>$ sudo groupadd influxdb -g 114\n$ sudo groupadd grafana  -g 115\n$ sudo useradd  influxdb -u 114 -g 114 -s /usr/sbin/nologin\n$ sudo useradd  grafana  -u 115 -g 115 -s /usr/sbin/nologin\n$ sudo mkdir /home/k8s/influxdb /home/k8s/grafana\n$ sudo chown -R influxdb:influxdb /home/k8s/influxdb\n$ sudo chown -R  grafana:grafana  /home/k8s/grafana\n$ sudo ls -hal /home/k8s/influxdb /home/k8s/grafana\n/home/k8s/grafana:\ntotal 8.0K\ndrwxr-xr-x 2 grafana grafana 4.0K May  2 23:54 .\ndrwxr-xr-x 5 root    root    4.0K May  2 23:54 ..\n\n/home/k8s/influxdb:\ntotal 8.0K\ndrwxr-xr-x 2 influxdb influxdb 4.0K May  2 23:54 .\ndrwxr-xr-x 5 root     root     4.0K May  2 23:54 ..\n</code></pre> <p>The apply the new deployment to start InfluxDB and Grafana:</p> <pre><code>$ kubectl apply -f alfred/monitoring.yaml\nnamespace/monitoring created\npersistentvolume/influxdb-pv created\npersistentvolumeclaim/influxdb-pv-claim created\ndeployment.apps/influxdb created\nservice/influxdb-svc created\npersistentvolume/grafana-pv created\npersistentvolumeclaim/grafana-pv-claim created\ndeployment.apps/grafana created\nservice/grafana-svc created\ningress.networking.k8s.io/grafana-ingress created\ningress.networking.k8s.io/grafana-ingress-tailscale created\n\n$ kubectl get all -n monitoring\nNAME                            READY   STATUS    RESTARTS   AGE\npod/cm-acme-http-solver-4kh85   1/1     Running   0          18s\npod/grafana-895c94879-j5dnr     1/1     Running   0          22s\npod/influxdb-5974bf664f-zwwrx   1/1     Running   0          23s\n\nNAME                                TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)           AGE\nservice/cm-acme-http-solver-p4cxg   NodePort   10.109.30.68   &lt;none&gt;        8089:32080/TCP    18s\nservice/grafana-svc                 NodePort   10.111.30.81   &lt;none&gt;        13000:30300/TCP   22s\nservice/influxdb-svc                NodePort   10.104.23.44   &lt;none&gt;        18086:30086/TCP   23s\n\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana    1/1     1            1           22s\ndeployment.apps/influxdb   1/1     1            1           23s\n\nNAME                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-895c94879     1         1         1       22s\nreplicaset.apps/influxdb-5974bf664f   1         1         1       23s\n\n$ kubectl get ingress -n monitoring\nNAME                        CLASS       HOSTS                                     ADDRESS                              PORTS     AGE\ngrafana-ingress             nginx       grafana-alfred.very-very-dark-gray.top    192.168.0.124                        80, 443   41h\ngrafana-ingress-tailscale   tailscale   *                                         grafana-alfred.royal-penny.ts.net    80, 443   3m33s\n</code></pre> <p>After a couple of minutes both services are running fine and Grafana is available at https://grafana-alfred.royal-penny.ts.net and https://grafana-alfred.very-very-dark-gray.top but it will be empty because before anything and report to InfluxDB, it must be initialized.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#secure-influxdb","title":"Secure InfluxDB","text":"<p>Secure InfluxDB by first creating an <code>admin</code> user with a password:</p> <pre><code>$ influx -host localhost -port 30086\nConnected to http://localhost:30086 version v1.11.8\nInfluxDB shell version: 1.6.7~rc0\n&gt; CREATE USER admin WITH PASSWORD '**********' WITH ALL PRIVILEGES\n</code></pre> <p>After that, re-login with the password and create databases <code>monitoring</code> (for Continuous Monitoring) and <code>home_assistant</code> (for Home Assistant). </p> <pre><code>$ influx -host localhost -port 30086 -username admin -password '**********'\nConnected to http://localhost:30086 version v1.11.8\nInfluxDB shell version: 1.6.7~rc0\n\n&gt; CREATE DATABASE monitoring\n&gt; USE monitoring\nUsing database monitoring\n\n&gt; CREATE RETENTION POLICY \"90_days\" ON \"monitoring\" DURATION 30d REPLICATION 1\n&gt; ALTER RETENTION POLICY \"90_days\" on \"monitoring\" DURATION 30d REPLICATION 1 DEFAULT\n\n&gt; CREATE DATABASE home_assistant\n&gt; USE home_assistant\nUsing database home_assistant\n\n&gt; CREATE RETENTION POLICY \"800_days\" ON \"home_assistant\" DURATION 30d REPLICATION 1\n&gt; ALTER RETENTION POLICY \"800_days\" on \"home_assistant\" DURATION 30d REPLICATION 1 DEFAULT\n\n&gt; CREATE USER conmon WITH password ''\n&gt; GRANT ALL PRIVILEGES ON \"monitoring\" TO \"conmon\"\n</code></pre> <p>InfluxDB Authentication is enabled by the <code>INFLUXDB_HTTP_AUTH_ENABLED</code> setting in the above deployment. The user <code>conmon</code> with an empty password, granted read and write access to the <code>monitoring</code> database, is created to allow the <code>conmon</code> script to report to both this system (without authentication) and remotely to <code>octavo</code> (with authentication). The <code>conmon</code> user can then be used to both report metrics from the <code>conmon</code> script and read metrics from Grafana.</p> /usr/local/bin/conmon<pre><code>#!/bin/bash\n#\n# Export system monitoring metrics to influxdb.\n\n# InfluxDB target.\n# HTTP works only with a 'conmon' user with empty password.\nDBNAME=monitoring\nTARGET_HTTP='http://localhost:30086'\nTARGET_HTTPS='https://influxdb.very-very-dark-gray.top:443'\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#grafana-dashboards","title":"Grafana Dashboards","text":"<p>With the databases in place and <code>conmon</code> reporting metrics to the local InfluxDB, create Data sources for the above databases using the relevant credentails (or just use <code>admin</code> for all of them). Once Data Sources are successfully setup, the relevant dashboards can now be exported from Grafana in <code>octavo</code> using the Export as JSON functionality from each existing dashboard, with the option to Export the dashboard to use in another instance enabled.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#home-assistant","title":"Home Assistant","text":"<p>To install Home Assistant in Kubernetes, abalage/home-assistant-k8s offers a very good starting point, including the use of <code>kustomize</code> which will be useful to have the same base deployment in two systems with minimal differences.</p> <p>Additional tweaks  are needed to make discovery work, based off the docker-compose:</p> <pre><code>services:\n  homeassistant:\n    container_name: homeassistant\n    image: \"ghcr.io/home-assistant/home-assistant:stable\"\n    volumes:\n      - /PATH_TO_YOUR_CONFIG:/config\n      - /etc/localtime:/etc/localtime:ro\n      - /run/dbus:/run/dbus:ro\n    restart: unless-stopped\n    privileged: true\n    network_mode: host\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#base-deployment","title":"Base deployment","text":"<p>In addition to <code>privileged: true</code> and <code>network_mode: host</code> it is also necessary to add certain <code>capabilities</code> to make network discover work. It is not yet clear whether this will be actually necessary for the currently available devices, mostly just TP-Link Tapo devices. Further adjustments include adding a hard-coded <code>namespace</code> and adding two different <code>Ingress</code>; one for use with Tailscale and the other one for use with Cloudflare Tunnel.</p> <ul> <li><code>configmap.yaml</code> includes a minimal <code>configuration.yaml</code> needed to allow      requests through the reverse proxy.</li> <li><code>persistent-volume.yaml</code> provides a simple physical volume (and claim)     to store files in a local directory.</li> <li><code>deployment.yaml</code> defines how the Home Assistant pod runs and updates.</li> <li><code>service.yaml</code> defines how the pod's port 8123 is exposed, to be used by...</li> <li><code>ingress.yaml</code> which maps specific hostnames (FQDN) to the pod's port 8123.</li> <li><code>kustomization.yaml</code> presents all the above to be kustomized for each server.</li> </ul>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#caveat-on-reverse-proxies","title":"Caveat on reverse proxies","text":"<p>The initial configuration created by Home Assistant includes mostly empty files, which does not allow request coming through reverse proxies. Services are running, but Cloudflare and Tailscale are getting nothing but <code>400: Bad Request</code>:</p> <pre><code>$ curl 2&gt;/dev/null \\\n  -H \"Host: home-assistant-alfred.very-very-dark-gray.top\" \\\n  -k https://192.168.0.151/ \n400: Bad Request\n\n$ curl -I -k https://home-assistant-alfred.royal-penny.ts.net/\nHTTP/2 400 \ncontent-type: text/plain; charset=utf-8\ndate: Sun, 20 Apr 2025 19:42:46 GMT\nserver: Python/3.13 aiohttp/3.11.16\ncontent-length: 16\n</code></pre> <p>Inspecting the logs from the <code>home-assistant</code> pod shows this is in fact an issue in Home Assistant itself:</p> <pre><code>$ kubectl logs -n home-assistant \\\n  $(kubectl get pods -n home-assistant | grep home-assistant | cut -f1 -d' ') -f\ns6-rc: info: service legacy-services successfully started\n2025-04-20 21:42:26.465 ERROR (MainThread) [homeassistant.components.http.forwarded] A request from a reverse proxy was received from 10.244.0.105, but your HTTP integration is not set-up for reverse proxies\n</code></pre> <p>This warning became an error in 2021.7 and the solution is to add the following into Home Assistant's <code>configuration.yaml</code> to allow the reverse proxy (allowing the entire Kubernetes IP range):</p> /home/k8s/home-assistant/configuration.yaml<pre><code>http:\n  use_x_forwarded_for: true\n  trusted_proxies:\n    - 10.244.0.0/16\n</code></pre> <p>Changes to files in <code>/home/k8s/home-assistant</code> will persist through pod and node  restarts because this config directory is mounted to a persistent volume. However, such changes to config files will not be effective without restarting the pod or node. A <code>ConfigMap</code> can be used to achieve a more permanent solution for the above, which is why it is included below. Such a <code>ConfigMap</code> is found in the manifests presented in Running your Home Assistant on Kubernetes \u2014 Part II, but these are problematic because (apparently) the use of <code>!</code> in the <code>ConfigMap</code> leads to Home Assistant failing to parse the config and going into recovery mode.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#base-deployment-manifests","title":"Base deployment manifests","text":"<p>Warning</p> <p>Sometimes the use of <code>!include</code> in the <code>ConfigMap</code> or else Home Assistant will fail to parse it, go into recovery mode and start without allowing requests coming in through the reverse proxy. The following <code>ConfigMap</code> omits such lines to address the Caveat on reverse proxies.</p> <code>base/configmap.yaml</code> <pre><code>---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: home-assistant-configmap\ndata:\n  configuration.yaml: |-\n    default_config:\n    http:\n      use_x_forwarded_for: true\n      trusted_proxies:\n        - 10.244.0.0/16\n</code></pre> <code>base/persistent-volume.yaml</code> <pre><code>---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: home-assistant-pv-config\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/home-assistant\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: home-assistant-config-root\nspec:\n  storageClassName: manual\n  volumeName: home-assistant-pv-config\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> <code>base/deployment.yaml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: home-assistant\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: home-assistant\n  name: home-assistant\nspec:\n  revisionHistoryLimit: 3\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: home-assistant\n  template:\n    metadata:\n      labels:\n        app: home-assistant\n    spec:\n    # securityContext:\n    #   fsGroup: 33\n    #   fsGroupChangePolicy: \"OnRootMismatch\"\n      containers:\n        - name: home-assistant-app\n          image: \"ghcr.io/home-assistant/home-assistant:stable\"\n          imagePullPolicy: Always\n          securityContext:\n            capabilities:\n              add:\n                - NET_ADMIN\n                - NET_RAW\n                - NET_BROADCAST\n            privileged: true\n          envFrom:\n            - configMapRef:\n                name: home-assistant-config\n          ports:\n            - name: http\n              containerPort: 8123\n              protocol: TCP\n          resources: {}\n          livenessProbe:\n            tcpSocket:\n              port: 8123\n            initialDelaySeconds: 0\n            failureThreshold: 3\n            timeoutSeconds: 1\n            periodSeconds: 10\n          readinessProbe:\n            tcpSocket:\n              port: 8123\n            initialDelaySeconds: 0\n            failureThreshold: 3\n            timeoutSeconds: 1\n            periodSeconds: 10\n          startupProbe:\n            tcpSocket:\n              port: 8123\n            initialDelaySeconds: 0\n            failureThreshold: 30\n            timeoutSeconds: 1\n            periodSeconds: 5\n          volumeMounts:\n            - name: ha-config-root\n              mountPath: /config\n            - name: configmap-file\n              mountPath: /config/configuration.yaml\n              subPath: configuration.yaml\n      restartPolicy: Always\n      hostNetwork: true\n      volumes:\n        - name: ha-config-root\n          persistentVolumeClaim:\n            claimName: home-assistant-config-root\n        - name: configmap-file\n          configMap:\n            name: home-assistant-configmap\n</code></pre> <code>base/service.yaml</code> <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: home-assistant\n  name: home-assistant-svc\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8123\n    targetPort: http\n    protocol: TCP\n    name: http\n</code></pre> <code>base/ingress.yaml</code> <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: home-assistant-nginx\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: REPLACEME  # FQDN for Cloudflare Tunnel or port-forwarded\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: home-assistant-svc\n              port:\n                name: \"http\"\n  tls:\n    - hosts:\n        - REPLACEME  # FQDN for Cloudflare Tunnel or port-forwarded\n      secretName: tls-secret\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: home-assistant-tailscale\nspec:\n  ingressClassName: tailscale\n  defaultBackend:\n    service:\n      name: home-assistant-svc\n      port:\n        name: \"http\"\n  tls:\n    - hosts:\n        - REPLACEME  # Hostname-only for Tailscale [Funnel]\n</code></pre> <code>base/kustomization.yaml</code> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: home-assistant\nresources:\n- configmap.yaml\n- deployment.yaml\n- ingress.yaml\n- persistent-volume.yaml\n- service.yaml\nconfigMapGenerator:\n- name: home-assistant-config\n  literals:\n  - TZ=\"UTC\"\nlabels:\n- includeSelectors: true\n  pairs:\n    app.kubernetes.io/name: home-assistant\n    app: home-assistant\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#kustomized-deployment","title":"Kustomized deployment","text":"<p>For each server to start this deployment on, create a <code>kustomization.yaml</code> file under a directory named after the server and adjust the relevant values:</p> <ul> <li>Set <code>TZ</code> under <code>configMapGenerator</code> to the correct time zone.</li> <li>Set <code>/spec/rules/0/host</code> and <code>/spec/tls/0/hosts/0</code> for the     <code>Ingress</code> named <code>home-assistant-nginx</code> to the FQDN from Cloudflare.</li> <li>Set <code>/spec/rules/0/host</code> and <code>/spec/tls/0/hosts/0</code> for the     <code>Ingress</code> named <code>home-assistant-tailscale</code> to the Machine name      from Tailscale (just the hostname, not the FQDN).</li> </ul> alfred/kustomization.yaml<pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- ../base\n\nconfigMapGenerator:\n- name: home-assistant-config\n  behavior: replace\n  literals:\n  - TZ=\"Europe/Madrid\"\n\npatches:\n  - target: #FQDN for Cloudflare Tunnel or port-forwarded\n      kind: Ingress\n      name: home-assistant-nginx\n    patch: |-\n      - op: replace\n        path: /spec/rules/0/host\n        value: home-assistant-alfred.very-very-dark-gray.top\n      - op: replace\n        path: /spec/tls/0/hosts/0\n        value: home-assistant-alfred.very-very-dark-gray.top\n  - target: # Hostname-only for Tailscale [Funnel]\n      kind: Ingress\n      name: home-assistant-tailscale\n    patch: |-\n      - op: replace\n        path: /spec/tls/0/hosts/0\n        value: home-assistant-alfred\n</code></pre> <p>There are two different checks to make before applying this deployment:</p> <ol> <li> <p>Run <code>kubectl kustomize alfred</code> to inspect that the generated deployment manifest     does replace the correct values.</p> </li> <li> <p>Run <code>kubectl apply -k alfred --dry-run=client</code> to verify that <code>kubectl</code> finds     on errors in the generated manifest.</p> </li> </ol>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#prepare-remote-access","title":"Prepare remote access","text":"<p>Before applying the deployment, Cloudflare Tunnel and Access should be setup so that the Let's Encrypt certificate can be obtained and the Home Assistant frontend is easily accessible while at the same time well secured.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#cloudflare","title":"Cloudflare","text":"<p>Since Cloudflare Tunnel is already working, go in to Zero Trust and go to Networks &gt; Tunnels, find the relevant connector (alfred) and add a public hostname named <code>home-assistant-alfred</code> to point https://home-assistant-alfred.very-very-dark-gray.top/ to the appropriate targets as seen in Let's Encrypt via tunnel:</p> <ol> <li>Requests for files under <code>.well-known</code> must go to <code>http://localhost:32080</code> so that     HTTPS certificates can be issued.</li> <li>Requests for everything else (<code>*</code>) must to to the <code>LoadBalancer</code> IP of Nginx     over HTTPS (<code>https://192.168.0.151</code>) with the TLS &gt; Origin Server Name     configured as <code>home-assistant-alfred.very-very-dark-gray.top</code><ul> <li>TLS &gt; No TLS Verify should not be necessary, unless there are issues     obtaining Let's Encrypt certificates.</li> </ul> </li> </ol> <p>Restrict access to this public hostname by creating an Access application in Cloudflare Access, so that only trusted users can actually access https://home-assistant-alfred.very-very-dark-gray.top/ (with a bypass policy for files under <code>.well-known</code>).</p> <p>If all the above has been correctly setup, requests for files under <code>.well-known</code> will receive <code>error code: 502</code> because the <code>Ingress</code> is not deployed yet, and requests for any other files should receive a redirect to <code>cloudflareaccess.com</code>.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#tailscale_1","title":"Tailscale","text":"<p>No preparation is necessary for Tailscale if the server already has the Tailscale Kubernetes operator installed. However, Tailscale DNS typically take a day or two to propagate after the deployment is applied and the <code>home-assistant-alfred</code> machine is created.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#deployment-start","title":"Deployment Start","text":"<pre><code>$ sudo mkdir /home/k8s/home-assistant/\n$ sudo ls -lah /home/k8s/home-assistant/\ntotal 8.0K\ndrwxr-xr-x 2 root root 4.0K Apr 20 22:44 .\ndrwxr-xr-x 3 root root 4.0K Apr 20 22:44 ..\n\n$ kubectl apply -k alfred\nnamespace/home-assistant created\nconfigmap/home-assistant-config-79257h2772 created\nconfigmap/home-assistant-configmap created\nservice/home-assistant-svc created\npersistentvolume/home-assistant-pv-config created\npersistentvolumeclaim/home-assistant-config-root created\ndeployment.apps/home-assistant created\ningress.networking.k8s.io/home-assistant-nginx created\ningress.networking.k8s.io/home-assistant-tailscale created\n\n$ kubectl get all -n home-assistant\nNAME                                  READY   STATUS    RESTARTS   AGE\npod/home-assistant-74fc8db56f-ws6j4   1/1     Running   0          2m21s\n\nNAME                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/home-assistant-svc   ClusterIP   10.99.217.239   &lt;none&gt;        8123/TCP   2m21s\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/home-assistant   1/1     1            1           2m21s\n\nNAME                                        DESIRED   CURRENT   READY   AGE\nreplicaset.apps/home-assistant-74fc8db56f   1         1         1       2m21s\n\n$ kubectl get ingress -n home-assistant\nNAME                       CLASS       HOSTS                                           ADDRESS                                    PORTS     AGE\nhome-assistant-nginx       nginx       home-assistant-alfred.very-very-dark-gray.top   192.168.0.121                              80, 443   2m26s\nhome-assistant-tailscale   tailscale   *                                               home-assistant-alfred.royal-penny.ts.net   80, 443   2m26s\n\n$ sudo ls -lah /home/k8s/home-assistant\ntotal 808K\ndrwxr-xr-x 6 root root 4.0K Apr 20 23:24 .\ndrwxr-xr-x 3 root root 4.0K Apr 20 23:22 ..\ndrwxr-xr-x 4 root root 4.0K Apr 20 23:24 blueprints\ndrwxr-xr-x 2 root root 4.0K Apr 20 23:24 .cloud\n-rw-r--r-- 1 root root    0 Apr 20 23:23 configuration.yaml\n-rw-r--r-- 1 root root    8 Apr 20 23:24 .HA_VERSION\n-rw-r--r-- 1 root root    0 Apr 20 23:24 home-assistant.log\n-rw-r--r-- 1 root root    0 Apr 20 23:24 home-assistant.log.1\n-rw-r--r-- 1 root root    0 Apr 20 23:24 home-assistant.log.fault\n-rw-r--r-- 1 root root 4.0K Apr 20 23:24 home-assistant_v2.db\n-rw-r--r-- 1 root root  32K Apr 20 23:40 home-assistant_v2.db-shm\n-rw-r--r-- 1 root root 741K Apr 20 23:40 home-assistant_v2.db-wal\ndrwxr-xr-x 2 root root 4.0K Apr 20 23:39 .storage\ndrwxr-xr-x 2 root root 4.0K Apr 20 23:24 tts\n\n$ kubectl get svc -A | grep acme\nhome-assistant         cm-acme-http-solver-7qb57                         NodePort       10.103.172.33    &lt;none&gt;          8089:32447/TCP               0s\n</code></pre> <p>After less than a minute, the ACME solver is patched to listen on port <code>32080</code>, and after just about another minute the solver is gone. At that point Home Assistant is available at both https://home-assistant-alfred.royal-penny.ts.net/ and https://home-assistant-alfred.very-very-dark-gray.top/; ready to start the onboarding process.</p> <p>Although it should not be necessary, it is possible to restore now the original <code>configuration.yaml</code> file, with the additional lines to allow traffic through reverse proxies (under <code>http</code>):</p> /home/k8s/home-assistant/configuration.yaml<pre><code># Loads default set of integrations. Do not remove.\ndefault_config:\n\n# Load frontend themes from the themes folder\nfrontend:\n  themes: !include_dir_merge_named themes\n\nautomation: !include automations.yaml\nscript: !include scripts.yaml\nscene: !include scenes.yaml\n\nhttp:\n  use_x_forwarded_for: true\n  trusted_proxies:\n    - 10.244.0.0/16\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#deployment-in-lexicon","title":"Deployment in <code>lexicon</code>","text":"<p>To test the portability of this deployment, applied it to <code>lexicon</code> and it seemed to work, except for an error that shows in the logs every 30 seconds:</p> <pre><code>2025-04-21 11:14:49.560 ERROR (MainThread) [async_upnp_client.ssdp] Received error: [Errno 126] Required key not available, transport: &lt;_SelectorDatagramTransport fd=40 read=polling write=&lt;idle, bufsize=0&gt;&gt;, socket: &lt;asyncio.TransportSocket fd=40, family=2, type=2, proto=0, laddr=('0.0.0.0', 55239)&gt;\n2025-04-21 11:15:19.561 ERROR (MainThread) [async_upnp_client.ssdp] Received error: [Errno 126] Required key not available, transport: &lt;_SelectorDatagramTransport fd=40 read=polling write=&lt;idle, bufsize=0&gt;&gt;, socket: &lt;asyncio.TransportSocket fd=40, family=2, type=2, proto=0, laddr=('0.0.0.0', 55239)&gt;\n2025-04-21 11:15:49.561 ERROR (MainThread) [async_upnp_client.ssdp] Received error: [Errno 126] Required key not available, transport: &lt;_SelectorDatagramTransport fd=40 read=polling write=&lt;idle, bufsize=0&gt;&gt;, socket: &lt;asyncio.TransportSocket fd=40, family=2, type=2, proto=0, laddr=('0.0.0.0', 55239)&gt;\n2025-04-21 11:16:19.562 ERROR (MainThread) [async_upnp_client.ssdp] Received error: [Errno 126] Required key not available, transport: &lt;_SelectorDatagramTransport fd=40 read=polling write=&lt;idle, bufsize=0&gt;&gt;, socket: &lt;asyncio.TransportSocket fd=40, family=2, type=2, proto=0, laddr=('0.0.0.0', 55239)&gt;\n</code></pre> <p>There are multiple threads with pretty much exactly the same error lines in the Home Assistant Community, but all of them die out with a solution other than restart Home Assistant or unplug and replug the network cable:</p> <ul> <li>2023-01-08: Network Crashing</li> <li>2021-11-04: HA Blue loosing connection - Network unreachable</li> <li>2024-01-09: HA crashed with no obvious reason</li> <li>2024-05-28: [async_upnp_client.ssdp] Received error: [Errno 101] Network unreachable</li> <li>2025-02-07: Network failed without reason</li> </ul> <p>This workaround by <code>/u/TristanDJr</code> (2023-01-28) seems to embrace the unfixable nature of the issue by constantly checking for loss of network connectivity and resetting the WiFi interface when trigged. However, these errors persisted after rebooting the server at least twice, over several hours. And yet, despite all those errors, Home Assistant seems to be working very well, as it was able to automatically discover several devices in the network.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#bluetooth-failed-setup","title":"Bluetooth failed setup","text":"<p>The one warning Home Assistant was showing after onboarding was related to the Bluetooth controller:</p> <pre><code>Intel Corporate None (DC:21:48:43:B7:C2)  \nNo devices or entities  \nFailed setup, will retry: hci0 (DC:21:48:43:B7:C2): hci0 (DC:21:48:43:B7:C2): DBus service not found; make sure the DBus socket is available: [Errno 2] No such file or directory\n</code></pre> <p>Making the DBus socket available in the container by mounting <code>/run/dbus</code> helped a bit:</p> base/deployment.yaml<pre><code>          volumeMounts:\n            - name: ha-config-root\n              mountPath: /config\n            - name: configmap-file\n              mountPath: /config/configuration.yaml\n              subPath: configuration.yaml\n            - name: dev-dbus\n              mountPath: /run/dbus\n      restartPolicy: Always\n      hostNetwork: true\n      volumes:\n        - name: ha-config-root\n          persistentVolumeClaim:\n            claimName: home-assistant-config-root\n        - name: configmap-file\n          configMap:\n            name: home-assistant-configmap\n        - name: dev-dbus\n          hostPath:\n            path: /run/dbus\n</code></pre> <pre><code>$ kubectl apply -k lexicon\nnamespace/home-assistant unchanged\nconfigmap/home-assistant-config-59kccc4bcd unchanged\nconfigmap/home-assistant-configmap unchanged\nservice/home-assistant-svc unchanged\npersistentvolume/home-assistant-pv-config unchanged\npersistentvolumeclaim/home-assistant-config-root unchanged\ndeployment.apps/home-assistant configured\ningress.networking.k8s.io/home-assistant-nginx unchanged\ningress.networking.k8s.io/home-assistant-tailscale unchanged\n</code></pre> <p>After reloading the integration, the error was different:</p> <pre><code>Failed setup, will retry: hci0 (DC:21:48:43:B7:C2): hci0 (DC:21:48:43:B7:C2): Failed to start Bluetooth: [org.freedesktop.DBus.Error.ServiceUnknown] The name org.bluez was not provided by any .service files; Try power cycling the Bluetooth hardware.\n</code></pre> <p>This suggests all 3 steps in the Home Assistant documentation are required, including Switching from dbus-daemon to dbus-broker and Installing BlueZ.</p> <pre><code># apt install bluez dbus-broker -y\n\n# systemctl --global enable dbus-broker.service\n</code></pre> <p>After this, without reloading the Bluetooth integration, it was instantely fixed and ready to use.</p> <p>The same 3 steps were then to <code>alfred</code> so that Bluetooth setup works well when the onboarding is done.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#protect-critical-plugs","title":"Protect critical plugs","text":"<p>A few smart plugs must never be powered off, at least not accidentally. To prevent that from happening, these can be hidden (not disabled) so they don't show in dashboards. They remain available under Settings &gt; Devices &amp; services, where they can still be powered off if necessary.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#home-assistant-community-store","title":"Home Assistant Community Store","text":"<p>Several of the following improvements require installing components from the HACS (Home Assistant Community Store).</p> <p>This involves running the installation script from inside the pod:</p> <pre><code>$ kubectl get pods -n home-assistant\nNAME                              READY   STATUS    RESTARTS      AGE\nhome-assistant-686974fbcb-rqv9t   1/1     Running   1 (17h ago)   26h\n\n$ kubectl exec --stdin --tty home-assistant-686974fbcb-rqv9t -n home-assistant -- /bin/bash\n</code></pre> <p>Once inside the pods, confirm that <code>wget</code> and <code>unzip</code> are available, then run:</p> <pre><code>lexicon:/config# wget -O install-hacs.sh https://get.hacs.xyz\nConnecting to get.hacs.xyz (172.67.68.101:443)\nConnecting to raw.githubusercontent.com (185.199.109.133:443)\nsaving to 'install-hacs.sh'\ninstall-hacs.sh      100% |**************************************************************|  4990  0:00:00 ETA\n'install-hacs.sh' saved\n\nlexicon:/config# bash install-hacs.sh \nINFO: Trying to find the correct directory...\nINFO: Found Home Assistant configuration directory at '/config'\nINFO: Creating custom_components directory...\nINFO: Changing to the custom_components directory...\nINFO: Downloading HACS\nConnecting to github.com (140.82.121.4:443)\nConnecting to github.com (140.82.121.4:443)\nConnecting to objects.githubusercontent.com (185.199.110.133:443)\nsaving to 'hacs.zip'\nhacs.zip             100% |**************************************************************| 18.1M  0:00:00 ETA\n'hacs.zip' saved\nINFO: Creating HACS directory...\nINFO: Unpacking HACS...\n\nINFO: Verifying versions\nINFO: Current version is 2025.4.3, minimum version is 2024.4.1\n\nINFO: Removing HACS zip file...\nINFO: Installation complete.\n\nINFO: Remember to restart Home Assistant before you configure it\n\nlexicon:/config# \nexit\n</code></pre> <p>One installed, restart Home Assistant using the power button icon at the top-right under Settings &gt; Hardware.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#how-to-add-cards","title":"How to add cards","text":"<p>Hello World Card explains the basics of installing and using custom cards.</p> <p>To add a Resource go to Settings &gt; Dashboards and pick Resources from the 3-dot menu in the top-right corner.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#real-time-power-monitoring","title":"Real-time power monitoring","text":"<p>There is no built-in dashboard for real-time power usage, which is precisely what I need to re-implement  Continuous Monitoring for TP-Link Tapo devices.</p> <p>There are multiple solutions to evaluate, all of which seem to require varying degrees of crafting:</p> <ul> <li>A dashboard and used a bunch of mushrooms in concert with the sections layout     with a graph of the last hour of usage.</li> <li>ha-tdv-bar shows individual power usage, but not total.</li> <li>Mini Graph Card supports showing better charts, but it does not     support showing stacked graphs or     making the title show the total sum of all values.</li> <li>Bubble Card     is a minimalist card collection for Home Assistant with a nice pop-up touch.</li> <li>Mushroom     is a collection of cards for Home Assistant Dashboard UI.</li> <li>HA COMPONENT KIT to create seamless, highly customizable interfaces for Home Assistant dashboards.</li> <li>Understand templates to elevate your game.</li> </ul>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#ha-tdv-bar","title":"ha-tdv-bar","text":"<p>The ha-tdv-bar cards makes it very easy to show the individual power usage of multiple devices in a single card, but offers no easy way to show the total power usage across them all.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#mini-graph-card","title":"Mini Graph Card","text":"<p>Lovelace Mini Graph Card supports showing better charts, where power usage can be in a single graph of multiple ones, but it does not support showing stacked graphs or making the title show the total sum of all values.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#multiple-cards-combined","title":"Multiple cards combined","text":"<p>Dashboard real-time power meter with device-level detail combines Config Template Card, Custom Card Mod (Add css styles to any lovelace card), Bar Card to create a nice power consumption meter. It also leverages Virtual Energy Meters with PowerCalc to create virtual sensors to monitor estimated power usage from devices that have a constant power usage.</p> <p>Real Time Device Power Meter for Home Assistant explains pretty well how to make this, including the creation of virtual sensors to account for power usage from devices without actual sensors. This involves creating a few YAML files directly in the Home Assistant <code>/config</code> directory (and then restarting Home Assistant to pick them up).</p> <p>Create the directory  <code>/home/k8s/home-assistant/templates/sensors/</code> to store the virtual sensors and update <code>configuration.yaml</code> to load files from it:</p> /home/k8s/home-assistant/configuration.yaml<pre><code>default_config:\n\n# Load frontend themes from the themes folder\nfrontend:\n  themes: !include_dir_merge_named themes\n\nautomation: !include automations.yaml\nscript: !include scripts.yaml\nscene: !include scenes.yaml\n\nhttp:\n  use_x_forwarded_for: true\n  trusted_proxies:\n    - 10.244.0.0/16\n\ntemplate:\n  - sensor: !include_dir_merge_list  templates/sensors/\n</code></pre> <p>Then create a virtual sensor <code>all_power</code> with the aggregate sum of all power sensors:</p> /home/k8s/home-assistant/template/sensors/power.yaml<pre><code>- name: all_power\n  unique_id: all_power\n  unit_of_measurement: W\n  device_class: power\n  state_class: measurement\n  state: &gt;-\n    {{ \n      states('sensor.mystrom_device_power')|float(0)\n      + states('sensor.homelab_current_consumption')|float(0)\n      + states('sensor.dell_1908fpc_current_consumption')|float(0)\n      + states('sensor.bed_heater_current_consumption')|float(0)\n      + states('sensor.bedlamp_current_consumption')|float(0)\n    }}\n</code></pre> <p>At this point it is necessary to restart Home Assistant, or at least reload YAML files.</p> <p>Note</p> <p>Home Assistant may fail to reload YAML files or restart due to references in <code>configuration.yaml</code> to non-existant files; this can be easily solved by creating those files: <code>touch automations.yaml scripts.yaml scenes.yaml</code>.</p> <p>Once YAML files are reloaded, a new virtual sensor <code>all_power</code> should be available to create cards with it, e.g. a gauge:</p> <pre><code>type: gauge\nmin: 0\nentity: sensor.all_power\nname: Current Electricity Usage\nmax: 500\nneedle: true\nseverity:\n  green: 0\n  yellow: 100\n  red: 400\nunit: W\n</code></pre> Entity is currently unavailable: sensor.all_power <p>Sometimes the new entity becomes unavailable. This appears to go away after a few seconds, or a full Home Assistant restart, without leaving any clue in the logs.</p> <p>Combined with the previous cards (ha-tdv-bar and Mini Graph Card) this all comes together to offer a useful overview:</p> <p></p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#a-better-bar-card","title":"A better bar card","text":"<p>For a better bar card, install the Custom Card Mod manually and then add this card to the dashboard:</p> Custom template card for individual power usage sensors. <pre><code>type: 'custom:config-template-card'\nvariables:\n  - entity: sensor.mystrom_device_power\n    name: Office\n  - entity: sensor.homelab_current_consumption\n    name: Homelab\n  - entity: sensor.dell_1908fpc_current_consumption\n    name: Dell 1908FPc\n  - entity: sensor.bed_heater_current_consumption\n    name: Bed heater\n  - entity: sensor.tv_smart_plug_power\n    name: TV\n  - entity: sensor.bedlamp_current_consumption\n    name: Bed lamp\nentities: \n  # Note: this list of entities may seem redundant, but is necessary to inform\n  # config-template-card which entities to watch for updates.\n  - sensor.mystrom_device_power\n  - sensor.homelab_current_consumption\n  - sensor.dell_1908fpc_current_consumption\n  - sensor.bed_heater_current_consumption\n  - sensor.bedlamp_current_consumption\nelement:\n  type: 'custom:bar-card'\n  entities: |- \n    ${ vars.filter(v =&gt; {\n      let ent = states[v.entity];\n      if(ent === undefined || ent.state === undefined) {\n        console.warn(`Power meter: Entity ${v.entity} not found`);\n      }\n      else if(ent.state === 'unknown') {\n        console.warn(`Power meter: Entity ${v.entity} state is unknown`);\n      }\n      else if(isNaN(ent.state)) {\n        console.warn(`Power meter: Entity ${v.entity} state is not a number`);\n      }\n      else return Number(ent.state) &gt; 5;\n    }).sort((v1,v2) =&gt; states[v2.entity].state - states[v1.entity].state)}\n    direction: right\n    entity_row: true\n    min: 0\n    max: ${ Math.max(...vars.map(v =&gt; states[v.entity]).filter(e =&gt; !!e).map(e =&gt; e.state).filter(n =&gt; !isNaN(n))) }\n  height: 20px\n  stack: vertical\n  decimal: 0\n  icon: 'mdi:flash'\n  positions:\n    icon: off\n    indicator: outside\n    name: inside\n    value: inside\n  severity:\n  - color: '#a1a1a18a'\n    from: 0\n    to: 2\n  - color: '#3ea8328a'\n    from: 2\n    to: 10\n  - color: '#85a8328a'\n    from: 10\n    to: 50\n  - color: '#a8a4328a'\n    from: 50\n    to: 200\n  - color: '#a887328a'\n    from: 200\n    to: 500\n  - color: '#a867328a'\n    from: 500\n    to: 1000\n  - color: '#a846328a'\n    from: 1000\n    to: 3000\n  - color: '#a832328a'\n    from: 3000\n    to: 10000\n  style: |-\n    #states &gt; * {\n      margin: 1px;\n    }\n    bar-card-name,\n    bar-card-value {\n      font-size: 0.9rem;\n      color: #ffffffaa;\n      font-weight: bold;\n    }\n    bar-card-value  {\n      font-weight: bolder;\n    }\n    bar-card-indicator {\n      margin-top: 4px;\n      transform: scaleY(-1);\n    }\n</code></pre> <p>Todo</p> <p>Find out why this doesn't work at all. It doesn't even show anything when editing the dashboard and can't even be edited afterwards, it can only be deleted.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#influxdb","title":"InfluxDB","text":"<p>The influxdb integration makes it possible to transfer all state changes to an external InfluxDB database. This could be used to transfer all data to the InfluxDB use for Continuous Monitoring and use a similar Grafana dashboard for Continuous Monitoring for TP-Link Tapo devices.</p> <p>To get started, add <code>influxdb:</code> to the main <code>configuration.yaml</code> and restart Home Assistant, then find the InfluxDB will be available under Settings &gt; Devices &amp; services. This integration cannot be configured from the UI; instead create a new InfluxDB database <code>home_assistant</code> using the InfluxDB CLI, with a retention policy of 800 days (well over 2 years):</p> <pre><code>$ influx -host localhost -port 30086 -username admin -password ''\npassword: \nConnected to http://localhost:30086 version 1.8.10\nInfluxDB shell version: 1.6\n\n&gt; CREATE DATABASE home_assistant\n&gt; CREATE RETENTION POLICY \"800_days\" ON \"home_assistant\" DURATION 800d REPLICATION 1\n&gt; ALTER RETENTION POLICY \"800_days\" on \"home_assistant\" DURATION 800d REPLICATION 1 DEFAULT\n</code></pre> <p>To start sending metrics to InfluxDB, create a configuration based on the full configuration for 1.xx installations, without rules to <code>exclude</code> or <code>include</code> specific entities; no need to add filters (by default all entities are sent to InfluxDB):</p> <pre><code>influxdb:\n  host: 192.168.0.6\n  port: 30086\n  database: home_assistant\n  username: admin\n  password: MY_PASSWORD\n  max_retries: 3\n  default_measurement: state\n  tags:\n    instance: prod\n    source: hass\n  component_config_glob:\n    sensor.*humidity*:\n      override_measurement: humidity\n    sensor.*temperature*:\n      override_measurement: temperature\n</code></pre> <p>The <code>component_config_glob</code> is a workaround found in forum thread InfluxDB to store data in intervals to replace the default <code>measurement</code> names, set as the unit of each entity, (for example <code>\u00b0C</code> for temperature entities), with more sensible names. This is particularly useful for those measurements with <code>%</code> as their unit, since that unit is used for measurements of very different kinds such as humidity, battery level, cpu load and storage capacity used.</p> <p>Note</p> <p>If the InfluxDB is reached over HTTPS the above config need to have <code>ssl: true</code> and, depending on whether SSL certificates are valid or not, also <code>verify_ssl: true</code> (or <code>false</code>).  </p> <p>After restarting Home Assistant metrics start flowing into new measurements into the new InfluxDB database. These can be used to create dashboards in Grafana, after adding the new database as a new InfluxDB data source.</p> <p>The InfluxDB <code>sensor</code> allows using values from a InfluxDB database to populate a sensor state. This can be used to present statistics as Home Assistant sensors, if used with the influxdb history integration. It can also be used with an external data source. All this sound very interesting for potential future expansions!</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#persist-configuration-in-configmap","title":"Persist configuration in <code>ConfigMap</code>","text":"<p>For months after deploying the above monitoring, it was observed that Home Assistant would regularly lose the configuration and revert to its default, leading to both the <code>sensor.all_power</code> entity not being available (indefinitely) and InfluxDB no longer receiving any metrics (also indefinitely). For a long time, The only workaround found was to revert <code>configuration.yaml</code> to the default, restart Home Assistant from the web admin UI (not rebooting the node, not restarting the deployment), then restore the configuration and restart again.</p> <p>After much thinking about it (and not finding anyone anywhere ever having had the same problem), it occurred to me that what was happening is that somehow Home Assistant was reloading the default configuration from the <code>ConfigMap</code>, so the solution was to add the configuration to that in <code>alfred/kustomization.yaml</code>:</p> alfred/kustomization.yaml<pre><code>  - target: # Hostname-only for Tailscale [Funnel]\n      kind: Ingress\n      name: home-assistant-tailscale\n    patch: |-\n      - op: replace\n        path: /spec/tls/0/hosts/0\n        value: home-assistant-alfred\n  - target: # Home Assistant config\n      kind: ConfigMap\n      name: home-assistant-configmap\n    patch: |-\n      - op: replace\n        path: /data/configuration.yaml\n        value: |-\n          default_config:\n          http:\n            use_x_forwarded_for: true\n            trusted_proxies:\n              - 10.244.0.0/16\n          template:\n            - sensor: !include_dir_merge_list  templates/sensors/\n          influxdb:\n            host: 192.168.0.6\n            port: 30086\n            database: home_assistant\n            username: admin\n            password: MY_PASSWORD\n            max_retries: 3\n            default_measurement: state\n            tags:\n              instance: prod\n              source: hass\n            component_config_glob:\n              sensor.*humidity*:\n                override_measurement: humidity\n              sensor.*temperature*:\n                override_measurement: temperature\n</code></pre>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#audiobookshelf","title":"Audiobookshelf","text":"<p>Audiobookshelf running on <code>octavo</code> is probably the first service I'd miss if it becomes unavailable or unusable, and since <code>alfred</code> has a roomy 2000 GB NVMe SSD, it can serve as a backup for the main service. This does not have to be a complete copy of Audiobookshelf on <code>octavo</code> but it does need to be kept in sync with it; and this sync must be pausable and reversable in the event of the main service becoming the secondary service for an extended period of time.</p> <p>To test the syncing strategy, Audiobookshelf on <code>alfred</code> will start entirely empty:</p> <pre><code>$ sudo groupadd audiobookshelf -g 117\n$ sudo useradd  audiobookshelf -u 117 -g 117 -s /usr/sbin/nologin\n$ sudo mkdir /home/k8s/audiobookshelf\n$ sudo chown -R audiobookshelf:audiobookshelf /home/k8s/audiobookshelf\n$ sudo ls -hal /home/k8s/audiobookshelf\ntotal 8.0K\ndrwxr-xr-x 2 audiobookshelf audiobookshelf 4.0K May  4 18:25 .\ndrwxr-xr-x 6 root           root           4.0K May  4 18:25 ..\n\n$ sudo mkdir /home/depot\n$ sudo chown pi:pi /home/depot\n$ mkdir -p /home/depot/audio/Audiobooks /home/depot/audio/Podcasts\n$ ls -hal /home/depot/audio/\ntotal 16K\ndrwxr-xr-x 4 pi pi 4.0K May  4 18:32 .\ndrwxr-xr-x 3 pi pi 4.0K May  4 18:32 ..\ndrwxr-xr-x 2 pi pi 4.0K May  4 18:32 Audiobooks\ndrwxr-xr-x 2 pi pi 4.0K May  4 18:32 Podcasts\n</code></pre> <p>Taking the deployment of Audiobookshelf in <code>octavo</code> as the starting point, the deployment for <code>alfred</code> changes only a few details:</p> <ul> <li>Change the mounting points for audio files to <code>/home/depot/audio</code> in the local     filesystem, because the NAS will be too far away from the final destination.</li> <li>Remove the <code>audiobookshelf-ingress</code> because this system will likely not be     directly reachable from outside its network on destination, and     Cloudflare Tunnel is not suitable for streaming audio.</li> <li>Add <code>audiobookshelf-ingress-tailscale</code> for access through Tailscale.<ul> <li>Add the <code>nginx.ingress.kubernetes.io/websocket-services</code> annotation which is     required by Audiobookshelf.</li> </ul> </li> </ul> Audiobookshelf deployment on <code>alfred</code> alfred/audiobookshelf.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: audiobookshelf\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-config\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/audiobookshelf/config\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-metadata\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/audiobookshelf/metadata\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-audiobooks\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/depot/audio/Audiobooks\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-podcasts\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/depot/audio/Podcasts\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-depot-podcasts\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/depot/audio/Podcasts\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-config\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-config\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-metadata\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-metadata\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-audiobooks\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-audiobooks\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-podcasts\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-podcasts\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-depot-podcasts\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-depot-podcasts\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: audiobookshelf\n  name: audiobookshelf\n  namespace: audiobookshelf\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: audiobookshelf\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: audiobookshelf\n    spec:\n      containers:\n        - image: ghcr.io/advplyr/audiobookshelf:latest\n          imagePullPolicy: Always\n          name: audiobookshelf\n          env:\n          - name: PORT\n            value: \"13378\"\n          ports:\n          - containerPort: 13378\n          resources: {}\n          stdin: true\n          tty: true\n          volumeMounts:\n          - mountPath: /config\n            name: audiobookshelf-config\n          - mountPath: /metadata\n            name: audiobookshelf-metadata\n          - mountPath: /audiobooks\n            name: audiobookshelf-audiobooks\n          - mountPath: /podcasts\n            name: audiobookshelf-podcasts\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 117\n            runAsGroup: 117\n      restartPolicy: Always\n      volumes:\n      - name: audiobookshelf-config\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-config\n      - name: audiobookshelf-metadata\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-metadata\n      - name: audiobookshelf-audiobooks\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-audiobooks\n      - name: audiobookshelf-podcasts\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-depot-podcasts\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: audiobookshelf-svc\n  namespace: audiobookshelf\nspec:\n  type: NodePort\n  ports:\n  - port: 13388\n    nodePort: 31378\n    targetPort: 13378\n  selector:\n    app: audiobookshelf\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: audiobookshelf-ingress-tailscale\n  namespace: audiobookshelf\n  annotations:\n    nginx.ingress.kubernetes.io/websocket-services: audiobookshelf-svc\nspec:\n  ingressClassName: tailscale\n  defaultBackend:\n    service:\n      name: audiobookshelf-svc\n      port:\n        number: 13378\n  tls:\n    - hosts:\n        - audiobookshelf-alfred\n</code></pre> <p>The apply the new deployment to start InfluxDB and Grafana:</p> <pre><code>$ kubectl apply -f alfred/audiobookshelf.yaml\nnamespace/audiobookshelf created\npersistentvolume/audiobookshelf-pv-config created\npersistentvolume/audiobookshelf-pv-metadata created\npersistentvolume/audiobookshelf-pv-audiobooks created\npersistentvolume/audiobookshelf-pv-podcasts created\npersistentvolume/audiobookshelf-pv-depot-podcasts created\npersistentvolumeclaim/audiobookshelf-pvc-config created\npersistentvolumeclaim/audiobookshelf-pvc-metadata created\npersistentvolumeclaim/audiobookshelf-pvc-audiobooks created\npersistentvolumeclaim/audiobookshelf-pvc-podcasts created\npersistentvolumeclaim/audiobookshelf-pvc-depot-podcasts created\ndeployment.apps/audiobookshelf created\nservice/audiobookshelf-svc created\ningress.networking.k8s.io/audiobookshelf-ingress-tailscale created\n\n$ kubectl get all -n audiobookshelf\nNAME                                 READY   STATUS    RESTARTS   AGE\npod/audiobookshelf-b49c49757-74rv5   1/1     Running   0          46s\n\nNAME                         TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE\nservice/audiobookshelf-svc   NodePort   10.108.207.194   &lt;none&gt;        13388:31378/TCP   46s\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/audiobookshelf   1/1     1            1           46s\n\nNAME                                       DESIRED   CURRENT   READY   AGE\nreplicaset.apps/audiobookshelf-b49c49757   1         1         1       46s\n\n$ kubectl get ingress -n audiobookshelf\nNAME                               CLASS       HOSTS   ADDRESS                                    PORTS     AGE\naudiobookshelf-ingress-tailscale   tailscale   *       audiobookshelf-alfred.royal-penny.ts.net   80, 443   52s\n</code></pre> <p>After nearly a minute the service is running, but the web UI is not yet available because somehow the process is having access denied to its own files:</p> <pre><code>$ klogs audiobookshelf audiobookshelf\n[2025-05-04 17:10:17.354] FATAL: [Server] Unhandled rejection: [Error: EACCES: permission denied, mkdir '/metadata/logs'] {\n  errno: -13,\n  code: 'EACCES',\n  syscall: 'mkdir',\n  path: '/metadata/logs'\n} \npromise: Promise {\n  &lt;rejected&gt; [Error: EACCES: permission denied, mkdir '/metadata/logs'] {\n    errno: -13,\n    code: 'EACCES',\n    syscall: 'mkdir',\n    path: '/metadata/logs'\n  }\n}\n</code></pre> <p>This error repeats at a very high rate and the directory is indeed empty, because somehow the directories have been claimed by <code>root</code> and need to be re-claimed by <code>audiobookshelf</code>:</p> <pre><code>$ sudo ls -lad /home/k8s/audiobookshelf/*\ndrwxr-xr-x  6 root root 4096 May  4 19:11 /home/k8s/audiobookshelf/config\ndrwxr-xr-x  2 root root 4096 May  4 19:11 /home/k8s/audiobookshelf/metadata\n\n$ sudo chown -R audiobookshelf:audiobookshelf /home/k8s/audiobookshelf/\n$ sudo ls -lad /home/k8s/audiobookshelf/*\ndrwxr-xr-x 3 audiobookshelf audiobookshelf 4096 May  4 19:11 /home/k8s/audiobookshelf/config\ndrwxr-xr-x 6 audiobookshelf audiobookshelf 4096 May  4 19:11 /home/k8s/audiobookshelf/metadata\n</code></pre> <p>After some more time, because Tailscale DNS need time to propagate, it becomes available at https://audiobookshelf-alfred.royal-penny.ts.net, but it will be empty because it has not yet been synced with <code>octavo</code>.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#audiobookshelf-sync-scripts","title":"Audiobookshelf sync scripts","text":"<p>To keep alfred in sync with octavo, the trick is to copy <code>/home/k8s/audiobookshelf</code> over while the service is not running; so long as the <code>config/absdatabase.sqlite</code> database is copied over, Audiobookshelf on <code>alfred</code> will mirror exactly the status in <code>octavo</code>.</p> Sync script: Audiobookshelf and audio from <code>octavo</code> <pre><code>#!/bin/bash\n#\n# Sync specific directories from /home/nas (NAS) to /home/depot (NVMe).\n\n# Audiobooks: active (authors).\nfor d in Andy.Weir Carl.Sagan Douglas.Adams James.S.A.Corey Terry.Pratchett; do\n  rsync -ua \\\n    root@octavo.royal-penny.ts.net:/home/nas/public/audio/Audiobooks/$d \\\n    /home/depot/audio/Audiobooks\ndone\n\n# Podcasts: active.\nfor d in Linux.Matters Making.It Nextlander; do\n  rsync -ua \\\n    root@octavo.royal-penny.ts.net:/home/depot/audio/Podcasts/$d \\\n    /home/depot/audio/Podcasts/\ndone\n\n# Audiobookshelf: stop, sync, start.\nkubectl scale -n audiobookshelf deployment audiobookshelf --replicas=0\nsleep 20\nsudo rsync -ua --del \\\n  root@octavo.royal-penny.ts.net:/home/k8s/audiobookshelf/* \\\n  /home/k8s/audiobookshelf/\nkubectl scale -n audiobookshelf deployment audiobookshelf --replicas=1\n</code></pre> <p>Running this hourly should provide a good mirror.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#navidrome","title":"Navidrome","text":"<p>Navidrome would be missed too, should it become unavailable. Following the same method as above, taking the deployment of Navidrome in <code>octavo</code> as the starting point, the deployment for <code>alfred</code> changes about the same details as with Audiobookshelf:</p> <ul> <li>Change the mounting points for audio files to <code>/home/depot/audio</code> in the local     filesystem, because the NAS will be too far away from the final destination.</li> <li>Remove the <code>navidrome-ingress</code> because this system will likely not be     directly reachable from outside its network on destination, and     Cloudflare Tunnel is not suitable for streaming.</li> <li>Add <code>navidrome-ingress-tailscale</code> for access through Tailscale.</li> <li>Modify the <code>ND_BASEURL</code> variable to reflect the expected FQND from Tailscale.</li> </ul> Navidrome deployment on <code>alfred</code> alfred/navidrome.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: navidrome\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: navidrome-pv-data\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/navidrome\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: navidrome-pv-music\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 100Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/depot/audio/Music\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: navidrome-pvc-data\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  volumeName: navidrome-pv-data\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: navidrome-pvc-music\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  volumeName: navidrome-pv-music\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: navidrome\n  name: navidrome\n  namespace: navidrome\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: navidrome\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: navidrome\n    spec:\n      containers:\n        - image: deluan/navidrome:latest\n          imagePullPolicy: Always\n          name: navidrome\n          env:\n          - name: ND_BASEURL\n            value: \"https://navidrome-alfred.royal-penny.ts.net/\"\n          - name: ND_LOGLEVEL\n            value: \"info\"\n          - name: ND_SCANSCHEDULE\n            value: \"1h\"\n          - name: ND_SESSIONTIMEOUT\n            value: \"24h\"\n          ports:\n          - containerPort: 4533\n          resources: {}\n          stdin: true\n          tty: true\n          volumeMounts:\n          - mountPath: /data\n            name: navidrome-data\n          - mountPath: /music\n            name: navidrome-music\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 116\n            runAsGroup: 116\n      restartPolicy: Always\n      volumes:\n      - name: navidrome-data\n        persistentVolumeClaim:\n          claimName: navidrome-pvc-data\n      - name: navidrome-music\n        persistentVolumeClaim:\n          claimName: navidrome-pvc-music\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: navidrome-svc\n  namespace: navidrome\nspec:\n  type: NodePort\n  ports:\n  - port: 4533\n    nodePort: 30533\n    targetPort: 4533\n  selector:\n    app: navidrome\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: navidrome-ingress-tailscale\n  namespace: navidrome\nspec:\n  ingressClassName: tailscale\n  defaultBackend:\n    service:\n      name: navidrome-svc\n      port:\n        number: 4533\n  tls:\n    - hosts:\n        - navidrome-alfred\n</code></pre> <p>Before deploying Navidrome, it makes sense to also import its <code>/data</code> directory from <code>octavo</code>, at least once if not on a regular sync:</p> <pre><code>$ sudo groupadd navidrome -g 116\n$ sudo useradd  navidrome -u 116 -g 116 -s /usr/sbin/nologin\n$ sudo rsync -ua root@octavo.royal-penny.ts.net:/home/k8s/navidrome /home/k8s/\n$ sudo chown -R navidrome:navidrome /home/k8s/navidrome\n$ sudo ls -hal /home/k8s/navidrome\ntotal 32M\ndrwxr-xr-x 3 navidrome navidrome 4.0K Apr 30 00:27 .\ndrwxr-xr-x 7 root      root      4.0K May  4 22:37 ..\ndrwxr-xr-x 5 navidrome navidrome 4.0K Dec 23 06:09 cache\n-rw-r--r-- 1 navidrome navidrome  32M Apr 30 00:25 navidrome.db\n-rw-r--r-- 1 navidrome navidrome  32K May  3 23:14 navidrome.db-shm\n-rw-r--r-- 1 navidrome navidrome 165K May  3 23:14 navidrome.db-wal\n</code></pre> <p>Then apply the deployment and allow time for Tailscale DNS to propagate.</p> <pre><code>$ kubectl apply -f alfred/navidrome.yaml\nnamespace/navidrome created\npersistentvolume/navidrome-pv-data created\npersistentvolume/navidrome-pv-music created\npersistentvolumeclaim/navidrome-pvc-data created\npersistentvolumeclaim/navidrome-pvc-music created\ndeployment.apps/navidrome created\nservice/navidrome-svc created\ningress.networking.k8s.io/navidrome-ingress-tailscale created\n\n$ kubectl get all -n navidrome\nNAME                             READY   STATUS    RESTARTS   AGE\npod/navidrome-6f6bf7d87c-qrtz2   1/1     Running   0          25s\n\nNAME                    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nservice/navidrome-svc   NodePort   10.104.114.45   &lt;none&gt;        4533:30533/TCP   25s\n\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/navidrome   1/1     1            1           25s\n\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/navidrome-6f6bf7d87c   1         1         1       25s\n\n$ kubectl get ingress -n navidrome\nNAME                          CLASS       HOSTS   ADDRESS                                PORTS     AGE\nnavidrome-ingress-tailscale   tailscale   *       navidrome-alfred.royal-penny.ts.net    80, 443   30s\n</code></pre> <p>Eventually the service is available at https://navidrome-alfred.royal-penny.ts.net and everything works just fine. Otherwise, consider removing the contents of <code>/home/k8s/navidrome</code> to let Navidrome initialize itself anew.</p>"},{"location":"blog/2025/02/22/home-assistant-on-kubernetes-on-raspberry-pi-5-alfred/#conclusion","title":"Conclusion","text":"<p>One does not simply... anything!</p> <p>All this started with a fairly simple goal: run Home Assistant on a Raspberry Pi 5. Adding Kubernetes to the mix certainly adds complexity, but at the same time enables future expansion by easily adding more services later.</p> <p>The choice of hardware turned out to be the least problematic part of this project, although the Raspberry Pi OS can be attributed the issues leading to troubleshooting the Kubernetes bootstrap. Installing Kubernetes v1.32.2 may be responsible for most of the other issues and complications encountered, such as Flannel troubles, trouble with the Kubernetes dashboard, MetalLB speaker, Ingress directives and possibly even more toubles with Flannel. And then Home Assistant itself had to present its own issues with reverse proxies.</p> <p>On the plus side, this project has been a good test run to learn about Cloudflare Tunnel (and Cloudflare Access), Tailscale and Kustomize, all of which will be reused to setup remote access and services in the new  Kubernetes homelab server with Ubuntu Server 24.04 (octavo).</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/","title":"Adopting Firefox and Bitwarden as daily drivers","text":"<p>Google Chrome has been my daily driver for a really long time; so long, in fact, that all I remember was the initial frustration when it first came out without a release for the GNU/Linux platforms. I don't remember why, or even whether, I was so eager to jump ship, and at this point I can only guess that the old ship was the one I'm preparing to jump back to: Firefox.</p> <p>It seems on-line life has gotten a wee bit more comp-lic-ated that it was back in 2008, when smartphone apps were a new thing, YouTube had only 720p video, and Spotify was brand new...</p> <p>And one of the worst thing back in those days was that a single web browser had so much market share, many (too many) developers would not care much about people, including paying customers, using any other browser. The landscape has since changed, only to essentially come back round to the same old problem:</p> <p> Source: StatCounter Global Stats - Browser Market Share</p> <p>These days there are other problems; developers and companies understand that users must be respected even if they don't use the one most popular web browser, operating system, screen resolution... well, at least more developers care, but that's not enough; still too many don't, and so We Have A Browser Monopoly Again and Firefox is The Only Alternative Out There.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#the-pln","title":"The PLN","text":"<p>With a significantly more complex on-line life and stronger dependencies on many more on-line services, one does not simply leave everything behind. Chrome used to be particularly useful thanks to many extensions, which would make development easier or enable workarounds on half-broken sites. These days I find its integrated password manager with cloud-based storage, along with very conviniently accessible multiple profiles, the more useful features in a modern browser. Firefox stores passwords only locally, which essentially means no multi-device password manager, but at least multiple profiles are supported if only less convenient to use.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#settings","title":"Settings","text":"<p>Default settings are seldom the best for anyone; if anything, they are the one side does not exactly fit anybody at best, or what they want for you at worst. Either way, the following changes to default settings are, if not necessarily vital, at least worth considering:</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#general","title":"General","text":""},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#startup","title":"Startup","text":"<p>\u2611 Open previous windows and tabs</p> <p>I like to enable this option to easily go back to consistently restore previous sessions each time. There is a mostly fixed set of sites I like to keep pinned because I used them on a daily basis, and beyond those I also like being able to just close the browser with the peace of mind that I can easily and quickly go badk to all of them by just launching the browser again.</p> <p>\u2610 Ask before closing multiple tabs</p> <p>This option is useful to avoid accidentally closing a window, although even then it is not too hard to get those tabs back: just reopen the relevant profile and then use the <code>Control</code>+<code>Shift</code>+<code>T</code> shortcut to reopen recently closed tabs.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#language-and-appearance","title":"Language and Appearance","text":""},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#website-appearance-dark","title":"Website appearance: Dark","text":"<p>I prefer dark themes on most websites, so I like to set this to Dark.</p> <p>There are many more themes available to further customize Firefox appearance; these may be useful later to make different profiles visually distinct.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#zoom","title":"Zoom","text":"<p>Working regularly on high DPI screens such as 4K and 2K monitors, I like to keep Default zoom at 120%.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#language","title":"Language","text":"<p>This is an interesting one if you are a multilingual person; even if you can just read additional languages, it's useful to Set Alternatives and add those languages, so that websites that are available in those languages are not unnecessarily translated.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#digital-rights-management-drm-content","title":"Digital Rights Management (DRM) Content","text":"<p>\u2610 Play DRM-controlled content</p> <p>This option is necessary to watch video in the most protective platforms, e.g. Netflix Error F7701-1003 requires enabling DRM to play videos, Spotify requires it to play music, etc.</p> <p>It may make sense to have a dedicated profile with this enabled, while leaving it disabled on all/most other profiles to avoid misuse or DRM Apply this workaround  to avoid epetitive nagging for those profiles that are meant to never play DRM.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#home","title":"Home","text":""},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#firefox-home-content","title":"Firefox Home Content","text":"<p>\u2610 Web Search</p> <p>I see no need to have a search bar in the home page, since the same search is readily available in the location bar already and that's where the keyboard focus goes by default. The screen space saved by removing this can be better used to display other, more useful links.</p> <p>\u2611 Shortcuts</p> <p>Shortcuts are good, but I rather not have \u2610 Sponsored shortcuts.</p> <p>\u2611 Recent activity</p> <p>Recent activity provides bigger page thumbnails from various sources, all of which seem interesting enough to leave enabled and even allow this to take 2 or more rows.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#privacy-security","title":"Privacy &amp; Security","text":""},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#browser-privacy","title":"Browser Privacy","text":""},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#website-privacy-preferences","title":"Website Privacy Preferences","text":"<p>\u2611 Tell websites not to sell or share my data</p> <p>This enables Global Privacy Control which operates as a \u201cDo Not Sell\u201d mechanism in some US states such as California, Colorado and Connecticut. It may also be used to indicate an opt-out of targeted advertising or general request to limit the sale or sharing of your personal data in those jurisdictions, as well as in jurisdictions such as the EU, UK, Nevada, Utah and Virginia.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#passwords","title":"Passwords","text":"<p>\u2610 Ask to save passwords</p> <p>Firefox built-in password manager stores passwords only locally, optionally protected by a primary password. While this is fine for a single person using a single computer, as soon as multiple users and/or devices are involved, it becomes more recommendable to disable this in favor of a 3rd-party password manager.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#autofill","title":"Autofill","text":"<p>\u2610 Save and fill payment methods</p> <p>This should only be enabled on specific profiles used for shopping.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#firefox-data-collection-and-use","title":"Firefox Data Collection and Use","text":"<p>\u2610 Allow Firefox to send technical and interaction data to Mozilla</p> <p>Firefox Privacy Notice explains many ways in which such technical and interaction data, including To serve relevant content and advertising on Firefox New Tab which is not going to work anyway after disabling \u2610 Sponsored shortcuts.</p> <p>\u2610 Allow Firefox to send backlogged crash reports on your behalf</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#website-advertising-preferences","title":"Website Advertising Preferences","text":"<p>\u2610 Allow websites to perform privacy-preserving ad measurement</p> <p>Disable Privacy-Preserving Attribution because, quite frankly, even Privacy-Preserving is not unlikely to end up leaking a bit too much.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#security","title":"Security","text":""},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#https-only-mode","title":"HTTPS-Only Mode","text":"<p>\ud83d\udd18 Enable HTTPS-Only Mode in all windows \u2299 Enable HTTPS-Only Mode in private windows only \u2299 Don\u2019t enable HTTPS-Only Mode</p> <p>HTTPS-Only Mode in Firefox should be enabled by default everywhere; given how HTTPS is actually supported, often even enforced, by most websites. Accessing HTTP-only sites remains easy and exceptions can be added to make them permanent.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#dns-over-https","title":"DNS over HTTPS","text":"<p>Enable DNS over HTTPS using: Default Protection</p> <p>DNS over HTTPS protection levels higher than default are only useful when a specific DNS provider is preferred, or when failing to resolve DNS queries is preferred over using non-HTTPS resolvers.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#sync","title":"Sync","text":"<p>This provides a secure method to synchronize data across Mozilla applications using a Mozilla account. Currently exclusive to Firefox, Sync ensures data is effortlessly shared and up-to-date. This enables seamless browsing across multiple Firefox instances with bookmarks, logins and passwords, addresses, credit cards, extensions and important settings kept consistent and accessible across all devices.</p> <p>Create a Mozilla account and keep the Recovery Keys safe. These can be saved later using a password manager.</p> <p>Displays a shortcut to prompts when you select text. Firefox sends the text, page title, and prompt to the chatbot.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#firefox-labs","title":"Firefox Labs","text":""},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#customize-your-browsing","title":"Customize your browsing","text":"<p>\u2610 AI chatbot should probably stay disabled until it has been thoroughly tested and confirmed more useful than harmful. So far the available community feedback seems overwhelmingly negative.</p> <p>This may be interesting in the future if/when employing a local LLM, there is a way to change <code>about:config</code> to set <code>browser.ml.chat.provider</code> to any URL and, if that URL accepts passing in prompts with <code>?q=</code>, the context menu works too.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#profiles","title":"Profiles","text":"<p>Firefox Profile Manager may not be as convenient (and pretty) as Chrome's, but it does let you create, remove or switch profiles.</p> <p>Open <code>about:profiles</code> in a tab to see the list of profiles, create new ones, make any one of them the default profile, and launch any in a new window. To directly launch a specific profile by clicking on an icon, create desktop launchers making use of the  <code>-P \"profile_name\"</code> command line flag, or use <code>-P</code> alone to launch only the  Profile Manager.</p> <p>All the above Settings are specific to each profile, so that each user can have their own profile with individual settings, etc. The most important settings are also stored online via Sync, so it also makes sense to keep each profile synced to its own Mozilla account.</p> <p>That means maintaining a number of Mozilla accounts, each with either their own unique email address and password, which can lead to keeping track of many accounts and passwords; too many to have only in one's head (or not-so-secure places), so better setup a password manager.</p> <p>My preferred practice is to create a profile for each active account; meaning accounts (Apple, Google, etc.) that I use on a daily basis, not necessarily accounts like a dedicated one for the Android TV in the living room that is used by the whole family.</p> <p>Right after creating each profile,</p> <ol> <li>Install the Bitwarden Password Manager     (assuming the password manager is already set up),     so that it can be used when creating the Mozilla account.</li> <li>Adjust all the above settings; in particular, disable the     built-in option to \u2610 Ask to save passwords so that Bitwarden is used.</li> <li>Install a visually distinct Theme.</li> <li>Create an account at accounts.firefox.com     and save the credentials to Bitwarden.</li> <li>Enable Sync in Firefox settings (requires logging in again).</li> <li>Install whatever other Extensions are relevant.</li> </ol>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#themes","title":"Themes","text":"<p>Themes are not only pretty to look at (although some of them get old real fast), they are also useful to make each profile visually distinct. Scrolling through the top rated themes I liked a few:</p> <ul> <li>Animated Miku Dance</li> <li>ANIMATED sea sunset 02 by candelora</li> <li>chihiro</li> <li>Cyberpunk [Animated]</li> <li>Fire Fox, The Anime Girl : Nebula Space [Blue]</li> <li>Hacker (Animated)</li> <li>Halloween Moon Bats</li> <li>Kirby9</li> <li>Nyan Cat (Animated)</li> <li>Praise the sun (animated)</li> <li>Purple starfield - Animated</li> <li>Sakura Blossoms &amp; Birds by MaDonna</li> <li>Spirited Away (Animated)</li> <li>TARDIS \u235f animated</li> <li>Totoro 12</li> </ul>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#extentions","title":"Extentions","text":"<p>There are many many lots of Firefox Extensions to tweak how it behaves, or how websites look like, or behave. What follows is only a handful I found worth at least trying, to see how it goes...</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#dark-reader","title":"Dark Reader","text":"<p>Dark Reader makes most websites look like they have a dark mode. It turns out a bit hit and miss, some websites will look good enough, other not really good, while a few will look pretty bad, sometimes to the point of being unusable.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#ublock-origin","title":"uBlock Origin","text":"<p>uBlock Origin is probably the most transformative extension; makes the Internet bearable. (again). Ads are not even the worst part of the  Internet enshittification, so this extension helps dealing with many more of those things that make the Internet a sorry place.</p> <p>After installing the extension, I like to set it to prefer Dark theme (under Settings) and enable all the additional Filter lists: Cookie notices, Social widgets, Annoyances, Regions, languages.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#sponsor-block","title":"Sponsor Block","text":"<p>SponsorBlock makes YouTube bearable (again). Sponsors may or may not be the better solution for content creators to sustain their business, but after well over a decade putting up with sponsor segments, I'm quite ready to not see them again. Most (if not all) of what I watch on YouTube, especially those I watch regularly, are creators I really like and support financially directly; they'll be okay. I get precious little time to catch up with their videos, so I set this up:</p> <p>Under Behavior:  </p> <ul> <li>Unpaid/Self Promotion: Auto skip</li> <li>Interaction Reminder (Subscribe): Auto skip</li> <li>Highlight: Auto skip to the start</li> <li>Intermission/Intro Animation: Auto skip</li> </ul> <p>Under Interface:  </p> <ul> <li>Disable Show notice after a segment is skipped</li> <li>Enable Hide clutter in segment list group</li> <li>Enable Hide prompts about new features</li> <li>Enable Hide donation link</li> <li>Enable Hide options not available without extra payment</li> </ul> <p>Undeer Backup/Restore, save the settings to file (<code>SponsorBlockConfig.json</code>) to then easily apply the same settings to other profiles.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#return-youtube-dislike","title":"Return YouTube Dislike","text":"<p>Return YouTube Dislike should not be as needed as it sadly may still be; as much as YouTube is full of great content, there is also many many lots of the opposite and the number of dislikes should be a useful signal to keep an eye on.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#privacy-badger","title":"Privacy Badger","text":"<p>Privacy Badger automatically learns to block invisible trackers.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#search-by-image","title":"Search by Image","text":"<p>Search by Image makes effortless reverse image searches possible, and comes with support for more than 30 search engines.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#augmented-steam-and-steamdb","title":"Augmented Steam and SteamDB","text":"<p>Augmented Steam and SteamDB add small but nice-to-have improvements to the Steam store, which I'm mostly curious about even though I may visit that store only a few times per year.</p> <p>The most welcome features I would say are the ones that get rid of repetitive, useless and/or annoying behaviors, such as</p> <ul> <li>Skip age check page</li> <li>Remove \"You are leaving Steam\" link filter from external sites</li> <li>Hide \"Open in Steam\" banner</li> </ul>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#activitywatch","title":"ActivityWatch","text":"<p>ActivityWatch is necessary (or just useful) to report active browser tags to the Self-hosted time tracking with ActivityWatch.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#account-isolation-containers","title":"Account-isolation Containers","text":"<p>Firefox Multi-Account Containers (explained in Multi-Account Containers), and the Facebook Container in particular, seems interesting for cases where Facebook or similar web apps are unavoidable. Since I'm pretty good at avoiding Facebook, this shall stay filed under Interesting, Revisit Later If Needed.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#noscript-security-suite","title":"NoScript Security Suite","text":"<p>NoScript Security Suite seems a bit too strong and restrictive, for instance Discord web app turns into an empty void when this is enabled. More concerning would be e-Banking web apps or other real-life mission-critical web apps that depend, for entirely legit reasons, on scripting techniques that this extention would flag as malicious.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#dearrow","title":"DeArrow","text":"<p>DeArrow is a paid browser extension: unlimited-use license key for 1$, 6-hour free trial. The idea of moving away from clickbait and sensationalism is mildly appealing, but some of the examples seem to go a little too far in the opposite directions. Either way, even if not something to jump into right away, this is something to be aware of.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#password-manager","title":"Password Manager","text":"<p>If had a penny for each time I've heard someone saying people, get yourself a password manager! I could probably afford myself a harty meal. Adding those who also have shared their quest to passwordmanagerized the whole family, I should have enough for an unreasonably hefty dessert.</p> <p>Google's password manager is integrated well enough with Chrome and Android, so that for quite a few years I have seldom needed to remember or lookup more than a few passwords, but I do run into its limitations more often than I'd like. Other browsers in Android and other applications on desktop don't get any of those benefits, which often means having to ask Chrome for the password or, when that is not an option, storing a few passwords in an alternative storage which, to put it mildly, is not quite as secure as a proper password manager.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#a-2025-comparison","title":"A 2025 Comparison","text":"<p>Choosing a password manager can be a bit of a daunting task; although it is probably not as hard to switch password managers are cars or houses, it is still something one does not simply do in a few minutes. In the spirit of contrasting multiple sources, without even trying too hard to verify how reputable each of these sources are, and not bothering at all to figure out whether each of these has been written by actual human beins with carbon-based brain, I've put together this multi-source comparision (sources below):</p> Ranking Engadget Safety Detectives PCMag security.org tom's guide TechRadar Password Manager Zapier Cybernews 1 1Password 1Password RoboForm RoboForm 1Password NordPass 1Password 1Password NordPass 2 Bitwarden Dashlane NordPass NordPass Bitwarden Dashlane Dashlane Bitwarden RoboForm 3 NordPass RoboForm Proton Pass Total Password Keeper RoboForm Keeper Dashlane 1Password 4 Dashlane NordPass 1Password Dashlane NordPass Apple Passwords Keeper 5 Keeper Dashlane 1Password Bitdefender Dashlane 6 LastPass Proton Pass Bitwarden 7 LogMeOnce Dashlane 8 Keeper Apple Passwords 9 Bitwarden 10 Enpass Sources (in reverse chronological order): <ul> <li>The best password manager for 2025   from Engadget, Mar 7, 2025.</li> <li>10 Best Password Managers for Windows in 2025   from Safety Detectives, March 7, 2025. </li> <li>The Best Password Managers for 2025   from PCMag, Feb 21, 2025.</li> <li>The Best Free Password Managers of 2025   from security.org, Feb 19, 2025.</li> <li>The best password managers in 2025   from tom's guide, February 11, 2025.</li> <li>The 4 best password managers in 2025   from Zapier, February 6, 2025</li> <li>The best password manager for 2025   from Cybernews, February 4, 2025</li> <li>Best password manager of 2025   from TechRadar, January 9, 2025.</li> <li>The Best Password Managers of 2025   from Password Manager, Updated June 23, 2023 (!!??)</li> </ul> <p>The first two sources claim to have been updated just yesterday, while the last one was updated in 2023 to list the best ones in 2025... take that with a chunk of salt.</p> <p>Aggreating these rankings by giving each contestant 1 point each time they rank in the first position, 1/2 pont each time they rank in the second position, and so on, only three stand out:</p> <ol> <li>1Password with 5.78 points.</li> <li>NordPass with 3.83 points.</li> <li>Bitwarden with 3.27 points.</li> </ol> <p>Comparing Bitwarden pricing and NordPass pricing for personal use, although they both offer a Free tier, only Bitwarden seems to offer enough functionality to make that free tier realistically useful in the long run. 1Password pricing includes no Free tier, only a 14-day free trial.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#bitwarden","title":"Bitwarden","text":"<p>And so the adventure begins at bitwarden.com/go/start-free, creating a new account using my main everyday email address. The email takes a minute or two to come in, with the link to finish the process by setting a primary password. Funnily enough, the unrecoverable primary password for the new password manager can be stored in the old password manager.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#import-passwords","title":"Import Passwords","text":"<p>Import Data to your Vault should be the first step after creating the account, and that means importing passwords from Google Chrome since that is the one that has been in use for years.</p> <p>Since multiple Chrome profiles have been in used over the years, although most of them had been out of use for years too, this involved repeating a few steps for each profile:</p> <ol> <li>Open <code>chrome://password-manager</code> to check if there are any passwords.</li> <li>If there are, then open <code>chrome://password-manager/settings</code> to download them,    this will download a <code>Chrome Passwords.csv</code> file.</li> <li>Create a Folder named after the profile, just to keep things tidy.</li> <li>Go to Import Data (under    Tools) and upload the <code>Chrome Passwords.csv</code> into the corresponding    Folder.</li> </ol>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#chrome-extension","title":"Chrome extension","text":"<p>bitwarden.com/browser-start suggests to install the relevant browser extension once data has been imported. Because Bitwarden supports multiple devices, the same account (vault) can be used at the same time from Chrome and Firefox.</p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#firefox-add-on","title":"Firefox Add-on","text":"<p>Bitwarden Password Manager is just as easy to setup: install, pin to the toolbar, log in. The whole dance needs to be done for each and every profile, </p>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#desktop-applications","title":"Desktop applications","text":"<p>Installing the Linux desktop app and command line tool is super easy with <code>snap</code>:</p> <pre><code>$ snap search bitwarden\nName         Version   Publisher    Notes  Summary\nbitwarden    2025.2.1  bitwarden\u2713   -      Bitwarden\nbw           2025.1.3  bitwarden\u2713   -      Bitwarden CLI - A secure and free password manager for all of your devices.\n...\n\n$ sudo snap install bitwarden bw\nbitwarden 2025.2.1 from 8bit Solutions LLC (bitwarden\u2713) installed\nbw 2025.1.3 from 8bit Solutions LLC (bitwarden\u2713) installed\n</code></pre>"},{"location":"blog/2025/03/08/adopting-firefox-and-bitwarden-as-daily-drivers/#final-touches","title":"Final touches","text":"<p>Once everything is setup with all the profiles and accounts, or even during the processing of setting it all up, a few extra steps were taken to take the opportunity and spend a little extra time to make sure everything was more consistent and better organized than before, including</p> <ul> <li>Generating and securely storing recovery codes for every account that   supports them. At first it seemd those would be only the ones with 2FA   enabled, but somehow a few more accounts did produce recovery codes, while   others did not.</li> <li>Designate criteria to decide when to use each profile, such as main profile   only for the most personal stuff, a second profile for work-related stuff, and   most importantly one or more additional profiles for everything else. The   purpose of this separation is to avoid being distracted by work-related stuff   while shopping around, and vice-versa, and other such situations.</li> <li>Review old profiles in all browsers for anything worth keeping, such as   long-forgotten tabs with interesting content that would be worth bookmarking   for future reference, but this time in a better-suited profile.</li> </ul> <p>Last but not least, reconsidering Bitwarden Premium plans. Once passwords have been migrated, even organized in folders, and all behind a single main password, and all for free, the value of upgrading to Premium is a no-brainer. Upgrading to a Family plan is not yet worth it.</p>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/","title":"Remote access options for self-hosted services","text":"<p>Running self-hosted services behind a router that allows port forwarding is mostly as simple as forwarding a few ports, mainly 443 for everything over HTTPS and port 80 for automatically renewing Let's Encrypt certificates.</p> <p>Otherwise, being behind a router that either doens't allow port forwarding, or just doesn't work well, or being behind CGNAT, may require the use of some sort of tunnels to route inbound traffic using outbound connections. This can also be useful even in the above case, when multiple systems need to be reachable on port 80.</p> Cloudflare tunnels do not enable access on port 80. <p>Cloudflare redirects port 80 to 443, to upgrade HTTP connections to HTTPS. That means ACME HTTP-01 challenges to renew Let's Encrypt certificates need to be routed to the relevant port (80 or 32080) based on the request path; see Let's Encrypt via tunnel.</p>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#options-considered","title":"Options considered","text":"<p>Making applications (ports) reachable from outside will require a different approach and implementation depending on the type of traffic served by the application behind each port:</p> <ul> <li> <p>HTML non-sensitive content can easibly be made available through a   Cloudflare Tunnel.   These are free to use and easy to setup, with the caveats that</p> <ul> <li>Traffic must be decrypted by Cloudflare, to apply certain traffic filters, even   if everything is then (re-)encrypted between Cloudflare and each service.</li> <li>Traffic must be either HTML content or low-bandwidth non-HTML content.   Cloudflare does not allow, at least when using their tunnels for free, to stream   high-bandwidth multi-media content: audio, video, photos, videogame client-to-server   communication, etc.</li> </ul> </li> <li> <p>HTML sensitive content may also be made available through a   Cloudflare Tunnel;   Zero Trust Access   can be used to limit access to each application (port) based on a number of rules,   including user's identity as verified by 3P SSO providers (e.g. Google account).</p> </li> <li> <p>Non-HTML sensitive content may also be made available through a   Cloudflare Tunnel;   Non-HTTP applications require connecting your private network to Cloudflare   which, again, means that Cloudflare is decrypting traffic (to inspected it and apply   filters) to then re-encrypt it again. Then again, this may only be used for   low-bandwidth non-HTML content, e.g. SSH sessions. Ideally with password   authentication disabled!</p> </li> <li> <p>Non-HTML sensitive or high-bandwidth content can also be made reachable   externally, through the use of  Tailscale Funnel   (or similar) to establish fully-encrypted   (TLS passthrough) tunnels between sites (LANs) and a few trusted clients. However,   this is only suitable for low-bandwidth applications,   and there is no way of knowing how tightly bandwidth is restricted.</p> </li> <li> <p>Non-HTML high-bandwidth content (sensitive or not) seems to be only practical to   expose by opening and forwarding ports; as much as this seems to be the method most   people recommend against. This is also the method that just won't work when working   with a defective or restriced router, or behind CGNAT.</p> </li> </ul>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#options-chosen","title":"Options chosen","text":"<p>With the above considerations, each of those options would seem to best match the relevant self-hosted services as follows:</p> <ul> <li> <p>Cloudflare Tunnel    can be used for non-sensitive, or at least not-too-sensitive, HTML content:</p> <ul> <li>automatically renewing Let's Encrypt certificates    over plain HTTP on port 80.</li> <li>InfluxDB   already serving over HTTPS and not-too-sensitive; basic PC monitoring.</li> </ul> </li> <li> <p>Cloudflare Tunnel    can also be used for sensitive HTML content, when combined with   Zero Trust Access   to restrict access to each service:</p> <ul> <li>Grafana</li> <li>UniFi Controller</li> <li>Visual Studio Code Server</li> <li>Kubernetes Dashboard</li> <li>Firefly III</li> <li>Homebox</li> </ul> </li> <li> <p>Tailscale Funnel   must be used for those applications that transfer mostly non-HTML content:</p> <ul> <li>Audiobookshelf</li> <li>Komga: eBook library</li> <li>Navidrome</li> <li>Plex Media Server</li> <li>SSH</li> </ul> </li> <li> <p>Port forwarding may still be needed for non-HTTP services that are meant to be   accessible by technically not trusted clients, i.e. without Tailscale. This would   be mostl video game servers, of which there is so far only</p> <ul> <li>Minecraft Server</li> </ul> </li> </ul> <p>DNS records are setup to point the various <code>&lt;service&gt;.&lt;node&gt;.uu.am</code> to the external IP address of the router, with port 80 redirected to one node's port <code>32080</code>, so that HTTPS certificates can be renewed automatically, and port 443 pointing to Nginx on one node. While it is possible to access Nginx on additional nodes by forwarding a different port, the automated renewals of HTTPS certificates is only possible over port 80.</p> <p>This limitation may mean the node that is rechable through the external port 80 is the only one that can serve high-bandwidth content directly via port forwarding. All other nodes will need to use Cloudflare tunnels, and thus be restricted to HTML-only or low-bandwidth traffic, or use Tailscale Funnel to serve high-bandwidth traffic, and thus be restricted to specific clients.</p>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#cloudflare","title":"Cloudflare","text":"<p>To get started with Cloudflare, create account and a team (e.g. <code>high-energy-building</code>), select the Free plan, enter billing details and checkout. Make sure to enable 2FA authentiction and take a look around for other settings to personalize.</p>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#add-the-first-site","title":"Add the first site","text":"<p>Adding a site is required before creating a tunnel. This can be an external domain, although it requires replacing that domain's DNS with the one from Cloudflare. It seems most convenient to have a dedicated domain for self-hosted applications, to be used mostly, if not exclusively, through Cloudflare tunnels. For this purpose, I've registered <code>very-very-dark-gray.top</code> with Porkbun and cleared the default DNS records, which removed their parking domain.</p> <p>Start at the Account Home in dash.cloudflare.com, enter the new domain and select the option to Manually enter DNS records since there is none anyway. Scroll down to select the Free plan and continue to start adding DNS records.</p> <p>Having no DNS records yet, 3 warnings are shown about missing an MX record to for emails to <code>@very-very-dark-gray.top</code> addresses and missing A/AAAA/CNAME records for the root domain and <code>www</code>. All that is fine, there is no need for those.</p> <p>As a quick test, add a subdomain for <code>alfred</code> pointing to the router's external IPv4 address and let it be proxied by Cloudflare to see how that goes. Then replace the DNS for this domain with the ones from Cloudflare's, and wait.</p> <p>Once the change is live, go under DNS &gt; Settings in Cloudflare and Enable DNSSEC.</p>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#cloudflare-tunnels","title":"Cloudflare Tunnels","text":"<p>Tunnels are very easy to setup; accessing applications behind them not necessarily so much. Cloudflare Tunnels in Alfred involved an unfair amount of troubleshooting to get the Kubernetes dashboard to work at https://kubernetes-alfred.very-very-dark-gray.top/, eventually using it's own Let's Encrypt certificates.</p>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#cloudflare-access","title":"Cloudflare Access","text":"<p>Zero Trust Web Access builds on top of the previous setup; once an applications is made available through a tunnel, restricting access to a fixed set of users (e.g. Google accounts) requires only a few more steps:</p> <ol> <li> <p>Set up Google as an identity provider,     which requires creating an OAuth Client ID in the Google Cloud Console.</p> <p>The OAuth client will be limited to a list of (max 100) test users, which is more than enough but those users have to be manually added in the Google Cloud Console, under /auth/audience?project=very-very-dark-gray.</p> </li> <li> <p>Create a Policy (e.g Google Account) to Allow only a few users, with     only one Include rule to match those users' email addresses.</p> </li> <li> <p>Create an Access application</p> <ul> <li>Choose self-hosted application and name it after the public     host it will expose (e.g. <code>kubernetes-alfred</code>).</li> <li>Add a Public hostname (https://kubernetes-alfred.very-very-dark-gray.top/)</li> <li>Add only the Google Account policy, so that     only users that match that policy are allowed.</li> <li>To authenicate those users, enable only the Google     authentication in Login methods.</li> <li>Finally, enable Instant Auth.</li> <li>Then accept the defaults in the next two pages and Save.</li> </ul> </li> <li> <p>If this application will need to renew its own Let's Encrypt certificates,     create another Policy called to Bypass all authentication mechanisms,     and set the selector to Everyone as in the examples for the action     Bypass.</p> </li> <li> <p>Then create another Access application as the previou one, with these changes:</p> <ul> <li>Set the same as <code>kubernetes-alfred-well-known</code>.</li> <li>Add the same Public hostname with Path set to <code>.well-known</code>.</li> <li>Add only the Bypass policy.</li> <li>Then accept the defaults in the next two pages and Save.</li> </ul> </li> </ol> <p>So long as only one policy is added to the applications and that policy has only one Include rule, only those users added to that Include rule are allowed in.</p> <p>Additional sources of inspiration (not reference):</p> <ul> <li>Cloudflare Tunnels with SSO/OAuth working for immich</li> <li>HOWTO: Secure Cloudflare Tunnels remote access</li> </ul>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#tailscale","title":"Tailscale","text":"<p>Tailscale quickstart explains the basics and how to get started as a personal user.</p>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#create-a-tailnet","title":"Create a tailnet","text":"<p>Create a tailnet by singing up with your choice of identity provider; e.g. signging up with a <code>@gmail.com</code> account automatically set the tailnet up so that other <code>@gmail.com</code> users can be invited to form part of the \"team\". Choosing the Personal Plan keeps the service free of charge, there is not even a requirement to setup a valid billing method (e.g. credit card).</p> <p>The quickstart wizard will wait until Tailscale is installed on 2 systems, e.g. a server and a PC. The installation process is as simple as running their <code>install.sh</code> script:</p> Installation of Tailscale on alfred <pre><code>pi@alfred:~ $ curl -fsSL https://tailscale.com/install.sh | sh\nInstalling Tailscale for debian bookworm, using method apt\n+ sudo mkdir -p --mode=0755 /usr/share/keyrings\n+ curl -fsSL https://pkgs.tailscale.com/stable/debian/bookworm.noarmor.gpg\n+ sudo tee /usr/share/keyrings/tailscale-archive-keyring.gpg\n+ sudo chmod 0644 /usr/share/keyrings/tailscale-archive-keyring.gpg\n+ curl -fsSL https://pkgs.tailscale.com/stable/debian/bookworm.tailscale-keyring.list\n+ sudo tee /etc/apt/sources.list.d/tailscale.list\n# Tailscale packages for debian bookworm\ndeb [signed-by=/usr/share/keyrings/tailscale-archive-keyring.gpg] https://pkgs.tailscale.com/stable/debian bookworm main\n+ sudo chmod 0644 /etc/apt/sources.list.d/tailscale.list\n+ sudo apt-get update\nHit:1 http://deb.debian.org/debian bookworm InRelease\nHit:2 http://archive.raspberrypi.com/debian bookworm InRelease                                               \nHit:3 https://download.docker.com/linux/debian bookworm InRelease                                            \nHit:4 http://deb.debian.org/debian-security bookworm-security InRelease                                      \nHit:5 http://deb.debian.org/debian bookworm-updates InRelease                                                \nHit:6 https://baltocdn.com/helm/stable/debian all InRelease                                                  \nHit:7 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.32/deb  InRelease\nGet:8 https://pkgs.tailscale.com/stable/debian bookworm InRelease               \nGet:9 https://pkgs.tailscale.com/stable/debian bookworm/main armhf Packages [12.2 kB]\nGet:10 https://pkgs.tailscale.com/stable/debian bookworm/main arm64 Packages [12.3 kB]\nGet:11 https://pkgs.tailscale.com/stable/debian bookworm/main all Packages [354 B]\nFetched 31.4 kB in 1s (29.4 kB/s)  \nReading package lists... Done\n+ sudo apt-get install -y tailscale tailscale-archive-keyring\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages were automatically installed and are no longer required:\n  linux-headers-6.6.51+rpt-common-rpi linux-kbuild-6.6.51+rpt\nUse 'sudo apt autoremove' to remove them.\nThe following NEW packages will be installed:\n  tailscale tailscale-archive-keyring\n0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 29.6 MB of archives.\nAfter this operation, 56.1 MB of additional disk space will be used.\nGet:2 https://pkgs.tailscale.com/stable/debian bookworm/main all tailscale-archive-keyring all 1.35.181 [3,082 B]\nGet:1 https://pkgs.tailscale.com/stable/debian bookworm/main arm64 tailscale arm64 1.82.0 [29.6 MB]   \nFetched 29.6 MB in 15s (2,040 kB/s)                                                                          \nSelecting previously unselected package tailscale.\n(Reading database ... 91338 files and directories currently installed.)\nPreparing to unpack .../tailscale_1.82.0_arm64.deb ...\nUnpacking tailscale (1.82.0) ...\nSelecting previously unselected package tailscale-archive-keyring.\nPreparing to unpack .../tailscale-archive-keyring_1.35.181_all.deb ...\nUnpacking tailscale-archive-keyring (1.35.181) ...\nSetting up tailscale-archive-keyring (1.35.181) ...\nSetting up tailscale (1.82.0) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/tailscaled.service \u2192 /lib/systemd/system/tailscaled.service.\n+ [ false = true ]\n+ set +x\nInstallation complete! Log in to start using Tailscale by running:\n\nsudo tailscale up\n</code></pre> Installation of Tailscale on rapture <pre><code>$ curl -fsSL https://tailscale.com/install.sh | sh\nInstalling Tailscale for ubuntu noble, using method apt\n+ sudo mkdir -p --mode=0755 /usr/share/keyrings\n+ curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/noble.noarmor.gpg\n+ sudo tee /usr/share/keyrings/tailscale-archive-keyring.gpg\n+ sudo chmod 0644 /usr/share/keyrings/tailscale-archive-keyring.gpg\n+ curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/noble.tailscale-keyring.list\n+ sudo tee /etc/apt/sources.list.d/tailscale.list\n# Tailscale packages for ubuntu noble\ndeb [signed-by=/usr/share/keyrings/tailscale-archive-keyring.gpg] https://pkgs.tailscale.com/stable/ubuntu noble main\n+ sudo chmod 0644 /etc/apt/sources.list.d/tailscale.list\n+ sudo apt-get update\nHit:1 http://ch.archive.ubuntu.com/ubuntu noble InRelease\nHit:2 https://brave-browser-apt-release.s3.brave.com stable InRelease                                        \nHit:3 https://repo.steampowered.com/steam stable InRelease                                                   \nHit:4 http://ch.archive.ubuntu.com/ubuntu noble-updates InRelease                                            \nHit:5 http://archive.ubuntu.com/ubuntu noble InRelease                                                       \nHit:6 http://ch.archive.ubuntu.com/ubuntu noble-backports InRelease                                          \nHit:7 https://packages.microsoft.com/repos/code stable InRelease                                             \nHit:8 https://dl.google.com/linux/chrome/deb stable InRelease                                                \nHit:9 http://archive.ubuntu.com/ubuntu noble-updates InRelease                                               \nHit:10 https://esm.ubuntu.com/apps/ubuntu noble-apps-security InRelease                                      \nHit:11 https://esm.ubuntu.com/apps/ubuntu noble-apps-updates InRelease                                       \nHit:12 https://esm.ubuntu.com/infra/ubuntu noble-infra-security InRelease                        \nHit:13 https://esm.ubuntu.com/infra/ubuntu noble-infra-updates InRelease                         \nHit:14 http://security.ubuntu.com/ubuntu noble-security InRelease          \nGet:15 https://pkgs.tailscale.com/stable/ubuntu noble InRelease\nGet:16 https://pkgs.tailscale.com/stable/ubuntu noble/main all Packages [354 B]\nGet:17 https://pkgs.tailscale.com/stable/ubuntu noble/main i386 Packages [12.2 kB]\nGet:18 https://pkgs.tailscale.com/stable/ubuntu noble/main amd64 Packages [12.7 kB]\nFetched 31.8 kB in 1s (23.3 kB/s)                \nReading package lists... Done\nN: Skipping acquire of configured file 'main/binary-i386/Packages' as repository 'https://brave-browser-apt-release.s3.brave.com stable InRelease' doesn't support architecture 'i386'\n+ sudo apt-get install -y tailscale tailscale-archive-keyring\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tailscale tailscale-archive-keyring\n0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 31.5 MB of archives.\nAfter this operation, 59.1 MB of additional disk space will be used.\nGet:2 https://pkgs.tailscale.com/stable/ubuntu noble/main all tailscale-archive-keyring all 1.35.181 [3,082 B]\nGet:1 https://pkgs.tailscale.com/stable/ubuntu noble/main amd64 tailscale amd64 1.82.0 [31.5 MB]       \nFetched 31.5 MB in 14s (2,258 kB/s)                                                                          \nSelecting previously unselected package tailscale.\n(Reading database ... 491341 files and directories currently installed.)\nPreparing to unpack .../tailscale_1.82.0_amd64.deb ...\nUnpacking tailscale (1.82.0) ...\nSelecting previously unselected package tailscale-archive-keyring.\nPreparing to unpack .../tailscale-archive-keyring_1.35.181_all.deb ...\nUnpacking tailscale-archive-keyring (1.35.181) ...\nSetting up tailscale-archive-keyring (1.35.181) ...\nSetting up tailscale (1.82.0) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/tailscaled.service \u2192 /usr/lib/systemd/system/tailscaled.service.\n+ [ false = true ]\n+ set +x\nInstallation complete! Log in to start using Tailscale by running:\n\nsudo tailscale up\n</code></pre> <p>After installing the software, running <code>sudo tailscale up</code> will provide a URL to  authenticate and the system will show up as ready in the wizard:</p> <pre><code>$ sudo tailscale up\n\nTo authenticate, visit:\n\n        https://login.tailscale.com/a/______________\n\nSuccess.\n</code></pre> <p>Repeat the process with the second system and the wizzard will show both as active; along with a useful test to check that all is working well:</p> <p>Every device in your Tailscale network has a private 100.x.y.z IP address that you can reach no matter where you are. And every protocol works \u2014 SSH, RDP, HTTP, Minecraft \u2014 use whatever you want while Tailscale is running.</p> <p></p> <p>And indeed that just works; an SSH connection to <code>pi@100.113.110.3</code> instantly connects to <code>alfred</code> and SSH key authentication just works (after accepting this new hostname).</p> <p>From this point on, one can connect more devices, by repeating the above 2 steps: install Tailscale, then authenticate it to join this tailnet. It may be a good idea to add all desired devices before proceeding to additional configuration changes, such as</p> <ul> <li>Set up DNS, to more easily connect to devices.</li> <li>Share a node with users on other networks.</li> <li>Set up access controls to limit which devices     can talk to each other.</li> </ul>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#set-up-dns","title":"Set up DNS","text":"<p>The first step when setting up DNS, although optional, should be to rename the tailnet to a more memorable (\"fun\") name. There is a limited number of 4 randomly generated names to pick from, which one can reroll many times but will never, probably by design, contain only dictionary words. Some of the \"funniest\" names offered were:</p> <ul> <li><code>blenny-godzilla.ts.net</code></li> <li><code>chicken-fujita.ts.net</code></li> <li><code>ocelot-betelgeuse.ts.net</code></li> <li><code>raptor-penny.ts.net</code></li> <li><code>royal-penny.ts.net</code></li> <li><code>risk-truck.ts.net</code></li> <li><code>xantu-lizard.ts.net</code></li> </ul> <p>The tailnet name is not too important, so just pick the first one that isn't too long or hard to spell, then try re-rolling a few times just in a case a better one shows up.</p> <p>MagicDNS being enabled by default, it should be possible to ping or SSH directly to <code>alfred.royal-penny.ts.net</code>, etc.</p> <pre><code>$ ping alfred.royal-penny.ts.net\nPING alfred.royal-penny.ts.net (100.113.110.3) 56(84) bytes of data.\n64 bytes from alfred.royal-penny.ts.net (100.113.110.3): icmp_seq=1 ttl=64 time=5.51 ms\n64 bytes from alfred.royal-penny.ts.net (100.113.110.3): icmp_seq=2 ttl=64 time=4.92 ms\n64 bytes from alfred.royal-penny.ts.net (100.113.110.3): icmp_seq=3 ttl=64 time=5.37 ms\n^C\n--- alfred.royal-penny.ts.net ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2002ms\nrtt min/avg/max/mdev = 4.921/5.264/5.505/0.249 ms\n</code></pre> <p>Note</p> <p>MagicDNS seems to need some time to propagate, so that for several minutes (possibly a few hours) after connecting each host, its name will not resolve.</p> <p>Once DNS has propagated, it is also possible to SSH directly to any host by its FQDN.</p>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#dns-alias-with-cname","title":"DNS alias with <code>CNAME</code>","text":"<p>To make web services more easily available, one can add a <code>CNAME</code> record for a publicly reachable domain to redirect every hostname under a subdomain to a specirfic host (node) in the tailnet, e.g. redirect everything under <code>.alfred.very-very-dark-gray.top</code> to the <code>alfred</code> node, disabling traffic proxying so that this is only a DNS only redirect:</p> <p></p> <p>Unfortunately, this does not support adding a port number, so the services would only be available when listening on standard ports, e.g. when using a <code>LoadBalancer</code> IP in Kubernetes nodes. However, whether it's because a <code>LoadBalancer</code> IP is not the same as the node's IP, or something else, HTTPS connections are rejected on port 443 (but not 22):</p> <pre><code>$ telnet 100.113.110.3 443\nTrying 100.113.110.3...\ntelnet: Unable to connect to remote host: Connection refused\n\n$ telnet 100.113.110.3 22\nTrying 100.113.110.3...\nConnected to 100.113.110.3.\nEscape character is '^]'.\nSSH-2.0-OpenSSH_9.2p1 Debian-2+deb12u5\n^]\ntelnet&gt; \nConnection closed.\n</code></pre> <p>It seems services can only be accessed through <code>NodePort</code> ports, </p> <pre><code>$ curl -k https://kubernetes.alfred.very-very-dark-gray.top/\ncurl: (7) Failed to connect to kubernetes.alfred.very-very-dark-gray.top port 443 after 1 ms: Couldn't connect to server\n\n$ curl 2&gt;/dev/null \\\n  -H \"Host: kubernetes-alfred.very-very-dark-gray.top\" \\\n  -k https://kubernetes.alfred.very-very-dark-gray.top:32035/ \\\n| head\n&lt;!--\nCopyright 2017 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\n</code></pre> <p>However, exposing port 80 and redirecting it to a different <code>NodePort</code>, as needed for automatically renewing Let's Encrypt certificates, does not seem to be possible. HTTPS access should be setup differently, as follows.</p>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#set-up-https-access","title":"Set up HTTPS access","text":"<p>Exposing Kubernetes services throuth Tailscale requires the use of the Kubernetes operator, which involves the following one-time (per node/host) setup and one additional <code>Ingress</code> per service.</p>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#tailscale-kubernetes-operator","title":"Tailscale Kubernetes operator","text":"<p>Securely Exposing Applications on Kubernetes With Tailscale using the Tailscale Kubernetes operator and the Tailscale Ingress Controller enables access over HTTPS with valid (signed) SSL certificates from within your tailnet.</p> <p>Create a <code>tailscale</code> namespace and allow privilege escalation via a namespace label:</p> <pre><code>$ kubectl create namespace tailscale\nnamespace/tailscale created\n\n$ kubectl label namespace tailscale pod-security.kubernetes.io/enforce=privileged\nnamespace/tailscale labeled\n</code></pre> <p>Edit Tailscale ACL to set the following <code>tagOwners</code>:</p> <pre><code>\"tagOwners\": {\n   \"tag:k8s-operator\": [],\n   \"tag:k8s\": [\"tag:k8s-operator\"],\n}\n</code></pre> <p>Create an Oauth Client, with a unique name (e.g. <code>k8s-operator</code> or <code>k8s-operator-alfred</code>) and read/write access to Devices &gt; Core and Keys &gt; Auth Keys, with the <code>k8s-operator</code> tag in both. Copy the Client ID and secret, keep them safe because these will never be provided again. Then install the Tailscale Kubernetes operator with Helm using these:</p> <pre><code>$ helm repo add tailscale https://pkgs.tailscale.com/helmcharts &amp;&amp; helm repo update\n\"tailscale\" has been added to your repositories\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"kubernetes-dashboard\" chart repository\n...Successfully got an update from the \"ingress-nginx\" chart repository\n...Successfully got an update from the \"jetstack\" chart repository\n...Successfully got an update from the \"tailscale\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\n\n$ helm upgrade \\\n  --install \\\n  tailscale-operator \\\n  tailscale/tailscale-operator \\\n  --namespace=tailscale \\\n  --create-namespace \\\n  --set-string oauth.clientId=\"_________________\" \\\n  --set-string oauth.clientSecret=\"tskey-client-_________________-__________________________________\" \\\n  --wait\nRelease \"tailscale-operator\" does not exist. Installing it now.\nNAME: tailscale-operator\nLAST DEPLOYED: Sun Apr 13 16:58:26 2025\nNAMESPACE: tailscale\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>Before proceeding further, enabling HTTPS is required to make Kubernetes accessible over Tailscale Ingress operator. After enabling HTTPS in the Tailscale console, a certificate must be requested from each server by running <code>tailscale cert</code>:</p> <pre><code>$ sudo tailscale cert alfred.royal-penny.ts.net\nWrote public cert to alfred.royal-penny.ts.net.crt\nWrote private key to alfred.royal-penny.ts.net.key\n</code></pre> <p>Using the Tailscale Ingress Controller is now possible by adding a new <code>Ingress</code>, e.g. the Kubernetes dashboard in <code>alfred</code> can be now exposed at https://kubernetes-alfred.royal-penny.ts.net by adding this <code>Ingress</code> in <code>nginx-ingress.yaml</code>:</p> dashboard/nginx-ingress.yaml<pre><code>---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard-ingress-tailscale\n  namespace: kubernetes-dashboard\nspec:\n  defaultBackend:\n    service:\n      name: kubernetes-dashboard-kong-proxy\n      port:\n        number: 443\n  ingressClassName: tailscale\n  tls:\n    - hosts:\n        - kubernetes-alfred\n</code></pre> This new <code>Ingress</code> needs its own unique <code>metadata.name</code>. <p>This new <code>Ingress</code> needs its own unique <code>name</code>, hence the addition of <code>-tailscale</code> to the <code>name</code> above.</p> <p>Otherwise, while DNS takes hours about 2 days to propagate access to the Tailscale Ingress, the previously working setup to access the Ingress for https://kubernetes-alfred.very-very-dark-gray.top/ starts returning 404 Not Found immediately. The reason could not be determined even by adding <code>error-log-level: debug</code> to <code>nginx-baremetal.yaml</code>, but the issue becomes apparent when listing all <code>Ingress</code> before and after adding the Tailscale <code>Ingress</code>:</p> <p>Before: </p><pre><code>$ kubectl get ingress -A\nNAMESPACE              NAME                           CLASS            HOSTS                                                             ADDRESS         PORTS     AGE\nkubernetes-dashboard   kubernetes-dashboard           internal-nginx   localhost                                                                         80, 443   53d\nkubernetes-dashboard   kubernetes-dashboard-ingress   nginx            kubernetes-alfred.very-very-dark-gray.top,k8s.alfred.uu.am        192.168.0.121   80, 443   26d\n</code></pre><p></p> <p>After: </p><pre><code>$ kubectl get ingress -A\nNAMESPACE              NAME                           CLASS            HOSTS       ADDRESS                                 PORTS     AGE\nkubernetes-dashboard   kubernetes-dashboard           internal-nginx   localhost                                           80, 443   53d\nkubernetes-dashboard   kubernetes-dashboard-ingress   tailscale        *           kubernetes-alfred.royal-penny.ts.net    80, 443   26d\n</code></pre><p></p> <p>The <code>nginx</code> (class) <code>Ingress</code> is replaced by the <code>tailscale</code> (class) <code>Ingress</code> and thus https://kubernetes-alfred.very-very-dark-gray.top/ is no longer mapped. Avoid this by using unique <code>metadata.name</code> values within each <code>namespace</code>.</p> <p>Removing the Tailscale Ingress immediately restored access to the previously setup Nginx Ingress the first time, but doing the same 2 days later did not have the same effect; the Dashboard at the Cloudflare subdomain did not became available again, while the Dashboard finally available on the Tailscale subdomain also became unavailable.</p> <p>To make this even worse, removing the Ingress and adding it back later will result in a different Tailscale IP address being assigned to it, so that's another 2 days of waiting until DNS records propagate.</p> <pre><code>$ kubectl apply -f dashboard/nginx-ingress.yaml \ningress.networking.k8s.io/kubernetes-dashboard-ingress unchanged\ningress.networking.k8s.io/kubernetes-dashboard-ingress configured\n\n$ kubectl get ingress -A\nNAMESPACE              NAME                                     CLASS            HOSTS                                                             ADDRESS                                 PORTS     AGE\nkubernetes-dashboard   kubernetes-dashboard                     internal-nginx   localhost                                                                                                 80, 443   53d\nkubernetes-dashboard   kubernetes-dashboard-ingress             nginx            kubernetes-alfred.very-very-dark-gray.top,k8s.alfred.uu.am        192.168.0.121                           80, 443   26d\nkubernetes-dashboard   kubernetes-dashboard-ingress-tailscale   tailscale        *                                                                 kubernetes-alfred.royal-penny.ts.net    80, 443   54s\n</code></pre> <p>After applying this change, the new service <code>kubernetes-alfred</code> shows up in the list of Machines in the Tailscale console:</p> <p></p> <p>Access can be tested with <code>curl</code> (from within the tailnet):</p> <pre><code>$ curl 2&gt;/dev/null \\\n  -k https://kubernetes-alfred.royal-penny.ts.net/ \\\n| head\n&lt;!--\nCopyright 2017 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\n</code></pre> Then again, MagicDNS takes a while to propagate to public DNS. <p>Even from another host in the same tailnet, this test will fail for the first few hours:</p> <pre><code>$ curl -k https://kubernetes-alfred.royal-penny.ts.net/ \ncurl: (6) Could not resolve host: kubernetes-alfred.royal-penny.ts.net\n</code></pre> <p>This is because, even when querying the tailnet DNS server, the new host is not yet resolved:</p> <pre><code>$ dig kubernetes-alfred.royal-penny.ts.net 100.100.100.100  \n\n; &lt;&lt;&gt;&gt; DiG 9.18.30-0ubuntu0.24.04.2-Ubuntu &lt;&lt;&gt;&gt; kubernetes-alfred.royal-penny.ts.net 100.100.100.100\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NXDOMAIN, id: 57292\n;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 65494\n;; QUESTION SECTION:\n;kubernetes-alfred.royal-penny.ts.net. IN A\n</code></pre> <p>In the meantime, only in the tailnet node where the Tailscale Kubernetes operator is running, the new host is already resolved:</p> <pre><code>$ dig kubernetes-alfred.royal-penny.ts.net 100.100.100.100  \n\n; &lt;&lt;&gt;&gt; DiG 9.18.33-1~deb12u2-Debian &lt;&lt;&gt;&gt; kubernetes-alfred.royal-penny.ts.net 100.100.100.100\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 41209\n;; flags: qr aa rd ra ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;kubernetes-alfred.royal-penny.ts.net. IN A\n\n;; ANSWER SECTION:\nkubernetes-alfred.royal-penny.ts.net. 600 IN A 100.74.213.20\n</code></pre> <p>From that host, even querying the default (system) DNS already resolves the new host:</p> <pre><code>$ dig kubernetes-alfred.royal-penny.ts.net\n\n; &lt;&lt;&gt;&gt; DiG 9.18.33-1~deb12u2-Debian &lt;&lt;&gt;&gt; kubernetes-alfred.royal-penny.ts.net\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 22456\n;; flags: qr aa rd ra ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;kubernetes-alfred.royal-penny.ts.net. IN A\n\n;; ANSWER SECTION:\nkubernetes-alfred.royal-penny.ts.net. 600 IN A 100.74.213.20\n</code></pre> <p>Eventually, after 2 days, the new host is finally resolved:</p> <pre><code>$ dig kubernetes-alfred.royal-penny.ts.net\n\n; &lt;&lt;&gt;&gt; DiG 9.18.30-0ubuntu0.24.04.2-Ubuntu &lt;&lt;&gt;&gt; kubernetes-alfred.royal-penny.ts.net\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 10204\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 65494\n;; QUESTION SECTION:\n;kubernetes-alfred.royal-penny.ts.net. IN A\n\n;; ANSWER SECTION:\nkubernetes-alfred.royal-penny.ts.net. 600 IN A 100.74.213.20\n\n;; Query time: 1 msec\n;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)\n;; WHEN: Tue Apr 15 23:54:58 CEST 2025\n;; MSG SIZE  rcvd: 82\n</code></pre> <p>At that point the Kubernetes dashboard is finally accessible at https://kubernetes-alfred.royal-penny.ts.net/ from hosts in the same tailnet:</p> <pre><code>$ curl 2&gt;/dev/null -k https://kubernetes-alfred.royal-penny.ts.net/ | head\n&lt;!--\nCopyright 2017 The Kubernetes Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\n</code></pre>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#private-access-through-invite","title":"Private access through invite","text":"<p>Once a node is reachable by using a public FQDN, other users (e.g. friends and familiy) can be invited through Tailscale to access that one service from their computers or phones.</p>"},{"location":"blog/2025/03/28/remote-access-options-for-self-hosted-services/#public-access-through-funnel","title":"Public access through Funnel","text":"<p>Tailscale Funnel lets you route traffic from the broader internet to a local service, like a web app, for anyone to access\u2014even if they don't use Tailscale. This can be used to expose low-bandwidth sensitive applications over HTTPS, with the caveat that traffic is not protected from abuse as with Cloudflare. Tailscale Funnel are slow by design, so this method only really works well for non-HTTP applications that are low-bandwidth, e.g. SSH or perhaps gaming servers.</p> <p>Exposing a Service to the public internet using Ingress and Tailscale Funnel enables access to local (Kubernetes) services also to clients that are not in the same tailnet, or even in their own tailnet. This is as simple as adding the following annotation to the Tailscale <code>Ingress</code> for a Kubernetes service:</p> dashboard/nginx-ingress.yaml<pre><code>---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    tailscale.com/funnel: \"true\"\n  name: kubernetes-dashboard-ingress-tailscale\n  namespace: kubernetes-dashboard\nspec:\n  defaultBackend:\n    service:\n      name: kubernetes-dashboard-kong-proxy\n      port:\n        number: 443\n  ingressClassName: tailscale\n  tls:\n    - hosts:\n        - kubernetes-alfred\n</code></pre> <p>Before applying this change, Edit Tailscale ACL to allow Kubernetes Operator proxy services to use Tailscale Funnel:</p> <ol> <li>Expand the Funnel section and select Add Funnel to policy.</li> <li>Edit the <code>nodeAttrs</code> to allow nodes created by the Kubernetes operator to use Funnel:</li> </ol> <pre><code>  \"nodeAttrs\": [\n    {\n      // Funnel policy, which lets tailnet members control Funnel\n      // for their own devices.\n      // Learn more at https://tailscale.com/kb/1223/tailscale-funnel/\n      \"target\": [\"tag:k8s\"], // tag that Tailscale Operator uses to tag proxies; defaults to 'tag:k8s'\n      \"attr\":   [\"funnel\"],\n    },\n  ],\n</code></pre> <p>Then applying the change to the <code>Ingress</code> manifest:</p> <pre><code>$ kubectl apply -f dashboard/nginx-ingress.yaml \ningress.networking.k8s.io/kubernetes-dashboard-ingress unchanged\ningress.networking.k8s.io/kubernetes-dashboard-ingress configured\n</code></pre> <p>Once DNS has propagated to resolve https://kubernetes-alfred.royal-penny.ts.net to the machine's Tailscale IP address (<code>100.74.213.20</code>); enabling the <code>funnel</code> annotation and applying the change results in yet another Tailscale IP address begin assigned to it, so there we go to wait for DNS to propagate again to test access using this method.</p> <p>Eventually, after another couple of days for DNS to propagate, the Kubernetes dashboard is available at https://kubernetes-alfred.royal-penny.ts.net also for computers not in the same tailnet.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/","title":"Kubernetes homelab server with Ubuntu Server 24.04 (octavo)","text":"<p>Need. More. Server. Need. More. POWER!!!</p> <p>Just a bit more, maybe quite a bit more, to run services that are more CPU-intensive than those already running in the current  Single-node Kubernetes cluster on an Intel NUC: lexicon.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#hardware","title":"Hardware","text":"<p>Ubuntu's list of Recommended and Certified Hardware is just as short and ancient as it was 2 years ago, but NUC systems have a now long history of being well suppored under Linux, so this time I'm taking chances:</p> <ul> <li>ASUS NUC 13 Pro Tall PC Kit RNUC13ANHI700000I w/ Intel Core i7-1360P ($570)</li> <li>Kingston FURY Impact 1x 32GB, 3200 MHz, DDR4-RAM, SODIMM ($110)</li> <li>Kingston FURY Renegade 4000 GB, M.2 2280 ($250)</li> </ul>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#bootable-usb-stick","title":"Bootable USB stick","text":"<p>Get Ubuntu Server (24.04.2 LTS) and  create a bootable USB stick on Ubuntu.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#install-ubuntu-server-2404","title":"Install Ubuntu Server 24.04","text":"<p>Installing Ubuntu Server 24.04 went smoothly and without any problems, the NUC booted from the USB stick and secure boot, enabled by default, never presented any problem. Screen and USB keyword worked seamlessly through a DisplayPort+USB+Ethernet Cable Matters Hub on a Thunderbolt port in the NUC.</p> <p>Once the intaller boots, the installation steps were:</p> <ol> <li>Choose language and keyboard layout.</li> <li>Choose Ubuntu Server (default, not (minimized)).<ul> <li>Checked the option to Search for third-party drivers.</li> </ul> </li> <li>Networking: DHCP on wired network.<ul> <li>IP address <code>.155</code> is assgined to the <code>enx5c857e3e1129</code> interface, the    RTL8153 Gigabit Ethernet NIC in the Cable Matters Hub.</li> <li>The <code>enp86s0</code> interface is the NUC's integrated 2.5Gbps NIC (Intel I226-V),    which during the installation had no network cable attached.</li> </ul> </li> <li>Pick a local Ubuntu mirror to install packages from.</li> <li>Setup a Custom storage layout as follows<ol> <li>Select the disk (Kingston FURY Renegade 4000 GB) to Use As Boot Device.     This automatically creates a 1GB partition for <code>/boot/efi</code> (formatted as <code>btrfs</code>).</li> <li>Create a 60G partition to mount as <code>/</code> (formatted as <code>btrfs</code>).</li> <li>Create a 60G partition to reverse for a future OS.</li> <li>Create a 60G partition to mount as <code>/var/lib</code> (formatted as <code>btrfs</code>).</li> <li>Create a  partition using all remaining space (3.46T) to mount as <code>/home</code>     (formatted as <code>btrfs</code>).</li> </ol> </li> <li>Confirm partitions &amp; changes.</li> <li>Set up a Profile: username (<code>ponder</code>), hostname (<code>octavo</code>) and password.</li> <li>Skip Upgrade to Ubuntu Pro (to be done later).</li> <li>Install OpenSSH server and allow password authentication (for now).</li> <li>A selection of snap packages is available at this point, none were selected.</li> <li>Confirm all previous choices and start to install software.</li> <li>Once the installation is complete, remove the UBS stick and hit Enter to reboot.</li> </ol>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#disable-swap","title":"Disable swap","text":"<p>With 32 GB on a single DIMM and the possiblity to double that in the future, there is no need for <code>swap</code> and it can be problematic for Kubernetes later, so this should be disabled: remove the relevant line in <code>/etc/fstab</code>, reboot and delete the swap file (typically <code>/swap.img</code> or <code>/swapfile</code>).</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#tweak-openssh-server","title":"Tweak OpenSSH server","text":"<p>Set the <code>root</code> password by first escalating with <code>sudo su -</code> and then using the <code>passwd</code> command to set the password. This password will hardly ever be used, but should be set so that it can be use in case of emergency.</p> <p>Copy SSH public keys into the <code>.ssh/authorized_keys</code> of both <code>ponder</code> and <code>root</code>, then disable password authentication in the OpenSSH server:</p> <pre><code># vi /etc/ssh/sshd_config\nPasswordAuthentication no\n# systemctl restart ssh\n</code></pre> <p>Test SSH connections as both <code>ponder</code> and <code>root</code> from all the relevant hosts in the LAN.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#setup-fail2ban","title":"Setup Fail2Ban","text":"<p>On top of disabling password authentication, the SSH server will be less busy is those pesky bad actors are blocked from reaching it's port. To do this, install fail2ban:</p> <code>apt install fail2ban -y</code> <pre><code># apt install fail2ban -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  python3-pyasyncore python3-pyinotify whois\nSuggested packages:\n  mailx monit sqlite3 python-pyinotify-doc\nThe following NEW packages will be installed:\n  fail2ban python3-pyasyncore python3-pyinotify whois\n0 upgraded, 4 newly installed, 0 to remove and 1 not upgraded.\nNeed to get 496 kB of archives.\nAfter this operation, 2,572 kB of additional disk space will be used.\nGet:1 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 python3-pyasyncore all 1.0.2-2 [10.1 kB]\nGet:2 http://ch.archive.ubuntu.com/ubuntu noble-updates/universe amd64 fail2ban all 1.0.2-3ubuntu0.1 [409 kB]\nGet:3 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 python3-pyinotify all 0.9.6-2ubuntu1 [25.0 kB]\nGet:4 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 whois amd64 5.5.22 [51.7 kB]\nFetched 496 kB in 0s (2,059 kB/s) \nSelecting previously unselected package python3-pyasyncore.\n(Reading database ... 86910 files and directories currently installed.)\nPreparing to unpack .../python3-pyasyncore_1.0.2-2_all.deb ...\nUnpacking python3-pyasyncore (1.0.2-2) ...\nSelecting previously unselected package fail2ban.\nPreparing to unpack .../fail2ban_1.0.2-3ubuntu0.1_all.deb ...\nUnpacking fail2ban (1.0.2-3ubuntu0.1) ...\nSelecting previously unselected package python3-pyinotify.\nPreparing to unpack .../python3-pyinotify_0.9.6-2ubuntu1_all.deb ...\nUnpacking python3-pyinotify (0.9.6-2ubuntu1) ...\nSelecting previously unselected package whois.\nPreparing to unpack .../whois_5.5.22_amd64.deb ...\nUnpacking whois (5.5.22) ...\nSetting up whois (5.5.22) ...\nSetting up python3-pyasyncore (1.0.2-2) ...\nSetting up fail2ban (1.0.2-3ubuntu0.1) ...\n/usr/lib/python3/dist-packages/fail2ban/tests/fail2banregextestcase.py:224: SyntaxWarning: invalid escape sequence '\\s'\n  \"1490349000 test failed.dns.ch\", \"^\\s*test &lt;F-ID&gt;\\S+&lt;/F-ID&gt;\"\n/usr/lib/python3/dist-packages/fail2ban/tests/fail2banregextestcase.py:435: SyntaxWarning: invalid escape sequence '\\S'\n  '^'+prefix+'&lt;F-ID&gt;User &lt;F-USER&gt;\\S+&lt;/F-USER&gt;&lt;/F-ID&gt; not allowed\\n'\n/usr/lib/python3/dist-packages/fail2ban/tests/fail2banregextestcase.py:443: SyntaxWarning: invalid escape sequence '\\S'\n  '^'+prefix+'User &lt;F-USER&gt;\\S+&lt;/F-USER&gt; not allowed\\n'\n/usr/lib/python3/dist-packages/fail2ban/tests/fail2banregextestcase.py:444: SyntaxWarning: invalid escape sequence '\\d'\n  '^'+prefix+'Received disconnect from &lt;F-ID&gt;&lt;ADDR&gt; port \\d+&lt;/F-ID&gt;'\n/usr/lib/python3/dist-packages/fail2ban/tests/fail2banregextestcase.py:451: SyntaxWarning: invalid escape sequence '\\s'\n  _test_variants('common', prefix=\"\\s*\\S+ sshd\\[&lt;F-MLFID&gt;\\d+&lt;/F-MLFID&gt;\\]:\\s+\")\n/usr/lib/python3/dist-packages/fail2ban/tests/fail2banregextestcase.py:537: SyntaxWarning: invalid escape sequence '\\['\n  'common[prefregex=\"^svc\\[&lt;F-MLFID&gt;\\d+&lt;/F-MLFID&gt;\\] connect &lt;F-CONTENT&gt;.+&lt;/F-CONTENT&gt;$\"'\n/usr/lib/python3/dist-packages/fail2ban/tests/servertestcase.py:1375: SyntaxWarning: invalid escape sequence '\\s'\n  \"`{ nft -a list chain inet f2b-table f2b-chain | grep -oP '@addr-set-j-w-nft-mp\\s+.*\\s+\\Khandle\\s+(\\d+)$'; } | while read -r hdl; do`\",\n/usr/lib/python3/dist-packages/fail2ban/tests/servertestcase.py:1378: SyntaxWarning: invalid escape sequence '\\s'\n  \"`{ nft -a list chain inet f2b-table f2b-chain | grep -oP '@addr6-set-j-w-nft-mp\\s+.*\\s+\\Khandle\\s+(\\d+)$'; } | while read -r hdl; do`\",\n/usr/lib/python3/dist-packages/fail2ban/tests/servertestcase.py:1421: SyntaxWarning: invalid escape sequence '\\s'\n  \"`{ nft -a list chain inet f2b-table f2b-chain | grep -oP '@addr-set-j-w-nft-ap\\s+.*\\s+\\Khandle\\s+(\\d+)$'; } | while read -r hdl; do`\",\n/usr/lib/python3/dist-packages/fail2ban/tests/servertestcase.py:1424: SyntaxWarning: invalid escape sequence '\\s'\n  \"`{ nft -a list chain inet f2b-table f2b-chain | grep -oP '@addr6-set-j-w-nft-ap\\s+.*\\s+\\Khandle\\s+(\\d+)$'; } | while read -r hdl; do`\",\nCreated symlink /etc/systemd/system/multi-user.target.wants/fail2ban.service \u2192 /usr/lib/systemd/system/fail2ban.service.\nSetting up python3-pyinotify (0.9.6-2ubuntu1) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nScanning processes...                                                                                         \nScanning processor microcode...                                                                               \nScanning linux images...                                                                                      \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\n</code></pre> <p>Enable the service so it starts after each system restart:</p> <pre><code># systemctl enable --now fail2ban\nSynchronizing state of fail2ban.service with SysV service script with /usr/lib/systemd/systemd-sysv-install.\n</code></pre> <p>The default configuration should be enough because this system will probably not expose its port 22 to the Internet, but instead accessed remotely via Tailscale. If port 22 is later expoed to the Internet, see lexicon's Fail2ban setup.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#tweak-network-config","title":"Tweak network config","text":"<p>This system will use static IP addresses <code>.8</code> so they can be added to the <code>/etc/hosts</code> file of the relevant hosts in the LAN, then the file can be copied into the new server, preserving the line that points <code>127.0.0.1</code> to itself.</p> <p>While using the Cable Matters hub, the LAN IP address is setup on the <code>enx5c857e3e1129</code> interface:</p> <pre><code># ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n       valid_lft forever preferred_lft forever\n2: enp86s0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether 48:21:0b:6d:3e:9b brd ff:ff:ff:ff:ff:ff\n3: enx5c857e3e1129: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 5c:85:7e:3e:11:29 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.0.155/24 metric 100 brd 192.168.0.255 scope global dynamic enx5c857e3e1129\n       valid_lft 85920sec preferred_lft 85920sec\n    inet6 fe80::5e85:7eff:fe3e:1129/64 scope link \n       valid_lft forever preferred_lft forever\n4: wlo1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether d0:65:78:a5:8b:dd brd ff:ff:ff:ff:ff:ff\n    altname wlp0s20f3\n</code></pre> <code>dmesg</code> lines for Intel I226-V 2.5Gbps NIC (<code>igc</code>) and USB NIC (<code>r8152</code>): <pre><code>[    1.736823] Intel(R) 2.5G Ethernet Linux Driver\n[    1.738154] Copyright(c) 2018 Intel Corporation.\n[    1.741460] igc 0000:56:00.0: enabling device (0000 -&gt; 0002)\n[    1.744312] igc 0000:56:00.0: PTM enabled, 4ns granularity\n[    1.756421] ahci 0000:00:17.0: version 3.0\n[    1.756759] xhci_hcd 0000:00:0d.0: xHCI Host Controller\n--\n[    1.821412] intel-lpss 0000:00:15.1: enabling device (0004 -&gt; 0006)\n[    1.823953] idma64 idma64.1: Found Intel integrated DMA 64-bit\n[    1.842492] igc 0000:56:00.0: 4.000 Gb/s available PCIe bandwidth (5.0 GT/s PCIe x1 link)\n[    1.843094] igc 0000:56:00.0 eth0: MAC: 48:21:0b:6d:3e:9b\n[    2.056525] typec port0: bound usb3-port6 (ops connector_ops)\n[    2.058975] typec port0: bound usb2-port1 (ops connector_ops)\n[    2.061054] usb 3-1: new full-speed USB device number 2 using xhci_hcd\n[    2.086270] ata2: SATA link down (SStatus 4 SControl 300)\n[    2.092729] igc 0000:56:00.0 enp86s0: renamed from eth0\n[    2.100868] ucsi_acpi USBC000:00: UCSI_GET_PDOS failed (-95)\n[    2.114965] nvme 0000:01:00.0: platform quirk: setting simple suspend\n[    2.116031] nvme nvme0: pci function 0000:01:00.0\n[    2.125670] nvme nvme0: Shutdown timeout set to 10 seconds\n[    2.129358] nvme nvme0: 16/0/0 default/read/poll queues\n--\n[    4.164720] usbcore: registered new device driver r8152-cfgselector\n[    4.352207] r8152-cfgselector 3-6.2: reset high-speed USB device number 6 using xhci_hcd\n[    4.581005] r8152 3-6.2:1.0: load rtl8153a-4 v2 02/07/20 successfully\n[    4.621215] r8152 3-6.2:1.0 eth0: v1.12.13\n[    4.624728] usbcore: registered new interface driver r8152\n[    4.660226] usbcore: registered new interface driver cdc_ether\n[    4.669749] r8152 3-6.2:1.0 enx5c857e3e1129: renamed from eth0\n[    4.720488] usb 3-6.4.1: new high-speed USB device number 8 using xhci_hcd\n[    4.808357] raid6: avx2x4   gen() 29500 MB/s\n[    4.825355] raid6: avx2x2   gen() 36875 MB/s\n[    4.842356] raid6: avx2x1   gen() 35157 MB/s\n[    4.842362] raid6: using algorithm avx2x2 gen() 36875 MB/s\n</code></pre> <p>Servers are better setup with static IP addresses in a know range, and for this system the <code>.8</code>addresses have been reserved and can be setup in the Netplan configuration:</p> /etc/netplan/50-cloud-init.yaml<pre><code># Dual static IP on LAN, nothing else.\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp86s0:\n      dhcp4: no\n      dhcp6: no\n      # Ser IP address &amp; subnet mask\n      addresses: [ 10.0.0.8/24, 192.168.0.8/24 ]\n    enx5c857e3e1129:\n      dhcp4: no\n      dhcp6: no\n      # Ser IP address &amp; subnet mask\n      addresses: [ 10.0.0.9/24, 192.168.0.9/24 ]\n      # Set default gateway\n      routes:\n       - to: default\n         via: 192.168.0.1\n      # Set DNS name servers\n      nameservers:\n        addresses: [ 77.109.128.2, 213.144.129.20 ]\n</code></pre> <p>Note</p> <p>Adding <code>routes</code> to both Ethernet interfaces will result in warnings, and is not necessary so long as one can SSH in from another host in the LAN.</p> <p>After applying the configuration with <code>netplan apply</code> both Ethernet NICs will always have static IP addresses:</p> <pre><code># ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n       valid_lft forever preferred_lft forever\n2: enp86s0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc mq state DOWN group default qlen 1000\n    link/ether 48:21:0b:6d:3e:9b brd ff:ff:ff:ff:ff:ff\n3: enx5c857e3e1129: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 5c:85:7e:3e:11:29 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.7/24 brd 10.0.0.255 scope global enx5c857e3e1129\n       valid_lft forever preferred_lft forever\n    inet 192.168.0.7/24 brd 192.168.0.255 scope global enx5c857e3e1129\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5e85:7eff:fe3e:1129/64 scope link \n       valid_lft forever preferred_lft forever\n4: wlo1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether d0:65:78:a5:8b:dd brd ff:ff:ff:ff:ff:ff\n    altname wlp0s20f3\n</code></pre> <p>The server can now be relocated to the 2.5Gbps network, where no screen is available. Once relocated to the 2.5Gbps network, no longer using the Cable Matters hub, and finished booting, SSH back into the server using the <code>.8</code> address:</p> <pre><code># ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n       valid_lft forever preferred_lft forever\n2: enp86s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 48:21:0b:6d:3e:9b brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.8/24 brd 10.0.0.255 scope global enp86s0\n       valid_lft forever preferred_lft forever\n    inet 192.168.0.8/24 brd 192.168.0.255 scope global enp86s0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::4a21:bff:fe6d:3e9b/64 scope link \n       valid_lft forever preferred_lft forever\n3: wlo1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\n    link/ether d0:65:78:a5:8b:dd brd ff:ff:ff:ff:ff:ff\n    altname wlp0s20f3\n</code></pre> <p>To re-enable the server's access to the Internet, move the <code>routes</code> to the <code>enp86s0</code> interface and <code>netplan apply</code> once again:</p> /etc/netplan/50-cloud-init.yaml<pre><code># Dual static IP on LAN, nothing else.\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp86s0:\n      dhcp4: no\n      dhcp6: no\n      # Ser IP address &amp; subnet mask\n      addresses: [ 10.0.0.8/24, 192.168.0.8/24 ]\n      # Set default gateway\n      routes:\n       - to: default\n         via: 192.168.0.1\n      # Set DNS name servers\n      nameservers:\n        addresses: [ 77.109.128.2, 213.144.129.20 ]\n    enx5c857e3e1129:\n      dhcp4: no\n      dhcp6: no\n      # Ser IP address &amp; subnet mask\n      addresses: [ 10.0.0.9/24, 192.168.0.9/24 ]\n</code></pre> <p>At this point the network configuration is finalized.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#set-correct-timezone","title":"Set correct timezone","text":"<p>The installation process does not offer setting the system timezone and defaults to UTC:</p> <pre><code># timedatectl\n               Local time: Sun 2025-04-13 20:47:28 UTC\n           Universal time: Sun 2025-04-13 20:47:28 UTC\n                 RTC time: Sun 2025-04-13 20:47:28\n                Time zone: Etc/UTC (UTC, +0000)\nSystem clock synchronized: yes\n              NTP service: active\n          RTC in local TZ: no\n</code></pre> <p>Since this is not the local timezone anywhere, it is better (more convenient) to set the local timezone, e.g.</p> <pre><code># timedatectl set-timezone \"Europe/Amsterdam\"\n\n# timedatectl\n               Local time: Sun 2025-04-13 22:48:45 CEST\n           Universal time: Sun 2025-04-13 20:48:45 UTC\n                 RTC time: Sun 2025-04-13 20:48:45\n                Time zone: Europe/Amsterdam (CEST, +0200)\nSystem clock synchronized: yes\n              NTP service: active\n          RTC in local TZ: no\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#update-system-packages","title":"Update system packages","text":"<p>Updating system packages is recommended after installing from USB media:</p> <pre><code># apt update &amp;&amp; apt full-upgrade -y\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#tweak-bash-prompt","title":"Tweak Bash prompt","text":"<p>Tweak Bash prompt for <code>root</code> to make user name red, host name blue and path green; with this in <code>.bashrc</code>:</p> .bashrc<pre><code># uncomment for a colored prompt, if the terminal has the capability; turned\n# off by default to not distract the user: the focus in a terminal window\n# should be on the output of commands, not on the prompt\nforce_color_prompt=yes\n\nif [ -n \"$force_color_prompt\" ]; then\n    if [ -x /usr/bin/tput ] &amp;&amp; tput setaf 1 &gt;&amp;/dev/null; then\n        # We have color support; assume it's compliant with Ecma-48\n        # (ISO/IEC-6429). (Lack of such support is extremely rare, and such\n        # a case would tend to support setf rather than setaf.)\n        color_prompt=yes\n    else\n        color_prompt=\n    fi\nfi\n\nif [ \"$color_prompt\" = yes ]; then\n    PS1='\\[\\e]0;\\u@\\h: \\w\\a\\]${debian_chroot:+($debian_chroot)}\\[\\033[01;31m\\]\\u@\\[\\033[01;34m\\]\\h\\[\\033[00m\\] \\[\\033[01;32m\\]\\w \\$\\[\\033[00m\\] '\nelse\n    PS1='${debian_chroot:+($debian_chroot)}\\u@\\h \\w \\$ '\nfi\nunset color_prompt force_color_prompt\n</code></pre> <p>Other users' Bash prompt is left as default, which renders all green. The idea is that <code>root</code>'s prompt is visually different, to remind me that with great power comes great responsibility.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#upgrade-to-ubuntu-pro","title":"Upgrade to Ubuntu Pro","text":"<p>This step was skipped during the installation process so the server is not attached to an Ubuntu Pro account:</p> <pre><code># pro security-status\n684 packages installed:\n    684 packages from Ubuntu Main/Restricted repository\n\nTo get more information about the packages, run\n    pro security-status --help\nfor a list of available options.\n\nThis machine is receiving security patching for Ubuntu Main/Restricted\nrepository until 2029.\nThis machine is NOT attached to an Ubuntu Pro subscription.\n\nUbuntu Pro with 'esm-infra' enabled provides security updates for\nMain/Restricted packages until 2034.\n\nTry Ubuntu Pro with a free personal subscription on up to 5 machines.\nLearn more at https://ubuntu.com/pro\n</code></pre> <p>Take the command and token from https://ubuntu.com/pro/dashboard and attach the server to an active Ubuntu Pro account:</p> <pre><code># pro attach ______________________________\nEnabling Ubuntu Pro: ESM Apps\nUbuntu Pro: ESM Apps enabled\nEnabling Ubuntu Pro: ESM Infra\nUbuntu Pro: ESM Infra enabled\nEnabling Livepatch\nLivepatch enabled\nThis machine is now attached to 'Ubuntu Pro - free personal subscription'\n\nSERVICE          ENTITLED  STATUS       DESCRIPTION\nanbox-cloud      yes       disabled     Scalable Android in the cloud\nesm-apps         yes       enabled      Expanded Security Maintenance for Applications\nesm-infra        yes       enabled      Expanded Security Maintenance for Infrastructure\nlandscape        yes       disabled     Management and administration tool for Ubuntu\nlivepatch        yes       enabled      Canonical Livepatch service\nrealtime-kernel* yes       disabled     Ubuntu kernel with PREEMPT_RT patches integrated\nusg              yes       disabled     Security compliance and audit tools\n\n * Service has variants\n\nNOTICES\nOperation in progress: pro attach\n\nFor a list of all Ubuntu Pro services and variants, run 'pro status --all'\nEnable services with: pro enable &lt;service&gt;\n\n     Account: ponder.stibbons@uu.am\nSubscription: Ubuntu Pro - free personal subscription\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#weekly-btrfs-scrub","title":"Weekly btrfs scrub","text":"<p>To keep BTRFS file systems healthy, it is recommended to run a weekly scrub to check everything for consistency. For this, run the script from <code>crontab</code> every Saturday morning, early enough that it will be done by the time anyone wakes up:</p> <code>/usr/local/bin/btrfs-scrub-all</code> <pre><code>#! /bin/bash\n\n# By Marc MERLIN &lt;marc_soft@merlins.org&gt; 2014/03/20\n# License: Apache-2.0\n# http://marc.merlins.org/perso/btrfs/post_2014-03-19_Btrfs-Tips_-Btrfs-Scrub-and-Btrfs-Filesystem-Repair.html\n\nwhich btrfs &gt;/dev/null || exit 0\nexport PATH=/usr/local/bin:/sbin:$PATH\n\nFILTER='(^Dumping|balancing, usage)'\ntest -n \"$DEVS\" || DEVS=$(grep '\\&lt;btrfs\\&gt;' /proc/mounts | awk '{ print $1 }' | sort -u)\nfor btrfs in $DEVS\ndo\n    tail -n 0 -f /var/log/syslog | grep -i \"BTRFS\" | grep -Evi '(disk space caching is enabled|unlinked .* orphans|turning on discard|device label .* devid .* transid|enabling SSD mode|BTRFS: has skinny extents|BTRFS: device label)' &amp;\n    mountpoint=\"$(grep \"$btrfs\" /proc/mounts | awk '{ print $2 }' | sort | head -1)\"\n    logger -s \"Quick Metadata and Data Balance of $mountpoint ($btrfs)\" &gt;&amp;2\n    # Even in 4.3 kernels, you can still get in places where balance\n    # won't work (no place left, until you run a -m0 one first)\n    btrfs balance start -musage=0 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    btrfs balance start -musage=20 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    # After metadata, let's do data:\n    btrfs balance start -dusage=0 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    btrfs balance start -dusage=20 -v $mountpoint 2&gt;&amp;1 | grep -Ev \"$FILTER\"\n    # And now we do scrub. Note that scrub can fail with \"no space left\n    # on device\" if you're very out of balance.\n    logger -s \"Starting scrub of $mountpoint\" &gt;&amp;2\n    echo btrfs scrub start -Bd $mountpoint\n    ionice -c 3 nice -10 btrfs scrub start -Bd $mountpoint\n    pkill -f 'tail -n 0 -f /var/log/syslog'\n    logger \"Ended scrub of $mountpoint\" &gt;&amp;2\ndone\n</code></pre> <pre><code># crontab -l | grep btrfs\n# m h  dom mon dow   command\n50 5 * * 6 /usr/local/bin/btrfs-scrub-all\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#stop-apparmor-spew-in-the-logs","title":"Stop <code>apparmor</code> spew in the logs","text":"<p>As seen in previous installs of Ubuntu 24.04 on Rapture and Raven, there is some log spam from <code>audit</code> in the <code>dmesg</code> logs. To stop this  simply install <code>auditd</code>:</p> <code>apt install auditd</code> <pre><code># apt install auditd -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  libauparse0t64\nSuggested packages:\n  audispd-plugins\nThe following NEW packages will be installed:\n  auditd libauparse0t64\n0 upgraded, 2 newly installed, 0 to remove and 1 not upgraded.\nNeed to get 274 kB of archives.\nAfter this operation, 893 kB of additional disk space will be used.\nGet:1 http://ch.archive.ubuntu.com/ubuntu noble-updates/main amd64 libauparse0t64 amd64 1:3.1.2-2.1build1.1 [58.9 kB]\nGet:2 http://ch.archive.ubuntu.com/ubuntu noble-updates/main amd64 auditd amd64 1:3.1.2-2.1build1.1 [215 kB]\nFetched 274 kB in 0s (1,148 kB/s)\nSelecting previously unselected package libauparse0t64:amd64.\n(Reading database ... 87398 files and directories currently installed.)\nPreparing to unpack .../libauparse0t64_1%3a3.1.2-2.1build1.1_amd64.deb ...\nAdding 'diversion of /lib/x86_64-linux-gnu/libauparse.so.0 to /lib/x86_64-linux-gnu/libauparse.so.0.usr-is-merged by libauparse0t64'\nAdding 'diversion of /lib/x86_64-linux-gnu/libauparse.so.0.0.0 to /lib/x86_64-linux-gnu/libauparse.so.0.0.0.usr-is-merged by libauparse0t64'\nUnpacking libauparse0t64:amd64 (1:3.1.2-2.1build1.1) ...\nSelecting previously unselected package auditd.\nPreparing to unpack .../auditd_1%3a3.1.2-2.1build1.1_amd64.deb ...\nUnpacking auditd (1:3.1.2-2.1build1.1) ...\nSetting up libauparse0t64:amd64 (1:3.1.2-2.1build1.1) ...\nSetting up auditd (1:3.1.2-2.1build1.1) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/auditd.service \u2192 /usr/lib/systemd/system/auditd.service.\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.4) ...\nScanning processes...                                                                                         \nScanning processor microcode...                                                                               \nScanning linux images...                                                                                      \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#nas-nfs-mount","title":"NAS NFS mount","text":"<p>Add the NFS mount in <code>/etc/fstab</code> to mount <code>/home/nas</code>, create the directory and mount it.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#continuous-monitoring","title":"Continuous Monitoring","text":"<p>Install Continuous Monitoring and report metrics to <code>lexicon</code> on its <code>NodePort</code> (30086).</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#remote-access","title":"Remote Access","text":"<p>Remote access options for self-hosted services may no longer require opening any ports in the router for this server, although that option remains available should it become necessary.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#cloudflare-tunnel","title":"Cloudflare Tunnel","text":"<p>Cloudflare Tunnels in <code>alfred</code> proved to be a good solution for making web sites externally available but still protected behind SSO with Zero Trust Web Access. Since all that was already setup, and there are no services running in this server yet, all there is to do here and now is just install <code>cloudflared</code> and join.</p> <p>Install the latest <code>cloudflared</code> using the instructions provided for Ubuntu 24.04:</p> <pre><code># curl -fsSL https://pkg.cloudflare.com/cloudflare-main.gpg \\\n  | tee /usr/share/keyrings/cloudflare-main.gpg &gt;/dev/null\n\n# echo 'deb [signed-by=/usr/share/keyrings/cloudflare-main.gpg] https://pkg.cloudflare.com/cloudflared noble main' \\\n  | tee /etc/apt/sources.list.d/cloudflared.list\n\n# install cloudflared\nsudo apt-get update &amp;&amp; sudo apt-get install cloudflared\n</code></pre> <p>Then run the command to connect to the new tunnel:</p> <pre><code># cloudflared service install eyJhIjoiMD...\n2025-04-18T21:45:27Z INF Using Systemd\n2025-04-18T21:45:27Z INF Linux service for cloudflared installed successfully\n</code></pre> <p>Although a public hostname is not yet necessary for this tunnel, create one for https://kubernetes-octavo.very-very-dark-gray.top/ to be used later for the Kubernetes Dashboard, just pointing to https://localhost for now. Make sure to enable the TLS option No TLS Verify, so the tunnel can be used without HTTPS certificates since they do not seem to be necessary when using Cloudflare tunnels.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#tailscale","title":"Tailscale","text":"<p>Install Tailscale and connect the server to the alredy existing tailnet:</p> Installation of Tailscale on octavo <pre><code># curl -fsSL https://tailscale.com/install.sh | sh\nInstalling Tailscale for ubuntu noble, using method apt\n+ mkdir -p --mode=0755 /usr/share/keyrings\n+ curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/noble.noarmor.gpg\n+ tee /usr/share/keyrings/tailscale-archive-keyring.gpg\n+ chmod 0644 /usr/share/keyrings/tailscale-archive-keyring.gpg\n+ curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/noble.tailscale-keyring.list\n+ tee /etc/apt/sources.list.d/tailscale.list\n# Tailscale packages for ubuntu noble\ndeb [signed-by=/usr/share/keyrings/tailscale-archive-keyring.gpg] https://pkgs.tailscale.com/stable/ubuntu noble main\n+ chmod 0644 /etc/apt/sources.list.d/tailscale.list\n+ apt-get update\nHit:1 http://ch.archive.ubuntu.com/ubuntu noble InRelease\nGet:2 http://ch.archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]                                   \nHit:3 https://pkg.cloudflare.com/cloudflared noble InRelease                                                 \nHit:4 http://ch.archive.ubuntu.com/ubuntu noble-backports InRelease                                          \nHit:5 http://security.ubuntu.com/ubuntu noble-security InRelease                                             \nGet:6 https://pkgs.tailscale.com/stable/ubuntu noble InRelease                                            \nGet:7 https://pkgs.tailscale.com/stable/ubuntu noble/main all Packages [354 B]\nGet:8 https://pkgs.tailscale.com/stable/ubuntu noble/main amd64 Packages [12.8 kB]\nGet:9 https://esm.ubuntu.com/apps/ubuntu noble-apps-security InRelease [7,595 B]\nGet:10 https://esm.ubuntu.com/apps/ubuntu noble-apps-updates InRelease [7,480 B]\nGet:11 https://esm.ubuntu.com/infra/ubuntu noble-infra-security InRelease [7,474 B]\nGet:12 https://esm.ubuntu.com/infra/ubuntu noble-infra-updates InRelease [7,473 B]\nFetched 176 kB in 3s (64.1 kB/s)      \nReading package lists... Done\n+ apt-get install -y tailscale tailscale-archive-keyring\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tailscale tailscale-archive-keyring\n0 upgraded, 2 newly installed, 0 to remove and 1 not upgraded.\nNeed to get 31.5 MB of archives.\nAfter this operation, 59.1 MB of additional disk space will be used.\nGet:2 https://pkgs.tailscale.com/stable/ubuntu noble/main all tailscale-archive-keyring all 1.35.181 [3,082 B]\nGet:1 https://pkgs.tailscale.com/stable/ubuntu noble/main amd64 tailscale amd64 1.82.5 [31.5 MB]\nFetched 31.5 MB in 13s (2,462 kB/s)                                                                          \nSelecting previously unselected package tailscale.\n(Reading database ... 125870 files and directories currently installed.)\nPreparing to unpack .../tailscale_1.82.5_amd64.deb ...\nUnpacking tailscale (1.82.5) ...\nSelecting previously unselected package tailscale-archive-keyring.\nPreparing to unpack .../tailscale-archive-keyring_1.35.181_all.deb ...\nUnpacking tailscale-archive-keyring (1.35.181) ...\nSetting up tailscale-archive-keyring (1.35.181) ...\nSetting up tailscale (1.82.5) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/tailscaled.service \u2192 /usr/lib/systemd/system/tailscaled.service.\nScanning processes...                                                                                         \nScanning processor microcode...                                                                               \nScanning linux images...                                                                                      \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\n+ [ false = true ]\n+ set +x\nInstallation complete! Log in to start using Tailscale by running:\n\ntailscale up\n</code></pre> <p>After installing the software, running <code>tailscale up</code> will provide a URL to  authenticate and add the server to the tailnet:</p> <pre><code># tailscale up\n\nTo authenticate, visit:\n\n        https://login.tailscale.com/a/______________\n\nSuccess.\n</code></pre> <p>Once added to the tailnet, an SSH connection to <code>octavo.royal-penny.ts.net</code> instantly connects to <code>octavo</code> and SSH key authentication just works (after accepting this new hostname).</p> <p>Additional setup will be needed later on, once services are running on Kubernetes:</p> <ol> <li>Install the Tailscale Kubernetes operator     and add an <code>Ingress</code> to use it.</li> <li>Optional: enable Public access through Funnel     for services that need to be accessible from outside the tailnet.</li> </ol>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes on Raspberry Pi 5 (<code>alfred</code>) showed quite a few new hurdles caused by newer versions of Kubernetes (v1.32.2) and a few components; this most recent installation will be the main guide this time. On top of that, Applications Installed and each application-specific post linked from there, will provide most of the guidance to install those applications to be migrated (here) to the new server. The old Single-node Kubernetes cluster on an Intel NUC: lexicon may not add much at this point.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#github-repository","title":"GitHub Repository","text":"<p>Use the same GitHub Repository as before and create a new directory for the new server:</p> <pre><code>$ git clone git@github.com:xxxx/kubernetes-deployments.git\n$ cd kubernetes-deployments/\n$ mkdir octavo\n</code></pre> Initial Git+SSH setup <p>Each new server requires an initial setup to authenticate with GitHub, using  a new SSH key, adding to the authorized SSH keys in the GitHub account, and coping <code>.gitconfig</code> from a previous server.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#storage-requirements","title":"Storage Requirements","text":"<p>Docker and <code>containerd</code> store images under <code>/var/lib</code> by default, which is why the when installing Ubuntu Server a dedicated partition is created for <code>/var/lib</code>, so that it will (should) not be necessary to move images to the <code>/home</code> partition.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#install-helm","title":"Install Helm","text":"<p>Install helm from APT because it will be required to install certain components later (e.g. Kubernetes Dashboard):</p> <pre><code>$ curl https://baltocdn.com/helm/signing.asc \\\n  | sudo gpg --dearmor -o /etc/apt/keyrings/helm.gpg\n$ echo \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" \\\n  | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\n$ sudo apt-get update\n$ sudo apt-get install -y helm\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#install-kubernetes","title":"Install Kubernetes","text":"<p>Kubernetes current stable release is now v1.33.0 which is only the 1st patch in 1.33 and came out only a few days ago, so instead install the more stable v1.32. Install kubeadm, kubelet and kubectl from Debian packages:</p> <pre><code>$ curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key \\\n  | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n$ sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n$ echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' \\\n  | sudo tee /etc/apt/sources.list.d/kubernetes.list\n$ sudo apt-get update\n</code></pre> <p>Once the APT repository is ready, install the packages:</p> <code>$ sudo apt-get install -y kubelet kubeadm kubectl</code> <pre><code>$ sudo apt-get install -y kubelet kubeadm kubectl\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  conntrack cri-tools kubernetes-cni\nThe following NEW packages will be installed:\n  conntrack cri-tools kubeadm kubectl kubelet kubernetes-cni\n0 upgraded, 6 newly installed, 0 to remove and 1 not upgraded.\nNeed to get 92.7 MB of archives.\nAfter this operation, 338 MB of additional disk space will be used.\nGet:1 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 conntrack amd64 1:1.4.8-1ubuntu1 [37.9 kB]\nGet:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.32/deb  cri-tools 1.32.0-1.1 [16.3 MB]\nGet:3 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.32/deb  kubeadm 1.32.4-1.1 [12.2 MB]\nGet:4 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.32/deb  kubectl 1.32.4-1.1 [11.2 MB]\nGet:5 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.32/deb  kubernetes-cni 1.6.0-1.1 [37.8 MB]\nGet:6 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.32/deb  kubelet 1.32.4-1.1 [15.2 MB]\nFetched 92.7 MB in 1s (66.3 MB/s)\nSelecting previously unselected package conntrack.\n(Reading database ... 140718 files and directories currently installed.)\nPreparing to unpack .../0-conntrack_1%3a1.4.8-1ubuntu1_amd64.deb ...\nUnpacking conntrack (1:1.4.8-1ubuntu1) ...\nSelecting previously unselected package cri-tools.\nPreparing to unpack .../1-cri-tools_1.32.0-1.1_amd64.deb ...\nUnpacking cri-tools (1.32.0-1.1) ...\nSelecting previously unselected package kubeadm.\nPreparing to unpack .../2-kubeadm_1.32.4-1.1_amd64.deb ...\nUnpacking kubeadm (1.32.4-1.1) ...\nSelecting previously unselected package kubectl.\nPreparing to unpack .../3-kubectl_1.32.4-1.1_amd64.deb ...\nUnpacking kubectl (1.32.4-1.1) ...\nSelecting previously unselected package kubernetes-cni.\nPreparing to unpack .../4-kubernetes-cni_1.6.0-1.1_amd64.deb ...\nUnpacking kubernetes-cni (1.6.0-1.1) ...\nSelecting previously unselected package kubelet.\nPreparing to unpack .../5-kubelet_1.32.4-1.1_amd64.deb ...\nUnpacking kubelet (1.32.4-1.1) ...\nSetting up conntrack (1:1.4.8-1ubuntu1) ...\nSetting up kubectl (1.32.4-1.1) ...\nSetting up cri-tools (1.32.0-1.1) ...\nSetting up kubernetes-cni (1.6.0-1.1) ...\nSetting up kubeadm (1.32.4-1.1) ...\nSetting up kubelet (1.32.4-1.1) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nScanning processes...                                                                                         \nScanning processor microcode...                                                                               \nScanning linux images...                                                                                      \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\n</code></pre> <p>And then, because updating Kubernetes is a rather involved process, hold them:</p> <pre><code>$ sudo apt-mark hold kubelet kubeadm kubectl\nkubelet set on hold.\nkubeadm set on hold.\nkubectl set on hold.\n</code></pre> <p>Note that the latest patch at this time is v1.32.4:</p> <pre><code># kubectl version --output=yaml\nclientVersion:\n  buildDate: \"2025-04-22T16:03:58Z\"\n  compiler: gc\n  gitCommit: 59526cd4867447956156ae3a602fcbac10a2c335\n  gitTreeState: clean\n  gitVersion: v1.32.4\n  goVersion: go1.23.6\n  major: \"1\"\n  minor: \"32\"\n  platform: linux/amd64\nkustomizeVersion: v5.5.0\n\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n</code></pre> <p>Enabling shell autocompletion for <code>kubectl</code> is very easy, since bash-completion is already installed:</p> <pre><code>$ kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl &gt; /dev/null\n$ sudo chmod a+r /etc/bash_completion.d/kubectl\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#enable-the-kubelet-service","title":"Enable the kubelet service","text":"<p>This step is only really necessary later, before  bootstrapping the cluster with <code>kubeadm</code>, but it can be done any time; the service will just be waiting:</p> <pre><code>$ sudo systemctl enable --now kubelet\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#install-container-runtime","title":"Install container runtime","text":""},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#networking-setup","title":"Networking setup","text":"<p>Enabling IPv4 packet forwarding is required for Kubernetes network and is not enabled by default:</p> <pre><code>$ sudo sysctl net.ipv4.ip_forward\nnet.ipv4.ip_forward = 0\n\n$ cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.ipv4.ip_forward = 1\nEOF\n\n$ sudo sysctl --system\n* Applying /usr/lib/sysctl.d/10-apparmor.conf ...\n* Applying /etc/sysctl.d/10-bufferbloat.conf ...\n* Applying /etc/sysctl.d/10-console-messages.conf ...\n* Applying /etc/sysctl.d/10-ipv6-privacy.conf ...\n* Applying /etc/sysctl.d/10-kernel-hardening.conf ...\n* Applying /etc/sysctl.d/10-magic-sysrq.conf ...\n* Applying /etc/sysctl.d/10-map-count.conf ...\n* Applying /etc/sysctl.d/10-network-security.conf ...\n* Applying /etc/sysctl.d/10-ptrace.conf ...\n* Applying /etc/sysctl.d/10-zeropage.conf ...\n* Applying /usr/lib/sysctl.d/50-pid-max.conf ...\n* Applying /usr/lib/sysctl.d/99-protect-links.conf ...\n* Applying /etc/sysctl.d/99-sysctl.conf ...\n* Applying /etc/sysctl.d/k8s.conf ...\n* Applying /etc/sysctl.conf ...\nkernel.apparmor_restrict_unprivileged_userns = 1\nnet.core.default_qdisc = fq_codel\nkernel.printk = 4 4 1 7\nnet.ipv6.conf.all.use_tempaddr = 2\nnet.ipv6.conf.default.use_tempaddr = 2\nkernel.kptr_restrict = 1\nkernel.sysrq = 176\nvm.max_map_count = 1048576\nnet.ipv4.conf.default.rp_filter = 2\nnet.ipv4.conf.all.rp_filter = 2\nkernel.yama.ptrace_scope = 1\nvm.mmap_min_addr = 65536\nkernel.pid_max = 4194304\nfs.protected_fifos = 1\nfs.protected_hardlinks = 1\nfs.protected_regular = 2\nfs.protected_symlinks = 1\nnet.ipv4.ip_forward = 1\n</code></pre> <p>This alone was not enough for the successful deployment of the  Network plugin in <code>alfred</code> required after bootstraping the cluster. Additional setup proved to be required to let iptables see bridged traffic, supposedly required only up to v1.29, when omitting these steps led to the <code>kube-flannel</code> deployment failing to start up (stuck in a crash loop):</p> <pre><code>$ sudo modprobe overlay\n$ sudo modprobe br_netfilter\n$ sudo tee /etc/modules-load.d/k8s.conf&lt;&lt;EOF\nbr_netfilter\noverlay\nEOF\n\n$ sudo tee /etc/sysctl.d/k8s.conf&lt;&lt;EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nEOF\n</code></pre> <p>Reboot the system to make sure that the changes are permanent:</p> <pre><code>$ sudo sysctl -a | egrep 'net.ipv4.ip_forward |net.bridge.bridge-nf-call-ip'\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\n\n$ lsmod | egrep 'overlay|bridge'\noverlay               212992  0\nbridge                421888  1 br_netfilter\nstp                    12288  1 bridge\nllc                    16384  2 bridge,stp\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#install-containerd","title":"Install containerd","text":"<p>Installing a container runtime comes next, with <code>containerd</code> being the runtime of choice. Install using the apt repository:</p> <pre><code>$ sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg \\\n  -o /etc/apt/keyrings/docker.asc\n$ sudo chmod a+r /etc/apt/keyrings/docker.asc\n$ echo \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n$ sudo apt-get update\n</code></pre> <p>Once the APT repository is ready, install the packages:</p> <code>$ sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</code> <pre><code>$ sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  docker-ce-rootless-extras libltdl7 libslirp0 pigz slirp4netns\nSuggested packages:\n  cgroupfs-mount | cgroup-lite\nThe following NEW packages will be installed:\n  containerd.io docker-buildx-plugin docker-ce docker-ce-cli docker-ce-rootless-extras docker-compose-plugin\n  libltdl7 libslirp0 pigz slirp4netns\n0 upgraded, 10 newly installed, 0 to remove and 1 not upgraded.\nNeed to get 120 MB of archives.\nAfter this operation, 440 MB of additional disk space will be used.\nGet:1 http://ch.archive.ubuntu.com/ubuntu noble/universe amd64 pigz amd64 2.8-1 [65.6 kB]\nGet:2 https://download.docker.com/linux/ubuntu noble/stable amd64 containerd.io amd64 1.7.27-1 [30.5 MB]\nGet:3 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 libltdl7 amd64 2.4.7-7build1 [40.3 kB]\nGet:4 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 libslirp0 amd64 4.7.0-1ubuntu3 [63.8 kB]\nGet:5 http://ch.archive.ubuntu.com/ubuntu noble/universe amd64 slirp4netns amd64 1.2.1-1build2 [34.9 kB]\nGet:6 https://download.docker.com/linux/ubuntu noble/stable amd64 docker-buildx-plugin amd64 0.23.0-1~ubuntu.24.04~noble [34.6 MB]\nGet:7 https://download.docker.com/linux/ubuntu noble/stable amd64 docker-ce-cli amd64 5:28.1.1-1~ubuntu.24.04~noble [15.8 MB]\nGet:8 https://download.docker.com/linux/ubuntu noble/stable amd64 docker-ce amd64 5:28.1.1-1~ubuntu.24.04~noble [19.2 MB]\nGet:9 https://download.docker.com/linux/ubuntu noble/stable amd64 docker-ce-rootless-extras amd64 5:28.1.1-1~ubuntu.24.04~noble [6,092 kB]\nGet:10 https://download.docker.com/linux/ubuntu noble/stable amd64 docker-compose-plugin amd64 2.35.1-1~ubuntu.24.04~noble [13.8 MB]\nFetched 120 MB in 1s (94.1 MB/s)             \nSelecting previously unselected package pigz.\n(Reading database ... 140777 files and directories currently installed.)\nPreparing to unpack .../0-pigz_2.8-1_amd64.deb ...\nUnpacking pigz (2.8-1) ...\nSelecting previously unselected package containerd.io.\nPreparing to unpack .../1-containerd.io_1.7.27-1_amd64.deb ...\nUnpacking containerd.io (1.7.27-1) ...\nSelecting previously unselected package docker-buildx-plugin.\nPreparing to unpack .../2-docker-buildx-plugin_0.23.0-1~ubuntu.24.04~noble_amd64.deb ...\nUnpacking docker-buildx-plugin (0.23.0-1~ubuntu.24.04~noble) ...\nSelecting previously unselected package docker-ce-cli.\nPreparing to unpack .../3-docker-ce-cli_5%3a28.1.1-1~ubuntu.24.04~noble_amd64.deb ...\nUnpacking docker-ce-cli (5:28.1.1-1~ubuntu.24.04~noble) ...\nSelecting previously unselected package docker-ce.\nPreparing to unpack .../4-docker-ce_5%3a28.1.1-1~ubuntu.24.04~noble_amd64.deb ...\nUnpacking docker-ce (5:28.1.1-1~ubuntu.24.04~noble) ...\nSelecting previously unselected package docker-ce-rootless-extras.\nPreparing to unpack .../5-docker-ce-rootless-extras_5%3a28.1.1-1~ubuntu.24.04~noble_amd64.deb ...\nUnpacking docker-ce-rootless-extras (5:28.1.1-1~ubuntu.24.04~noble) ...\nSelecting previously unselected package docker-compose-plugin.\nPreparing to unpack .../6-docker-compose-plugin_2.35.1-1~ubuntu.24.04~noble_amd64.deb ...\nUnpacking docker-compose-plugin (2.35.1-1~ubuntu.24.04~noble) ...\nSelecting previously unselected package libltdl7:amd64.\nPreparing to unpack .../7-libltdl7_2.4.7-7build1_amd64.deb ...\nUnpacking libltdl7:amd64 (2.4.7-7build1) ...\nSelecting previously unselected package libslirp0:amd64.\nPreparing to unpack .../8-libslirp0_4.7.0-1ubuntu3_amd64.deb ...\nUnpacking libslirp0:amd64 (4.7.0-1ubuntu3) ...\nSelecting previously unselected package slirp4netns.\nPreparing to unpack .../9-slirp4netns_1.2.1-1build2_amd64.deb ...\nUnpacking slirp4netns (1.2.1-1build2) ...\nSetting up docker-buildx-plugin (0.23.0-1~ubuntu.24.04~noble) ...\nSetting up containerd.io (1.7.27-1) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/containerd.service \u2192 /usr/lib/systemd/system/containerd.service.\nSetting up docker-compose-plugin (2.35.1-1~ubuntu.24.04~noble) ...\nSetting up libltdl7:amd64 (2.4.7-7build1) ...\nSetting up docker-ce-cli (5:28.1.1-1~ubuntu.24.04~noble) ...\nSetting up libslirp0:amd64 (4.7.0-1ubuntu3) ...\nSetting up pigz (2.8-1) ...\nSetting up docker-ce-rootless-extras (5:28.1.1-1~ubuntu.24.04~noble) ...\nSetting up slirp4netns (1.2.1-1build2) ...\nSetting up docker-ce (5:28.1.1-1~ubuntu.24.04~noble) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/docker.service \u2192 /usr/lib/systemd/system/docker.service.\nCreated symlink /etc/systemd/system/sockets.target.wants/docker.socket \u2192 /usr/lib/systemd/system/docker.socket.\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.4) ...\nScanning processes...                                                                                         \nScanning processor microcode...                                                                               \nScanning linux images...                                                                                      \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\n</code></pre> <p>In just a few moments <code>docker</code> is already running:</p> <code>$ systemctl status docker</code> <pre><code>$ systemctl status docker\n\u25cf docker.service - Docker Application Container Engine\n    Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; preset: enabled)\n    Active: active (running) since Sat 2025-04-26 15:17:27 CEST; 52s ago\nTriggeredBy: \u25cf docker.socket\n      Docs: https://docs.docker.com\n  Main PID: 198205 (dockerd)\n      Tasks: 22\n    Memory: 25.1M (peak: 27.5M)\n        CPU: 335ms\n    CGroup: /system.slice/docker.service\n            \u2514\u2500198205 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\n\nApr 26 15:17:27 octavo dockerd[198205]: time=\"2025-04-26T15:17:27.329995883+02:00\" level=info msg=\"detected 127.0.0.53 nameserver, assuming systemd-resolved, so using resolv.conf: /run/systemd/resolve/resolv.conf\"\nApr 26 15:17:27 octavo dockerd[198205]: time=\"2025-04-26T15:17:27.356938837+02:00\" level=info msg=\"Creating a containerd client\" address=/run/containerd/containerd.sock timeout=1m0s\nApr 26 15:17:27 octavo dockerd[198205]: time=\"2025-04-26T15:17:27.391167891+02:00\" level=info msg=\"Loading containers: start.\"\nApr 26 15:17:27 octavo dockerd[198205]: time=\"2025-04-26T15:17:27.623474369+02:00\" level=info msg=\"Loading containers: done.\"\nApr 26 15:17:27 octavo dockerd[198205]: time=\"2025-04-26T15:17:27.651891549+02:00\" level=info msg=\"Docker daemon\" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1\nApr 26 15:17:27 octavo dockerd[198205]: time=\"2025-04-26T15:17:27.652003229+02:00\" level=info msg=\"Initializing buildkit\"\nApr 26 15:17:27 octavo dockerd[198205]: time=\"2025-04-26T15:17:27.686521864+02:00\" level=info msg=\"Completed buildkit initialization\"\nApr 26 15:17:27 octavo dockerd[198205]: time=\"2025-04-26T15:17:27.694113345+02:00\" level=info msg=\"Daemon has completed initialization\"\nApr 26 15:17:27 octavo dockerd[198205]: time=\"2025-04-26T15:17:27.694180217+02:00\" level=info msg=\"API listen on /run/docker.sock\"\nApr 26 15:17:27 octavo systemd[1]: Started docker.service - Docker Application Container Engine.\n</code></pre> <p>The server is indeed reachable and its version can be checked:</p> <code>$ sudo docker version</code> <pre><code>$ sudo docker version\nClient: Docker Engine - Community\nVersion:           28.1.1\nAPI version:       1.49\nGo version:        go1.23.8\nGit commit:        4eba377\nBuilt:             Fri Apr 18 09:52:14 2025\nOS/Arch:           linux/amd64\nContext:           default\n\nServer: Docker Engine - Community\nEngine:\n  Version:          28.1.1\n  API version:      1.49 (minimum version 1.24)\n  Go version:       go1.23.8\n  Git commit:       01f442b\n  Built:            Fri Apr 18 09:52:14 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\ncontainerd:\n  Version:          1.7.27\n  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da\nrunc:\n  Version:          1.2.5\n  GitCommit:        v1.2.5-0-g59923ef\ndocker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre> <p>And the basic <code>hello-world</code> example just works:</p> <code>$ sudo docker run hello-world</code> <pre><code>$ sudo docker run hello-world\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\ne6590344b1a5: Pull complete \nDigest: sha256:c41088499908a59aae84b0a49c70e86f4731e588a737f1637e73c8c09d995654\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#configure-containerd-for-kubernetes","title":"Configure containerd for Kubernetes","text":"<p>The default configutation that comes with <code>containerd</code>, at least when installed from the APT repository, needs two adjustments to work with Kubernetes:</p> <ol> <li>Enable the use of      <code>systemd</code> cgroup driver,      because Ubuntu (22.04+) uses both <code>systemd</code> and      cgroup v2.</li> <li>Enable CRI integration, disabled in <code>/etc/containerd/config.toml</code>,      but needed to use <code>containerd</code> with Kubernetes.</li> </ol> <p>The safest method to set these configurations is to do it based off of the default configuration:</p> <pre><code>$ containerd config default \\\n | sed 's/disabled_plugins.*/disabled_plugins = []/' \\\n | sed 's/SystemdCgroup = false/SystemdCgroup = true/' \\\n | sudo tee /etc/containerd/config.toml &gt; /dev/null\n\n$ sudo systemctl restart containerd\n$ sudo systemctl restart kubelet\n</code></pre> Resulting <code>/etc/containerd/config.toml</code> <p>The resulting configuration enables <code>SystemdCgroup</code> under <code>plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options</code>:</p> <pre><code>disabled_plugins = []\nimports = []\noom_score = 0\nplugin_dir = \"\"\nrequired_plugins = []\nroot = \"/var/lib/containerd\"\nstate = \"/run/containerd\"\ntemp = \"\"\nversion = 2\n\n[cgroup]\n  path = \"\"\n\n[debug]\n  address = \"\"\n  format = \"\"\n  gid = 0\n  level = \"\"\n  uid = 0\n\n[grpc]\n  address = \"/run/containerd/containerd.sock\"\n  gid = 0\n  max_recv_message_size = 16777216\n  max_send_message_size = 16777216\n  tcp_address = \"\"\n  tcp_tls_ca = \"\"\n  tcp_tls_cert = \"\"\n  tcp_tls_key = \"\"\n  uid = 0\n\n[metrics]\n  address = \"\"\n  grpc_histogram = false\n\n[plugins]\n\n  [plugins.\"io.containerd.gc.v1.scheduler\"]\n    deletion_threshold = 0\n    mutation_threshold = 100\n    pause_threshold = 0.02\n    schedule_delay = \"0s\"\n    startup_delay = \"100ms\"\n\n  [plugins.\"io.containerd.grpc.v1.cri\"]\n    cdi_spec_dirs = [\"/etc/cdi\", \"/var/run/cdi\"]\n    device_ownership_from_security_context = false\n    disable_apparmor = false\n    disable_cgroup = false\n    disable_hugetlb_controller = true\n    disable_proc_mount = false\n    disable_tcp_service = true\n    drain_exec_sync_io_timeout = \"0s\"\n    enable_cdi = false\n    enable_selinux = false\n    enable_tls_streaming = false\n    enable_unprivileged_icmp = false\n    enable_unprivileged_ports = false\n    ignore_deprecation_warnings = []\n    ignore_image_defined_volumes = false\n    image_pull_progress_timeout = \"5m0s\"\n    image_pull_with_sync_fs = false\n    max_concurrent_downloads = 3\n    max_container_log_line_size = 16384\n    netns_mounts_under_state_dir = false\n    restrict_oom_score_adj = false\n    sandbox_image = \"registry.k8s.io/pause:3.8\"\n    selinux_category_range = 1024\n    stats_collect_period = 10\n    stream_idle_timeout = \"4h0m0s\"\n    stream_server_address = \"127.0.0.1\"\n    stream_server_port = \"0\"\n    systemd_cgroup = false\n    tolerate_missing_hugetlb_controller = true\n    unset_seccomp_profile = \"\"\n\n    [plugins.\"io.containerd.grpc.v1.cri\".cni]\n      bin_dir = \"/opt/cni/bin\"\n      conf_dir = \"/etc/cni/net.d\"\n      conf_template = \"\"\n      ip_pref = \"\"\n      max_conf_num = 1\n      setup_serially = false\n\n    [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n      default_runtime_name = \"runc\"\n      disable_snapshot_annotations = true\n      discard_unpacked_layers = false\n      ignore_blockio_not_enabled_errors = false\n      ignore_rdt_not_enabled_errors = false\n      no_pivot = false\n      snapshotter = \"overlayfs\"\n\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.default_runtime]\n        base_runtime_spec = \"\"\n        cni_conf_dir = \"\"\n        cni_max_conf_num = 0\n        container_annotations = []\n        pod_annotations = []\n        privileged_without_host_devices = false\n        privileged_without_host_devices_all_devices_allowed = false\n        runtime_engine = \"\"\n        runtime_path = \"\"\n        runtime_root = \"\"\n        runtime_type = \"\"\n        sandbox_mode = \"\"\n        snapshotter = \"\"\n\n        [plugins.\"io.containerd.grpc.v1.cri\".containerd.default_runtime.options]\n\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes]\n\n        [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n          base_runtime_spec = \"\"\n          cni_conf_dir = \"\"\n          cni_max_conf_num = 0\n          container_annotations = []\n          pod_annotations = []\n          privileged_without_host_devices = false\n          privileged_without_host_devices_all_devices_allowed = false\n          runtime_engine = \"\"\n          runtime_path = \"\"\n          runtime_root = \"\"\n          runtime_type = \"io.containerd.runc.v2\"\n          sandbox_mode = \"podsandbox\"\n          snapshotter = \"\"\n\n          [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n            BinaryName = \"\"\n            CriuImagePath = \"\"\n            CriuPath = \"\"\n            CriuWorkPath = \"\"\n            IoGid = 0\n            IoUid = 0\n            NoNewKeyring = false\n            NoPivotRoot = false\n            Root = \"\"\n            ShimCgroup = \"\"\n            SystemdCgroup = true\n\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.untrusted_workload_runtime]\n        base_runtime_spec = \"\"\n        cni_conf_dir = \"\"\n        cni_max_conf_num = 0\n        container_annotations = []\n        pod_annotations = []\n        privileged_without_host_devices = false\n        privileged_without_host_devices_all_devices_allowed = false\n        runtime_engine = \"\"\n        runtime_path = \"\"\n        runtime_root = \"\"\n        runtime_type = \"\"\n        sandbox_mode = \"\"\n        snapshotter = \"\"\n\n        [plugins.\"io.containerd.grpc.v1.cri\".containerd.untrusted_workload_runtime.options]\n\n    [plugins.\"io.containerd.grpc.v1.cri\".image_decryption]\n      key_model = \"node\"\n\n    [plugins.\"io.containerd.grpc.v1.cri\".registry]\n      config_path = \"\"\n\n      [plugins.\"io.containerd.grpc.v1.cri\".registry.auths]\n\n      [plugins.\"io.containerd.grpc.v1.cri\".registry.configs]\n\n      [plugins.\"io.containerd.grpc.v1.cri\".registry.headers]\n\n      [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors]\n\n    [plugins.\"io.containerd.grpc.v1.cri\".x509_key_pair_streaming]\n      tls_cert_file = \"\"\n      tls_key_file = \"\"\n\n  [plugins.\"io.containerd.internal.v1.opt\"]\n    path = \"/opt/containerd\"\n\n  [plugins.\"io.containerd.internal.v1.restart\"]\n    interval = \"10s\"\n\n  [plugins.\"io.containerd.internal.v1.tracing\"]\n\n  [plugins.\"io.containerd.metadata.v1.bolt\"]\n    content_sharing_policy = \"shared\"\n\n  [plugins.\"io.containerd.monitor.v1.cgroups\"]\n    no_prometheus = false\n\n  [plugins.\"io.containerd.nri.v1.nri\"]\n    disable = true\n    disable_connections = false\n    plugin_config_path = \"/etc/nri/conf.d\"\n    plugin_path = \"/opt/nri/plugins\"\n    plugin_registration_timeout = \"5s\"\n    plugin_request_timeout = \"2s\"\n    socket_path = \"/var/run/nri/nri.sock\"\n\n  [plugins.\"io.containerd.runtime.v1.linux\"]\n    no_shim = false\n    runtime = \"runc\"\n    runtime_root = \"\"\n    shim = \"containerd-shim\"\n    shim_debug = false\n\n  [plugins.\"io.containerd.runtime.v2.task\"]\n    platforms = [\"linux/amd64\"]\n    sched_core = false\n\n  [plugins.\"io.containerd.service.v1.diff-service\"]\n    default = [\"walking\"]\n    sync_fs = false\n\n  [plugins.\"io.containerd.service.v1.tasks-service\"]\n    blockio_config_file = \"\"\n    rdt_config_file = \"\"\n\n  [plugins.\"io.containerd.snapshotter.v1.aufs\"]\n    root_path = \"\"\n\n  [plugins.\"io.containerd.snapshotter.v1.blockfile\"]\n    fs_type = \"\"\n    mount_options = []\n    root_path = \"\"\n    scratch_file = \"\"\n\n  [plugins.\"io.containerd.snapshotter.v1.btrfs\"]\n    root_path = \"\"\n\n  [plugins.\"io.containerd.snapshotter.v1.devmapper\"]\n    async_remove = false\n    base_image_size = \"\"\n    discard_blocks = false\n    fs_options = \"\"\n    fs_type = \"\"\n    pool_name = \"\"\n    root_path = \"\"\n\n  [plugins.\"io.containerd.snapshotter.v1.native\"]\n    root_path = \"\"\n\n  [plugins.\"io.containerd.snapshotter.v1.overlayfs\"]\n    mount_options = []\n    root_path = \"\"\n    sync_remove = false\n    upperdir_label = false\n\n  [plugins.\"io.containerd.snapshotter.v1.zfs\"]\n    root_path = \"\"\n\n  [plugins.\"io.containerd.tracing.processor.v1.otlp\"]\n\n  [plugins.\"io.containerd.transfer.v1.local\"]\n    config_path = \"\"\n    max_concurrent_downloads = 3\n    max_concurrent_uploaded_layers = 3\n\n    [[plugins.\"io.containerd.transfer.v1.local\".unpack_config]]\n      differ = \"\"\n      platform = \"linux/amd64\"\n      snapshotter = \"overlayfs\"\n\n[proxy_plugins]\n\n[stream_processors]\n\n  [stream_processors.\"io.containerd.ocicrypt.decoder.v1.tar\"]\n    accepts = [\"application/vnd.oci.image.layer.v1.tar+encrypted\"]\n    args = [\"--decryption-keys-path\", \"/etc/containerd/ocicrypt/keys\"]\n    env = [\"OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf\"]\n    path = \"ctd-decoder\"\n    returns = \"application/vnd.oci.image.layer.v1.tar\"\n\n  [stream_processors.\"io.containerd.ocicrypt.decoder.v1.tar.gzip\"]\n    accepts = [\"application/vnd.oci.image.layer.v1.tar+gzip+encrypted\"]\n    args = [\"--decryption-keys-path\", \"/etc/containerd/ocicrypt/keys\"]\n    env = [\"OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf\"]\n    path = \"ctd-decoder\"\n    returns = \"application/vnd.oci.image.layer.v1.tar+gzip\"\n\n[timeouts]\n  \"io.containerd.timeout.bolt.open\" = \"0s\"\n  \"io.containerd.timeout.metrics.shimstats\" = \"2s\"\n  \"io.containerd.timeout.shim.cleanup\" = \"5s\"\n  \"io.containerd.timeout.shim.load\" = \"5s\"\n  \"io.containerd.timeout.shim.shutdown\" = \"3s\"\n  \"io.containerd.timeout.task.state\" = \"2s\"\n\n[ttrpc]\n  address = \"\"\n  gid = 0\n  uid = 0\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#bootstrap-with-kubeadm","title":"Bootstrap with <code>kubeadm</code>","text":"<p>Creating a cluster with <code>kubeadm</code> is the next big step towards creating the Kubernetes cluster.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#initialize-the-control-plane","title":"Initialize the control-plane","text":"<p>Having reviewed the requirements and installed all the components already, initialize the control-plane node with flags:</p> <ul> <li><code>--cri-socket=unix:/run/containerd/containerd.sock</code> to make sure Kubernetes uses the containerd runtime.</li> <li><code>--pod-network-cidr=10.244.0.0/16</code> as      required by flannel,     which is the network plugin to be installed later.</li> </ul> <code>$ sudo kubeadm init</code> <pre><code>$ sudo kubeadm init \\\n  --cri-socket=unix:/run/containerd/containerd.sock \\\n  --pod-network-cidr=10.244.0.0/16\nI0426 16:55:42.979255   33862 version.go:261] remote version is much newer: v1.33.0; falling back to: stable-1.32\n[init] Using Kubernetes version: v1.32.4\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action beforehand using 'kubeadm config images pull'\nW0426 16:55:43.482424   33862 checks.go:846] detected that the sandbox image \"registry.k8s.io/pause:3.8\" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use \"registry.k8s.io/pause:3.10\" as the CRI sandbox image.\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local octavo] and IPs [10.96.0.1 10.0.0.8]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [localhost octavo] and IPs [10.0.0.8 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [localhost octavo] and IPs [10.0.0.8 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"super-admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\"\n[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s\n[kubelet-check] The kubelet is healthy after 501.89299ms\n[api-check] Waiting for a healthy API server. This can take up to 4m0s\n[api-check] The API server is healthy after 3.503255484s\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Skipping phase. Please see --upload-certs\n[mark-control-plane] Marking the node octavo as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n[mark-control-plane] Marking the node octavo as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]\n[bootstrap-token] Using token: 7k3y4k.r8ebhonqgtm6anyr\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 10.0.0.8:6443 --token 7k3y4k.r8ebhonqgtm6anyr \\\n        --discovery-token-ca-cert-hash sha256:18d968e92516e1a2808166d90a7d7c8b6f7b37cbac6328c49793863f9ae2b982 \n</code></pre> <p>Once this is done, Kubernetes control plane is running at <code>10.0.0.8:6443</code></p> <pre><code># export KUBECONFIG=/etc/kubernetes/admin.conf\n\n# kubectl cluster-info\nKubernetes control plane is running at https://10.0.0.8:6443\nCoreDNS is running at https://10.0.0.8:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n\n# kubectl version --output=yaml\nclientVersion:\n  buildDate: \"2025-04-22T16:03:58Z\"\n  compiler: gc\n  gitCommit: 59526cd4867447956156ae3a602fcbac10a2c335\n  gitTreeState: clean\n  gitVersion: v1.32.4\n  goVersion: go1.23.6\n  major: \"1\"\n  minor: \"32\"\n  platform: linux/amd64\nkustomizeVersion: v5.5.0\nserverVersion:\n  buildDate: \"2025-04-22T15:56:15Z\"\n  compiler: gc\n  gitCommit: 59526cd4867447956156ae3a602fcbac10a2c335\n  gitTreeState: clean\n  gitVersion: v1.32.4\n  goVersion: go1.23.6\n  major: \"1\"\n  minor: \"32\"\n  platform: linux/amd6\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#setup-kubectl-access","title":"Setup <code>kubectl</code> access","text":"<p>To run <code>kubectl</code> as a non-root user, copy the Kubernetes config file under the <code>~/.kube</code> directory and set the right permissions to it:</p> <pre><code>$ mkdir $HOME/.kube\n$ sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config\n[sudo] password for ponder: \n$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n$ ls -l $HOME/.kube/config\n-rw------- 1 ponder ponder 5648 Apr 26 17:03 /home/ponder/.kube/config\n$ kubectl cluster-info\nKubernetes control plane is running at https://10.0.0.8:6443\nCoreDNS is running at https://10.0.0.8:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#troubleshooting-bootstrap","title":"Troubleshooting bootstrap","text":"<p>The first time around, <code>sudo kubeadm init</code> failed. This didn't seem to have been swap not being disabled at first, but in the end disabling swap seems to have been the (only) solution.</p> <code>$ sudo kubeadm init</code> <pre><code>$ sudo kubeadm init \\\n  --cri-socket=unix:/run/containerd/containerd.sock \\\n  --pod-network-cidr=10.244.0.0/16\n\nI0426 15:49:34.806666  537229 version.go:261] remote version is much newer: v1.33.0; falling back to: stable-1.32\n[init] Using Kubernetes version: v1.32.4\n[preflight] Running pre-flight checks\n        [WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action beforehand using 'kubeadm config images pull'\nW0426 15:49:35.293520  537229 checks.go:846] detected that the sandbox image \"registry.k8s.io/pause:3.8\" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use \"registry.k8s.io/pause:3.10\" as the CRI sandbox image.\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local octavo] and IPs [10.96.0.1 10.0.0.8]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [localhost octavo] and IPs [10.0.0.8 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [localhost octavo] and IPs [10.0.0.8 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"super-admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\"\n[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s\n[kubelet-check] The kubelet is not healthy after 4m0.000397441s\n\nUnfortunately, an error has occurred:\n        The HTTP call equal to 'curl -sSL http://127.0.0.1:10248/healthz' returned error: Get \"http://127.0.0.1:10248/healthz\": context deadline exceeded\n\n\nThis error is likely caused by:\n        - The kubelet is not running\n        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)\n\nIf you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:\n        - 'systemctl status kubelet'\n        - 'journalctl -xeu kubelet'\n\nAdditionally, a control plane component may have crashed or exited when started by the container runtime.\nTo troubleshoot, list all containers using your preferred container runtimes CLI.\nHere is one example how you may list all running Kubernetes containers by using crictl:\n        - 'crictl --runtime-endpoint unix:/run/containerd/containerd.sock ps -a | grep kube | grep -v pause'\n        Once you have found the failing container, you can inspect its logs with:\n        - 'crictl --runtime-endpoint unix:/run/containerd/containerd.sock logs CONTAINERID'\nerror execution phase wait-control-plane: could not initialize a Kubernetes cluster\nTo see the stack trace of this error execute with --v=5 or higher\n</code></pre> <p>Swap was disabled (and the server was restarted) only after this happened, but it does not seem likely to have been the cause.</p> <p>The kubelet is running but possibly unhealthy, <code>systemctl status kubelet</code> shows the same line every 5 seconds:</p> <pre><code>Apr 26 16:11:29 octavo kubelet[3081]: E0426 16:11:29.654554    3081 kubelet.go:3002] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\n</code></pre> <p>Many errors and warning are logged and shown by <code>journalctl -xeu kubelet</code>, up to the point where the above error is repeated every 5 seconds:</p> <code>$ journalctl -xeu kubelet</code> <pre><code>$ journalctl -xeu kubelet\nApr 26 16:00:24 octavo systemd[1]: Started kubelet.service - kubelet: The Kubernetes Node Agent.\n\u2591\u2591 Subject: A start job for unit kubelet.service has finished successfully\n\u2591\u2591 Defined-By: systemd\n\u2591\u2591 Support: http://www.ubuntu.com/support\n\u2591\u2591 \n\u2591\u2591 A start job for unit kubelet.service has finished successfully.\n\u2591\u2591 \n\u2591\u2591 The job identifier is 192.\nApr 26 16:00:24 octavo kubelet[3081]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.\nApr 26 16:00:24 octavo kubelet[3081]: Flag --pod-infra-container-image has been deprecated, will be removed in 1.35. Image garbage collector will get sandbox image information from CRI.\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.873338    3081 server.go:215] \"--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.878116    3081 server.go:520] \"Kubelet version\" kubeletVersion=\"v1.32.4\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.878137    3081 server.go:522] \"Golang settings\" GOGC=\"\" GOMAXPROCS=\"\" GOTRACEBACK=\"\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.878359    3081 server.go:954] \"Client rotation is on, will bootstrap in background\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.879746    3081 certificate_store.go:130] Loading cert/key pair from \"/var/lib/kubelet/pki/kubelet-client-current.pem\".\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.881693    3081 dynamic_cafile_content.go:161] \"Starting controller\" name=\"client-ca-bundle::/etc/kubernetes/pki/ca.crt\"\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.884220    3081 log.go:32] \"RuntimeConfig from runtime service failed\" err=\"rpc error: code = Unimplemented desc = unknown method RuntimeConfig for service runtime.v1.RuntimeService\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.884242    3081 server.go:1421] \"CRI implementation should be updated to support RuntimeConfig when KubeletCgroupDriverFromCRI feature gate has been enabled. Falling back to using cgroupDriver from kubelet config.\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.892519    3081 server.go:772] \"--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.892745    3081 container_manager_linux.go:268] \"Container manager verified user specified cgroup-root exists\" cgroupRoot=[]\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.892782    3081 container_manager_linux.go:273] \"Creating Container Manager object based on Node Config\" nodeConfig={\"NodeName\":\"octavo\",\"RuntimeCgroupsName\":\"\",\"SystemCgroupsName\":\"\",\"KubeletCgroupsName\":\"\",\"KubeletOOMScoreAdj\":-999,\"ContainerRuntime\":\"\",\"CgroupsPerQOS\":true,\"CgroupRoot\":\"/\",\"CgroupDriver\":\"systemd\",\"Kubele&gt;\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.892963    3081 topology_manager.go:138] \"Creating topology manager with none policy\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.892973    3081 container_manager_linux.go:304] \"Creating device plugin manager\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.893096    3081 state_mem.go:36] \"Initialized new in-memory state store\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.893362    3081 kubelet.go:446] \"Attempting to sync node with API server\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.893383    3081 kubelet.go:341] \"Adding static pod path\" path=\"/etc/kubernetes/manifests\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.893401    3081 kubelet.go:352] \"Adding apiserver pod source\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.893412    3081 apiserver.go:42] \"Waiting for node sync before watching apiserver pods\"\nApr 26 16:00:24 octavo kubelet[3081]: W0426 16:00:24.893859    3081 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get \"https://10.0.0.8:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&amp;limit=500&amp;resourceVersion=0\": dial tcp 10.0.0.8:6443: connect: connection refused\nApr 26 16:00:24 octavo kubelet[3081]: W0426 16:00:24.893887    3081 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get \"https://10.0.0.8:6443/api/v1/nodes?fieldSelector=metadata.name%3Doctavo&amp;limit=500&amp;resourceVersion=0\": dial tcp 10.0.0.8:6443: connect: connection refused\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.893911    3081 reflector.go:166] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \\\"https://10.0.0.8:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&amp;limit=500&amp;resourceVersion=0\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" lo&gt;\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.893993    3081 reflector.go:166] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \\\"https://10.0.0.8:6443/api/v1/nodes?fieldSelector=metadata.name%3Doctavo&amp;limit=500&amp;resourceVersion=0\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" logger=\"Unhan&gt;\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.894044    3081 kuberuntime_manager.go:269] \"Container runtime initialized\" containerRuntime=\"containerd\" version=\"1.7.27\" apiVersion=\"v1\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.894351    3081 kubelet.go:890] \"Not starting ClusterTrustBundle informer because we are in static kubelet mode\"\nApr 26 16:00:24 octavo kubelet[3081]: W0426 16:00:24.894385    3081 probe.go:272] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.894658    3081 watchdog_linux.go:99] \"Systemd watchdog is not enabled\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.894673    3081 server.go:1287] \"Started kubelet\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.894770    3081 server.go:169] \"Starting to listen\" address=\"0.0.0.0\" port=10250\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.894761    3081 ratelimit.go:55] \"Setting rate limiting for endpoint\" service=\"podresources\" qps=100 burstTokens=10\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.895021    3081 server.go:243] \"Starting to serve the podresources API\" endpoint=\"unix:/var/lib/kubelet/pod-resources/kubelet.sock\"\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.895066    3081 event.go:368] \"Unable to write event (may retry after sleeping)\" err=\"Post \\\"https://10.0.0.8:6443/api/v1/namespaces/default/events\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" event=\"&amp;Event{ObjectMeta:{octavo.1839e3187cc326ed  default    0 0001-01-01 00:00:00 +0000 UTC &lt;nil&gt; &lt;nil&gt; map[] map[] [] [&gt;\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.895314    3081 fs_resource_analyzer.go:67] \"Starting FS ResourceAnalyzer\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.895343    3081 dynamic_serving_content.go:135] \"Starting controller\" name=\"kubelet-server-cert-files::/var/lib/kubelet/pki/kubelet.crt::/var/lib/kubelet/pki/kubelet.key\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.895416    3081 volume_manager.go:297] \"Starting Kubelet Volume Manager\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.895449    3081 desired_state_of_world_populator.go:150] \"Desired state populator starts to run\"\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.895492    3081 kubelet_node_status.go:466] \"Error getting the current node from lister\" err=\"node \\\"octavo\\\" not found\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.895521    3081 reconciler.go:26] \"Reconciler: start to sync state\"\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.896004    3081 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https://10.0.0.8:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/octavo?timeout=10s\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" interval=\"200ms\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.896248    3081 factory.go:219] Registration of the crio container factory failed: Get \"http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info\": dial unix /var/run/crio/crio.sock: connect: no such file or directory\nApr 26 16:00:24 octavo kubelet[3081]: W0426 16:00:24.897181    3081 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get \"https://10.0.0.8:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&amp;resourceVersion=0\": dial tcp 10.0.0.8:6443: connect: connection refused\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.897377    3081 reflector.go:166] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \\\"https://10.0.0.8:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&amp;resourceVersion=0\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" logger=\"UnhandledEr&gt;\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.897627    3081 server.go:479] \"Adding debug handlers to kubelet server\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.898015    3081 factory.go:221] Registration of the containerd container factory successfully\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.898031    3081 factory.go:221] Registration of the systemd container factory successfully\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.898126    3081 kubelet.go:1555] \"Image garbage collection failed once. Stats initialization may not have completed yet\" err=\"invalid capacity 0 on image filesystem\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.907984    3081 cpu_manager.go:221] \"Starting CPU manager\" policy=\"none\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.907991    3081 cpu_manager.go:222] \"Reconciling\" reconcilePeriod=\"10s\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.908003    3081 state_mem.go:36] \"Initialized new in-memory state store\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.909090    3081 policy_none.go:49] \"None policy: Start\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.909098    3081 memory_manager.go:186] \"Starting memorymanager\" policy=\"None\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.909104    3081 state_mem.go:35] \"Initializing new in-memory state store\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.948672    3081 manager.go:519] \"Failed to read data from checkpoint\" checkpoint=\"kubelet_internal_checkpoint\" err=\"checkpoint is not found\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.948805    3081 eviction_manager.go:189] \"Eviction manager: starting control loop\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.948817    3081 container_log_manager.go:189] \"Initializing container log rotate workers\" workers=1 monitorPeriod=\"10s\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.948983    3081 plugin_manager.go:118] \"Starting Kubelet Plugin Manager\"\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.949523    3081 eviction_manager.go:267] \"eviction manager: failed to check if we have separate container filesystem. Ignoring.\" err=\"no imagefs label for configured runtime\"\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.949568    3081 eviction_manager.go:292] \"Eviction manager: failed to get summary stats\" err=\"failed to get node info: node \\\"octavo\\\" not found\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.952661    3081 kubelet_network_linux.go:50] \"Initialized iptables rules.\" protocol=\"IPv4\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.953773    3081 kubelet_network_linux.go:50] \"Initialized iptables rules.\" protocol=\"IPv6\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.953793    3081 status_manager.go:227] \"Starting to sync pod status with apiserver\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.953815    3081 watchdog_linux.go:127] \"Systemd watchdog is not enabled or the interval is invalid, so health checking will not be started.\"\nApr 26 16:00:24 octavo kubelet[3081]: I0426 16:00:24.953821    3081 kubelet.go:2382] \"Starting kubelet main sync loop\"\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.953861    3081 kubelet.go:2406] \"Skipping pod synchronization\" err=\"PLEG is not healthy: pleg has yet to be successful\"\nApr 26 16:00:24 octavo kubelet[3081]: W0426 16:00:24.954293    3081 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get \"https://10.0.0.8:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&amp;resourceVersion=0\": dial tcp 10.0.0.8:6443: connect: connection refused\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.954314    3081 reflector.go:166] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \\\"https://10.0.0.8:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&amp;resourceVersion=0\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" logger=\"Unha&gt;\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.051160    3081 kubelet_node_status.go:75] \"Attempting to register node\" node=\"octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.051955    3081 kubelet_node_status.go:107] \"Unable to register node with API server\" err=\"Post \\\"https://10.0.0.8:6443/api/v1/nodes\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" node=\"octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.096663    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"ca-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/ad9e0985cbbefcb442de36a1fa2a7651-ca-certs\\\") pod \\\"kube-controller-manager-octavo\\\" (UID: \\\"ad9e0985cbbefcb442de36a1fa2a7651\\\") \" pod=\"kube-system/kube-controller-&gt;\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.096703    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"usr-local-share-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/ad9e0985cbbefcb442de36a1fa2a7651-usr-local-share-ca-certificates\\\") pod \\\"kube-controller-manager-octavo\\\" (UID: \\\"ad9e0985cbbefcb442de36a1f&gt;\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.096736    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"ca-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/9143a118ddce4da18128e4a82a4a703f-ca-certs\\\") pod \\\"kube-apiserver-octavo\\\" (UID: \\\"9143a118ddce4da18128e4a82a4a703f\\\") \" pod=\"kube-system/kube-apiserver-octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.096755    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"k8s-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/9143a118ddce4da18128e4a82a4a703f-k8s-certs\\\") pod \\\"kube-apiserver-octavo\\\" (UID: \\\"9143a118ddce4da18128e4a82a4a703f\\\") \" pod=\"kube-system/kube-apiserver-octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.096790    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"usr-local-share-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/9143a118ddce4da18128e4a82a4a703f-usr-local-share-ca-certificates\\\") pod \\\"kube-apiserver-octavo\\\" (UID: \\\"9143a118ddce4da18128e4a82a4a703f\\\"&gt;\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.096824    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"etc-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/ad9e0985cbbefcb442de36a1fa2a7651-etc-ca-certificates\\\") pod \\\"kube-controller-manager-octavo\\\" (UID: \\\"ad9e0985cbbefcb442de36a1fa2a7651\\\") \" pod=\"kube-s&gt;\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.096849    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"flexvolume-dir\\\" (UniqueName: \\\"kubernetes.io/host-path/ad9e0985cbbefcb442de36a1fa2a7651-flexvolume-dir\\\") pod \\\"kube-controller-manager-octavo\\\" (UID: \\\"ad9e0985cbbefcb442de36a1fa2a7651\\\") \" pod=\"kube-system/kube&gt;\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.096876    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"k8s-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/ad9e0985cbbefcb442de36a1fa2a7651-k8s-certs\\\") pod \\\"kube-controller-manager-octavo\\\" (UID: \\\"ad9e0985cbbefcb442de36a1fa2a7651\\\") \" pod=\"kube-system/kube-controlle&gt;\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.096896    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"etcd-certs\\\" (UniqueName: \\\"kubernetes.io/host-path/6a67e3805915a015cc4032245f92dbab-etcd-certs\\\") pod \\\"etcd-octavo\\\" (UID: \\\"6a67e3805915a015cc4032245f92dbab\\\") \" pod=\"kube-system/etcd-octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.096920    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"etcd-data\\\" (UniqueName: \\\"kubernetes.io/host-path/6a67e3805915a015cc4032245f92dbab-etcd-data\\\") pod \\\"etcd-octavo\\\" (UID: \\\"6a67e3805915a015cc4032245f92dbab\\\") \" pod=\"kube-system/etcd-octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.096941    3081 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https://10.0.0.8:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/octavo?timeout=10s\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" interval=\"400ms\"\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.096967    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kubeconfig\\\" (UniqueName: \\\"kubernetes.io/host-path/ad9e0985cbbefcb442de36a1fa2a7651-kubeconfig\\\") pod \\\"kube-controller-manager-octavo\\\" (UID: \\\"ad9e0985cbbefcb442de36a1fa2a7651\\\") \" pod=\"kube-system/kube-control&gt;\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.097001    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"usr-share-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/ad9e0985cbbefcb442de36a1fa2a7651-usr-share-ca-certificates\\\") pod \\\"kube-controller-manager-octavo\\\" (UID: \\\"ad9e0985cbbefcb442de36a1fa2a7651\\\") \"&gt;\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.097020    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"kubeconfig\\\" (UniqueName: \\\"kubernetes.io/host-path/9a50c799dacb081ee9c958350b51d8ec-kubeconfig\\\") pod \\\"kube-scheduler-octavo\\\" (UID: \\\"9a50c799dacb081ee9c958350b51d8ec\\\") \" pod=\"kube-system/kube-scheduler-octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.097037    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"etc-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/9143a118ddce4da18128e4a82a4a703f-etc-ca-certificates\\\") pod \\\"kube-apiserver-octavo\\\" (UID: \\\"9143a118ddce4da18128e4a82a4a703f\\\") \" pod=\"kube-system/kub&gt;\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.097054    3081 reconciler_common.go:251] \"operationExecutor.VerifyControllerAttachedVolume started for volume \\\"usr-share-ca-certificates\\\" (UniqueName: \\\"kubernetes.io/host-path/9143a118ddce4da18128e4a82a4a703f-usr-share-ca-certificates\\\") pod \\\"kube-apiserver-octavo\\\" (UID: \\\"9143a118ddce4da18128e4a82a4a703f\\\") \" pod=\"kub&gt;\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.247411    3081 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"octavo\\\" not found\" node=\"octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.251941    3081 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"octavo\\\" not found\" node=\"octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.253846    3081 kubelet_node_status.go:75] \"Attempting to register node\" node=\"octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.254394    3081 kubelet_node_status.go:107] \"Unable to register node with API server\" err=\"Post \\\"https://10.0.0.8:6443/api/v1/nodes\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" node=\"octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.275473    3081 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"octavo\\\" not found\" node=\"octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.301544    3081 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"octavo\\\" not found\" node=\"octavo\"\nApr 26 16:00:24 octavo kubelet[3081]: E0426 16:00:24.974201    3081 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https://10.0.0.8:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/octavo?timeout=10s\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" interval=\"800ms\"\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.134490    3081 kubelet_node_status.go:75] \"Attempting to register node\" node=\"octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.135362    3081 kubelet_node_status.go:107] \"Unable to register node with API server\" err=\"Post \\\"https://10.0.0.8:6443/api/v1/nodes\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" node=\"octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: W0426 16:00:25.220172    3081 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get \"https://10.0.0.8:6443/api/v1/nodes?fieldSelector=metadata.name%3Doctavo&amp;limit=500&amp;resourceVersion=0\": dial tcp 10.0.0.8:6443: connect: connection refused\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.220349    3081 reflector.go:166] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \\\"https://10.0.0.8:6443/api/v1/nodes?fieldSelector=metadata.name%3Doctavo&amp;limit=500&amp;resourceVersion=0\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" logger=\"Unhan&gt;\nApr 26 16:00:25 octavo kubelet[3081]: W0426 16:00:25.451675    3081 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get \"https://10.0.0.8:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&amp;limit=500&amp;resourceVersion=0\": dial tcp 10.0.0.8:6443: connect: connection refused\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.451792    3081 reflector.go:166] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \\\"https://10.0.0.8:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&amp;limit=500&amp;resourceVersion=0\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" lo&gt;\nApr 26 16:00:25 octavo kubelet[3081]: W0426 16:00:25.487336    3081 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get \"https://10.0.0.8:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&amp;resourceVersion=0\": dial tcp 10.0.0.8:6443: connect: connection refused\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.487463    3081 reflector.go:166] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \\\"https://10.0.0.8:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&amp;resourceVersion=0\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" logger=\"Unha&gt;\nApr 26 16:00:25 octavo kubelet[3081]: W0426 16:00:25.504868    3081 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get \"https://10.0.0.8:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&amp;resourceVersion=0\": dial tcp 10.0.0.8:6443: connect: connection refused\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.504969    3081 reflector.go:166] \"Unhandled Error\" err=\"k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \\\"https://10.0.0.8:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&amp;resourceVersion=0\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" logger=\"UnhandledEr&gt;\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.775421    3081 controller.go:145] \"Failed to ensure lease exists, will retry\" err=\"Get \\\"https://10.0.0.8:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/octavo?timeout=10s\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" interval=\"1.6s\"\nApr 26 16:00:25 octavo kubelet[3081]: I0426 16:00:25.938373    3081 kubelet_node_status.go:75] \"Attempting to register node\" node=\"octavo\"\nApr 26 16:00:25 octavo kubelet[3081]: E0426 16:00:25.939217    3081 kubelet_node_status.go:107] \"Unable to register node with API server\" err=\"Post \\\"https://10.0.0.8:6443/api/v1/nodes\\\": dial tcp 10.0.0.8:6443: connect: connection refused\" node=\"octavo\"\nApr 26 16:00:26 octavo kubelet[3081]: E0426 16:00:26.436928    3081 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"octavo\\\" not found\" node=\"octavo\"\nApr 26 16:00:26 octavo kubelet[3081]: E0426 16:00:26.438217    3081 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"octavo\\\" not found\" node=\"octavo\"\nApr 26 16:00:26 octavo kubelet[3081]: E0426 16:00:26.439369    3081 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"octavo\\\" not found\" node=\"octavo\"\nApr 26 16:00:26 octavo kubelet[3081]: E0426 16:00:26.440112    3081 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"octavo\\\" not found\" node=\"octavo\"\nApr 26 16:00:27 octavo kubelet[3081]: E0426 16:00:27.442281    3081 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"octavo\\\" not found\" node=\"octavo\"\nApr 26 16:00:27 octavo kubelet[3081]: E0426 16:00:27.442300    3081 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"octavo\\\" not found\" node=\"octavo\"\nApr 26 16:00:27 octavo kubelet[3081]: E0426 16:00:27.442354    3081 kubelet.go:3190] \"No need to create a mirror pod, since failed to get node info from the cluster\" err=\"node \\\"octavo\\\" not found\" node=\"octavo\"\nApr 26 16:00:27 octavo kubelet[3081]: I0426 16:00:27.540869    3081 kubelet_node_status.go:75] \"Attempting to register node\" node=\"octavo\"\nApr 26 16:00:27 octavo kubelet[3081]: E0426 16:00:27.804881    3081 nodelease.go:49] \"Failed to get node when trying to set owner ref to the node lease\" err=\"nodes \\\"octavo\\\" not found\" node=\"octavo\"\nApr 26 16:00:27 octavo kubelet[3081]: I0426 16:00:27.998050    3081 kubelet_node_status.go:78] \"Successfully registered node\" node=\"octavo\"\nApr 26 16:00:28 octavo kubelet[3081]: I0426 16:00:28.072723    3081 kubelet.go:3194] \"Creating a mirror pod for static pod\" pod=\"kube-system/etcd-octavo\"\nApr 26 16:00:28 octavo kubelet[3081]: E0426 16:00:28.079480    3081 kubelet.go:3196] \"Failed creating a mirror pod\" err=\"pods \\\"etcd-octavo\\\" is forbidden: no PriorityClass with name system-node-critical was found\" pod=\"kube-system/etcd-octavo\"\nApr 26 16:00:28 octavo kubelet[3081]: I0426 16:00:28.079531    3081 kubelet.go:3194] \"Creating a mirror pod for static pod\" pod=\"kube-system/kube-apiserver-octavo\"\nApr 26 16:00:28 octavo kubelet[3081]: E0426 16:00:28.086877    3081 kubelet.go:3196] \"Failed creating a mirror pod\" err=\"pods \\\"kube-apiserver-octavo\\\" is forbidden: no PriorityClass with name system-node-critical was found\" pod=\"kube-system/kube-apiserver-octavo\"\nApr 26 16:00:28 octavo kubelet[3081]: I0426 16:00:28.086955    3081 kubelet.go:3194] \"Creating a mirror pod for static pod\" pod=\"kube-system/kube-controller-manager-octavo\"\nApr 26 16:00:28 octavo kubelet[3081]: E0426 16:00:28.092824    3081 kubelet.go:3196] \"Failed creating a mirror pod\" err=\"pods \\\"kube-controller-manager-octavo\\\" is forbidden: no PriorityClass with name system-node-critical was found\" pod=\"kube-system/kube-controller-manager-octavo\"\nApr 26 16:00:28 octavo kubelet[3081]: I0426 16:00:28.092885    3081 kubelet.go:3194] \"Creating a mirror pod for static pod\" pod=\"kube-system/kube-scheduler-octavo\"\nApr 26 16:00:28 octavo kubelet[3081]: E0426 16:00:28.094961    3081 kubelet.go:3196] \"Failed creating a mirror pod\" err=\"pods \\\"kube-scheduler-octavo\\\" is forbidden: no PriorityClass with name system-node-critical was found\" pod=\"kube-system/kube-scheduler-octavo\"\nApr 26 16:00:28 octavo kubelet[3081]: I0426 16:00:28.372556    3081 apiserver.go:52] \"Watching apiserver\"\nApr 26 16:00:28 octavo kubelet[3081]: I0426 16:00:28.442538    3081 kubelet.go:3194] \"Creating a mirror pod for static pod\" pod=\"kube-system/kube-apiserver-octavo\"\nApr 26 16:00:28 octavo kubelet[3081]: E0426 16:00:28.444604    3081 kubelet.go:3196] \"Failed creating a mirror pod\" err=\"pods \\\"kube-apiserver-octavo\\\" is forbidden: no PriorityClass with name system-node-critical was found\" pod=\"kube-system/kube-apiserver-octavo\"\nApr 26 16:00:28 octavo kubelet[3081]: I0426 16:00:28.472896    3081 desired_state_of_world_populator.go:158] \"Finished populating initial desired state of world\"\nApr 26 16:00:29 octavo kubelet[3081]: I0426 16:00:29.338842    3081 kubelet.go:3194] \"Creating a mirror pod for static pod\" pod=\"kube-system/etcd-octavo\"\nApr 26 16:00:31 octavo kubelet[3081]: I0426 16:00:31.031022    3081 kubelet.go:3194] \"Creating a mirror pod for static pod\" pod=\"kube-system/kube-controller-manager-octavo\"\nApr 26 16:00:32 octavo kubelet[3081]: I0426 16:00:32.884678    3081 kubelet.go:3194] \"Creating a mirror pod for static pod\" pod=\"kube-system/kube-scheduler-octavo\"\nApr 26 16:00:34 octavo kubelet[3081]: I0426 16:00:34.459610    3081 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"kube-system/etcd-octavo\" podStartSLOduration=5.459582191 podStartE2EDuration=\"5.459582191s\" podCreationTimestamp=\"2025-04-26 16:00:29 +0200 CEST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-01 00:00:0&gt;\nApr 26 16:00:34 octavo kubelet[3081]: I0426 16:00:34.483008    3081 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"kube-system/kube-controller-manager-octavo\" podStartSLOduration=3.482988267 podStartE2EDuration=\"3.482988267s\" podCreationTimestamp=\"2025-04-26 16:00:31 +0200 CEST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=&gt;\nApr 26 16:00:34 octavo kubelet[3081]: I0426 16:00:34.483102    3081 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"kube-system/kube-scheduler-octavo\" podStartSLOduration=2.483094445 podStartE2EDuration=\"2.483094445s\" podCreationTimestamp=\"2025-04-26 16:00:32 +0200 CEST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-&gt;\nApr 26 16:00:35 octavo kubelet[3081]: I0426 16:00:35.015311    3081 kuberuntime_manager.go:1702] \"Updating runtime config through cri with podcidr\" CIDR=\"10.244.0.0/24\"\nApr 26 16:00:35 octavo kubelet[3081]: I0426 16:00:35.016020    3081 kubelet_network.go:61] \"Updating Pod CIDR\" originalPodCIDR=\"\" newPodCIDR=\"10.244.0.0/24\"\nApr 26 16:00:36 octavo kubelet[3081]: I0426 16:00:36.698529    3081 kubelet.go:3194] \"Creating a mirror pod for static pod\" pod=\"kube-system/kube-apiserver-octavo\"\nApr 26 16:00:36 octavo kubelet[3081]: I0426 16:00:36.724746    3081 pod_startup_latency_tracker.go:104] \"Observed pod startup duration\" pod=\"kube-system/kube-apiserver-octavo\" podStartSLOduration=0.724721039 podStartE2EDuration=\"724.721039ms\" podCreationTimestamp=\"2025-04-26 16:00:36 +0200 CEST\" firstStartedPulling=\"0001-01-01 00:00:00 +0000 UTC\" lastFinishedPulling=\"0001-01-&gt;\nApr 26 16:02:24 octavo kubelet[3081]: E0426 16:02:24.440322    3081 kubelet_node_status.go:460] \"Node not becoming ready in time after startup\"\nApr 26 16:02:24 octavo kubelet[3081]: E0426 16:02:24.460985    3081 kubelet.go:3002] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\nApr 26 16:02:29 octavo kubelet[3081]: E0426 16:02:29.463219    3081 kubelet.go:3002] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\nApr 26 16:02:34 octavo kubelet[3081]: E0426 16:02:34.465049    3081 kubelet.go:3002] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\nApr 26 16:02:39 octavo kubelet[3081]: E0426 16:02:39.467066    3081 kubelet.go:3002] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\nApr 26 16:02:44 octavo kubelet[3081]: E0426 16:02:44.468109    3081 kubelet.go:3002] \"Container runtime network not ready\" networkReady=\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\n...\n</code></pre> <p>Despite all these errors, <code>kubelet</code> and <code>containerd</code> are running, and something is listening on port 6443, but the <code>kubernetes-admin</code> has no access in any way. Regenerating <code>kubeconfig</code> file for the admin user did not help either. Running <code>kubectl proxy</code> starts serving on port 8001 but all requests are still denied:</p> <pre><code>$ curl 2&gt;/dev/null -X GET \\\n  http://127.0.0.1:8001/api/v1/nodes/octavo/proxy/configz \\\n| jq .\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"nodes \\\"octavo\\\" is forbidden: User \\\"kubernetes-admin\\\" cannot get resource \\\"nodes/proxy\\\" in API group \\\"\\\" at the cluster scope\",\n  \"reason\": \"Forbidden\",\n  \"details\": {\n    \"name\": \"octavo\",\n    \"kind\": \"nodes\"\n  },\n  \"code\": 403\n}\n</code></pre> <p>At this point all that was left to do was tear down the cluster:</p> <code>$ sudo kubeadm reset</code> <pre><code>$ sudo kubeadm reset -f \\\n  --cri-socket=unix:/run/containerd/containerd.sock \\\n  --cleanup-tmp-dir\n[reset] Reading configuration from the \"kubeadm-config\" ConfigMap in namespace \"kube-system\"...\n[reset] Use 'kubeadm init phase upload-config --config your-config.yaml' to re-upload it.\nW0426 16:44:31.464050  537333 reset.go:143] [reset] Unable to fetch the kubeadm-config ConfigMap from cluster: failed to get config map: configmaps \"kubeadm-config\" is forbidden: User \"kubernetes-admin\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"\n[preflight] Running pre-flight checks\nW0426 16:44:31.464336  537333 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory\n[reset] Deleted contents of the etcd data directory: /var/lib/etcd\n[reset] Stopping the kubelet service\n[reset] Unmounting mounted directories in \"/var/lib/kubelet\"\n[reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki /etc/kubernetes/tmp]\n[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]\n\nThe reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d\n\nThe reset process does not reset or clean up iptables rules or IPVS tables.\nIf you wish to reset iptables, you must do so manually by using the \"iptables\" command.\n\nIf your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)\nto reset your system's IPVS tables.\n\nThe reset process does not clean your kubeconfig files and you must remove them manually.\nPlease, check the contents of the $HOME/.kube/config file.\n\n$ sudo mv /etc/kubernetes/ /etc/bad-kubernetes/\n$ sudo mv /etc/cni /etc/bad-cni\n\n$ sudo iptables -L\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \nts-input   all  --  anywhere             anywhere            \nKUBE-FIREWALL  all  --  anywhere             anywhere            \n\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination         \nts-forward  all  --  anywhere             anywhere            \nDOCKER-USER  all  --  anywhere             anywhere            \nDOCKER-FORWARD  all  --  anywhere             anywhere            \n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination         \nKUBE-FIREWALL  all  --  anywhere             anywhere            \n\nChain DOCKER (1 references)\ntarget     prot opt source               destination         \nDROP       all  --  anywhere             anywhere            \n\nChain DOCKER-BRIDGE (1 references)\ntarget     prot opt source               destination         \nDOCKER     all  --  anywhere             anywhere            \n\nChain DOCKER-CT (1 references)\ntarget     prot opt source               destination         \nACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED\n\nChain DOCKER-FORWARD (1 references)\ntarget     prot opt source               destination         \nDOCKER-CT  all  --  anywhere             anywhere            \nDOCKER-ISOLATION-STAGE-1  all  --  anywhere             anywhere            \nDOCKER-BRIDGE  all  --  anywhere             anywhere            \nACCEPT     all  --  anywhere             anywhere            \n\nChain DOCKER-ISOLATION-STAGE-1 (1 references)\ntarget     prot opt source               destination         \nDOCKER-ISOLATION-STAGE-2  all  --  anywhere             anywhere            \n\nChain DOCKER-ISOLATION-STAGE-2 (1 references)\ntarget     prot opt source               destination         \nDROP       all  --  anywhere             anywhere            \n\nChain DOCKER-USER (1 references)\ntarget     prot opt source               destination         \nRETURN     all  --  anywhere             anywhere            \n\nChain KUBE-FIREWALL (2 references)\ntarget     prot opt source               destination         \nDROP       all  -- !localhost/8          localhost/8          /* block incoming localnet connections */ ! ctstate RELATED,ESTABLISHED,DNAT\n\nChain KUBE-KUBELET-CANARY (0 references)\ntarget     prot opt source               destination         \n\nChain ts-forward (1 references)\ntarget     prot opt source               destination         \nMARK       all  --  anywhere             anywhere             MARK xset 0x40000/0xff0000\nACCEPT     all  --  anywhere             anywhere             mark match 0x40000/0xff0000\nDROP       all  --  100.64.0.0/10        anywhere            \nACCEPT     all  --  anywhere             anywhere            \n\nChain ts-input (1 references)\ntarget     prot opt source               destination         \nACCEPT     all  --  octavo.royal-penny.ts.net  anywhere            \nRETURN     all  --  100.115.92.0/23      anywhere            \nDROP       all  --  100.64.0.0/10        anywhere            \nACCEPT     all  --  anywhere             anywhere            \nACCEPT     udp  --  anywhere             anywhere             udp dpt:41641\n\n$ sudo iptables -L -t nat\nChain PREROUTING (policy ACCEPT)\ntarget     prot opt source               destination         \nDOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL\n\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination         \nDOCKER     all  --  anywhere            !localhost/8          ADDRTYPE match dst-type LOCAL\n\nChain POSTROUTING (policy ACCEPT)\ntarget     prot opt source               destination         \nts-postrouting  all  --  anywhere             anywhere            \nMASQUERADE  all  --  172.17.0.0/16        anywhere            \n\nChain DOCKER (2 references)\ntarget     prot opt source               destination         \nRETURN     all  --  anywhere             anywhere            \n\nChain KUBE-KUBELET-CANARY (0 references)\ntarget     prot opt source               destination         \n\nChain ts-postrouting (1 references)\ntarget     prot opt source               destination         \nMASQUERADE  all  --  anywhere             anywhere             mark match 0x40000/0xff0000\n\n$ sudo iptables -L -t mangle\nChain PREROUTING (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain POSTROUTING (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain KUBE-IPTABLES-HINT (0 references)\ntarget     prot opt source               destination         \n\nChain KUBE-KUBELET-CANARY (0 references)\ntarget     prot opt source               destination         \n\n\n$ sudo iptables -F\n$ sudo iptables -t nat -F\n$ sudo iptables -t mangle -F\n$ sudo iptables -X\n\n\n$ sudo iptables -L\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\n$ sudo iptables -t nat\nChain PREROUTING (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain POSTROUTING (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain DOCKER (0 references)\ntarget     prot opt source               destination         \n\nChain KUBE-KUBELET-CANARY (0 references)\ntarget     prot opt source               destination         \n\nChain ts-postrouting (0 references)\ntarget     prot opt source               destination         \n\n$ sudo iptables -t mangle\nChain PREROUTING (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain POSTROUTING (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain KUBE-IPTABLES-HINT (0 references)\ntarget     prot opt source               destination         \n\nChain KUBE-KUBELET-CANARY (0 references)\ntarget     prot opt source               destination\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#network-plugin","title":"Network plugin","text":"<p>Installing a Pod network add-on is the next required step and, once again, in lack of other suggestions, deploying flannel manually like in previous clusters seems the way to go:</p> <pre><code>$ wget \\\n  https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n\n$ kubectl apply -f kube-flannel.yml\nnamespace/kube-flannel created\nserviceaccount/flannel created\nclusterrole.rbac.authorization.k8s.io/flannel created\nclusterrolebinding.rbac.authorization.k8s.io/flannel created\nconfigmap/kube-flannel-cfg created\ndaemonset.apps/kube-flannel-ds created\n\n$ kubectl get all -n kube-flannel \nNAME                        READY   STATUS    RESTARTS   AGE\npod/kube-flannel-ds-m8h8n   1/1     Running   0          8s\n\nNAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/kube-flannel-ds   1         1         1       1            1           &lt;none&gt;          8s\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#enable-single-node-cluster","title":"Enable single-node cluster","text":"<p>Control plane node isolation is required for a single-node cluster, because otherwise a cluster will not schedule Pods on the control plane nodes for security reasons. This is reflected in the <code>Taints</code> found in the node details:</p> <pre><code>$ kubectl get nodes -o wide\nNAME     STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\noctavo   Ready    control-plane   14m   v1.32.4   192.168.0.8   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-58-generic   containerd://1.7.27\n\n$ kubectl describe node octavo\nName:               octavo\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=octavo\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"0a:7f:28:09:c7:77\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.0.0.8\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 26 Apr 2025 16:55:47 +0200\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\n</code></pre> <p>Remove this taint to allow other pods to be scheduled:</p> <pre><code>$ kubectl taint nodes --all node-role.kubernetes.io/control-plane-\nnode/octavo untainted\n\n$ kubectl describe node octavo | grep -i taint\nTaints:             &lt;none&gt;\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#allow-external-load-balancers","title":"Allow external load balancers","text":"<p>The <code>node.kubernetes.io/exclude-from-external-load-balancers</code> label highlighted above will later lead to the problem of MetalLB is not advertising my service from my control-plane nodes or from my single node cluster; the recommened solution is to remove this label:</p> <pre><code>$ kubectl label nodes octavo \\\n  node.kubernetes.io/exclude-from-external-load-balancers-\nnode/octavo unlabeled\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#test-pod-scheduling","title":"Test pod scheduling","text":"<p>Before moving forward, run a test pod to confirm that pods can be scheduled:</p> <pre><code>$ kubectl apply -f https://k8s.io/examples/pods/commands.yaml\npod/command-demo created\n\n$ kubectl get all\nNAME               READY   STATUS              RESTARTS   AGE\npod/command-demo   0/1     ContainerCreating   0          1s\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   24m\n</code></pre> <p>After a minute or two, the pod becomes <code>Completed</code>, indicating a successful run. With the cluster now ready to run pods and services, move on to installing more components that will be used by the actual services: MetalLB Load Balancer, Kubernets Dashboard, Ingress Controller, HTTPS certificates with Let\u2019s Encrypt, including automatic renewal. LocalPath PV provisioner for simple persistent storage in local file systems would seem unnecessary, based on experience with previous clusters.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#logs-reader-helper","title":"Logs reader helper","text":"<p>Troubleshooting podcs and services often involves reading or watching the logs, which involves combining two <code>kubectl</code> commands to find the relevant pod/service and requesting the logs. To makes this easier, put the following in <code>~/bin/klogs</code> (and add <code>~/bin/</code> to your path if not there already):</p> bin/klogs<pre><code>#!/bin/bash\n#\n# Watch logs from Kubernetes pod/service.\n#\n# Usage: klogs &lt;namespace&gt; &lt;pod/service&gt;\n\nns=$1\npd=$2\nkubectl logs -n $ns \\\n  $(kubectl get pods -n $ns | grep $pd | cut -f1 -d' ') -f\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#metallb-load-balancer","title":"MetalLB Load Balancer","text":"<p>A Load Balancer is going to be necessary for the Dashboard and other services, to expose individual services via open ports on the server (<code>NodePort</code>) or virtual IP addresses. Installation By Manifest is as simple as applying the provided manifest:</p> <pre><code>$ wget \\\n  https://raw.githubusercontent.com/metallb/metallb/v0.14.9/config/manifests/metallb-native.yaml\n\n$ kubectl apply -f metallb/metallb-native.yaml\nnamespace/metallb-system created\ncustomresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/communities.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io created\ncustomresourcedefinition.apiextensions.k8s.io/servicel2statuses.metallb.io created\nserviceaccount/controller created\nserviceaccount/speaker created\nrole.rbac.authorization.k8s.io/controller created\nrole.rbac.authorization.k8s.io/pod-lister created\nclusterrole.rbac.authorization.k8s.io/metallb-system:controller created\nclusterrole.rbac.authorization.k8s.io/metallb-system:speaker created\nrolebinding.rbac.authorization.k8s.io/controller created\nrolebinding.rbac.authorization.k8s.io/pod-lister created\nclusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created\nclusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created\nconfigmap/metallb-excludel2 created\nsecret/metallb-webhook-cert created\nservice/metallb-webhook-service created\ndeployment.apps/controller created\ndaemonset.apps/speaker created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/metallb-webhook-configuration created\n</code></pre> <p>Soon enough the deployment should have the controller and speaker running:</p> <pre><code>$ kubectl get all -n metallb-system\nNAME                             READY   STATUS    RESTARTS   AGE\npod/controller-bb5f47665-vt57w   1/1     Running   0          62s\npod/speaker-92c2g                1/1     Running   0          62s\n\nNAME                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice/metallb-webhook-service   ClusterIP   10.98.162.115   &lt;none&gt;        443/TCP   62s\n\nNAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\ndaemonset.apps/speaker   1         1         1       1            1           kubernetes.io/os=linux   62s\n\nNAME                         READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/controller   1/1     1            1           62s\n\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/controller-bb5f47665   1         1         1       62s\n</code></pre> <p>MetalLB remains idle until configured, which is done by deploying resources into its namespace. A small range of IP addresses is advertised via Layer 2 Configuration, which does not not require the IPs to be bound to the network interfaces:</p> metallb/ipaddress-pool-octavo.yaml<pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: production\n  namespace: metallb-system\nspec:\n  addresses:\n    - 192.168.0.171-192.168.0.180\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: l2-advert\n  namespace: metallb-system\n</code></pre> <p>The range is based on the local DHCP server configuration and which IPs are  currently in use; this range has just not been leased so far. The reason to use IPs from the leased range is that the router only allows adding port forwarding rules for those. This range is intentionally on the same network range and subnet as the DHCP server so that no routing is needed to reach MetalLB IPs.</p> <pre><code>$ kubectl apply -f metallb/ipaddress-pool-octavo.yaml\nipaddresspool.metallb.io/production created\nl2advertisement.metallb.io/l2-advert created\n\n$ kubectl get ipaddresspool.metallb.io -n metallb-system \nNAME         AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES\nproduction   true          false             [\"192.168.0.171-192.168.0.180\"]\n\n$ kubectl get l2advertisement.metallb.io -n metallb-system \nNAME        IPADDRESSPOOLS   IPADDRESSPOOL SELECTORS   INTERFACES\nl2-advert\n\n$ kubectl describe ipaddresspool.metallb.io production -n metallb-system \nstem\nName:         production\nNamespace:    metallb-system\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  metallb.io/v1beta1\nKind:         IPAddressPool\nMetadata:\n  Creation Timestamp:  2025-04-26T15:40:23Z\n  Generation:          1\n  Resource Version:    3974\n  UID:                 3a7c5d52-ab54-4cb8-b339-2a81930bf199\nSpec:\n  Addresses:\n    192.168.0.171-192.168.0.180\n  Auto Assign:       true\n  Avoid Buggy I Ps:  false\nEvents:              &lt;none&gt;\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#2026-upgrade-to-helm-chart","title":"2026 Upgrade to Helm chart","text":"<p>Upgrading Kubernetes eventually reached a point where MetalLB needed to be upgraded too; upgrading to Kubernetes 1.35 requires MetalLB 0.15 or later.</p> <p>MetalLb was last using the <code>metallb-native.yaml</code> instead of its Helm chart, and there is a <code>IPAddressPool</code> defined in a separate manifest, so now in order to migrate to the Helm chart without losing IP assignments or causing a collision, the migration needs to perform a \"takeover\". Kubernetes allows Helm to manage existing resources if they are correctly labeled and annotated. The <code>IPAddressPool</code> and <code>L2Advertisement</code> are Custom Resources (CRD). Helm will not delete these during the upgrade if the namespace is handled correctly, but they should be backed up first:</p> <pre><code>$ kubectl get ipaddresspools.metallb.io,l2advertisements.metallb.io -A -o yaml \\\n  &gt; metallb-config-backup.yaml\n</code></pre> <p>Install the MetalLB Helm chart:</p> <pre><code>$ helm repo add metallb https://metallb.github.io/metallb\n\"metallb\" has been added to your repositories\n\n$ helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"metallb\" chart repository\n...\nUpdate Complete. \u2388Happy Helming!\u2388\n</code></pre> <p>To minimize downtime during the takeover, pre-pull the Docker images so that the Helm installation happens in seconds rather than minutes:</p> <pre><code># docker pull quay.io/metallb/speaker:v0.15.3\nv0.15.3: Pulling from metallb/speaker\nfd4aa3667332: Pull complete \n...\n07003b9acb06: Pull complete \nDigest: sha256:c6a5b25b2e1fba610a57b2db4bb8141d7c133569d561a8cc29e38ca5113efbc4\nStatus: Downloaded newer image for quay.io/metallb/speaker:v0.15.3\nquay.io/metallb/speaker:v0.15.3\n\n# docker pull quay.io/metallb/controller:v0.15.3\nv0.15.3: Pulling from metallb/controller\nfd4aa3667332: Already exists \n...\nddf74a63f7d8: Already exists \n6dff88f058c1: Pull complete \ndb83dd67de88: Pull complete \nDigest: sha256:6698ccc54c380913816ed1fd0758637ec87dd79da419c4ab170a2c26c158ab89\nStatus: Downloaded newer image for quay.io/metallb/controller:v0.15.3\nquay.io/metallb/controller:v0.15.3\n</code></pre> <p>Helm requires specific metadata to \"adopt\" existing resources. Without this, Helm will fail with an \"already exists\" error. To avoid this, annotate and label the namespace:</p> <pre><code>$ kubectl describe namespaces metallb-system \nName:         metallb-system\nLabels:       kubernetes.io/metadata.name=metallb-system\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/enforce=privileged\n              pod-security.kubernetes.io/warn=privileged\nAnnotations:  &lt;none&gt;\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n\n$ kubectl annotate namespace metallb-system meta.helm.sh/release-name=metallb\nnamespace/metallb-system annotated\n\n$ kubectl annotate namespace metallb-system meta.helm.sh/release-namespace=metallb-system\nnamespace/metallb-system annotated\n\n$ kubectl label namespace metallb-system app.kubernetes.io/managed-by=Helm\nnamespace/metallb-system labeled\n\n$ kubectl describe namespaces metallb-system\nName:         metallb-system\nLabels:       app.kubernetes.io/managed-by=Helm\n              kubernetes.io/metadata.name=metallb-system\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/enforce=privileged\n              pod-security.kubernetes.io/warn=privileged\nAnnotations:  meta.helm.sh/release-name: metallb\n              meta.helm.sh/release-namespace: metallb-system\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n</code></pre> <p>Then, to avoid conflicts, delete the existing deployment/daemonset while keeping the CRDs and Namespace. It is critical not to delete the namespace, as doing so would trigger a cascading deletion of <code>IPAddressPools</code> and potentially <code>Services</code> using them.</p> <p>Do not run <code>kubectl delete -f</code> on the <code>metallb-native.yaml</code> manifest. Instead, use the manifest to identify and delete only the functional components (Deployments, DaemonSets, and RBAC) while leaving the Namespace and CRDs intact. To delete everything except the namespace and the CRDs, use the <code>--selector</code> to target MetalLB's internal components:</p> <pre><code>$ kubectl delete -n metallb-system -l app=metallb \\\n  deployment,daemonset,service,serviceaccount,role,rolebinding \ndeployment.apps \"controller\" deleted from metallb-system namespace\ndaemonset.apps \"speaker\" deleted from metallb-system namespace\nserviceaccount \"controller\" deleted from metallb-system namespace\nserviceaccount \"speaker\" deleted from metallb-system namespace\nrole.rbac.authorization.k8s.io \"controller\" deleted from metallb-system namespace\nrole.rbac.authorization.k8s.io \"pod-lister\" deleted from metallb-system namespace\nrolebinding.rbac.authorization.k8s.io \"controller\" deleted from metallb-system namespace\nrolebinding.rbac.authorization.k8s.io \"pod-lister\" deleted from metallb-system namespace\n\n$ kubectl delete -l app=metallb clusterrole,clusterrolebinding\nclusterrole.rbac.authorization.k8s.io \"metallb-system:controller\" deleted\nclusterrole.rbac.authorization.k8s.io \"metallb-system:speaker\" deleted\nclusterrolebinding.rbac.authorization.k8s.io \"metallb-system:controller\" deleted\nclusterrolebinding.rbac.authorization.k8s.io \"metallb-system:speaker\" deleted\n\n$ kubectl delete validatingwebhookconfiguration metallb-webhook-configuration\nvalidatingwebhookconfiguration.admissionregistration.k8s.io \"metallb-webhook-configuration\" deleted\n</code></pre> <p>The CRDs (<code>IPAddressPool</code> and <code>L2Advertisement</code>) in the 0.14.9 manifest don't have the <code>app=metallb</code> label, so the commands above will not touch them and thus the IP Pool configuration is safe.</p> <p>However, the above deletions are not enough, attempting to install the Helm chart fails with <code>INSTALLATION FAILED</code> errors due to many other pre-existing <code>CustomResourceDefinition</code> objects such as <code>bfdprofiles.metallb.io</code>:</p> <pre><code>$ helm install metallb metallb/metallb \\\n  --namespace metallb-system \\\n  --version 0.15.3\nError: INSTALLATION FAILED: Unable to continue with install: CustomResourceDefinition\n\"bfdprofiles.metallb.io\" in namespace \"\" exists and cannot be imported into the\ncurrent release: invalid ownership metadata; label validation error: missing key\n\"app.kubernetes.io/managed-by\": must be set to \"Helm\"; annotation validation error:\nmissing key \"meta.helm.sh/release-name\": must be set to \"metallb\"; annotation\nvalidation error: missing key \"meta.helm.sh/release-namespace\": must be set to\n\"metallb-system\"\n</code></pre> <p>So there are two options: <code>delete</code> or <code>annotate</code>; a few need to be deleted:</p> <pre><code>$ kubectl delete secret metallb-webhook-cert -n metallb-system\nsecret \"metallb-webhook-cert\" deleted from metallb-system namespace\n\n$ kubectl delete configmap metallb-excludel2 -n metallb-system\nconfigmap \"metallb-excludel2\" deleted from metallb-system namespace\n\n$ kubectl delete service metallb-webhook-service -n metallb-system\nservice \"metallb-webhook-service\" deleted from metallb-system namespace\n</code></pre> <p>The rest need to be annotated (to preserve them):</p> <pre><code>$ CRDS=\"bfdprofiles.metallb.io bgpadvertisements.metallb.io bgppeers.metallb.io communities.metallb.io ipaddresspools.metallb.io l2advertisements.metallb.io\"\n$ for crd in $CRDS; do\n  kubectl annotate crd $crd meta.helm.sh/release-name=metallb --overwrite\n  kubectl annotate crd $crd meta.helm.sh/release-namespace=metallb-system --overwrite\n  kubectl label crd $crd app.kubernetes.io/managed-by=Helm --overwrite\ndone\ncustomresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/bfdprofiles.metallb.io labeled\ncustomresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/bgpadvertisements.metallb.io labeled\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.metallb.io labeled\ncustomresourcedefinition.apiextensions.k8s.io/communities.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/communities.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/communities.metallb.io labeled\ncustomresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/ipaddresspools.metallb.io labeled\ncustomresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io annotated\ncustomresourcedefinition.apiextensions.k8s.io/l2advertisements.metallb.io labeled\n\n$ kubectl annotate crd servicel2statuses.metallb.io meta.helm.sh/release-name=metallb --overwrite\ncustomresourcedefinition.apiextensions.k8s.io/servicel2statuses.metallb.io annotated\n\n$ kubectl annotate crd servicel2statuses.metallb.io meta.helm.sh/release-namespace=metallb-system --overwrite\ncustomresourcedefinition.apiextensions.k8s.io/servicel2statuses.metallb.io annotated\n\n$ kubectl label crd servicel2statuses.metallb.io app.kubernetes.io/managed-by=Helm --overwrite\ncustomresourcedefinition.apiextensions.k8s.io/servicel2statuses.metallb.io labeled\n</code></pre> <p>Once the old <code>controller</code> and <code>speaker</code> are gone, Helm can install 0.15.3 into the existing namespace:</p> <pre><code>$ helm install metallb metallb/metallb \\\n  --namespace metallb-system \\\n  --version 0.15.3\nNAME: metallb\nLAST DEPLOYED: Fri Jan 30 23:33:07 2026\nNAMESPACE: metallb-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nMetalLB is now running in the cluster.\n\nNow you can configure it via its CRs. Please refer to the metallb official docs\non how to use the CRs.\n</code></pre> <p>After the Helm install, the existing <code>IPAddressPool</code> is immediately active:</p> <pre><code>$ kubectl -n metallb-system get ipaddresspools\nNAME         AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES\nproduction   true          false             [\"192.168.0.171-192.168.0.180\",\"192.168.1.171-201.168.1.220\"]\n\n$ kubectl -n metallb-system describe ipaddresspools\nName:         production\nNamespace:    metallb-system\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  metallb.io/v1beta1\nKind:         IPAddressPool\nMetadata:\n  Creation Timestamp:  2025-04-26T15:40:23Z\n  Generation:          4\n  Resource Version:    62054111\n  UID:                 3a7c5d52-ab54-4cb8-b339-2a81930bf199\nSpec:\n  Addresses:\n    192.168.0.171-192.168.0.180\n    192.168.1.171-201.168.1.220\n  Auto Assign:       true\n  Avoid Buggy I Ps:  false\nStatus:\n  assignedIPv4:   3\n  assignedIPv6:   0\n  availableIPv4:  150995001\n  availableIPv6:  0\nEvents:           &lt;none&gt;\n</code></pre> <p>Had this not worked, the Speaker pod logs should be inspected. When the pools are active, the speaker should be announcing services like this:</p> <pre><code>$ kubectl logs -n metallb-system metallb-speaker-54w68 | grep -i pool\n...\n{\"caller\":\"main.go:444\",\"event\":\"serviceAnnounced\",\"ips\":[\"192.168.0.173\"],\"level\":\"info\",\"msg\":\"service has IP, announcing\",\"pool\":\"production\",\"protocol\":\"layer2\",\"ts\":\"2026-01-30T22:33:18Z\"}\n{\"caller\":\"main.go:444\",\"event\":\"serviceAnnounced\",\"ips\":[\"192.168.1.220\"],\"level\":\"info\",\"msg\":\"service has IP, announcing\",\"pool\":\"production\",\"protocol\":\"layer2\",\"ts\":\"2026-01-30T22:33:18Z\"}\n{\"caller\":\"main.go:444\",\"event\":\"serviceAnnounced\",\"ips\":[\"192.168.0.172\"],\"level\":\"info\",\"msg\":\"service has IP, announcing\",\"pool\":\"production\",\"protocol\":\"layer2\",\"ts\":\"2026-01-30T22:33:18Z\"}\n</code></pre> <p>In the event of the logs showing errors about <code>\"no pools found\"</code>, re-apply the specific <code>IPAddressPool</code> manifest:</p> <pre><code>$ kubectl apply -f metallb/ipaddress-pool-octavo.yaml\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#kubernets-dashboard","title":"Kubernets Dashboard","text":"<p>Install the Helm repository for the Kubernetes dashboard (this requires having previously installed Helm):</p> <pre><code>$ helm repo add \\\n  kubernetes-dashboard \\\n  https://kubernetes.github.io/dashboard/\n\"kubernetes-dashboard\" has been added to your repositories\n</code></pre> <p>And install the Kubernetes dashboard without any customization:</p> <pre><code>$ helm upgrade \\\n  --install kubernetes-dashboard \\\n    kubernetes-dashboard/kubernetes-dashboard \\\n  --create-namespace \\\n  --namespace kubernetes-dashboard\nRelease \"kubernetes-dashboard\" does not exist. Installing it now.\nNAME: kubernetes-dashboard\nLAST DEPLOYED: Sat Apr 26 17:56:44 2025\nNAMESPACE: kubernetes-dashboard\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\n*************************************************************************************************\n*** PLEASE BE PATIENT: Kubernetes Dashboard may need a few minutes to get up and become ready ***\n*************************************************************************************************\n\nCongratulations! You have just installed Kubernetes Dashboard in your cluster.\n\nTo access Dashboard run:\n  kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443\n\nNOTE: In case port-forward command does not work, make sure that kong service name is correct.\n      Check the services in Kubernetes Dashboard namespace using:\n        kubectl -n kubernetes-dashboard get svc\n\nDashboard will be available at:\n  https://localhost:8443\n</code></pre> <p>After a minute or two, all services are running:</p> <pre><code>$ kubectl get all -n kubernetes-dashboard\nNAME                                                        READY   STATUS    RESTARTS   AGE\npod/kubernetes-dashboard-api-64c997cbcc-cxbjt               1/1     Running   0          30s\npod/kubernetes-dashboard-auth-5cf6848ffd-5vcm7              1/1     Running   0          30s\npod/kubernetes-dashboard-kong-79867c9c48-dwncj              1/1     Running   0          30s\npod/kubernetes-dashboard-metrics-scraper-76df4956c4-bx6wj   1/1     Running   0          30s\npod/kubernetes-dashboard-web-56df7655d9-jc8ss               1/1     Running   0          30s\n\nNAME                                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/kubernetes-dashboard-api               ClusterIP   10.96.67.106    &lt;none&gt;        8000/TCP   30s\nservice/kubernetes-dashboard-auth              ClusterIP   10.110.81.112   &lt;none&gt;        8000/TCP   30s\nservice/kubernetes-dashboard-kong-proxy        ClusterIP   10.97.89.215    &lt;none&gt;        443/TCP    30s\nservice/kubernetes-dashboard-metrics-scraper   ClusterIP   10.111.10.215   &lt;none&gt;        8000/TCP   30s\nservice/kubernetes-dashboard-web               ClusterIP   10.99.213.93    &lt;none&gt;        8000/TCP   30s\n\nNAME                                                   READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/kubernetes-dashboard-api               1/1     1            1           30s\ndeployment.apps/kubernetes-dashboard-auth              1/1     1            1           30s\ndeployment.apps/kubernetes-dashboard-kong              1/1     1            1           30s\ndeployment.apps/kubernetes-dashboard-metrics-scraper   1/1     1            1           30s\ndeployment.apps/kubernetes-dashboard-web               1/1     1            1           30s\n\nNAME                                                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/kubernetes-dashboard-api-64c997cbcc               1         1         1       30s\nreplicaset.apps/kubernetes-dashboard-auth-5cf6848ffd              1         1         1       30s\nreplicaset.apps/kubernetes-dashboard-kong-79867c9c48              1         1         1       30s\nreplicaset.apps/kubernetes-dashboard-metrics-scraper-76df4956c4   1         1         1       30s\nreplicaset.apps/kubernetes-dashboard-web-56df7655d9               1         1         1       30s\n</code></pre> <p>The dashboard is now behind the <code>kubernetes-dashboard-kong-proxy</code> service, and the suggested <code>kubectl port-forward</code> command.can be used to map port 8443 to it:</p> <pre><code>$ kubectl -n kubernetes-dashboard port-forward \\\n  svc/kubernetes-dashboard-kong-proxy 8443:443 \\\n  --address 0.0.0.0\nForwarding from 0.0.0.0:8443 -&gt; 8443\n</code></pre> <p>The dasboard is then available at https://octavo:8443/:</p> <p></p> <p>Note</p> <p>Documentation pages omit the <code>--address 0.0.0.0</code> flag, but without it the dashboard is either unreachable or non-functional, see Troubleshooting Dashboard for details of how this issue was encountered before.</p> <p>Accessing the Dashboard UI requires creating a sample user; the setup in the tutorial creates an example admin user with all privileges, good enough for now:</p> dashboard/admin-sa-rbac.yaml<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  namespace: kubernetes-dashboard\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/service-account.name: \"admin-user\"   \ntype: kubernetes.io/service-account-token\n</code></pre> <pre><code>$ kubectl apply -f dashboard/admin-sa-rbac.yaml\nserviceaccount/admin-user created\nclusterrolebinding.rbac.authorization.k8s.io/admin-user created\nsecret/admin-user created\n</code></pre> <p>To login on the dashboard as the <code>admin</code> user, each time, generate a new tokenw with:</p> <pre><code>$ kubectl -n kubernetes-dashboard create token admin-user\n</code></pre> <p>The next step is to make the dashboard available at a stable URL, without running the <code>kubectl port-forward</code> command.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#new-in-2026-headlamp","title":"New in 2026: Headlamp","text":"<p>Kubernetes Dashboard was deprecated and archived in January 2026, and is no longer maintained due to lack of active maintainers and contributors. The suggested replacement is Headlamp, which can be deployed in-cluster with a (Pomerium) ingress.</p> <p>The easiest way to install Headlamp is to use their helm chart:</p> <pre><code>$ helm repo add headlamp https://kubernetes-sigs.github.io/headlamp/\n\"headlamp\" has been added to your repositories\n\n$ helm repo update\n\n$ helm install headlamp headlamp/headlamp \\\n  --namespace kube-system \\\n  -f headlamp-values.yaml\nNAME: headlamp\nLAST DEPLOYED: Sun Feb  1 00:27:44 2026\nNAMESPACE: kube-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace kube-system -l \"app.kubernetes.io/name=headlamp,app.kubernetes.io/instance=headlamp\" -o jsonpath=\"{.items[0].metadata.name}\")\n  export CONTAINER_PORT=$(kubectl get pod --namespace kube-system $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl --namespace kube-system port-forward $POD_NAME 8080:$CONTAINER_PORT\n2. Get the token using\n  kubectl create token headlamp --namespace kube-system\n</code></pre> <p><code>headlamp-values.yaml</code></p> <pre><code>config:\n  baseURL: \"https://headlamp.very-very-dark-gray.top/\"\n  watchPlugins: true\npluginsManager:\n  enabled: true\n  configContent: |\n    plugins:\n      - name: cert-manager\n        source: https://artifacthub.io/packages/headlamp/headlamp-plugins/headlamp_cert-manager\n        version: 0.1.0\n      - name: flux\n        source: https://artifacthub.io/packages/headlamp/headlamp-plugins/headlamp_flux\n        version: 0.5.0\n      - name: trivy\n        source: https://artifacthub.io/packages/headlamp/headlamp-trivy/headlamp_trivy\n        version: 0.3.1\n    installOptions:\n      parallel: true\n      maxConcurrent: 2\n  baseImage: node:lts-alpine\n  version: latest\npersistentVolumeClaim:\n  enabled: true\n  accessModes:\n    - ReadWriteOnce\n  size: \"1Gi\"\n  storageClassName: \"longhorn-nvme\"\n  volumeMode: Filesystem\n</code></pre> <p>To make Headlamp available via Pomerium ingress on external URL, add an <code>Ingress</code> manifest under <code>pomerium/pomerium-ingress/</code> (and load it from <code>kustomization.yaml</code>):</p> <p><code>pomerium/pomerium-ingress/headlamp.yaml</code></p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: headlamp-pomerium-ingress\n  namespace: kube-system\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.pomerium.io/pass_identity_headers: true\nspec:\n  ingressClassName: pomerium\n  rules:\n    - host: headlamp.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: headlamp\n                port:\n                  number: 80\n  tls:\n    - secretName: tls-secret-cloudflare\n      hosts:\n        - headlamp.very-very-dark-gray.top\n</code></pre> <p>Finally, to log into the Headlamp dashboard, request a token for <code>headlamp</code>:</p> <pre><code>$ kubectl create token headlamp --namespace kube-system\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#ingress-controller","title":"Ingress Controller","text":"<p>Warning</p> <p>The Nginx Ingress Controller was deprecated in 2025 and replaced with Pomerium.</p> <p>An Nginx Ingress Controller will be used to redirect HTTPS requests to different services depending on the <code>Host</code> header, while all those requests will be hitting the same IP address. The current Nginx Installation Guide essentially suggests several methods to install Nginx, of which the first is Helm:</p> <pre><code>$ helm repo add \\\n  ingress-nginx \\\n  https://kubernetes.github.io/ingress-nginx\n\"ingress-nginx\" has been added to your repositories\n</code></pre> <p>To enable the use of snippets annotations, used to hide server headers, override <code>allow-snippet-annotations</code> which is set to <code>false</code> by default to mitigate known vulnerability CVE-2021-25742. The <code>nginx.ingress.kubernetes.io/configuration-snippet</code> is rated <code>Critical</code>, so the annotation also requires raisig the <code>annotations-risk-level</code>. To tweak both of these in the Helm chart, use the following <code>nginx-values.yaml</code>:</p> nginx-values.yaml<pre><code>controller:\n  allowSnippetAnnotations: true\n  config:\n    annotations-risk-level: \"Critical\"\n</code></pre> <pre><code>$ helm upgrade \\\n  --install ingress-nginx \\\n  ingress-nginx/ingress-nginx \\\n  --create-namespace \\\n  --namespace ingress-nginx \\\n  --values nginx-values.yaml\nRelease \"ingress-nginx\" does not exist. Installing it now.\nNAME: ingress-nginx\nLAST DEPLOYED: Sat Apr 26 19:31:25 2025\nNAMESPACE: ingress-nginx\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThe ingress-nginx controller has been installed.\nIt may take a few minutes for the load balancer IP to be available.\nYou can watch the status by running 'kubectl get service --namespace ingress-nginx ingress-nginx-controller --output wide --watch'\n\nAn example Ingress that makes use of the controller:\n  apiVersion: networking.k8s.io/v1\n  kind: Ingress\n  metadata:\n    name: example\n    namespace: foo\n  spec:\n    ingressClassName: nginx\n    rules:\n      - host: www.example.com\n        http:\n          paths:\n            - pathType: Prefix\n              backend:\n                service:\n                  name: exampleService\n                  port:\n                    number: 80\n              path: /\n    # This section is only required if TLS is to be enabled for the Ingress\n    tls:\n      - hosts:\n        - www.example.com\n        secretName: example-tls\n\nIf TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:\n\n  apiVersion: v1\n  kind: Secret\n  metadata:\n    name: example-tls\n    namespace: foo\n  data:\n    tls.crt: &lt;base64 encoded cert&gt;\n    tls.key: &lt;base64 encoded key&gt;\n  type: kubernetes.io/tls\n</code></pre> <p>After just half a minute the service is available on the <code>LoadBalancer</code> IP address:</p> <pre><code>$ kubectl get all -n ingress-nginx\nNAME                                           READY   STATUS    RESTARTS   AGE\npod/ingress-nginx-controller-b49d9c7b9-w26hb   1/1     Running   0          25s\n\nNAME                                         TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                      AGE\nservice/ingress-nginx-controller             LoadBalancer   10.99.252.250   192.168.0.171   80:30278/TCP,443:30974/TCP   25s\nservice/ingress-nginx-controller-admission   ClusterIP      10.96.96.221    &lt;none&gt;          443/TCP                      25s\n\nNAME                                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/ingress-nginx-controller   1/1     1            1           25s\n\nNAME                                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/ingress-nginx-controller-b49d9c7b9   1         1         1       25\n</code></pre> <p>The first virtual IP address is assigned to the <code>ingress-nginx-controller</code> service and there is NGinx happily returning <code>404 Not found</code> and reachable from other hosts:</p> <pre><code>$ curl -k https://192.168.0.171/\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#kubernetes-dashboard-ingress","title":"Kubernetes Dashboard Ingress","text":"<p>With both Ngnix and the Kubernetes dashboard up and running, it is now possible to make the dashboard more conveniently accessible via Nginx. This <code>Ingress</code> is a slightly more complete one based on the example above, </p> dashboard/octavo-ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard-ingress\n  namespace: kubernetes-dashboard\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"false\"\n    nginx.ingress.kubernetes.io/whitelist-source-range: 10.244.0.0/16\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      more_set_headers \"server: hide\";\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: k8s.octavo\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard-kong-proxy\n                port:\n                  number: 443\n</code></pre> <p>This points to the <code>kubernetes-dashboard-kong-proxy</code> service which is the one listening on the standard HTTPS port 443, and the one previously targeted by the <code>kubectl port-forward</code> command above. After applying this <code>Ingress</code>, adding the <code>Host: k8s.octavo</code> header will get the request correctly reach the dashboard:</p> <pre><code>$ kubectl apply -f dashboard/octavo-ingress.yaml\ningress.networking.k8s.io/kubernetes-dashboard-ingress created\n\n$ curl 2&gt;/dev/null \\\n  -H \"Host: k8s.octavo\" \\\n  -k https://192.168.0.171/ \\\n| head -2\n&lt;!--\nCopyright 2017 The Kubernetes Authors.\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#cloudflare-ingress","title":"Cloudflare Ingress","text":"<p>Having a Cloudflare Tunnel already setup with  https://kubernetes-octavo.very-very-dark-gray.top/ (pointing to <code>https://localhost</code>), updating this to point to <code>https://192.168.0.171/</code> will make requests reach Nginx, and updating the <code>host</code> value above to the <code>kubernetes-octavo.very-very-dark-gray.top</code> makes the dashboard available at that address.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#tailscale-ingress","title":"Tailscale Ingress","text":"<p>To make the dashboard available over Tailscale, start by installing the Tailscale Kubernetes operator:</p> <pre><code>$ helm repo add tailscale https://pkgs.tailscale.com/helmcharts &amp;&amp; \\\n  helm repo update\n\"tailscale\" has been added to your repositories\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"kubernetes-dashboard\" chart repository\n...Successfully got an update from the \"ingress-nginx\" chart repository\n...Successfully got an update from the \"tailscale\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\n\n$ kubectl create namespace tailscale\nnamespace/tailscale created\n\n$ kubectl label namespace tailscale pod-security.kubernetes.io/enforce=privileged\nnamespace/tailscale labeled\n\n$ helm upgrade \\\n  --install \\\n  tailscale-operator \\\n  tailscale/tailscale-operator \\\n  --namespace=tailscale \\\n  --create-namespace \\\n  --set-string oauth.clientId=\"_________________\" \\\n  --set-string oauth.clientSecret=\"tskey-client-_________________-__________________________________\" \\\n  --wait\nRelease \"tailscale-operator\" does not exist. Installing it now.\nNAME: tailscale-operator\nLAST DEPLOYED: Sat Apr 26 20:15:26 2025\nNAMESPACE: tailscale\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n\n$ sudo tailscale cert octavo.royal-penny.ts.net\nWrote public cert to octavo.royal-penny.ts.net.crt\nWrote private key to octavo.royal-penny.ts.net.key\n</code></pre> <p>Using the Tailscale Ingress Controller, it is now possible to make the dashboard available at https://kubernetes-octavo.royal-penny.ts.net by adding a new <code>Ingress</code> (with its own unique <code>metadata.name</code>) in <code>octavo-ingress.yaml</code>:</p> dashboard/octavo-ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard-ingress\n  namespace: kubernetes-dashboard\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"false\"\n    nginx.ingress.kubernetes.io/whitelist-source-range: 10.244.0.0/16\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      more_set_headers \"server: hide\";\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: kubernetes-octavo.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard-kong-proxy\n                port:\n                  number: 443\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard-ingress-tailscale\n  namespace: kubernetes-dashboard\nspec:\n  defaultBackend:\n    service:\n      name: kubernetes-dashboard-kong-proxy\n      port:\n        number: 443\n  ingressClassName: tailscale\n  tls:\n    - hosts:\n        - kubernetes-octavo\n</code></pre> <pre><code>$ kubectl apply -f dashboard/octavo-ingress.yaml\ningress.networking.k8s.io/kubernetes-dashboard-ingress unchanged\ningress.networking.k8s.io/kubernetes-dashboard-ingress-tailscale created\n\n$ kubectl get ingress -A\nNAMESPACE              NAME                                     CLASS       HOSTS                                       ADDRESS                                PORTS     AGE\nkubernetes-dashboard   kubernetes-dashboard-ingress             nginx       kubernetes-octavo.very-very-dark-gray.top   192.168.0.171                          80        39m\nkubernetes-dashboard   kubernetes-dashboard-ingress-tailscale   tailscale   *                                           kubernetes-octavo.royal-penny.ts.net   80, 443   31s\n</code></pre> <p>After some time the dashboard is available also at https://kubernetes-octavo.royal-penny.ts.net/.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#https-certificates","title":"HTTPS certificates","text":"<p>The Kubernetes dashboard uses a self-signed certificate, and so does Nginx by default, which works in so far as encrypting traffic, but provides no guarantee that the traffic is coming from the actual servers and is just very annoying when browsers complain every time accessing the service. It is now time to get properly signed HTTPS certificates. This is the way.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#install-cert-manager","title":"Install <code>cert-manager</code>","text":"<p>cert-manager is the native Kubernetes certificate management controller of choice to issue certificates from Let's Encrypt to secure NGINX-ingress.</p> <p>Having already installed Helm (3.17), deployed the NGINX Ingress Controller, assigned <code>octavo.uu.am</code> to the router's external IP address, and deployed the Kubernetes dashboard service, the system is ready for Step 5 - Deploy cert-manager, starting with the installation with Helm:</p> <pre><code>$ helm repo add jetstack https://charts.jetstack.io --force-update\n\"jetstack\" has been added to your repositories\n\n$ helm install \\\n    cert-manager jetstack/cert-manager \\\n    --namespace cert-manager \\\n    --create-namespace \\\n    --version v1.17.2 \\\n    --set crds.enabled=true\nNAME: cert-manager\nLAST DEPLOYED: Sat Apr 26 21:05:57 2025\nNAMESPACE: cert-manager\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\ncert-manager v1.17.2 has been deployed successfully!\n\nIn order to begin issuing certificates, you will need to set up a ClusterIssuer\nor Issuer resource (for example, by creating a 'letsencrypt-staging' issuer).\n\nMore information on the different types of issuers and how to configure them\ncan be found in our documentation:\n\nhttps://cert-manager.io/docs/configuration/\n\nFor information on how to configure cert-manager to automatically provision\nCertificates for Ingress resources, take a look at the `ingress-shim`\ndocumentation:\n\nhttps://cert-manager.io/docs/usage/ingress/\n</code></pre> <p>The <code>helm install</code> command takes several seconds to come back with the above output, at which point the pods and services are all up and running:</p> <pre><code>$ kubectl get all -n cert-manager\nNAME                                           READY   STATUS    RESTARTS   AGE\npod/cert-manager-7d67448f59-c2fgn              1/1     Running   0          102s\npod/cert-manager-cainjector-666b8b6b66-fl6rp   1/1     Running   0          102s\npod/cert-manager-webhook-78cb4cf989-wb4wz      1/1     Running   0          102s\n\nNAME                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)            AGE\nservice/cert-manager              ClusterIP   10.103.213.89   &lt;none&gt;        9402/TCP           102s\nservice/cert-manager-cainjector   ClusterIP   10.108.222.22   &lt;none&gt;        9402/TCP           102s\nservice/cert-manager-webhook      ClusterIP   10.108.56.97    &lt;none&gt;        443/TCP,9402/TCP   102s\n\nNAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/cert-manager              1/1     1            1           102s\ndeployment.apps/cert-manager-cainjector   1/1     1            1           102s\ndeployment.apps/cert-manager-webhook      1/1     1            1           102s\n\nNAME                                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/cert-manager-7d67448f59              1         1         1       102s\nreplicaset.apps/cert-manager-cainjector-666b8b6b66   1         1         1       102s\nreplicaset.apps/cert-manager-webhook-78cb4cf989      1         1         1       102s\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#test-cert-manager","title":"Test <code>cert-manager</code>","text":"<p>Verify the installation by creating a simple self-signed certificate:</p> Test certificate: <code>test-resources.yaml</code> test-resources.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: cert-manager-test\n---\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: test-selfsigned\n  namespace: cert-manager-test\nspec:\n  selfSigned: {}\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: selfsigned-cert\n  namespace: cert-manager-test\nspec:\n  dnsNames:\n    - example.com\n  secretName: selfsigned-cert-tls\n  issuerRef:\n    name: test-selfsigned\n</code></pre> <p>Deploying this succeeds withing seconds, after which it can be cleaned up:</p> <code>kubectl apply -f test-resources.yaml</code> <pre><code>$ kubectl apply -f test-resources.yaml\nnamespace/cert-manager-test created\nissuer.cert-manager.io/test-selfsigned created\ncertificate.cert-manager.io/selfsigned-cert created\n\n$ kubectl describe certificate -n cert-manager-test\nName:         selfsigned-cert\nNamespace:    cert-manager-test\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         Certificate\nMetadata:\n  Creation Timestamp:  2025-04-26T19:11:05Z\n  Generation:          1\n  Resource Version:    21857\n  UID:                 e16b3f4e-494b-4866-b328-bb92759fd482\nSpec:\n  Dns Names:\n    example.com\n  Issuer Ref:\n    Name:       test-selfsigned\n  Secret Name:  selfsigned-cert-tls\nStatus:\n  Conditions:\n    Last Transition Time:  2025-04-26T19:11:05Z\n    Message:               Certificate is up to date and has not expired\n    Observed Generation:   1\n    Reason:                Ready\n    Status:                True\n    Type:                  Ready\n  Not After:               2025-07-25T19:11:05Z\n  Not Before:              2025-04-26T19:11:05Z\n  Renewal Time:            2025-06-25T19:11:05Z\n  Revision:                1\nEvents:\n  Type    Reason     Age   From                                       Message\n  ----    ------     ----  ----                                       -------\n  Normal  Issuing    8s    cert-manager-certificates-trigger          Issuing certificate as Secret does not exist\n  Normal  Generated  8s    cert-manager-certificates-key-manager      Stored new private key in temporary Secret resource \"selfsigned-cert-fzsgh\"\n  Normal  Requested  8s    cert-manager-certificates-request-manager  Created new CertificateRequest resource \"selfsigned-cert-1\"\n  Normal  Issuing    8s    cert-manager-certificates-issuing          The certificate has been successfully issued\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#configure-lets-encrypt","title":"Configure Let's Encrypt","text":"<p>Step 6 - Configure a Let's Encrypt Issuer shows how to create an <code>Issuer</code>, but in this system with multiple services running in different namespaces, a <code>ClusterIssuer</code> is needed instead:</p> cert-manager-issuer.yaml<pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: stibbons@uu.am\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n      - http01:\n          ingress:\n            class: nginx\n</code></pre> <pre><code>$ kubectl create -f cert-manager-issuer.yaml\nclusterissuer.cert-manager.io/letsencrypt-prod created\n\n$ kubectl describe clusterissuer.cert-manager.io/letsencrypt-prod\nName:         letsencrypt-prod\nNamespace:    \nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         ClusterIssuer\nMetadata:\n  Creation Timestamp:  2025-04-26T19:18:14Z\n  Generation:          1\n  Resource Version:    22532\n  UID:                 c7063ffb-d530-4f72-8b0a-7f323b39c593\nSpec:\n  Acme:\n    Email:  stibbons@uu.am\n    Private Key Secret Ref:\n      Name:  letsencrypt-prod\n    Server:  https://acme-v02.api.letsencrypt.org/directory\n    Solvers:\n      http01:\n        Ingress:\n          Class:  nginx\nStatus:\n  Acme:\n    Last Private Key Hash:  Qb3v8RLD0ixqgBLKVsI/tEgX16kauzYTQXAaC3pdqCE=\n    Last Registered Email:  stibbons@uu.am\n    Uri:                    https://acme-v02.api.letsencrypt.org/acme/acct/2364237637\n  Conditions:\n    Last Transition Time:  2025-04-26T19:18:15Z\n    Message:               The ACME account was registered with the ACME server\n    Observed Generation:   1\n    Reason:                ACMEAccountRegistered\n    Status:                True\n    Type:                  Ready\nEvents:                    &lt;none&gt;\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#end-to-end-test-lets-encrypt","title":"End-to-end test Let's Encrypt","text":"<p>Step 7 - Deploy a TLS Ingress Resource is the last step left to actually make pratical use of all the above. Based on the provided example, apply the equivalent changes to <code>dashboard/nginx-ingress.yaml</code>, using <code>cert-manager.io/cluster-issuer</code> instead of <code>cert-manager.io/issuer</code> and adding the <code>tls</code> section with just the relevant FQDN under <code>hosts</code>:</p> dashboard/nginx-ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard-ingress\n  namespace: kubernetes-dashboard\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"false\"\n    nginx.ingress.kubernetes.io/whitelist-source-range: 10.244.0.0/16\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      more_set_headers \"server: hide\";\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: kubernetes-octavo.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard-kong-proxy\n                port:\n                  number: 443\n  tls:\n    - secretName: tls-secret-cloudflare\n      hosts:\n        - kubernetes-octavo.very-very-dark-gray.top\n</code></pre> <p>Note</p> <p>The Tailscale <code>Ingress</code> is omitted here because it needs no change; there is no way to obtain and renew Let's Encrypt certificates on Tailscale hostnames.</p> <p>Applying the changes to this <code>Ingress</code> triggers the request for a certificate signed by Let's Encrypt; there will be a pending order and challenge for a new certificate:</p> <pre><code>$ kubectl apply -f dashboard/octavo-ingress.yaml\ningress.networking.k8s.io/kubernetes-dashboard-ingress configured\ningress.networking.k8s.io/kubernetes-dashboard-ingress-tailscale unchanged\n\n$ kubectl get svc -A | grep acme\nkubernetes-dashboard   cm-acme-http-solver-pg4lj                         NodePort       10.105.119.248   &lt;none&gt;          8089:31654/TCP               40s\n</code></pre> <p>The pod behind the service is listening and the logs can be monitored in real time:</p> <pre><code>$ klogs kubernetes-dashboard kubernetes-dashboard | grep acme | cut -f1 -d' ') -f\nI0426 19:51:06.525439       1 solver.go:52] \"starting listener\" logger=\"cert-manager.acmesolver\" expected_domain=\"kubernetes-octavo.very-very-dark-gray.top\" expected_token=\"A8NepZWnfMUnjHcB01FaBct-OtTlyevnybrzEu2d2lo\" expected_key=\"A8NepZWnfMUnjHcB01FaBct-OtTlyevnybrzEu2d2lo.iqPqqTpFo6Xc2HKxELaaa6msFZd96MSHPdgrxtrPdwM\" listen_port=8089\n</code></pre> <p>Now, instead of routing requests to the different <code>NodePort</code> assigned each time, it is more convinient to update route requests always to a fixed port 32080, and then <code>patch</code> the ACME resolve service to change the service's NodePort.</p> <p>The <code>patch</code> command is directed at the specific ACME resolver service in each namespace:</p> <pre><code>$ kubectl -n kubernetes-dashboard patch \\\n    service cm-acme-http-solver-pg4lj \\\n     -p '{\"spec\":{\"ports\": [{\"port\": 8089, \"nodePort\": 32080}]}}'\nservice/cm-acme-http-solver-pg4lj patched\n\n$ kubectl get svc -A | grep acme\nkubernetes-dashboard   cm-acme-http-solver-pg4lj                         NodePort       10.105.119.248   &lt;none&gt;          8089:32080/TCP               2m47s\n</code></pre> <p>Once the service is patched, external connections can be selectively routed to this port by adding a public hostname to route only requests for paths under <code>/.well-known</code> to port 32080 over plain HTTP. Combined with the patching of the ACME solver above, this makes solvers for <code>HTTP01</code> challenges reachable, so certificates can be issued and renewed.</p> <p></p> <p>Very soon after making the ACME solver reachable, the challenge is resolved and a valid certificate is obtained and installed.</p> <p>At this point it is no longer necessary to have No TLS Verify enable under because Nginx is now using a certificate signed by Let's Encrypt. To complete the end-to-end test, disable No TLS Verify and set Origin Server Name to the FQDN (<code>kubernetes-octavo.very-very-dark-gray.top</code>) so that Cloudflare accepts the new certificate.</p> <p>Although this is not really necessary when accessing services through a Cloudflare Tunnel, it does serve as a good end-to-end test and secures the communication between the Cloudflare connector and Nginx. Later, the same mechanism will make it possible to obtain and renew Let's Encrypt certificates for services exposed externally by means of forwarding the router's external port 443 to the <code>LoadBalancer</code> IP address of Nginx on this server, so that services can be accessed directly, securely and without bandwidth contraints through a different domain, e.g. https://k8s.octavo.uu.am.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#automatic-renewal","title":"Automatic renewal","text":"<p>The above <code>patch</code> operation can be automated to update each ACME resolver service when it starts:</p> cert-renewal-port-fwd.sh<pre><code>#!/bin/bash\n#\n# Patch the nodePort of running cert-manager renewal challenge, to listen\n# on port 32080 which is the one the router is forwarding port 80 to.\n\n# Check if there is a LetsEncrypt challenge resolver (acme) running.\nexport KUBECONFIG=/etc/kubernetes/admin.conf\nnamespace=$(kubectl get svc -A | grep acme | awk '{print $1}' | head -1)\nservice=$(kubectl get svc -A | grep acme | awk '{print $2}' | head -1)\n\n# Patch the service to listen on port 32080 (set up in router).\nif [ -n \"${namespace}\" ] &amp;&amp; [ -n \"${service}\" ]; then\n    kubectl \\\n      -n \"${namespace}\" patch service \"${service}\" \\\n      -p '{\"spec\":{\"ports\": [{\"port\": 8089, \"nodePort\":32080}]}}'\nfi\n</code></pre> <p>Install this script in a convenient location and setup <code>crontab</code> to run it every minute:</p> <pre><code># Hourly patch montly cert renewal solvers.\n* * * * * /home/pi/cert-renewal-port-fwd.sh\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#test-automatic-renewal","title":"Test automatic renewal","text":"<p>Expose the Kubernetes dashboard at https://k8s.octavo.uu.am to test the automatic renewal of Let's Encrypt certificates end-to-end. More generally, this is the process to expose services directly without tunnels for high-bandwidth applications:</p> <ol> <li>Add a DNS record to point <code>k8s.octavo.uu.am</code> to the router's external IP address.</li> <li>Add a port forwarding rule to redirect port 80 to port 32080 on the server. </li> <li>Add a port forwarding rule to redirect port 443 to the IP address of Nginx.</li> <li>In the <code>kubernetes-dashboard-ingress</code> (not a new one),<ul> <li>duplicate the <code>host</code> and <code>tls</code> objects,</li> <li>replace the <code>.top</code> FQDN with the <code>k8s.octavo.uu.am</code> and</li> <li>rename the new <code>tls.secretName</code> to avoid hitting Let's Encrypt rate limits.</li> </ul> </li> </ol> dashboard/nginx-ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard-ingress\n  namespace: kubernetes-dashboard\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"false\"\n    nginx.ingress.kubernetes.io/whitelist-source-range: 10.244.0.0/16\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      more_set_headers \"server: hide\";\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: kubernetes-octavo.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard-kong-proxy\n                port:\n                  number: 443\n    - host: k8s.octavo.uu.am\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard-kong-proxy\n                port:\n                  number: 443\n  tls:\n    - secretName: tls-secret-cloudflare\n      hosts:\n        - kubernetes-octavo.very-very-dark-gray.top\n    - secretName: tls-secret-uu-am\n      hosts:\n        - k8s.octavo.uu.am\n</code></pre> <p>Once the DNS record has propagated and port 443 is redirected, the dashboard is reachable only by adding the <code>Host</code> and ignoring SSL verifycation (<code>-k</code>):</p> <pre><code>$ curl 2&gt;/dev/null -k \\\n  -H \"Host: kubernetes-octavo.very-very-dark-gray.top\"\\\n  https://k8s.octavo.uu.am/ \\\n  | head -2\n&lt;!--\nCopyright 2017 The Kubernetes Authors.\n\n$ curl \\\n  -H \"Host: kubernetes-octavo.very-very-dark-gray.top\"\\\n  https://k8s.octavo.uu.am/\ncurl: (60) SSL certificate problem: self-signed certificate\nMore details here: https://curl.se/docs/sslcerts.html\n\ncurl failed to verify the legitimacy of the server and therefore could not\nestablish a secure connection to it. To learn more about this situation and\nhow to fix it, please visit the web page mentioned above.\n</code></pre> <p>Applying the changes to the <code>Ingress</code> will trigger the requests for a new certifiate and the automatic patching of the service will get the process completed within a couple of minmutes (provided port 80 is correctly redirected):</p> <pre><code>$ kubectl apply -f dashboard/octavo-ingress.yaml\ningress.networking.k8s.io/kubernetes-dashboard-ingress configured\ningress.networking.k8s.io/kubernetes-dashboard-ingress-tailscale unchanged\n\n$ kubectl get svc -A | grep acme\nkubernetes-dashboard   cm-acme-http-solver-wlk2b                         NodePort       10.102.244.182   &lt;none&gt;          8089:32562/TCP               0s\n\n$ klogs kubernetes-dashboard kubernetes-dashboard | grep acme | cut -f1 -d' ') -f\nI0427 06:42:50.918410       1 solver.go:52] \"starting listener\" logger=\"cert-manager.acmesolver\" expected_domain=\"k8s.octavo.uu.am\" expected_token=\"NtYo8LxQxMIGK78bsXvv65RwI4skIolgdtSrWNuLeRs\" expected_key=\"NtYo8LxQxMIGK78bsXvv65RwI4skIolgdtSrWNuLeRs.iqPqqTpFo6Xc2HKxELaaa6msFZd96MSHPdgrxtrPdwM\" listen_port=8089\nI0427 06:43:09.789648       1 solver.go:89] \"validating request\" logger=\"cert-manager.acmesolver\" host=\"k8s.octavo.uu.am\" path=\"/.well-known/acme-challenge/NtYo8LxQxMIGK78bsXvv65RwI4skIolgdtSrWNuLeRs\" base_path=\"/.well-known/acme-challenge\" token=\"NtYo8LxQxMIGK78bsXvv65RwI4skIolgdtSrWNuLeRs\" headers={\"Accept-Encoding\":[\"gzip\"],\"Connection\":[\"close\"],\"User-Agent\":[\"cert-manager-challenges/v1.17.2 (linux/amd64) cert-manager/f3ffb86641f75d94d01e5a2606b9871ff89645ef\"]}\n...\nI0427 06:43:18.934689       1 solver.go:112] \"got successful challenge request, writing key\" logger=\"cert-manager.acmesolver\" host=\"k8s.octavo.uu.am\" path=\"/.well-known/acme-challenge/NtYo8LxQxMIGK78bsXvv65RwI4skIolgdtSrWNuLeRs\" base_path=\"/.well-known/acme-challenge\" token=\"NtYo8LxQxMIGK78bsXvv65RwI4skIolgdtSrWNuLeRs\" headers={\"Accept\":[\"*/*\"],\"Accept-Encoding\":[\"gzip\"],\"Connection\":[\"close\"],\"User-Agent\":[\"Mozilla/5.0 (compatible; Let's Encrypt validation server; +https://www.letsencrypt.org)\"]}\nE0427 06:43:20.531057       1 main.go:42] \"error executing command\" err=\"http: Server closed\" logger=\"cert-manager\"\n</code></pre> <p>With the new certificate installed, the dashboard is now reachable at  https://k8s.octavo.uu.am/ without ignoring SSL verifycation (<code>-k</code>):</p> <pre><code>$ curl 2&gt;/dev/null \\\n  https://k8s.octavo.uu.am/ \\\n| head -2\n&lt;!--\nCopyright 2017 The Kubernetes Authors.\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#migration-from-lexicon","title":"Migration from Lexicon","text":"<p>With the new cluster up and running, the next step is to migrate (most of) the applications installed previously in <code>lexicon</code> over to <code>octavo</code>, while preserving their storage and status. </p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#audiobookshelf","title":"Audiobookshelf","text":"<p>Audiobookshelf has been easily my most used application for over a year, and already migrated it to <code>rapture</code> once as a test, so the process here is essentially the same. The deployment is only different in the <code>securityContext</code> UID/GID and FQDN where the service will be available at:</p> Kubernetes deployment: <code>audiobookshelf.yaml</code> audiobookshelf.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: audiobookshelf\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-config\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/audiobookshelf/config\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-metadata\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/audiobookshelf/metadata\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-audiobooks\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/nas/public/audio/Audiobooks\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: audiobookshelf-pv-podcasts\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/nas/public/audio/Podcasts\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-config\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-config\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-metadata\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-metadata\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-audiobooks\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-audiobooks\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: audiobookshelf-pvc-podcasts\n  namespace: audiobookshelf\nspec:\n  storageClassName: manual\n  volumeName: audiobookshelf-pv-podcasts\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: audiobookshelf\n  name: audiobookshelf\n  namespace: audiobookshelf\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: audiobookshelf\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: audiobookshelf\n    spec:\n      containers:\n        - image: ghcr.io/advplyr/audiobookshelf:latest\n          imagePullPolicy: Always\n          name: audiobookshelf\n          env:\n          - name: PORT\n            value: \"13378\"\n          ports:\n          - containerPort: 13378\n          resources: {}\n          stdin: true\n          tty: true\n          volumeMounts:\n          - mountPath: /config\n            name: audiobookshelf-config\n          - mountPath: /metadata\n            name: audiobookshelf-metadata\n          - mountPath: /audiobooks\n            name: audiobookshelf-audiobooks\n          - mountPath: /podcasts\n            name: audiobookshelf-podcasts\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 117\n            runAsGroup: 117\n      restartPolicy: Always\n      volumes:\n      - name: audiobookshelf-config\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-config\n      - name: audiobookshelf-metadata\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-metadata\n      - name: audiobookshelf-audiobooks\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-audiobooks\n      - name: audiobookshelf-podcasts\n        persistentVolumeClaim:\n          claimName: audiobookshelf-pvc-podcasts\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: audiobookshelf-svc\n  namespace: audiobookshelf\nspec:\n  type: NodePort\n  ports:\n  - port: 13388\n    nodePort: 31378\n    targetPort: 13378\n  selector:\n    app: audiobookshelf\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: audiobookshelf-ingress\n  namespace: audiobookshelf\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: audiobookshelf-svc\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: audiobookshelf.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: audiobookshelf-svc\n                port:\n                  number: 13378\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - audiobookshelf.very-very-dark-gray.top\n</code></pre> <p>First, stop Audiobookshelf in <code>lexicon</code> (it won't be used moving forward):</p> <pre><code>$ kubectl scale -n audiobookshelf deployment audiobookshelf --replicas=0\ndeployment.apps/audiobookshelf scaled\n</code></pre> <p>Then copy the <code>/config</code> and <code>/metadata</code> directories over from <code>lexicon</code> to <code>octavo</code>:</p> <pre><code>root@octavo ~ # groupadd audiobookshelf -g 117\nroot@octavo ~ # useradd  audiobookshelf -u 117 -g 117 -s /usr/sbin/nologin\nroot@octavo ~ # rsync -ua lexicon:/home/k8s/audiobookshelf /home/k8s/\nroot@octavo ~ # chown -R audiobookshelf:audiobookshelf /home/k8s/audiobookshelf\nroot@octavo ~ # ls -hal /home/k8s/audiobookshelf\ndrwxr-xr-x 1 audiobookshelf audiobookshelf  28 Feb 27  2024 .\ndrwxr-xr-x 1 root           root           104 Apr 28 22:22 ..\ndrwxr-xr-x 1 audiobookshelf audiobookshelf 102 Apr 28 21:27 config\ndrwxr-xr-x 1 audiobookshelf audiobookshelf 230 Jan 14 20:54 metadata\n</code></pre> <p>Finally, start the deployment in <code>octavo</code>:</p> <pre><code>$ kubectl apply -f audiobookshelf.yaml\nnamespace/audiobookshelf created\npersistentvolume/audiobookshelf-pv-config created\npersistentvolume/audiobookshelf-pv-metadata created\npersistentvolume/audiobookshelf-pv-audiobooks created\npersistentvolume/audiobookshelf-pv-podcasts created\npersistentvolumeclaim/audiobookshelf-pvc-config created\npersistentvolumeclaim/audiobookshelf-pvc-metadata created\npersistentvolumeclaim/audiobookshelf-pvc-audiobooks created\npersistentvolumeclaim/audiobookshelf-pvc-podcasts created\ndeployment.apps/audiobookshelf created\nservice/audiobookshelf-svc created\ningress.networking.k8s.io/audiobookshelf-ingress created\n\n$ kubectl get all -n audiobookshelf\nNAME                                  READY   STATUS    RESTARTS   AGE\npod/audiobookshelf-5b486f64b4-8zd2g   1/1     Running   0          20s\npod/cm-acme-http-solver-wd8rk         1/1     Running   0          16s\n\nNAME                                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\nservice/audiobookshelf-svc          NodePort   10.108.231.47   &lt;none&gt;        13388:31378/TCP   20s\nservice/cm-acme-http-solver-vblgl   NodePort   10.105.105.53   &lt;none&gt;        8089:31546/TCP    16s\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/audiobookshelf   1/1     1            1           20s\n\nNAME                                        DESIRED   CURRENT   READY   AGE\nreplicaset.apps/audiobookshelf-5b486f64b4   1         1         1       20s\n\n$ kubectl get svc -n audiobookshelf\nNAME                        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\naudiobookshelf-svc          NodePort   10.108.231.47   &lt;none&gt;        13388:31378/TCP   13s\ncm-acme-http-solver-vblgl   NodePort   10.105.105.53   &lt;none&gt;        8089:31546/TCP    9s\n\n$ kubectl get ingress -n audiobookshelf\nNAME                     CLASS   HOSTS                                    ADDRESS         PORTS     AGE\naudiobookshelf-ingress   nginx   audiobookshelf.very-very-dark-gray.top   192.168.0.171   80, 443   60s\n</code></pre> <p>After a couple for minutes Audiobookshelf is available at https://audiobookshelf.very-very-dark-gray.top/ and works fine.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#home-assistant","title":"Home Assistant","text":"<p>Home Assistant can be deployed in <code>octavo</code> in very much the same way it was on <code>lexicon</code>, using the same base manifests and a new <code>kustomization.yaml</code> file just to set different hostnames:</p> home-assistant/octavo/kustomization.yaml<pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- ../base\n\nconfigMapGenerator:\n- name: home-assistant-config\n  behavior: replace\n  literals:\n  - TZ=\"Europe/Amsterdam\"\n\npatches:\n  - target: #FQDN for Cloudflare Tunnel or port-forwarded\n      kind: Ingress\n      name: home-assistant-nginx\n    patch: |-\n      - op: replace\n        path: /spec/rules/0/host\n        value: home-assistant-octavo.very-very-dark-gray.top\n      - op: replace\n        path: /spec/tls/0/hosts/0\n        value: home-assistant-octavo.very-very-dark-gray.top\n  - target: # Hostname-only for Tailscale [Funnel]\n      kind: Ingress\n      name: home-assistant-tailscale\n    patch: |-\n      - op: replace\n        path: /spec/tls/0/hosts/0\n        value: home-assistant-octavo\n</code></pre> <p>Prepare remote access  first, then create <code>/home/k8s/home-assistant/</code> anew and apply the deployment.</p> <pre><code>$ sudo mkdir /home/k8s/home-assistant\n$ ls -lah /home/k8s/home-assistant\ntotal 0\ndrwxr-xr-x 1 root root  0 Apr 27 14:03 .\ndrwxr-xr-x 1 root root 28 Apr 27 14:03 ..\n\n$ kubectl apply -k octavo\nnamespace/home-assistant created\nconfigmap/home-assistant-config-59kccc4bcd created\nconfigmap/home-assistant-configmap created\nservice/home-assistant-svc created\npersistentvolume/home-assistant-pv-config created\npersistentvolumeclaim/home-assistant-config-root created\ndeployment.apps/home-assistant created\ningress.networking.k8s.io/home-assistant-nginx created\ningress.networking.k8s.io/home-assistant-tailscale created\n\n$ kubectl get all -n home-assistant\nNAME                                  READY   STATUS    RESTARTS   AGE\npod/cm-acme-http-solver-ssftt         1/1     Running   0          54s\npod/home-assistant-77bf44c47b-tqz2s   1/1     Running   0          59s\n\nNAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nservice/cm-acme-http-solver-f7jbv   NodePort    10.110.189.111   &lt;none&gt;        8089:31135/TCP   54s\nservice/home-assistant-svc          ClusterIP   10.107.114.233   &lt;none&gt;        8123/TCP         59s\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/home-assistant   1/1     1            1           59s\n\nNAME                                        DESIRED   CURRENT   READY   AGE\nreplicaset.apps/home-assistant-77bf44c47b   1         1         1       59s\n\n$ sudo ls -lah /home/k8s/home-assistant\ntotal 916K\ndrwxr-xr-x 1 root root  372 Apr 27 14:14 .\ndrwxr-xr-x 1 root root   28 Apr 27 14:03 ..\ndrwxr-xr-x 1 root root   32 Apr 27 14:14 blueprints\ndrwxr-xr-x 1 root root    0 Apr 27 14:14 .cloud\n-rw-r--r-- 1 root root    0 Apr 27 14:14 configuration.yaml\n-rw-r--r-- 1 root root    8 Apr 27 14:14 .HA_VERSION\n-rw-r--r-- 1 root root    0 Apr 27 14:14 home-assistant.log\n-rw-r--r-- 1 root root    0 Apr 27 14:14 home-assistant.log.1\n-rw-r--r-- 1 root root    0 Apr 27 14:14 home-assistant.log.fault\n-rw-r--r-- 1 root root 4.0K Apr 27 14:14 home-assistant_v2.db\n-rw-r--r-- 1 root root  32K Apr 27 14:30 home-assistant_v2.db-shm\n-rw-r--r-- 1 root root 866K Apr 27 14:30 home-assistant_v2.db-wal\ndrwxr-xr-x 1 root root  428 Apr 27 14:29 .storage\ndrwxr-xr-x 1 root root    0 Apr 27 14:14 tts\n</code></pre> <p>After less than a minute, the ACME solver is patched to listen on port <code>32080</code>, and after just about another minute the solver is gone. At that point Home Assistant is available at both https://home-assistant-octavo.royal-penny.ts.net/ and https://home-assistant-octavo.very-very-dark-gray.top/ ready to start the onboarding process.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#bluetooth-setup","title":"Bluetooth setup","text":"<p>Bluetooth failed setup in <code>lexicon</code> and the same is bound to repeat in <code>octavo</code> now; even though the base deployment manifests already mount <code>/run/dbus</code> it is also required to switch from dbus-daemon to dbus-broker and install BlueZ:</p> <pre><code>$ sudo apt install bluez dbus-broker -y\n\n$ sudo systemctl --global enable dbus-broker.service\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#restore-backup","title":"Restore backup","text":"<p>At this point a fresh backup from <code>lexicon</code> could be restored in <code>octavo</code> to initialize Home Assistant, including the tweaked <code>configuration.yaml</code> file, Home Assistant Community Store and all the cards and dashboards.</p> <p>However, restoring a backup fails consistently with an OS-level error that doesn't seem to make sense; the logs for the pod show that a process was unable to open the main <code>configuration.yaml</code> file, even though there was definitely nothing accessing it at the time:</p> <pre><code>$ klogs home-assistant home-assistant\ns6-rc: info: service s6rc-oneshot-runner: starting\ns6-rc: info: service s6rc-oneshot-runner successfully started\ns6-rc: info: service fix-attrs: starting\ns6-rc: info: service fix-attrs successfully started\ns6-rc: info: service legacy-cont-init: starting\ns6-rc: info: service legacy-cont-init successfully started\ns6-rc: info: service legacy-services: starting\nservices-up: info: copying legacy longrun home-assistant (no readiness notification)\ns6-rc: info: service legacy-services successfully started\n\n\n[12:58:06] INFO: Home Assistant Core finish process exit code 100\nINFO:homeassistant.backup_restore:Restoring /config/backups/lexicon-last_2025-04-27_14.40_32799259.tar\nTraceback (most recent call last):\n  File \"&lt;frozen runpy&gt;\", line 198, in _run_module_as_main\n  File \"&lt;frozen runpy&gt;\", line 88, in _run_code\n  File \"/usr/src/homeassistant/homeassistant/__main__.py\", line 227, in &lt;module&gt;\n    sys.exit(main())\n             ~~~~^^\n  File \"/usr/src/homeassistant/homeassistant/__main__.py\", line 186, in main\n    if restore_backup(config_dir):\n       ~~~~~~~~~~~~~~^^^^^^^^^^^^\n  File \"/usr/src/homeassistant/homeassistant/backup_restore.py\", line 197, in restore_backup\n    _extract_backup(\n    ~~~~~~~~~~~~~~~^\n        config_dir=config_dir,\n        ^^^^^^^^^^^^^^^^^^^^^^\n        restore_content=restore_content,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/usr/src/homeassistant/homeassistant/backup_restore.py\", line 143, in _extract_backup\n    _clear_configuration_directory(config_dir, keep)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/homeassistant/homeassistant/backup_restore.py\", line 87, in _clear_configuration_directory\n    entrypath.unlink()\n    ~~~~~~~~~~~~~~~~^^\n  File \"/usr/local/lib/python3.13/pathlib/_local.py\", line 746, in unlink\n    os.unlink(self)\n    ~~~~~~~~~^^^^^^\nOSError: [Errno 16] Resource busy: '/config/configuration.yaml'\n[12:58:08] INFO: Home Assistant Core finish process exit code 1\n[12:58:08] INFO: Home Assistant Core service shutdown\ns6-rc: info: service legacy-services: stopping\ns6-rc: info: service legacy-services successfully stopped\ns6-rc: info: service legacy-cont-init: stopping\ns6-rc: info: service legacy-cont-init successfully stopped\ns6-rc: info: service fix-attrs: stopping\ns6-rc: info: service fix-attrs successfully stopped\ns6-rc: info: service s6rc-oneshot-runner: stopping\ns6-rc: info: service s6rc-oneshot-runner successfully stopped\n</code></pre> <p>The error persists after Home Assistant (pod) is restarted:</p> <pre><code>$ klogs home-assistant home-assistant\ns6-rc: info: service s6rc-oneshot-runner: starting\ns6-rc: info: service s6rc-oneshot-runner successfully started\ns6-rc: info: service fix-attrs: starting\ns6-rc: info: service fix-attrs successfully started\ns6-rc: info: service legacy-cont-init: starting\ns6-rc: info: service legacy-cont-init successfully started\ns6-rc: info: service legacy-services: starting\nservices-up: info: copying legacy longrun home-assistant (no readiness notification)\ns6-rc: info: service legacy-services successfully started\n2025-04-27 14:58:16.784 WARNING (MainThread) [homeassistant.components.backup]\n  Backup restore failed with OSError: [Errno 16] Resource busy: '/config/configuration.yaml'\n</code></pre> <p>In the meantime, the Home Assistant onboarding page keeps waiting even though nothing is happening in the backend. Opening the same page on a new browser tab and selecting the option to restore the backup shows the same error:</p> <pre><code>The backup could not be restored. Please try again.\n\nError:  \n[Errno 16] Resource busy: '/config/configuration.yaml'\n</code></pre> <p>Trying again consistently leads to the same error time and again, even after stopping the pods, deleting the entire <code>/home/k8s/home-assistant</code> directory and starting anew with a fresh empty one.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#hardcore-migration","title":"Hardcore migration","text":"<p>The solution turned out to be to simply copy over the <code>/home/k8s/home-assistant</code> directory at the rigth time between stopping and starting the relevant services:</p> <ol> <li> <p>Stop Home Assistant on both servers by scaling the deployment to 0 replicas:</p> <pre><code>$ kubectl scale -n home-assistant deployment home-assistant --replicas=0\ndeployment.apps/home-assistant scaled\n</code></pre> </li> <li> <p>Copy <code>/home/k8s/home-assistant</code> over from <code>lexicon</code> to <code>octavo</code>:</p> <pre><code>root@octavo ~ # rm -rf /home/k8s/home-assistant\nroot@octavo ~ # rsync -ua lexicon:/home/k8s/home-assistant /home/k8s/\nroot@octavo ~ # ls -hal /home/k8s/home-assistant\ntotal 110M\ndrwxr-xr-x 1 root root  570 Apr 27 23:28 .\ndrwxr-xr-x 1 root root   76 Apr 28 21:14 ..\n-rw-r--r-- 1 root root    0 Apr 25 22:39 automations.yaml\ndrwxr-xr-x 1 root root  484 Apr 28 05:37 backups\ndrwxr-xr-x 1 root root   48 Apr 25 22:42 blueprints\ndrwxr-xr-x 1 root root    0 Apr 21 11:13 .cloud\n-rw-r--r-- 1 root root  670 Apr 27 19:37 configuration.yaml\ndrwxr-xr-x 1 root root   26 Apr 24 21:04 custom_components\ndrwxr-xr-x 1 root root    0 Apr 21 17:58 deps\n-rw-r--r-- 1 root root    8 Apr 26 05:08 .HA_VERSION\n-rw-r--r-- 1 root root 2.2M Apr 28 22:20 home-assistant.log\n-rw-r--r-- 1 root root 5.8K Apr 27 23:28 home-assistant.log.1\n-rw-r--r-- 1 root root    0 Apr 27 23:28 home-assistant.log.fault\n-rw-r--r-- 1 root root 103M Apr 28 22:18 home-assistant_v2.db\n-rw-r--r-- 1 root root  32K Apr 28 22:20 home-assistant_v2.db-shm\n-rw-r--r-- 1 root root 4.4M Apr 28 22:20 home-assistant_v2.db-wal\ndrwxr-xr-x 1 root root  448 Apr 23 21:07 image\n-rw-r--r-- 1 root root 4.9K Apr 22 22:59 install-hacs.sh\n-rw-r--r-- 1 root root    0 Apr 25 22:39 scenes.yaml\n-rw-r--r-- 1 root root    0 Apr 25 22:39 scripts.yaml\ndrwxr-xr-x 1 root root 1.3K Apr 28 22:13 .storage\ndrwxr-xr-x 1 root root   14 Apr 25 22:29 templates\ndrwxr-xr-x 1 root root    0 Apr 21 11:13 tts\ndrwxr-xr-x 1 root root   18 Apr 23 22:58 www\n</code></pre> </li> <li> <p>Start Home Assistant on <code>octavo</code> only (scaling back to 1 replica):</p> <pre><code>$ kubectl scale -n home-assistant deployment home-assistant --replicas=1\ndeployment.apps/home-assistant scaled\n</code></pre> </li> </ol> <p>Now Home Assistant is available at https://home-assistant-octavo.royal-penny.ts.net/ and https://home-assistant-octavo.very-very-dark-gray.top/ and working flawlessly, with the same user credentails, except for just one little quirk:</p> <p></p> <p><code>DC:21:48:43:B7:C2</code> is the Bluetooth adapter in <code>lexicon</code>, so it can be removed, and <code>D0:65:78:A5:8B:E1</code> is the Bluetooth adapter in <code>octavo</code> so that's the one to add.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#influxdb","title":"InfluxDB","text":"<p>The InfluxDB integration stopped reporting metrics to <code>octavo</code> even though the configuration remained valid; the integration had gone missing and would no longer be found under Settings &gt; Devices &amp; services. To restore reporting it was found necessary to</p> <ol> <li>Remove the <code>influxdb</code> section from Home Assistant's <code>configuration.yaml</code>.</li> <li>Restart Home Assistant.</li> <li>Add the <code>influxdb</code> section again to <code>configuration.yaml</code>.</li> <li>Restart Home Assistant again.</li> </ol> <p>This persuaded Home Assistant to add the InfluxDB integration back, and after that the integration simply resumed sending all metrics again.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#synology-dsm","title":"Synology DSM","text":"<p>Another little surprise from Home Assistant after moving to <code>octavo</code> is Synology DSM discovering the Synology DS423+ (luggage). Create a new NAS user in the admin group but without access to any files, but with access to all services:</p> <p></p> <p>Then add this user's credentials to the Synology DSM integration and specify port 5001 to use HTTPS, check the option for Uses an SSL certificate but uncheck the one for Verify SSL certificate. After sumitting, Home Assistant creates devices for the NAS, each volume and each drive. This adds 5 devides and 43 entities, to be used later to create a cool dashboard.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#influxdb-and-grafana","title":"InfluxDB and Grafana","text":"<p>InfluxDB and Grafana running in <code>lexicon</code> collect data from all sources reporting through Continuous Monitoring and serves the dashboards. This setup is about a year old and runing Grafana 10.4.2 and InfluxDB 1.8. Seems like time to update these:</p> <ul> <li>Upgrading InfluxDB from v1.8.10 to v1.11.7 is a large jump     and there is no reason to switch to v2 given     the future of Flux     (deprecated and not planned for InfluxDB v3).</li> <li>Breaking changes in Grafana v11.0     looks like it should be fine to update from 10.4.2, at least worth a try.</li> </ul> <p>At this time the latest versions available in Docker images are InfluxDB 1.11.8 and Grafana 11.6.1.</p> Combined deployment for InfluxDB and Grafana monitoring.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: monitoring\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: influxdb-pv\n  labels:\n    type: local\n  namespace: monitoring\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 30Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/k8s/influxdb\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: influxdb-pv-claim\n  namespace: monitoring\nspec:\n  storageClassName: manual\n  volumeName: influxdb-pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 30Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  labels:\n    app: influxdb\n  name: influxdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influxdb\n  template:\n    metadata:\n      labels:\n        app: influxdb\n    spec:\n      hostname: influxdb\n      containers:\n      - image: docker.io/influxdb:1.11.8\n        env:\n        - name: \"INFLUXDB_HTTP_AUTH_ENABLED\"\n          value: \"true\"\n        name: influxdb\n        volumeMounts:\n        - mountPath: /var/lib/influxdb\n          name: influxdb-data\n      securityContext:\n        runAsUser: 114\n        runAsGroup: 114\n      volumes:\n      - name: influxdb-data\n        persistentVolumeClaim:\n          claimName: influxdb-pv-claim\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: influxdb\n  name: influxdb-svc\n  namespace: monitoring\nspec:\n  ports:\n  - port: 18086\n    protocol: TCP\n    targetPort: 8086\n    nodePort: 30086\n  selector:\n    app: influxdb\n  type: NodePort\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: grafana-pv\n  labels:\n    type: local\n  namespace: monitoring\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 3Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/k8s/grafana\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: grafana-pv-claim\n  namespace: monitoring\nspec:\n  storageClassName: manual\n  volumeName: grafana-pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: monitoring\n  labels:\n    app: grafana\n  name: grafana\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n  template:\n    metadata:\n      labels:\n        app: grafana\n    spec:\n      containers:\n      - image: docker.io/grafana/grafana:11.6.1\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: \"GF_AUTH_ANONYMOUS_ENABLED\"\n          value: \"true\"\n        - name: \"GF_SECURITY_ADMIN_USER\"\n          value: \"admin\"\n        - name: \"GF_SECURITY_ADMIN_PASSWORD\"\n          value: \"__________________________\"\n        name: grafana\n        volumeMounts:\n          - name: grafana-data\n            mountPath: /var/lib/grafana\n      securityContext:\n        runAsUser: 115\n        runAsGroup: 115\n        fsGroup: 115\n      volumes:\n      - name: grafana-data\n        persistentVolumeClaim:\n          claimName: grafana-pv-claim\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: grafana\n  name: grafana-svc\n  namespace: monitoring\nspec:\n  ports:\n  - port: 13000\n    protocol: TCP\n    targetPort: 3000\n    nodePort: 30300\n  selector:\n    app: grafana\n  type: NodePort\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana-ingress\n  namespace: monitoring\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: grafana.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: grafana-svc\n                port:\n                  number: 3000\n  tls:\n    - secretName: tls-secret-grafana\n      hosts:\n        - grafana.very-very-dark-gray.top\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: influxdb-ingress\n  namespace: monitoring\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: influxdb.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: influxdb-svc\n                port:\n                  number: 8086\n  tls:\n    - secretName: tls-secret-influxdb\n      hosts:\n        - influxdb.very-very-dark-gray.top\n</code></pre> <p>Before deploying the above, dedicated users and home directories must be created for <code>influxdb</code> and <code>grafana</code>:</p> <pre><code>root@octavo ~ # groupadd influxdb -g 114\nroot@octavo ~ # groupadd grafana  -g 115\nroot@octavo ~ # useradd  influxdb -u 114 -g 114 -s /usr/sbin/nologin\nroot@octavo ~ # useradd  grafana  -u 115 -g 115 -s /usr/sbin/nologin\n</code></pre> <p>Then files need to be copied over at the right time between stopping the services in <code>lexicon</code> and starting them again:</p> <ol> <li> <p>Stop Grafana and InfluxDB (in this order) in <code>lexicon</code>:</p> <pre><code>$ kubectl scale -n monitoring deployment grafana  --replicas=0\ndeployment.apps/grafana scaled\n$ kubectl scale -n monitoring deployment influxdb --replicas=0\ndeployment.apps/influxdb scaled\n</code></pre> </li> <li> <p>Copy data over from <code>lexicon</code> to <code>octavo</code>:</p> <pre><code>root@octavo ~ # rsync -ua lexicon:/home/k8s/influxdb /home/k8s/\nroot@octavo ~ # rsync -ua lexicon:/home/k8s/grafana  /home/k8s/\nroot@octavo ~ # chown -R influxdb:influxdb /home/k8s/influxdb\nroot@octavo ~ # chown -R  grafana:grafana  /home/k8s/grafana\nroot@octavo ~ # ls -hal /home/k8s/influxdb /home/k8s/grafana\n/home/k8s/grafana:\ntotal 3.9M\ndrwxr-xr-x 1 grafana grafana   68 Apr 27 19:08 .\ndrwxr-xr-x 1 root    root      58 Apr 27 19:10 ..\ndrwxr-x--- 1 grafana grafana    2 Apr 20  2024 alerting\ndrwx------ 1 grafana grafana    0 Apr 20  2024 csv\n-rw-r----- 1 grafana grafana 3.9M Apr 27 19:08 grafana.db\ndrwx------ 1 grafana grafana    0 Apr 20  2024 pdf\ndrwxr-xr-x 1 grafana grafana    0 Apr 20  2024 plugins\ndrwx------ 1 grafana grafana    0 Apr 20  2024 png\n\n/home/k8s/influxdb:\ntotal 0\ndrwxr-xr-x 1 influxdb influxdb 22 Apr 20  2024 .\ndrwxr-xr-x 1 root     root     58 Apr 27 19:10 ..\ndrwxr-xr-x 1 influxdb influxdb 90 Apr 26 08:20 data\ndrwxr-xr-x 1 influxdb influxdb 14 Apr 27 02:08 meta\ndrwx------ 1 influxdb influxdb 90 Apr 26 08:20 wal\n</code></pre> </li> <li> <p>Start InfluxDB and Grafana (in this order) in <code>lexicon</code>:</p> <pre><code>$ kubectl scale -n monitoring deployment influxdb --replicas=1\ndeployment.apps/influxdb scaled\n$ kubectl scale -n monitoring deployment grafana  --replicas=1\ndeployment.apps/grafana scaled\n</code></pre> </li> </ol> <p>Finally, start the deployment in <code>octavo</code>:</p> <pre><code>$ kubectl apply -f monitoring.yaml\nnamespace/monitoring created\npersistentvolume/influxdb-pv created\npersistentvolumeclaim/influxdb-pv-claim created\ndeployment.apps/influxdb created\nservice/influxdb-svc created\npersistentvolume/grafana-pv created\npersistentvolumeclaim/grafana-pv-claim created\ndeployment.apps/grafana created\nservice/grafana-svc created\ningress.networking.k8s.io/grafana-ingress created\ningress.networking.k8s.io/influxdb-ingress created\n\n$ kubectl get all -n monitoring\nNAME                            READY   STATUS    RESTARTS   AGE\npod/cm-acme-http-solver-dkgn5   1/1     Running   0          59s\npod/grafana-6fff9dbb6c-v22hg    1/1     Running   0          62s\npod/influxdb-5974bf664f-8r5mf   1/1     Running   0          62s\n\nNAME                                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\nservice/cm-acme-http-solver-n8l8h   NodePort   10.98.3.183     &lt;none&gt;        8089:30378/TCP    59s\nservice/grafana-svc                 NodePort   10.110.29.239   &lt;none&gt;        13000:30300/TCP   62s\nservice/influxdb-svc                NodePort   10.110.65.108   &lt;none&gt;        18086:30086/TCP   62s\n\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana    1/1     1            1           62s\ndeployment.apps/influxdb   1/1     1            1           62s\n\nNAME                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-6fff9dbb6c    1         1         1       62s\nreplicaset.apps/influxdb-5974bf664f   1         1         1       62s\n\n$ kubectl get svc -n monitoring\nNAME                        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\ncm-acme-http-solver-n8l8h   NodePort   10.98.3.183     &lt;none&gt;        8089:30378/TCP    63s\ngrafana-svc                 NodePort   10.110.29.239   &lt;none&gt;        13000:30300/TCP   66s\ninfluxdb-svc                NodePort   10.110.65.108   &lt;none&gt;        18086:30086/TCP   66s\n\n$ kubectl get ingress -n monitoring\nNAME               CLASS   HOSTS                              ADDRESS         PORTS     AGE\ngrafana-ingress    nginx   grafana.very-very-dark-gray.top    192.168.0.171   80, 443   69s\ninfluxdb-ingress   nginx   influxdb.very-very-dark-gray.top   192.168.0.171   80, 443   69s\n</code></pre> <p>This worked surprisingly well, both services became quickly available at their assigned URLs, with authentication working as intended and all dashboards working as before. The only other change needed was getting data flowing into the new InfluxDB server, by updating the <code>conmon</code> scripts in all the reporting systems.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#clean-up-lexicon","title":"Clean up <code>lexicon</code>","text":"<p>Once InfluxDB is running in <code>octavo</code> there are a few additional monitoring scripts to relocate to <code>octavo</code> (because they've been running in <code>lexicon</code>), some of which require additional dependencies to be installed system-wide:</p> <ol> <li><code>conmon-speedtest</code> depens on     <code>speedtest-cli</code></li> <li><code>conmon-tapo.py</code> has more complex     Python dependencies.</li> </ol> <p>Once the scripts work, they need to run from <code>root</code>'s <code>crontab</code>:</p> <pre><code># crontab -e\n*/3 * * * * /home/k8s/code-server/conmon/conmon-speedtest\n*/5 * * * * /home/k8s/code-server/conmon/conmon-tapo.py\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#komga","title":"Komga","text":"<p>Komga (eBook library) was very easy to migrate from <code>lexicon</code>, with only the minor quirk that its <code>/config</code> directory had been inadvertently placed inside itself; fixing this, and the location of eBooks, are the only changes in this deployment:</p> Kubernetes deployment: <code>komga.yaml</code> komga.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: komga\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: komga-pv-config\n  namespace: komga\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/komga\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: komga-pv-books\n  namespace: komga\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/nas/public/ebooks\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: komga-pvc-config\n  namespace: komga\nspec:\n  storageClassName: manual\n  volumeName: komga-pv-config\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: komga-pvc-books\n  namespace: komga\nspec:\n  storageClassName: manual\n  volumeName: komga-pv-books\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: komga\n  name: komga\n  namespace: komga\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: komga\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: komga\n    spec:\n      containers:\n        - image: gotson/komga\n          imagePullPolicy: Always\n          name: komga\n          args: [\"user\", \"118:118\"]\n          env:\n          - name: TZ\n            value: \"Europe/Amsterdam\"\n          ports:\n          - containerPort: 25600\n          resources: {}\n          stdin: true\n          tty: true\n          volumeMounts:\n          - mountPath: /config\n            name: komga-config\n          - mountPath: /data\n            name: komga-books\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 118\n            runAsGroup: 118\n      restartPolicy: Always\n      volumes:\n      - name: komga-config\n        persistentVolumeClaim:\n          claimName: komga-pvc-config\n      - name: komga-books\n        persistentVolumeClaim:\n          claimName: komga-pvc-books\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: komga-svc\n  namespace: komga\nspec:\n  type: NodePort\n  ports:\n  - port: 25600\n    nodePort: 30600\n    targetPort: 25600\n  selector:\n    app: komga\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: komga-ingress\n  namespace: komga\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: komga-svc\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: komga.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: komga-svc\n                port:\n                  number: 25600\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - komga.very-very-dark-gray.top\n</code></pre> <p>Then again, the trick is to copy the <code>/config</code> directory between stopping the service in <code>lexicon</code> and starting it in <code>octavo</code>.</p> <p>First, stop komga in <code>lexicon</code> (it won't be used moving forward):</p> <pre><code>$ kubectl scale -n komga deployment komga --replicas=0\ndeployment.apps/komga scaled\n</code></pre> <p>Copy data over from <code>lexicon</code> to <code>octavo</code>, moving the <code>/config</code> directory one level up:</p> <pre><code>root@octavo ~ # groupadd komga -g 118\nroot@octavo ~ # useradd  komga -u 118 -g 118 -s /usr/sbin/nologin\nroot@octavo ~ # rsync -ua lexicon:/home/k8s/komga/config/ /home/k8s/komga\nroot@octavo ~ # chown -R komga:komga /home/k8s/komga\nroot@octavo ~ # ls -hal /home/k8s/komga\ntotal 14M\ndrwxr-xr-x 1 komga komga   74 Apr 28 23:09 .\ndrwxr-xr-x 1 root  root   114 Apr 28 23:26 ..\n-rw-r--r-- 1 komga komga  14M Apr 28 23:09 database.sqlite\ndrwxr-xr-x 1 komga komga  368 Apr 28 05:05 logs\ndrwxr-xr-x 1 komga komga  668 Mar 16 06:08 lucene\n-rw-r--r-- 1 komga komga 156K Apr 28 23:09 tasks.sqlite\n</code></pre> <p>Warning</p> <p>The trailing <code>/</code> in <code>lexicon:/home/k8s/komga/config/</code> is critical to make the destination directory <code>/home/k8s/komga</code> in <code>octavo</code> be that <code>data</code> directory, instead of containing it.</p> <p>Finally, start the deployment in <code>octavo</code>:</p> <pre><code>$ kubectl apply -f komga.yaml\nnamespace/komga created\npersistentvolume/komga-pv-config created\npersistentvolume/komga-pv-books created\npersistentvolumeclaim/komga-pvc-config created\npersistentvolumeclaim/komga-pvc-books created\ndeployment.apps/komga created\nservice/komga-svc created\ningress.networking.k8s.io/komga-ingress created\n\n$ kubectl get all -n komga\nNAME                            READY   STATUS    RESTARTS   AGE\npod/cm-acme-http-solver-v8rp6   1/1     Running   0          21s\npod/komga-5cc699fdcd-xwqlh      1/1     Running   0          24s\n\nNAME                                TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE\nservice/cm-acme-http-solver-m625s   NodePort   10.108.18.62     &lt;none&gt;        8089:31587/TCP    21s\nservice/komga-svc                   NodePort   10.105.251.174   &lt;none&gt;        25600:30600/TCP   24s\n\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/komga   1/1     1            1           24s\n\nNAME                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/komga-5cc699fdcd   1         1         1       24s\n\n$ kubectl get svc -n komga\nNAME        TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE\nkomga-svc   NodePort   10.105.251.174   &lt;none&gt;        25600:30600/TCP   67s\n\n$ kubectl get ingress -n komga\nNAME            CLASS   HOSTS                           ADDRESS         PORTS     AGE\nkomga-ingress   nginx   komga.very-very-dark-gray.top   192.168.0.171   80, 443   72s\n</code></pre> <p>After a couple for minutes Komga is available at https://komga.very-very-dark-gray.top/ and everything works fine.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#navidrome","title":"Navidrome","text":"<p>Navidrome (music streaming) was very easy to migrate from <code>lexicon</code>, with only the minor quirk that its <code>/data</code> directory had been inadvertently placed inside itself; fixing this, and the location of music files, are the only changes in this deployment:</p> Kubernetes deployment: <code>navidrome.yaml</code> navidrome.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: navidrome\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: navidrome-pv-data\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/navidrome\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: navidrome-pv-music\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 100Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/nas/public/audio/Music\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: navidrome-pvc-data\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  volumeName: navidrome-pv-data\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: navidrome-pvc-music\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  volumeName: navidrome-pv-music\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: navidrome\n  name: navidrome\n  namespace: navidrome\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: navidrome\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: navidrome\n    spec:\n      containers:\n        - image: deluan/navidrome:latest\n          imagePullPolicy: Always\n          name: navidrome\n          env:\n          - name: ND_BASEURL\n            value: \"https://navidrome.very-very-dark-gray.top/\"\n          - name: ND_LOGLEVEL\n            value: \"info\"\n          - name: ND_SCANSCHEDULE\n            value: \"1h\"\n          - name: ND_SESSIONTIMEOUT\n            value: \"24h\"\n          ports:\n          - containerPort: 4533\n          resources: {}\n          stdin: true\n          tty: true\n          volumeMounts:\n          - mountPath: /data\n            name: navidrome-data\n          - mountPath: /music\n            name: navidrome-music\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 116\n            runAsGroup: 116\n      restartPolicy: Always\n      volumes:\n      - name: navidrome-data\n        persistentVolumeClaim:\n          claimName: navidrome-pvc-data\n      - name: navidrome-music\n        persistentVolumeClaim:\n          claimName: navidrome-pvc-music\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: navidrome-svc\n  namespace: navidrome\nspec:\n  type: NodePort\n  ports:\n  - port: 4533\n    nodePort: 30533\n    targetPort: 4533\n  selector:\n    app: navidrome\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: navidrome-ingress\n  namespace: navidrome\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: navidrome-svc\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: navidrome.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: navidrome-svc\n                port:\n                  number: 4533\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - navidrome.very-very-dark-gray.top\n</code></pre> <p>Then again, the trick is to copy the <code>/data</code> directory between stopping the service in <code>lexicon</code> and starting it in <code>octavo</code>.</p> <p>First, stop Navidrome in <code>lexicon</code> (it won't be used moving forward):</p> <pre><code>$ kubectl scale -n navidrome deployment navidrome --replicas=0\ndeployment.apps/navidrome scaled\n</code></pre> <p>Copy data over from <code>lexicon</code> to <code>octavo</code>, moving the <code>/data</code> directory one level up:</p> <pre><code>root@octavo ~ # groupadd navidrome -g 116\nroot@octavo ~ # useradd  navidrome -u 116 -g 116 -s /usr/sbin/nologin\nroot@octavo ~ # rsync -ua lexicon:/home/k8s/navidrome/data/ /home/k8s/navidrome\nroot@octavo ~ # chown -R navidrome:navidrome /home/k8s/navidrome\nroot@octavo ~ # ls -hal /home/k8s/navidrome\ntotal 33M\ndrwxr-xr-x 1 navidrome navidrome   98 Apr 28 21:22 .\ndrwxr-xr-x 1 root      root        76 Apr 28 21:14 ..\ndrwxr-xr-x 1 navidrome navidrome   56 Dec 23 06:09 cache\n-rw-r--r-- 1 navidrome navidrome  32M Apr 28 21:08 navidrome.db\n-rw-r--r-- 1 navidrome navidrome  32K Apr 28 22:18 navidrome.db-shm\n-rw-r--r-- 1 navidrome navidrome 620K Apr 28 22:18 navidrome.db-wal\n</code></pre> <p>Warning</p> <p>The trailing <code>/</code> in <code>lexicon:/home/k8s/navidrome/data/</code> is critical to make the destination directory <code>/home/k8s/navidrome</code> in <code>octavo</code> be that <code>data</code> directory, instead of containing it.</p> <p>Finally, start the deployment in <code>octavo</code>:</p> <pre><code>$ kubectl apply -f navidrome.yaml\nNAME                             READY   STATUS              RESTARTS   AGE\npod/cm-acme-http-solver-tq8wv    1/1     Running             0          6s\npod/navidrome-588d4d77c7-pcxqm   0/1     ContainerCreating   0          9s\n\nNAME                                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nservice/cm-acme-http-solver-l9wp2   NodePort   10.97.50.141    &lt;none&gt;        8089:31185/TCP   6s\nservice/navidrome-svc               NodePort   10.110.51.110   &lt;none&gt;        4533:30533/TCP   9s\n\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/navidrome   0/1     1            0           9s\n\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/navidrome-588d4d77c7   1         1         0       9s\n\n$ kubectl get svc -n navidrome \nNAME                        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\ncm-acme-http-solver-l9wp2   NodePort   10.97.50.141    &lt;none&gt;        8089:31185/TCP   10s\nnavidrome-svc               NodePort   10.110.51.110   &lt;none&gt;        4533:30533/TCP   13s\n\n$ kubectl get ingress -n navidrome \nNAME                CLASS   HOSTS                               ADDRESS         PORTS     AGE\nnavidrome-ingress   nginx   navidrome.very-very-dark-gray.top   192.168.0.171   80, 443   17\n</code></pre> <p>After a couple for minutes Navidrome is available at https://navidrome.very-very-dark-gray.top/ and everything works fine.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#visual-studio-code-server","title":"Visual Studio Code Server","text":"<p>Visual Studio Code Server has been running in <code>lexicon</code> for nearly 2 years and it is still sometimes preferable to using Visual Studio Code on the desktop, so this service is also migrated over to <code>octavo</code> in very much the same fashion.</p> Kubernetes deployment: <code>code-server.yaml</code> code-server.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: code-server\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: code-server\nnamespace: code-server\nspec:\nports:\n- port: 80\n  targetPort: 8080\nselector:\n  app: code-server\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: code-server-pv\n  labels:\n    type: local\n  namespace: code-server\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/home/k8s/code-server\"\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: code-server-pv-claim\n  namespace: code-server\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: code-server\n  name: code-server\n  namespace: code-server\nspec:\n  selector:\n    matchLabels:\n      app: code-server\n  template:\n    metadata:\n      labels:\n        app: code-server\n    spec:\n      volumes:\n        - name: code-server-storage\n          persistentVolumeClaim:\n            claimName: code-server-pv-claim\n      containers:\n      - image: codercom/code-server\n        imagePullPolicy: IfNotPresent\n        name: code-server\n        ports:\n        - containerPort: 8080\n        env:\n        - name: PASSWORD\n          value: \"*************************\"\n        volumeMounts:\n          - mountPath: \"/home/coder\"\n            name: code-server-storage\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: code-server-ingress\n  namespace: code-server\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: \"code-server.very-very-dark-gray.top\"\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: code-server\n                port:\n                  number: 80\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - \"code-server.very-very-dark-gray.top\"\n</code></pre> <p>First, stop the service in <code>lexicon</code> (it won't be used moving forward):</p> <pre><code>$ kubectl scale -n code-server deployment code-server --replicas=0\ndeployment.apps/code-server scaled\n</code></pre> <p>Copy data over from <code>lexicon</code> to <code>octavo</code> (keeping ownership to <code>ponder</code>):</p> <pre><code>root@octavo ~ # rsync -ua lexicon:/home/k8s/code-server /home/k8s/\n</code></pre> <p>Start the deployment in <code>octavo</code>:</p> <pre><code>$ kubectl apply -f code-server.yaml\nnamespace/code-server created\nservice/code-server created\npersistentvolume/code-server-pv created\npersistentvolumeclaim/code-server-pv-claim created\ndeployment.apps/code-server created\ningress.networking.k8s.io/code-server-ingress created\n\n$ kubectl get all -n code-server\nNAME                               READY   STATUS    RESTARTS   AGE\npod/cm-acme-http-solver-44r69      1/1     Running   0          16s\npod/code-server-69d64cd9cd-llgqq   1/1     Running   0          18s\n\nNAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nservice/cm-acme-http-solver-x75d2   NodePort    10.106.2.52     &lt;none&gt;        8089:32080/TCP   16s\nservice/code-server                 ClusterIP   10.103.60.210   &lt;none&gt;        80/TCP           18s\n\nNAME                          READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/code-server   1/1     1            1           18s\n\nNAME                                     DESIRED   CURRENT   READY   AGE\nreplicaset.apps/code-server-69d64cd9cd   1         1         1       18s\n\n$ kubectl get svc -n code-server\nNAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\ncm-acme-http-solver-x75d2   NodePort    10.106.2.52     &lt;none&gt;        8089:32080/TCP   5s\ncode-server                 ClusterIP   10.103.60.210   &lt;none&gt;        80/TCP           7s\n\n$ kubectl get ingress -n code-server\nNAME                  CLASS   HOSTS                                   ADDRESS   PORTS     AGE\ncode-server-ingress   nginx   code-server.very-very-dark-gray.top               80, 443   11s\n</code></pre> <p>After a couple for minutes Visual Studio Code is available at https://code-server.very-very-dark-gray.top/ and everything works fine, although due to the new URL it needs to be re-authorized on the linked GitHub account.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#plex-media-server","title":"Plex Media Server","text":"<p>Migrating my Plex Media Server to Kubernetes on <code>lexicon</code> was relatively involved and this Plex server has already been replaced by Jellyfin, so it's not clear that this Plex server needs to be migrated in the same way as before.</p> <p>To make sure the service won't be missed, the deployment is now stopped:</p> <pre><code>$ kubectl scale -n plexserver deployment plexserver --replicas=0\ndeployment.apps/plexserver scaled\n</code></pre> <p>For now, its database will be copied over to <code>octavo</code> so the \"same\" Plex server can be restored later in necessary, although it may make more time to instead set it up as a new Plex server with only a subset of the library.</p> <pre><code>root@octavo ~ # rsync -ua lexicon:/home/k8s/plexmediaserver /home/k8s/\nroot@octavo ~ # ls -lan /home/k8s/plexmediaserver/\ntotal 0\ndrwxr-xr-x 1 998 998  42 Sep 16  2023 .\ndrwxr-xr-x 1   0   0 306 May  1 19:16 ..\ndrwxr-xr-x 1 998 998  38 Jul  6  2022 Library\ndrwxr-xr-x 1 998 998  38 Sep 15  2023 Library.backup\n</code></pre> <p>UID/GID 998 is already taken by <code>systemd-network</code> so the files would need to have their ownership changed if and when a dedicated <code>plex</code> user is created for a new Plex Media Server deployment.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#minecraft-server","title":"Minecraft Server","text":"<p>Running Minecraft Java Server for Bedrock clients on Kubernetes was fun while the kids wanted to play Minecraft, but they don't seem to be so inclined since some time ago and have admitted they won't be using it any time soon. Since the server was taking up a sizeable 10GB of RAM (to do essentially nothing with it), it has been scaled down to zero replicas and won't be setup in <code>octavo</code> until there is demand for it again. Maybe by then it can be added to a working Pterodactyl\u00ae.</p> <pre><code>$ kubectl scale -n minecraft-server deployment minecraft-server --replicas=0\ndeployment.apps/minecraft-server scaled\n</code></pre> <p>In the meantime, backups are kept in their configured paths for potential future use:</p> <pre><code>root@octavo ~ # rsync -ua lexicon:/home/k8s/minecraft-server /home/k8s/\nroot@octavo ~ # rsync -ua lexicon:/home/k8s/minecraft-server-backups /home/k8s/\nroot@octavo ~ # du -sh /home/k8s/minecraft-server*\n1.8G    /home/k8s/minecraft-server\n40G     /home/k8s/minecraft-server-backups\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#firefly-iii","title":"Firefly III","text":"<p>Self-hosted accountancy with Firefly III has not been used much; in all honesty keeping track of expenses and transactions is a lot less fun and a lot more grind than everything else going on. Even so, just in case this will be used in the future, it is worth migrating just to not throw it away.</p> <p>Warning</p> <p>Do not bother trying to change what user these processes run as, these are hard-coded in the images and (at least) the <code>mariadb</code> pod will fail to start if it runs as a different user.</p> Kubernetes deployment: <code>firefly-iii.yaml</code> firefly-iii.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: firefly-iii\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: firefly-iii-pv-mysql\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 20Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/firefly-iii/mysql\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: firefly-iii-pvc-mysql\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  storageClassName: manual\n  volumeName: firefly-iii-pv-mysql\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 20Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: firefly-iii-mysql\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  selector:\n    matchLabels:\n      app: firefly-iii\n      tier: mysql\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: firefly-iii\n        tier: mysql\n    spec:\n      containers:\n      - image: yobasystems/alpine-mariadb:latest\n        imagePullPolicy: Always\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"**************************\"\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: firefly-iii-pvc-mysql\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: firefly-iii-mysql-svc\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  type: NodePort\n  ports:\n  - port: 3306\n    nodePort: 30306\n    targetPort: 3306\n  selector:\n    app: firefly-iii\n    tier: mysql\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: firefly-iii-pv-upload\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/firefly-iii/upload\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: firefly-iii-pvc-upload\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  storageClassName: manual\n  volumeName: firefly-iii-pv-upload\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 10Gi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: firefly-iii-svc\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  type: NodePort\n  ports:\n  - port: 8080\n    nodePort: 30080\n    targetPort: 8080\n  selector:\n    app: firefly-iii\n    tier: frontend\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: firefly-iii\n  namespace: firefly-iii\n  labels:\n    app: firefly-iii\nspec:\n  selector:\n    matchLabels:\n      app: firefly-iii\n      tier: frontend\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: firefly-iii\n        tier: frontend\n    spec:\n      containers:\n      - image: fireflyiii/core\n        imagePullPolicy: Always\n        name: firefly-iii\n        env:\n        - name: APP_ENV\n          value: \"local\"\n        - name: APP_KEY\n          value: \"********************************\"\n        - name: DB_HOST\n          value: firefly-iii-mysql-svc\n        - name: DB_CONNECTION\n          value: mysql\n        - name: DB_DATABASE\n          value: \"fireflyiii\"\n        - name: DB_USERNAME\n          value: \"root\"\n        - name: DB_PASSWORD\n          value: \"**************************\"\n        - name: TRUSTED_PROXIES\n          value: \"**\"\n        ports:\n        - containerPort: 8080\n          name: firefly-iii\n        volumeMounts:\n        - mountPath: \"/var/www/html/storage/upload\"\n          name: firefly-iii-upload \n      volumes:\n        - name: firefly-iii-upload\n          persistentVolumeClaim:\n            claimName: firefly-iii-pvc-upload\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: firefly-iii-ingress\n  namespace: firefly-iii\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: firefly-iii-svc\n    nginx.ingress.kubernetes.io/proxy-buffer-size: \"16k\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: firefly-iii.very-very-dark-gray.top\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: firefly-iii-svc\n            port:\n              number: 8080\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - firefly-iii.very-very-dark-gray.top\n</code></pre> <p>First, stop the service in <code>lexicon</code> (it won't be used moving forward):</p> <pre><code>$ kubectl scale -n firefly-iii deployment firefly-iii --replicas=0\ndeployment.apps/firefly-iii scaled\n\n$ kubectl scale -n firefly-iii deployment firefly-iii-mysql --replicas=0\ndeployment.apps/firefly-iii-mysql scaled\n</code></pre> <p>Copy data over from <code>lexicon</code> to <code>octavo</code>:</p> <pre><code>root@octavo ~ # rsync -ua lexicon:/home/k8s/firefly-iii /home/k8s/\nroot@octavo ~ # ls -hal /home/k8s/firefly-iii\ndrwxr-xr-x 1 root     root      22 May 19  2024 .\ndrwxr-xr-x 1 root     root     254 May  1 15:20 ..\ndrwxr-xr-x 1 dhcpcd   lxd      566 May  1 15:32 mysql\ndrwxrwxr-x 1 www-data www-data   0 May 19  2024 upload\n</code></pre> <p>Start the deployment in <code>octavo</code>:</p> <pre><code>$ kubectl apply -f firefly-iii.yaml \nnamespace/firefly-iii created\npersistentvolume/firefly-iii-pv-mysql created\npersistentvolumeclaim/firefly-iii-pvc-mysql created\nservice/firefly-iii-mysql-svc created\npersistentvolume/firefly-iii-pv-upload created\npersistentvolumeclaim/firefly-iii-pvc-upload created\nservice/firefly-iii-svc created\ningress.networking.k8s.io/firefly-iii-ingress created\ndeployment.apps/firefly-iii created\ningress.networking.k8s.io/firefly-iii-ingress configured\n\n$ kubectl get all -n firefly-iii\nNAME                                     READY   STATUS    RESTARTS   AGE\npod/cm-acme-http-solver-rhfrm            1/1     Running   0          15s\npod/firefly-iii-7c6f8597c9-j7nlf         1/1     Running   0          3m27s\npod/firefly-iii-mysql-859cd77d57-85pjp   1/1     Running   0          4m1s\n\nNAME                                TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nservice/cm-acme-http-solver-4dln6   NodePort   10.99.155.239    &lt;none&gt;        8089:32080/TCP   15s\nservice/firefly-iii-mysql-svc       NodePort   10.105.106.204   &lt;none&gt;        3306:30306/TCP   10m\nservice/firefly-iii-svc             NodePort   10.98.129.183    &lt;none&gt;        8080:30080/TCP   10m\n\nNAME                                READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/firefly-iii         1/1     1            1           9m57s\ndeployment.apps/firefly-iii-mysql   1/1     1            1           9m57s\n\nNAME                                           DESIRED   CURRENT   READY   AGE\nreplicaset.apps/firefly-iii-7c6f8597c9         1         1         1       4m22s\nreplicaset.apps/firefly-iii-mysql-859cd77d57   1         1         1       4m23s\n\n$ kubectl get svc -n firefly-iii\nNAME                        TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\ncm-acme-http-solver-hgc27   NodePort   10.110.227.244   &lt;none&gt;        8089:32080/TCP   72s\nfirefly-iii-mysql-svc       NodePort   10.105.106.204   &lt;none&gt;        3306:30306/TCP   76s\nfirefly-iii-svc             NodePort   10.98.129.183    &lt;none&gt;        8080:30080/TCP   76s\n\n$ kubectl get ingress -n firefly-iii\nNAME                  CLASS   HOSTS                                 ADDRESS         PORTS     AGE\n\nfirefly-iii-ingress   nginx   firefly-iii.very-very-dark-gray.top   192.168.0.171   80, 443   79s\n</code></pre> <p>After a couple of minutes Firefly III is available at https://firefly-iii.very-very-dark-gray.top/ and ready to use with the migrated database and everything else.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#new-services-post-migration","title":"New services (post migration)","text":"<p>After all the above migrations, new services have been added to <code>octavo</code> that were not previously available in <code>lexicon</code>:</p> <ul> <li>Jellyfin to watch videos     from anywhere, including hardware acceleration for transcoding AV1 videos using      the onboard Intel GPU Irix Xe Graphics.</li> <li>Unifi Network Application     was never actually used in <code>lexicon</code>, so instead of migrating an     empty deployment this was deployed anew in <code>octavo</code> using a slightly updated     manifest to set newer versions of both images and updated UID/GID (119).</li> </ul>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#migration-to-new-isp","title":"Migration to new ISP","text":"<p>Replacing the router with a UniFi Gateway Fiber (UXG-FIBER) required first adopting the router, by connecting it to the LAN on both sides:</p> <ul> <li>Ethernet uplink port to get an IP on the current LAN to communicate   both with the Internet (old router) and LAN (to the UniFi Network app).   The IP range on the LAN is 192.168.0.0/24 as set by the old router,   this being its default settings.</li> <li>Ethernet downlink port sets its own static IP range, seemingly choosing   the 192.168.1.0/24 range automatically, it being the next available   <code>/24</code> network available.</li> </ul>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#take-1-unifi-network-application-on-new-lan","title":"Take 1: Unifi Network Application on new LAN","text":"<p>The Unifi Network Application being only available on the 0.0/24 network, it would not be reachable by the router once the old router is disconnected from the LAN. To make the Unifi Network Application reachable on the new router's LAN side, it needs an IP address on the 1.0/24 network, </p> <p>Kubernetes services can be exposed on a new LAN by adding a suitable range to the pool of IP addresses for MetalLB Load Balancer and adding a new <code>Service</code> using one of those IP addresses:</p> metallb/ipaddress-pool-octavo.yaml<pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: production\n  namespace: metallb-system\nspec:\n  addresses:\n    - 192.168.0.171-192.168.0.180\n    - 192.168.1.171-192.168.1.220\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: l2-advert\n  namespace: metallb-system\n</code></pre> <pre><code>$ kubectl apply -f metallb/ipaddress-pool-octavo.yaml\nipaddresspool.metallb.io/production configured\nl2advertisement.metallb.io/l2-advert unchanged\n\n$ kubectl get ipaddresspool.metallb.io -n metallb-system\nNAME         AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES\nproduction   true          false             [\"192.168.0.171-192.168.0.180\",\"192.168.1.201-192.168.1.220\"]\n</code></pre> unifi-network-app.yaml<pre><code>---\nkind: Service\napiVersion: v1\nmetadata:\n  name: unifi-tcp\n  namespace: unifi\n  annotations:\n    metallb.universe.tf/allow-shared-ip: unifi\nspec:\n  type: LoadBalancer\n  loadBalancerIP: 192.168.0.173\n  ports:\n  - name: mob-speedtest\n    protocol: TCP\n    port: 6789\n    targetPort: 6789\n  - name: device-inform\n    protocol: TCP\n    port: 8080\n    targetPort: 8080\n  - name: web-ui\n    protocol: TCP\n    port: 8443\n    targetPort: 8443\n  selector:\n    app: unifi\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: unifi-tcp-1\n  namespace: unifi\n  annotations:\n    metallb.universe.tf/allow-shared-ip: unifi\nspec:\n  type: LoadBalancer\n  loadBalancerIP: 192.168.1.220\n  ports:\n  - name: mob-speedtest\n    protocol: TCP\n    port: 6789\n    targetPort: 6789\n  - name: device-inform\n    protocol: TCP\n    port: 8080\n    targetPort: 8080\n  - name: web-ui\n    protocol: TCP\n    port: 8443\n    targetPort: 8443\n  selector:\n    app: unifi\n---\n</code></pre> <pre><code>$ kubectl apply -f unifi-network-app.yaml\nnamespace/unifi unchanged\npersistentvolume/mongo-pv-data unchanged\npersistentvolume/mongo-pv-init unchanged\npersistentvolumeclaim/mongo-pvc-data unchanged\npersistentvolumeclaim/mongo-pvc-init unchanged\ndeployment.apps/mongo unchanged\nservice/mongo-svc unchanged\npersistentvolume/unifi-pv-config unchanged\npersistentvolumeclaim/unifi-pvc-config unchanged\ndeployment.apps/unifi unchanged\nservice/unifi-tcp unchanged\nservice/unifi-tcp-1 created\nservice/unifi-udp unchanged\ningress.networking.k8s.io/unifi-ingress unchanged\n\n$ kubectl get services -A | grep '192.168.'\ningress-nginx              ingress-nginx-controller                                LoadBalancer   10.99.252.250    192.168.0.171   80:30278/TCP,443:30974/TCP                      150d\nunifi                      unifi-tcp                                               LoadBalancer   10.105.232.48    192.168.0.173   6789:31231/TCP,8080:32034/TCP,8443:30909/TCP    145d\nunifi                      unifi-tcp-1                                             LoadBalancer   10.109.194.34    192.168.1.220   6789:30860/TCP,8080:31627/TCP,8443:30889/TCP    6d1h\nunifi                      unifi-udp                                               LoadBalancer   10.108.54.45     192.168.0.173   3478:31805/UDP,10001:32694/UDP,1900:30234/UDP   145d\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#take-2-old-router-on-new-lan","title":"Take 2: Old router on new LAN","text":"<p>Turns out the UniFi router, once adopted, would never reporr to the Unifi Network Application on 192.168.1.220, even though the application is available on https://192.168.1.220:8443 just as well as on https://192.168.0.173:8443 because, unlike access points, the router has no usable SSH connection to <code>set-inform https://192.168.0.220:8080</code>.</p> <p>Eventually the solution was to set the old router to be leasing addresses (as DHCP server) on the 1.0/24 network, which let the UniFi router to re-configure its LAN ports to adopt the 0.0/24 network. After this change, the router was able again to reach the Unifi Network Application on 192.168.0.173 and everything was well once again.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#bridging-wireless","title":"Bridging wireless","text":"<p>This new server has a Wi-Fi 6E interface that is not needed, but can be used to improve the Wi-Fi coverage in the room it occupies. In this environment the Wireless network in is provided by a number of UniFi access points, none of which offer good coverage in the room where the NUC is.</p> <p>To provide bridged access point using the server's Wi-Fi interface, create a software bridge (<code>br0</code>) that unite the Ethernet and Wi-Fi interfaces into a single Layer 2 network, allowing the NUC to extend the existing UniFi network.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#prerequisites","title":"Prerequisites","text":"<p>Most Intel wireless chips (using the <code>iwlWi-Fi</code> driver) support AP mode. To check against this limitation, run <code>iw list</code> and confirm <code>Supported interface modes</code> includes <code>AP</code>:</p> <pre><code># iw list\nWiphy phy0\n        ...\n        Supported interface modes:\n                 * IBSS\n                 * managed\n                 * AP\n                 * AP/VLAN\n                 * monitor\n                 * P2P-client\n                 * P2P-GO\n                 * P2P-device\n</code></pre> <p>Install <code>hostapd</code> for the access point and <code>bridge-utils</code> for bridging support:</p> <code># apt install hostapd bridge-utils</code> <pre><code># apt install hostapd bridge-utils\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSuggested packages:\n  ifupdown\nThe following NEW packages will be installed:\n  bridge-utils hostapd\n0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 933 kB of archives.\nAfter this operation, 2,437 kB of additional disk space will be used.\nGet:1 http://ch.archive.ubuntu.com/ubuntu noble-updates/universe amd64 hostapd amd64 2:2.10-21ubuntu0.3 [899 kB]\nGet:2 http://ch.archive.ubuntu.com/ubuntu noble/main amd64 bridge-utils amd64 1.7.1-1ubuntu2 [33.9 kB]\nFetched 933 kB in 0s (32.6 MB/s)        \nSelecting previously unselected package hostapd.\n(Reading database ... 169804 files and directories currently installed.)\nPreparing to unpack .../hostapd_2%3a2.10-21ubuntu0.3_amd64.deb ...\nUnpacking hostapd (2:2.10-21ubuntu0.3) ...\nSelecting previously unselected package bridge-utils.\nPreparing to unpack .../bridge-utils_1.7.1-1ubuntu2_amd64.deb ...\nUnpacking bridge-utils (1.7.1-1ubuntu2) ...\nSetting up hostapd (2:2.10-21ubuntu0.3) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/hostapd.service \u2192 /usr/lib/systemd/system/hostapd.service.\nCreated symlink /etc/systemd/system/hostapd.service \u2192 /dev/null.\nSetting up bridge-utils (1.7.1-1ubuntu2) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nScanning processes...                                                                                            \nScanning candidates...                                                                                           \nScanning processor microcode...                                                                                  \nScanning linux images...                                                                                         \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nRestarting services...\n\nService restarts being deferred:\nsystemctl restart bluetooth.service\nsystemctl restart unattended-upgrades.service\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#bridge","title":"Bridge","text":"<p>Define a bridge in Netplan that includes the Ethernet interface (<code>enp86s0</code>) and assigns the static IP. The Wi-Fi interface (<code>wlo1</code>) will be added to the bridge by <code>hostapd</code> automatically once it is started later:</p> <p><code>/etc/netplan/50-cloud-init.yaml</code></p> <pre><code># Dual static IP on LAN, nothing else.\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp86s0:\n      dhcp4: no\n      dhcp6: no\n    wlo1:\n      dhcp4: no\n      dhcp6: no\n  bridges:\n    br0:\n      interfaces: [enp86s0]\n      parameters:\n        stp: false  # Disable Spanning Tree Protocol for home setups\n      # Ser IP address &amp; subnet mask\n      addresses: [ 10.0.0.8/24, 192.168.0.8/24 ]\n      # Set default gateway\n      routes:\n      - to: default\n        via: 192.168.0.1  # UniFi router gateway\n      # Set DNS name servers\n      nameservers:\n        addresses: [ 77.109.128.2, 213.144.129.20 ]\n</code></pre> <p>Note</p> <p>The Wi-Fi <code>wlo1</code> is under the <code>ethernets</code> section, and not under <code>Wi-Fis</code>, so that Netplan brings the physical interface up but perform no Layer 3 configuration (no IP/DHCP), allowing <code>hostapd</code> to take control of it and attach it to the bridge.</p> <p>Warning</p> <p>If using an SSH session, it may disconnect. Be prepared to connect with a physical keyboard/monitor immediately if things go wrong. Normally changes can tested with <code>netplan try</code> but not in this case because a bridge is involved:</p> <pre><code># netplan try\nbr0: reverting custom parameters for bridges and bonds is not supported\n\nPlease carefully review the configuration and use 'netplan apply' directly.\n</code></pre> <p>Applying Netplan config should create the bridge, without the Wi-Fi for now, and assign the IP addresses to the bridge:</p> <pre><code># netplan apply\n\n# brctl show br0\nbridge name     bridge id               STP enabled     interfaces\nbr0             8000.ae6c4282ecc1       no              enp86s0\n\n# ip addr show br0\n491: br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether ae:6c:42:82:ec:c1 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.8/24 brd 10.0.0.255 scope global br0\n       valid_lft forever preferred_lft forever\n    inet 192.168.0.8/24 brd 192.168.0.255 scope global br0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::ac6c:42ff:fe82:ecc1/64 scope link \n       valid_lft forever preferred_lft forever\n</code></pre>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#2ghz-ap-works","title":"2GHz AP (works)","text":"<p>Create the hostapd configuration file to define the SSID and bridge connection:</p> <p><code>/etc/hostapd/hostapd.conf</code></p> <pre><code>bridge=br0\ncountry_code=CH\ndriver=nl80211\ninterface=wlo1\n\n# 'g' for 2.4GHz or 'a' for 5GHz.\nhw_mode=g\n# Non-overlapping channel allowed by region CH\nchannel=1\n# High Throughput (HT) mode (Wi-Fi 4)\nieee80211n=1\n\n# SSID and Security settings\nssid=Discworld_IoT\nwpa=2\nwpa_passphrase=************\nwpa_key_mgmt=WPA-PSK\nwpa_pairwise=CCMP\nrsn_pairwise=CCMP\n</code></pre> <p>With <code>hw_mode=g</code> (2.4GHz) it is possible to use <code>ieee80211n=1</code> (enable Wi-Fi 4), but all the other <code>ieee80211...</code> flags will be ignored as they don't support 2.4GHz. </p> <p>The UniFi access points are on channels 6 and 11, as reported by the UniFi Network application. This leaves only Channel 1 as completely non-overlapping with them. When marking channel 1 as Excluded at 40MHz in UniFi (under Settings &gt; WiFi) it disables all channels up to 7, leaving only channel 11 in use by UniFi, and since UniFi is not using any channels at 40MHz, it is not recommended to enable the faster 40MHz channel width (with <code>ht_capab=[HT40+]</code>).</p> <p>Mark channel 1 in UniFi to mark it as Excluded at 20 MHz.</p> <p>In any case, modern smartphones and laptops are programmed to automatically drop down to 20MHz in the 2.4GHz band whenever they detect any other nearby Wi-Fi networks to prevent interference; even faster 40MHz channel width was forced, clients would likely refuse to use it. Sticking to the 20MHz channel width makes also roaming smoother, when a client moves from a UniFi AP to the NUC, the transition is smoother if the channel widths are consistent.</p> <p>Unmask and enable the <code>hostapd</code> service so it starts on boot:</p> <pre><code># systemctl unmask hostapd\nRemoved \"/etc/systemd/system/hostapd.service\".\n\n# systemctl enable hostapd\nSynchronizing state of hostapd.service with SysV service script with /usr/lib/systemd/systemd-sysv-install.\nExecuting: /usr/lib/systemd/systemd-sysv-install enable hostapd\n\n# systemctl start hostapd\nJob for hostapd.service failed because the control process exited with error code.\nSee \"systemctl status hostapd.service\" and \"journalctl -xeu hostapd.service\" for details.\n\n# systemctl status hostapd\n\u25cf hostapd.service - Access point and authentication server for Wi-Fi and Ethernet\n     Loaded: loaded (/usr/lib/systemd/system/hostapd.service; enabled; preset: enabled)\n     Active: active (running) since Thu 2026-01-22 07:26:28 CET; 2min 10s ago\n       Docs: man:hostapd(8)\n    Process: 1874042 ExecStart=/usr/sbin/hostapd -B -P /run/hostapd.pid $DAEMON_OPTS ${DAEMON_CONF} (code=exited, status=0/SUCCESS)\n   Main PID: 1874338 (hostapd)\n      Tasks: 1 (limit: 37734)\n     Memory: 1.0M (peak: 1.5M)\n        CPU: 64ms\n     CGroup: /system.slice/hostapd.service\n             \u2514\u25001874338 /usr/sbin/hostapd -B -P /run/hostapd.pid /etc/hostapd/hostapd.conf\n\nJan 22 07:26:28 octavo systemd[1]: Starting hostapd.service - Access point and authentication server for Wi-Fi and Ethernet...\nJan 22 07:26:28 octavo (hostapd)[1874042]: hostapd.service: Referenced but unset environment variable evaluates to an empty string: DAEMON_OPTS\nJan 22 07:26:28 octavo hostapd[1874042]: wlo1: interface state UNINITIALIZED-&gt;COUNTRY_UPDATE\nJan 22 07:26:28 octavo hostapd[1874042]: wlo1: interface state COUNTRY_UPDATE-&gt;ENABLED\nJan 22 07:26:28 octavo hostapd[1874042]: wlo1: AP-ENABLED\nJan 22 07:26:28 octavo systemd[1]: Started hostapd.service - Access point and authentication server for Wi-Fi and Ethernet.\n</code></pre> <p>Once the hostapd service is running, the interface should be correctly bridged:</p> <pre><code># brctl show br0\nbridge name     bridge id               STP enabled     interfaces\nbr0             8000.ae6c4282ecc1       no              enp86s0\n                                                        wlo1\n</code></pre> <p>Now both the Ethernet interface (<code>enp86s0</code>) and the Wi-Fi interface (<code>wlo1</code>) listed under the <code>interfaces</code> column for <code>br0</code>. This confirms that any client connecting to the NUC Wi-Fi will be Layer-2 bridged directly to the UniFi network.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#5ghz-ap-does-not-work","title":"5GHz AP (does not work)","text":"<p>Unfortunately the above setup would not work on the 5HGz band.</p> <p>Many Intel cards have Location Aware Regulatory (LAR) firmware that often prevents 5GHz AP mode in Linux because the card cannot \"see\" its regulatory domain without being a client first.</p> <p>The first step is to make sure the regulatory domain is set, by running <code>iw reg get</code> with the appropriate country code:</p> <pre><code># iw reg set FR\n\n# iw reg get\nglobal\ncountry 00: DFS-UNSET\n        (755 - 928 @ 2), (N/A, 20), (N/A), PASSIVE-SCAN\n        (2402 - 2472 @ 40), (N/A, 20), (N/A)\n        (2457 - 2482 @ 20), (N/A, 20), (N/A), AUTO-BW, PASSIVE-SCAN\n        (2474 - 2494 @ 20), (N/A, 20), (N/A), NO-OFDM, PASSIVE-SCAN\n        (5170 - 5250 @ 80), (N/A, 20), (N/A), AUTO-BW, PASSIVE-SCAN\n        (5250 - 5330 @ 80), (N/A, 20), (0 ms), DFS, AUTO-BW, PASSIVE-SCAN\n        (5490 - 5730 @ 160), (N/A, 20), (0 ms), DFS, PASSIVE-SCAN\n        (5735 - 5835 @ 80), (N/A, 20), (N/A), PASSIVE-SCAN\n        (57240 - 63720 @ 2160), (N/A, 0), (N/A)\n\nphy#0 (self-managed)\ncountry 00: DFS-UNSET\n        (2402 - 2437 @ 40), (6, 22), (N/A), AUTO-BW, NO-HT40MINUS, NO-80MHZ, NO-160MHZ\n        (2422 - 2462 @ 40), (6, 22), (N/A), AUTO-BW, NO-80MHZ, NO-160MHZ\n        (2447 - 2482 @ 40), (6, 22), (N/A), AUTO-BW, NO-HT40PLUS, NO-80MHZ, NO-160MHZ\n        (5170 - 5190 @ 160), (6, 22), (N/A), NO-OUTDOOR, AUTO-BW, IR-CONCURRENT, NO-HT40MINUS, NO-320MHZ, PASSIVE-SCAN\n        (5190 - 5210 @ 160), (6, 22), (N/A), NO-OUTDOOR, AUTO-BW, IR-CONCURRENT, NO-HT40PLUS, NO-320MHZ, PASSIVE-SCAN\n        (5210 - 5230 @ 160), (6, 22), (N/A), NO-OUTDOOR, AUTO-BW, IR-CONCURRENT, NO-HT40MINUS, NO-320MHZ, PASSIVE-SCAN\n        (5230 - 5250 @ 160), (6, 22), (N/A), NO-OUTDOOR, AUTO-BW, IR-CONCURRENT, NO-HT40PLUS, NO-320MHZ, PASSIVE-SCAN\n        (5250 - 5270 @ 160), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40MINUS, NO-320MHZ, PASSIVE-SCAN\n        (5270 - 5290 @ 160), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40PLUS, NO-320MHZ, PASSIVE-SCAN\n        (5290 - 5310 @ 160), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40MINUS, NO-320MHZ, PASSIVE-SCAN\n        (5310 - 5330 @ 160), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40PLUS, NO-320MHZ, PASSIVE-SCAN\n        (5490 - 5510 @ 160), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40MINUS, NO-320MHZ, PASSIVE-SCAN\n        (5510 - 5530 @ 160), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40PLUS, NO-320MHZ, PASSIVE-SCAN\n        (5530 - 5550 @ 160), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40MINUS, NO-320MHZ, PASSIVE-SCAN\n        (5550 - 5570 @ 160), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40PLUS, NO-320MHZ, PASSIVE-SCAN\n        (5570 - 5590 @ 160), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40MINUS, NO-320MHZ, PASSIVE-SCAN\n        (5590 - 5610 @ 160), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40PLUS, NO-320MHZ, PASSIVE-SCAN\n        (5610 - 5630 @ 160), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40MINUS, NO-320MHZ, PASSIVE-SCAN\n        (5630 - 5650 @ 160), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40PLUS, NO-320MHZ, PASSIVE-SCAN\n        (5650 - 5670 @ 80), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40MINUS, NO-160MHZ, NO-320MHZ, PASSIVE-SCAN\n        (5670 - 5690 @ 80), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40PLUS, NO-160MHZ, NO-320MHZ, PASSIVE-SCAN\n        (5690 - 5710 @ 80), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40MINUS, NO-160MHZ, NO-320MHZ, PASSIVE-SCAN\n        (5710 - 5730 @ 80), (6, 22), (0 ms), DFS, AUTO-BW, NO-HT40PLUS, NO-160MHZ, NO-320MHZ, PASSIVE-SCAN\n        (5735 - 5755 @ 80), (6, 22), (N/A), AUTO-BW, IR-CONCURRENT, NO-HT40MINUS, NO-160MHZ, NO-320MHZ, PASSIVE-SCAN\n        (5755 - 5775 @ 80), (6, 22), (N/A), AUTO-BW, IR-CONCURRENT, NO-HT40PLUS, NO-160MHZ, NO-320MHZ, PASSIVE-SCAN\n        (5775 - 5795 @ 80), (6, 22), (N/A), AUTO-BW, IR-CONCURRENT, NO-HT40MINUS, NO-160MHZ, NO-320MHZ, PASSIVE-SCAN\n        (5795 - 5815 @ 80), (6, 22), (N/A), AUTO-BW, IR-CONCURRENT, NO-HT40PLUS, NO-160MHZ, NO-320MHZ, PASSIVE-SCAN\n        (5815 - 5835 @ 40), (6, 22), (N/A), AUTO-BW, IR-CONCURRENT, NO-HT40MINUS, NO-80MHZ, NO-160MHZ, NO-320MHZ, PASSIVE-SCAN\n</code></pre> <p>Since <code>iw reg get</code> still says <code>country 00</code> the setting has not been effective. This shows a common Intel hardware limitation: the Wi-Fi card is \"self-managed\" and is ignoring the system's FR (France) regulatory setting, defaulting instead to a restrictive <code>00 (World)</code> domain. Also, almost every 5GHz frequency is marked with <code>PASSIVE-SCAN</code> or <code>IR-CONCURRENT</code>. In the Linux wireless stack, this is effectively the same as <code>no-IR</code>.</p> <p>Using the <code>iw</code> tool to check for list all the Wi-Fi channels with shows that everything except <code>Band 1</code> (2.4GHz) has either <code>No IR</code> (No Initial Radiation) or <code>(disabled)</code>, including all frequencies in the 5260\u20135700 MHz range (channels 52\u2013140):</p> <code>iw phy phy0 channels</code> <pre><code># iw phy phy0 channels\nBand 1:\n        * 2412 MHz [1] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40+\n        * 2417 MHz [2] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40+\n        * 2422 MHz [3] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40+\n        * 2427 MHz [4] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40+\n        * 2432 MHz [5] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40- HT40+\n        * 2437 MHz [6] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40- HT40+\n        * 2442 MHz [7] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40- HT40+\n        * 2447 MHz [8] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40- HT40+\n        * 2452 MHz [9] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40- HT40+\n        * 2457 MHz [10] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40-\n        * 2462 MHz [11] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40-\n        * 2467 MHz [12] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40-\n        * 2472 MHz [13] \n          Maximum TX power: 22.0 dBm\n          Channel widths: 20MHz HT40-\n        * 2484 MHz [14] (disabled)\nBand 2:\n        * 5180 MHz [36] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Channel widths: 20MHz HT40+ VHT80 VHT160\n        * 5200 MHz [40] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Channel widths: 20MHz HT40- VHT80 VHT160\n        * 5220 MHz [44] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Channel widths: 20MHz HT40+ VHT80 VHT160\n        * 5240 MHz [48] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Channel widths: 20MHz HT40- VHT80 VHT160\n        * 5260 MHz [52] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40+ VHT80 VHT160\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5280 MHz [56] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40- VHT80 VHT160\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5300 MHz [60] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40+ VHT80 VHT160\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5320 MHz [64] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40- VHT80 VHT160\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5340 MHz [68] (disabled)\n        * 5360 MHz [72] (disabled)\n        * 5380 MHz [76] (disabled)\n        * 5400 MHz [80] (disabled)\n        * 5420 MHz [84] (disabled)\n        * 5440 MHz [88] (disabled)\n        * 5460 MHz [92] (disabled)\n        * 5480 MHz [96] (disabled)\n        * 5500 MHz [100] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40+ VHT80 VHT160\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5520 MHz [104] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40- VHT80 VHT160\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5540 MHz [108] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40+ VHT80 VHT160\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5560 MHz [112] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40- VHT80 VHT160\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5580 MHz [116] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40+ VHT80 VHT160\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5600 MHz [120] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40- VHT80 VHT160\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5620 MHz [124] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40+ VHT80 VHT160\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5640 MHz [128] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40- VHT80 VHT160\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5660 MHz [132] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40+ VHT80\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5680 MHz [136] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40- VHT80\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5700 MHz [140] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40+ VHT80\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5720 MHz [144] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Radar detection\n          Channel widths: 20MHz HT40- VHT80\n          DFS state: usable (for 42089 sec)\n          DFS CAC time: 60000 ms\n        * 5745 MHz [149] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Channel widths: 20MHz HT40+ VHT80\n        * 5765 MHz [153] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Channel widths: 20MHz HT40- VHT80\n        * 5785 MHz [157] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Channel widths: 20MHz HT40+ VHT80\n        * 5805 MHz [161] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Channel widths: 20MHz HT40- VHT80\n        * 5825 MHz [165] \n          Maximum TX power: 22.0 dBm\n          No IR\n          Channel widths: 20MHz\n        * 5845 MHz [169] (disabled)\n        * 5865 MHz [173] (disabled)\n        * 5885 MHz [177] (disabled)\n        * 5905 MHz [181] (disabled)\nBand 4:\n        * 5955 MHz [1] (disabled)\n        * 5975 MHz [5] (disabled)\n        * 5995 MHz [9] (disabled)\n        * 6015 MHz [13] (disabled)\n        * 6035 MHz [17] (disabled)\n        * 6055 MHz [21] (disabled)\n        * 6075 MHz [25] (disabled)\n        * 6095 MHz [29] (disabled)\n        * 6115 MHz [33] (disabled)\n        * 6135 MHz [37] (disabled)\n        * 6155 MHz [41] (disabled)\n        * 6175 MHz [45] (disabled)\n        * 6195 MHz [49] (disabled)\n        * 6215 MHz [53] (disabled)\n        * 6235 MHz [57] (disabled)\n        * 6255 MHz [61] (disabled)\n        * 6275 MHz [65] (disabled)\n        * 6295 MHz [69] (disabled)\n        * 6315 MHz [73] (disabled)\n        * 6335 MHz [77] (disabled)\n        * 6355 MHz [81] (disabled)\n        * 6375 MHz [85] (disabled)\n        * 6395 MHz [89] (disabled)\n        * 6415 MHz [93] (disabled)\n        * 6435 MHz [97] (disabled)\n        * 6455 MHz [101] (disabled)\n        * 6475 MHz [105] (disabled)\n        * 6495 MHz [109] (disabled)\n        * 6515 MHz [113] (disabled)\n        * 6535 MHz [117] (disabled)\n        * 6555 MHz [121] (disabled)\n        * 6575 MHz [125] (disabled)\n        * 6595 MHz [129] (disabled)\n        * 6615 MHz [133] (disabled)\n        * 6635 MHz [137] (disabled)\n        * 6655 MHz [141] (disabled)\n        * 6675 MHz [145] (disabled)\n        * 6695 MHz [149] (disabled)\n        * 6715 MHz [153] (disabled)\n        * 6735 MHz [157] (disabled)\n        * 6755 MHz [161] (disabled)\n        * 6775 MHz [165] (disabled)\n        * 6795 MHz [169] (disabled)\n        * 6815 MHz [173] (disabled)\n        * 6835 MHz [177] (disabled)\n        * 6855 MHz [181] (disabled)\n        * 6875 MHz [185] (disabled)\n        * 6895 MHz [189] (disabled)\n        * 6915 MHz [193] (disabled)\n        * 6935 MHz [197] (disabled)\n        * 6955 MHz [201] (disabled)\n        * 6975 MHz [205] (disabled)\n        * 6995 MHz [209] (disabled)\n        * 7015 MHz [213] (disabled)\n        * 7035 MHz [217] (disabled)\n        * 7055 MHz [221] (disabled)\n        * 7075 MHz [225] (disabled)\n        * 7095 MHz [229] (disabled)\n        * 7115 MHz [233] (disabled)\n</code></pre> <p>This means the card is prohibited from being the \"master\" (Access Point) on these channels because it doesn't have regulatory permission to broadcast until it first hears an existing authorized AP on that frequency. In contrast, the output for 2.4GHz (2402-2437 MHz) does not have the <code>No IR</code> or <code>PASSIVE-SCAN</code> restriction, meaning it is guaranteed to work in AP mode.</p>"},{"location":"blog/2025/04/12/kubernetes-homelab-server-with-ubuntu-server-2404-octavo/#confimed-failed-attempt","title":"Confimed failed attempt","text":"<p>Because of the <code>PASSIVE-SCAN</code> flags, it was foreseeable that <code>hostapd</code> might refuse to start. An attempt was made to force it by adding the <code>ieee80211d=1</code> (Country Code broadcasting) and <code>ieee80211h=1</code> (DFS support) flags.</p> <p>UniFi is using channels 38 and 44 at 40MHz and channels 140, 149 and 157 are marked as \"Not available in your region\". At 80MHz only channel 42 is in use, while channels 132 and 149 are marked as \"Not available in your region\". This leaves only a small window available: the \"DFS\" Option, channels 52 through 140 are available but require DFS (Dynamic Frequency Selection). The card shows these as DFS and <code>PASSIVE-SCAN</code>. Channel 52 is the only one outside the range of UniFi APs and is generally supported by most devices.</p> <p>The attempt failed due to the foreseen hardware limitations; after trying to start the <code>hostapd</code> service it failed with <code>DFS start_dfs_cac() failed</code>:</p> <pre><code># systemctl status hostapd.service\n\u25cf hostapd.service - Access point and authentication server for Wi-Fi and Ethernet\n     Loaded: loaded (/usr/lib/systemd/system/hostapd.service; enabled; preset: enabled)\n     Active: activating (auto-restart) (Result: exit-code) since Thu 2026-01-22 07:01:57 CET; 1s ago\n       Docs: man:hostapd(8)\n    Process: 1166743 ExecStart=/usr/sbin/hostapd -B -P /run/hostapd.pid $DAEMON_OPTS ${DAEMON_CONF} (code=exited, status=1/FAILURE)\n        CPU: 4ms\n\nJan 22 07:02:00 octavo systemd[1]: Starting hostapd.service - Access point and authentication server for Wi-Fi and Ethernet...\nJan 22 07:02:00 octavo (hostapd)[1167941]: hostapd.service: Referenced but unset environment variable evaluates to an empty string: DAEMON_OPTS\nJan 22 07:02:00 octavo hostapd[1167941]: Failed to set up interface with /etc/hostapd/hostapd.conf\nJan 22 07:02:00 octavo hostapd[1167941]: Failed to initialize interface\nJan 22 07:02:00 octavo systemd[1]: hostapd.service: Control process exited, code=exited, status=1/FAILURE\nJan 22 07:02:00 octavo systemd[1]: hostapd.service: Failed with result 'exit-code'.\nJan 22 07:02:00 octavo systemd[1]: Failed to start hostapd.service - Access point and authentication server for Wi-Fi and Ethernet.\n\n# journalctl -xeu hostapd.service\nJan 22 07:04:32 octavo (hostapd)[1241470]: hostapd.service: Referenced but unset environment variable evaluates to an empty string: DAEMON_OPTS\nJan 22 07:04:33 octavo hostapd[1241470]: wlo1: interface state UNINITIALIZED-&gt;COUNTRY_UPDATE\nJan 22 07:04:33 octavo hostapd[1241470]: wlo1: interface state COUNTRY_UPDATE-&gt;DFS\nJan 22 07:04:33 octavo hostapd[1241470]: wlo1: DFS-CAC-START freq=5260 chan=52 sec_chan=0, width=0, seg0=0, seg1=0, cac_time=60s\nJan 22 07:04:33 octavo hostapd[1241470]: DFS start_dfs_cac() failed, -1\nJan 22 07:04:33 octavo hostapd[1241470]: Interface initialization failed\nJan 22 07:04:33 octavo hostapd[1241470]: wlo1: interface state DFS-&gt;DISABLED\nJan 22 07:04:33 octavo hostapd[1241470]: wlo1: AP-DISABLED\nJan 22 07:04:33 octavo hostapd[1241470]: wlo1: Unable to setup interface.\nJan 22 07:04:33 octavo hostapd[1241470]: wlo1: interface state DISABLED-&gt;DISABLED\nJan 22 07:04:33 octavo hostapd[1241470]: wlo1: AP-DISABLED\nJan 22 07:04:33 octavo hostapd[1241470]: wlo1: CTRL-EVENT-TERMINATING\nJan 22 07:04:33 octavo hostapd[1241470]: hostapd_free_hapd_data: Interface wlo1 wasn't started\nJan 22 07:04:33 octavo hostapd[1241470]: nl80211: deinit ifname=wlo1 disabled_11b_rates=0\nJan 22 07:04:33 octavo hostapd[1241470]: nl80211: Failed to remove interface wlo1 from bridge br0: Invalid argument\nJan 22 07:04:33 octavo systemd[1]: hostapd.service: Control process exited, code=exited, status=1/FAILURE\n</code></pre>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/","title":"Synology DS423+ for the homelab (luggage)","text":"<p>SPAAAAACE!!! I like Space.</p> <p>Despite all my efforts, hard drives keep filling up. Producing videos definitely does not help keeping drives from filling up and these days it's rather hard not to produce videos even accidentally. Besides that, having a single large storage unit allows making all the backups to a single place, at least once. Important data is already replicated in multiple disks and/or machines, having an additional large storage allows replacing one of those copies and thus alleviating disk pressure.</p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#hardware","title":"Hardware","text":"<p>The machine of choice has been a Synology DiskStation DS423+ with, for now, just 3x 12TB WD Red Plus Internal NAS HDD 3.5\" in a RAID 5 configuration, which provides ~20 TB of usable space. This is about tripple the amount of space needed right now, but those disks were on offer and, according to WD's product brief they should be mostly quiet.</p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#memory-expansion","title":"Memory expansion","text":"<p>Performance of this NAS may be improved by  adding third-party memory like the CT4G4SFS8266, although based on the community database from the Synology RAM Megathread it may be problematic, a better option reportedly being M471A5244CB0-CTD although this one seems harder to find. For now, this seems unnecessary.</p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#hardware-transcoding","title":"Hardware transcoding","text":"<p>Hardware transcoding should be possible with Jellyfin on Docker as well as with Plex Media Server. Then again, this will likely not be necessary, since there is a much more capable server to run these services on.</p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#future-of-hard-drive-compatiblity","title":"Future of hard-drive compatiblity","text":"<p>As reported by Tom's Hardware just two days ago:</p> <p>Quote</p> <p>Synology's new Plus Series NAS systems, designed for small and medium enterprises and advanced home users, can no longer use non-Synology or non-certified hard drives and get the full feature set of their device. Instead, Synology customers will have to use the company's self-branded hard drives. While you can still use non-supported drives for storage, Hardwareluxx [machine translated] reports that you\u2019ll lose several critical functions, including estimated hard drive health reports, volume-wide deduplication, lifespan analyses, and automatic firmware updates. The company also restricts storage pools and provides limited or zero support for third-party drives.</p> <p>This sounds pretty bad, but the situation is not so dire just yet, as reported by NAS Compares:</p> <p>Quote</p> <p>There will be no changes for Plus models released up to and including 2024 (excluding the XS Plus series and rack models). Furthermore, migrating hard drives from existing Synology NAS to a new Plus model will continue to be possible without restrictions. The use of compatible and unlisted hard drives will be subject to certain restrictions in the future, such as pool creation and support for issues and failures caused by the use of incompatible storage media. Volume-wide deduplication, lifespan analysis, and automatic hard drive firmware updates will only be available for Synology hard drives in the future. Tight integration of Synology NAS systems and hard drives reduces compatibility issues and increases system reliability and performance. At the same time, firmware updates and security patches can be deployed more efficiently, ensuring a high level of data security and more efficient support for Synology customers.</p> <p>It would seems building a NAS setup right now is just about the best time, as it would soon become no longer possible or supported, but by the time the current disks start failing it should be possible to replace them with the same brand of disks (not creating new volumes) and it should still be possible to even migrate them to a new NAS.</p> <p>However, given the increasing degree of vendor lock-in going on with Synology, the next NAS may well be a custom-built FreeNAS or, possibly better, openmediavault, which might as well run on the Kubernetes servers in the meantime.</p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#basic-setup","title":"Basic Setup","text":"<p>Find the IP address assigned to the NAS and add it in <code>/etc/hosts</code> with a fun name, then open its port 5000 over plain HTTP, e.g. https://luggage:5001/</p> <p></p> <p>After checking Device Info, the first (apparently not optional) step is to upgrade the DSM software to the latest version, which as of now is Version: 7.2.2-72806 Update 3 from 2025-02-04. The update process takes up to 10 minutes to complete, in fact it finished in less than 5.</p> <p>Setup an admin account with a non-obvious admin name and a very long and strong passwword, then select the option to be notified about DSM updates, to retain control over which updates are installed and when.</p> <p>Skip the options to create a Synology Account and install Synology apps, these can be revisited later. Skip also all the options to enable 2FA and MFA for now; this NAS will not be exposed externally.</p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#storage-pool-and-volume","title":"Storage Pool and Volume","text":"<p>Create a RAID 5 volume with the 3 disks available; they show up with only 10.9TB of usage capacity, so the volume would have 21.8TB of capacity. Set the volume capacity to the Max value (22331 GB) and formated as Btrfs.</p> <p>Leave the volume unencrypted, there is no need here to compromise performance, but above all this should allow recovering data by moving these disks to another NAS or even a PC, as it was the case years ago with the previous Synology NAS that died with 4x 1TB disks in it (about 15 years ago).</p> <p>The volume is now healthy, with a background job Optimizing in the background  that initially estimates about 21 hours but then updates it's ETA to 11 days once the volume has about 1 TB in it.</p> <p></p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#advanced-setup","title":"Advanced Setup","text":""},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#file-services","title":"File Services","text":"<p>SMB Service is enabled by default. Although this may be useful, there are no plans to use it. At the very least, one should change the workgroup name. The services that would be more useful in this homelab environment would be NFS and Rsync.</p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#nfs","title":"NFS","text":"<p>Enable NFS service under File Services in the Control Panel. Then, add NFS permissions to actually allow access, to either specific IPs or all (<code>*</code>), to mount NFS volumes. Install <code>nfs-common</code> and mount the shared folder (e.g. <code>NetBackup</code>) rather than the entire volume (which has no NFS permissions):</p> <pre><code>$ sudo apt install nfs-common\n\n$ sudo mount -t nfs luggage:/volume1/ /mnt/\nmount.nfs: access denied by server while mounting luggage:/volume1/\n\n$ sudo mount -t nfs luggage:/volume1/NetBackup /mnt/\n$ sudo ls /mnt/ -lah\ntotal 4.0K\ndrwxr-xr-x 1 root   root    22 Apr 18 17:05 .\ndrwxr-xr-x 1 root   root   132 Apr 18 16:13 ..\n</code></pre> <p>To mount the NAS after each restart, add a line in <code>/etc/fstab</code> with the default options:</p> <pre><code>luggage:/volume1/NetBackup /home/nas nfs defaults 0 0\n</code></pre> <p>Create the mount point and mount it to check access is granted:</p> <pre><code># mkdir /home/nas \n# mount /home/nas \n# df -h | grep nas\nluggage:/volume1/NetBackup   21T  1.3T   20T   6% /home/nas\n</code></pre> <p>When mounting for the first time, <code>dmesg</code> should NFS v4.1 could be used:</p> <pre><code>[37905.532214] FS-Cache: Loaded\n[37905.544295] RPC: Registered named UNIX socket transport module.\n[37905.544298] RPC: Registered udp transport module.\n[37905.544299] RPC: Registered tcp transport module.\n[37905.544299] RPC: Registered tcp NFSv4.1 backchannel transport module.\n[37905.562189] FS-Cache: Netfs 'nfs' registered for caching\n[37937.422623] NFS: Registering the id_resolver key type\n[37937.422635] Key type id_resolver registered\n[37937.422636] Key type id_legacy registered\n</code></pre>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#rsync","title":"Rsync","text":"<p>Enable Rsync as an alternative service, to incrementally upload files without mounting entire NFS volumes (shared folders).</p> <p>Create a separate user account with a long password without special characters (<code>!</code>, <code>$</code>, etc.), so it can be used with <code>sshpass</code> without the shell misinterpreting the password.</p> <p>Then enable the Rsync service, change the port and enable this accounts. Do not apply a speed limit.</p> <p>Install <code>sshpass</code> and store the password in <code>~/.ssh/nas-secret</code> (and <code>chmod 600</code> it), this way <code>rsync</code> can connect without a password promt. Files can be transfered only to/from a shared folder (e.g. <code>NetBackup</code>) rather than the entire volume:</p> <pre><code>$ SECRET=$(cat ~/.ssh/nas-secret)\n$ rsync -ahv --rsh=\"sshpass -p $SECRET ssh -p 2222\" Music luggage:/volume1/NetBackup/\nCould not chdir to home directory /var/services/homes/ponder: No such file or directory\n</code></pre>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#remote-access","title":"Remote access","text":""},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#cloudflare-tunnel","title":"Cloudflare Tunnel","text":"<p>Cloudflare Tunnels in Alfred proved to be a good solution for making web sites externally available but still protected behind SSO with Zero Trust Web Access. Instead of installing and running <code>cloudflared</code> in the NAS (which should be possible), simply create a tunnel using the connector already running in the new Kubernetes homelab server (octavo).</p> <p>Directly create a public hostname for this tunnel, to point https://luggage.very-very-dark-gray.top/ to the HTTPS endpoint on port 5001 using the static IP address. Make sure to enable the TLS option No TLS Verify, so the tunnel can be used without HTTPS certificates, because setting that up in the NAS isn't necessary.</p> <p>Then setup Cloudflare Access by adding a new application for the new public hostname, protected by a single identity provider (e.g. Google account). While not strictly necessary, this means only a few trusted users can even reach the login page of the NAS, after passing the identity check through Cloudflare, before having a chance to try to log in.</p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#setup-ssh","title":"Setup SSH","text":"<p>To SSH into a Synology NAS go to Terminal &amp; SNMP under the Control Panel and Enable SSH service. Once enabled, login is available only to users in the admin group:</p> <pre><code>$ ssh -l rincewind luggage\nThe authenticity of host 'luggage (192.168.0.55)' can't be established.\nED25519 key fingerprint is SHA256:atWMDkF65MUI0hieTZdRChhwEVclgIaYaL491X2gGyc.\nThis host key is known by the following other names/addresses:\n    ~/.ssh/known_hosts:140: [hashed name]\n    ~/.ssh/known_hosts:141: [hashed name]\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added 'luggage' (ED25519) to the list of known hosts.\nrincewind@luggage's password: \n\nUsing terminal commands to modify system configs, execute external binary\nfiles, add files, or install unauthorized third-party apps may lead to system\ndamages or unexpected behavior, or cause data loss. Make sure you are aware of\nthe consequences of each command and proceed at your own risk.\n\nWarning: Data should only be stored in shared folders. Data stored elsewhere\nmay be deleted when the system is updated/restarted.\n\nCould not chdir to home directory /var/services/homes/rincewind: No such file or directory\n</code></pre> <p>This user has <code>sudo</code> access, which of course comes with warnings:</p> <pre><code>rincewind@luggage:/$ sudo ls -l /volume1/\n\nWe trust you have received the usual lecture from the local System\nAdministrator. It usually boils down to these three things:\n\n    #1) Respect the privacy of others.\n    #2) Think before you type.\n    #3) With great power comes great responsibility.\n\nPassword: \ntotal 0\ndrwxr-xr-x  1 root       root       120 Apr 18 10:42 @database\ndrwxrwxrwx+ 1 root       root        56 Apr 18 10:57 @eaDir\nd---------+ 1 root       root        22 Apr 18 12:48 NetBackup\ndrwxr-xr-x  1 root       root        88 Apr 18 10:42 @S2S\ndrwxr-xr-x  1 SynoFinder SynoFinder  26 Apr 18 10:42 @SynoFinder-etc-volume\ndrwxr-xr-x  1 SynoFinder SynoFinder  84 Apr 18 10:42 @SynoFinder-log\ndrwxrwxrwt  1 root       root        40 Apr 18 10:57 @tmp\ndrwxr-xr-x  1 root       root        94 Apr 18 10:42 @userpreference\n</code></pre> <p>The warning seen above from <code>rsync</code> about home directory <code>/var/services/homes/ponder</code> not existing is due to <code>/var/services/homes</code> being a symlink to a non-existing file:</p> <pre><code>rincewind@luggage:/$ ls -l /var/services/homes \nlrwxrwxrwx 1 root root 24 Apr 18 10:20 /var/services/homes -&gt; /volume1/@fake_home_link\n</code></pre> <p>It might be possibly to change this, but the more intersting use of SSH access is to run commands as <code>root</code>. To do this more efficiently (without having to enter a very long password twice), it is actually possible to add SSH public keys for <code>root</code>:</p> <pre><code>rincewind@luggage:/$ echo 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDJC+vAO4ee18xVgXIp1l4upf0UJeigD1nd4uruntVcFNfgrYo3UK0QsybYvQ+G+88Yr8GQ+oIdRUe8hz5CVMBxhf5QwQ/wdnJS+gcLNGvTqK/fq80aarRkmY+MP8XCEogVjUdTiSNY/cl7Ox7tqCLITacav94JKaMMM4oKf6pLe9rayg5wM8ycW0k0aurA2GIjCJm6eRDRhhjoiOFrBtg7hinMAfCz3Gv9UyMIhPopba2YkG4ojG9S19f8bF6kGEH7QC8C2CADatfc53lm3a+rQxbI/z5OvtYyqlJnei7DrpS0WmQF/R6jdqmVYcUIQM7gknF8YleJyhglE3FUSCA7 ponder@rapture' \\\n  | sudo tee -a /root/.ssh/authorized_keys\n</code></pre> <p>Once the key is in place, <code>ponder</code> can just SSH in:</p> <pre><code>[ponder@rapture ~]$ ssh root@luggage\n\nUsing terminal commands to modify system configs, execute external binary\nfiles, add files, or install unauthorized third-party apps may lead to system\ndamages or unexpected behavior, or cause data loss. Make sure you are aware of\nthe consequences of each command and proceed at your own risk.\n\nWarning: Data should only be stored in shared folders. Data stored elsewhere\nmay be deleted when the system is updated/restarted.\n\nroot@luggage:~# \n</code></pre>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#setup-25g-nic","title":"Setup 2.5G NIC","text":"<p>Follow these instructions, which require SSH access, download the latest 7.2 release for <code>geminilake</code> from the releases page and install via the Synology Package Center.</p> <p>The trick is to Manual Install the drivers twice, running these commands via SSH between the first (failed) attempt and the second (which then succeeds):</p> <pre><code>root@luggage:~# install -m 4755 -o root -D /var/packages/r8152/target/r8152/spk_su /opt/sbin/spk_su\nroot@luggage:~# chmod a+rx /opt /opt/sbin\n</code></pre> <p>Rebooting the NAS was not possible because the recently created volume is still in the process of being optimized, but in any case the new RTL8152/RTL8153 driver is already running so we can already setup a static IP address on the new NIC under Network Interfaces in the Control panel, then switch the network cable to the new card (and update the PC's /etc/hosts to the static IP, if different), and after a few seconds the new card shows as Connected with a glorious 2500 Mbps:</p> <p></p> <p>Warning</p> <p>Even if the driver starts automatically when installing it, it is still necessary to <code>enable</code> it so that it starts again after rebooting the NAS:</p> <pre><code># synosystemctl enable pkgctl-r8152\n# synosystemctl start pkgctl-r8152\n</code></pre> <p>Copying the 400GB collection of Audiobooks over NFS gets the 2.5Gbps NICs to work it out!</p> <p></p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#conclusion","title":"Conclusion","text":"<p>The installation was easy enough, even setting up the 2.5Gbps NIC was not too painful. The worst part of this whole affair was dealing with NFS permissions not working properly at all for the first few hours.</p> <p>While it is possibly to run many other applications in containers, having at least one server with much more CPU (and GPU) and RAM available already makes this point moot; better to run services in the server/s and let the NAS just be the NAS.</p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#appendix-wtf-nfs-permissions","title":"Appendix: WTF NFS Permissions","text":"<p>NFS was problematic for the first few hours, specially around permissions. It seemed there was no way to have the correct level of access to the NFS share for all users:</p> <ul> <li><code>root</code> user able to create and write files, and set ownership and permissions.</li> <li>Other users (<code>ponder</code> and <code>pi</code>) able to at least read files.</li> <li>Ideally, all users able to create and write files, and set ownership and permissions.</li> </ul> <p>This all eventually started working correctly, after a few hours of trying every value of Squash from two separate systems.</p> <p>By default, the filesystem is owned by <code>root</code> by default, but directories can be created owned by other users:</p> <pre><code>$ sudo ls /mnt/ -lah\ntotal 4.0K\ndrwxrwxrwx  1 root   root     24 Apr 18 11:44 .\ndrwxr-xr-x 24 root   root   4.0K Nov  4 00:04 ..\ndrwxrwxrwx  1 ponder ponder    0 Apr 18 11:44 ponder\n\n$ sudo ls /mnt/ -lahn\ntotal 4.0K\ndrwxrwxrwx  1    0    0   24 Apr 18 11:44 .\ndrwxr-xr-x 24    0    0 4.0K Nov  4 00:04 ..\ndrwxrwxrwx  1 1000 1000    0 Apr 18 11:44 ponder\n</code></pre> <p>However, any users other than <code>root</code> cannot even see what is in the mounted volume, even if files show as owned by them:</p> <pre><code>$ ls -l /home/nas\nls: cannot open directory '/home/nas': Permission denied\n$ ls -ld /home/nas\ndrwxrwxrwx 1 root root 22 Apr 18 16:16 /home/nas\n\n# ls -l /home/nas/\ntotal 0\ndrwxr-xr-x 1 ponder ponder  5902 Apr  2 19:58 Music\n</code></pre> <p>For <code>ponder</code> and other users to actually have access to NFS mounted shares, one can allow access to all users, by  setting the Squash option (for each client host) to one of the following values:</p> <ul> <li>No mapping: <ul> <li>Only <code>root</code> has access (read and write).</li> <li>File ownership and permissions are visible on existing files.</li> <li>File ownership and permissions can be set on new files.</li> </ul> </li> <li>Map root to admin:<ul> <li>Only <code>root</code> has access.</li> <li>File ownership and permissions are visible on existing files.</li> <li>File ownership and permissions cannot be set on new files.</li> </ul> </li> <li>Map root to guest:<ul> <li>Nobody (not even <code>root</code>) gets read access!</li> </ul> </li> <li>Map all users to admin:<ul> <li>All users get read access.</li> <li>File ownership and permissions are visible on existing files.</li> <li>Nobody (not even <code>root</code>) gets write access!</li> </ul> </li> <li>Map all users to guest:<ul> <li>Nobody (not even <code>root</code>) gets read access!</li> </ul> </li> </ul> <p>In short:</p> <ul> <li>No mapping is the only option that allows write access; for <code>root</code> only.</li> <li>Map all users to admin: is the only option that allows access to all users;     read only.</li> </ul> <p>The following sections include more detailed testing.</p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#no-mapping","title":"No mapping","text":"<p>This is the default option. As seen above:</p> <ul> <li>only <code>root</code> has access,</li> <li><code>root</code> can set file ownership and permissons,</li> <li>nobody else can see anything.</li> </ul> <p>However, at least once, this option did actually allow non-root users to write new files and create directories, and even kept file ownership and permissions!</p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#map-root-to-admin","title":"Map root to admin","text":"<p>This does not grant access to any users other than <code>root</code> and it prevent <code>root</code> from setting file ownership or permissions:</p> <pre><code># chown 1000 /home/nas/audio/test/\nchown: changing ownership of '/home/nas/audio/test/': Operation not permitted\n</code></pre>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#map-root-to-guest","title":"Map root to guest","text":"<p>This prevents even <code>root</code> from seeing anything inside the shared folder:</p> <pre><code># ls -l /home/nas/audio\nls: cannot access '/home/nas/audio': Permission denied\n</code></pre>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#map-all-users-to-admin","title":"Map all users to admin","text":"<p>This enable all users to read and write files in the shared folder, but permissions are set based on the <code>admin</code> user in the NAS (UID 1024):</p> <pre><code>$ ls -l /home/nas/\ntotal 0\ndrwxr-xr-x 1 ponder ponder  5902 Apr  2 19:58 Music\n\n$ mkdir /home/nas/test\n$ ls -l /home/nas/\ntotal 0\ndrwxr-xr-x 1 ponder ponder  5902 Apr  2 19:58 Music\ndrwxrwxrwx 1   1024 users      0 Apr 18 16:35 test\n</code></pre> <p>This also prevents even <code>root</code> from setting permissions on files and directories, just as with Map root to admin.</p> <p>This doesn't prevent users from copying files into the NAS, but it does make it impossible to make a proper backup preserving file permissions and ownership:</p> <pre><code>$ rsync -uva Music /mnt/ponder/\nsending incremental file list\nrsync: [generator] failed to set permissions on \"/mnt/ponder/.\": Operation not permitted (1)\n./\n...\n\n$ sudo cp -av Music/ /mnt/ponder/\ncp: failed to preserve ownership for '/mnt/ponder/Music/60.Christmas.Carols.for.Kids/01 Frosty the Snowman.mp3': Operation not permitted\n...\n</code></pre> <p>Changing ownership of files seems to be out of the question, at least when allowing access to all users. Going back to No mapping only allows <code>root</code> to write files with, albeit the correct permissions, but the setting is enforced per host so there is no way to have a separate NAS user can be used for read-only access.</p>"},{"location":"blog/2025/04/18/synology-ds423-for-the-homelab-luggage/#map-all-users-to-guest","title":"Map all users to guest","text":"<p>Just as useless as Map root to guest.</p>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/","title":"Jellyfin on Kubernetes with Intel GPU","text":"<p>I don't often watch videos, but when I do, I find it useful to have them all in one Plex... at least, I did, until the recent  Important 2025 Plex Updates made it clear that it was going to have a significant cost; for a service I seldom use.</p> <p>The new pricing becomes effective today and, although I had a few weeks to ponder the possibility of purchasing a lifetime subscription for a lot less than it costs now, I decided to switch to Jellyfin and I'm glad I did!</p> <p>There are a number of articles about combining Jellyfin with many other components, such as NAS devices and other media-related applications: Deploying a Kubernetes-Based Media Server, Deploying Media Stack on Kubernetes with Longhorn and NFS Storage from about a year ago, and more complete although from 4 years ago Kubernetes Part 16: Deploy Jellyfin.</p> <p>Those articles go too far beyond my very basic needs, I just want to (very occassionally) watch videos that are neither movies nor TV shows; family videos and lectures purchased directly from their producers, such as Logos By Nick and Riley Brandt. I am more interested in enabling Intel GPU acceleration on Kubernetes (a feature that was always locked behing the Plex subscription, but not so with Jellyfin) in anticipation of other applications that may make even better use of it.</p>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#jellyfin","title":"Jellyfin","text":"<p>Deploying Jellyfin itself is relatively simple, the manifests are longer than those from Merox because three physical volumes (and claims) are included:</p> <ul> <li><code>/home/k8s/jellyfin</code> for Jellyfin's own files.</li> <li><code>/home/nas/public/videos</code> for those digital art courses from Nick and     Riley, plus some old lessons from Coursera.</li> <li><code>/home/nas/public/ponder/Videos</code> for family videos. These can later be made     accessible to specific users only.</li> </ul> Basic Jellyfin deployment: <code>jellyfin.yaml</code> jellyfin.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: media-center\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: config-pv\n  namespace: media-center\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/k8s/jellyfin\n  persistentVolumeReclaimPolicy: Retain\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: config-pvc\n  namespace: media-center\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  volumeName: config-pv\n  volumeMode: Filesystem\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: public-video-pv\n  namespace: media-center\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1000Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/nas/public/video\n  persistentVolumeReclaimPolicy: Retain\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: public-video-pvc\n  namespace: media-center\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1000Gi\n  volumeName: public-video-pv\n  volumeMode: Filesystem\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: ponder-video-pv\n  namespace: media-center\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 1000Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/nas/public/ponder/Videos\n  persistentVolumeReclaimPolicy: Retain\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: ponder-video-pvc\n  namespace: media-center\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1000Gi\n  volumeName: ponder-video-pv\n  volumeMode: Filesystem\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jellyfin\n  namespace: media-center\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jellyfin\n  template:\n    metadata:\n      labels:\n        app: jellyfin\n    spec:\n      containers:\n      - name: jellyfin\n        image: jellyfin/jellyfin\n        ports:\n        - containerPort: 8096\n        securityContext:\n          readOnlyRootFilesystem: true\n          runAsNonRoot: false\n          runAsUser: 1000\n          runAsGroup: 1000\n        resources:\n          requests:\n            cpu: 1000m\n            memory: 1Gi\n          limits:\n            cpu: 3500m\n            memory: 6Gi\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: family-videos\n          mountPath: /data/family-videos\n        - name: videos\n          mountPath: /data/videos\n        - name: tmp\n          mountPath: /tmp\n      securityContext:\n        runAsNonRoot: false\n        fsGroup: 1000\n        runAsUser: 1000\n        runAsGroup: 1000\n      volumes:\n      - name: config\n        persistentVolumeClaim:\n          claimName: config-pvc\n      - name: videos\n        persistentVolumeClaim:\n          claimName: public-video-pvc\n      - name: family-videos\n        persistentVolumeClaim:\n          claimName: ponder-video-pvc\n      - name: tmp\n        emptyDir:\n          medium: Memory\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: jellyfin-svc\n  namespace: media-center\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 8096\n  selector:\n    app: jellyfin\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: jellyfin-ingress\n  namespace: media-center\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: jellyfin-svc\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: jellyfin.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: jellyfin-svc\n                port:\n                  number: 80\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - jellyfin.very-very-dark-gray.top\n</code></pre> <p>Jellyfin will be running as the user that owns those not-so-public videos, this user already exists but what does not yet exist is the <code>/home/k8s/jellyfin</code> directory for Jellyfin's own files.</p> <pre><code>$ sudo mkdir /home/k8s/jellyfin\n$ sudo chown 1000:1000 /home/k8s/jellyfin\n\n$ kubectl apply -f jellyfin.yaml\nnamespace/media-center created\npersistentvolume/config-pv created\npersistentvolumeclaim/config-pvc created\npersistentvolume/public-video-pv created\npersistentvolumeclaim/public-video-pvc created\npersistentvolume/ponder-video-pv created\npersistentvolumeclaim/ponder-video-pvc created\ndeployment.apps/jellyfin created\nservice/jellyfin-svc created\ningress.networking.k8s.io/jellyfin-ingress created\n\n$ kubectl get all -n media-center\nNAME                            READY   STATUS    RESTARTS   AGE\npod/cm-acme-http-solver-qjwgz   1/1     Running   0          25s\npod/jellyfin-97f9fb4f8-6rgzz    0/1     Pending   0          28s\n\nNAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nservice/cm-acme-http-solver-nk5gr   NodePort    10.108.15.161   &lt;none&gt;        8089:32620/TCP   25s\nservice/jellyfin-svc                ClusterIP   10.98.226.91    &lt;none&gt;        80/TCP           28s\n\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/jellyfin   0/1     1            0           28s\n\nNAME                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/jellyfin-97f9fb4f8   1         1         0       28s\n\n$ kubectl get svc -n media-center\nNAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\ncm-acme-http-solver-nk5gr   NodePort    10.108.15.161   &lt;none&gt;        8089:32620/TCP   15s\njellyfin-svc                ClusterIP   10.98.226.91    &lt;none&gt;        80/TCP           18s\n\n$ kubectl get ingress -n media-center\nNAME                        CLASS    HOSTS                                   ADDRESS   PORTS     AGE\ncm-acme-http-solver-hwpt7   &lt;none&gt;   jellyfin.very-very-dark-gray.top             80        17s\njellyfin-ingress            nginx    jellyfin.very-very-dark-gray.top             80, 443   20s\n</code></pre> <p>After a couple of minutes Jellyfin is available (and ready to be setup) at http://jellyfin.very-very-dark-gray.top</p>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#thumbnails-for-family-videos","title":"Thumbnails for family videos","text":"<p>For family videos and generally those that have no public metadata, there are no candidate images to show are their thubmnail / poster. Instead, the solution is to go to Dashboard &gt; Libraries and, for each library with such videos, select Manage library and under Image fetchers (Videos) make sure the Screen Grabber is at the top of the priority list. When changing this after a library has been already scanned, it is also necessary to select Refresh metadata (for each library, in the Home screen) and set the option to Replace all existing images.</p> <p>If thumbnails fail to show up, refer to the section about troubleshooting no thumbnails for family videos.</p>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#av1-cpu-transcoding","title":"AV1 CPU transcoding","text":"<p>Playing a long AV1 video no hardware acceleration increases CPU load by 300-500% above the baseline, and raises core temperatures by 5-15 \u00baC:</p> <p></p> <p>All the while the GPU shows no load at all:</p> <pre><code># intel_gpu_top -o - \n...\n Freq MHz      IRQ RC6     Power W             RCS             BCS             VCS            VECS \n req  act       /s   %   gpu   pkg       %  se  wa       %  se  wa       %  se  wa       %  se  wa \n   0    0        0 100  0.00  9.17    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00 11.49    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00  7.03    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00 10.03    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00  8.41    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00 10.59    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00  8.42    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00  9.60    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00  8.22    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00 10.36    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00 10.82    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00  8.38    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00  8.60    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00  9.88    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00  8.54    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00 10.19    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00  8.08    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n   0    0        0 100  0.00  9.63    0.00   0   0    0.00   0   0    0.00   0   0    0.00   0   0 \n</code></pre>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#intel-gpu","title":"Intel GPU","text":"<p>The above doesn't immediately suggest Jellyfin needs GPU support, but since there a GPU in the server...</p>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#avaiilable-hardware-and-codecs","title":"Avaiilable hardware and codecs","text":"<p>This server is an ASUS NUC 13 Pro Tall PC Kit RNUC13ANHI700000I w/ Intel Core i7-1360P with an 13th generation Intel Core i7 CPU and Intel Raptor Lake-P [UHD Graphics] (<code>8086:a720</code>) GPU:</p> <pre><code># lspci -vnn | grep -E 'VGA|3D'\n00:02.0 VGA compatible controller [0300]: Intel Corporation Raptor Lake-P [UHD Graphics] [8086:a720] (rev 04) (prog-if 00 [VGA controller])\n\n# cat /proc/cpuinfo \nprocessor       : 0\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 186\nmodel name      : 13th Gen Intel(R) Core(TM) i7-1360P\nstepping        : 2\nmicrocode       : 0x4124\ncpu MHz         : 538.627\ncache size      : 18432 KB\nphysical id     : 0\nsiblings        : 16\ncore id         : 0\ncpu cores       : 12\napicid          : 0\ninitial apicid  : 0\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 32\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect user_shstk avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize arch_lbr ibt flush_l1d arch_capabilities\nvmx flags       : vnmi preemption_timer posted_intr invvpid ept_x_only ept_ad ept_1gb flexpriority apicv tsc_offset vtpr mtf vapic ept vpid unrestricted_guest vapic_reg vid ple shadow_vmcs ept_mode_based_exec tsc_scaling usr_wait_pause\nbugs            : spectre_v1 spectre_v2 spec_store_bypass swapgs eibrs_pbrsb rfds bhi\nbogomips        : 5222.40\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 39 bits physical, 48 bits virtual\npower management:\n</code></pre> <p>The GPU is detected as a Raptor Lake-P [UHD Graphics], except by <code>intel_gpu_top</code>:</p> <pre><code># intel_gpu_top\nintel-gpu-top: Intel Alderlake_p (Gen12) @ /dev/dri/card0 -    0/   0 MHz; 100% RC6;  0.00/ 5.97 W\n\n# lspci -k | grep -EA3 'VGA|3D|Display'\n00:02.0 VGA compatible controller: Intel Corporation Raptor Lake-P [UHD Graphics] (rev 04)\n        DeviceName: Onboard - Video\n        Subsystem: Intel Corporation Raptor Lake-P [UHD Graphics]\n        Kernel driver in use: i915\n\n# lshw -c video\n  *-display                 \n       description: VGA compatible controller\n       product: Raptor Lake-P [UHD Graphics]\n       vendor: Intel Corporation\n       physical id: 2\n       bus info: pci@0000:00:02.0\n       version: 04\n       width: 64 bits\n       clock: 33MHz\n       capabilities: pciexpress msi pm vga_controller bus_master cap_list rom\n       configuration: driver=i915 latency=0\n       resources: iomemory:610-60f iomemory:400-3ff irq:214 memory:6123000000-6123ffffff memory:4000000000-400fffffff ioport:3000(size=64) memory:c0000-dffff memory:611c000000-6122ffffff memory:6000000000-60dfffffff\n</code></pre> <p>The list of supported codes includes: AV1, H264, HEVC, JPEG, MPEG2, VC1, VC1 Advanced, VC1 Simple, VP8, VP9.</p> Video codecs as reported by <code>vainfo</code> <pre><code># vainfo --display drm --device /dev/dri/renderD128\nlibva info: VA-API version 1.20.0\nlibva info: Trying to open /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so\nlibva info: Found init function __vaDriverInit_1_20\nlibva info: va_openDriver() returns 0\nvainfo: VA-API version: 1.20 (libva 2.12.0)\nvainfo: Driver version: Intel iHD driver for Intel(R) Gen Graphics - 24.1.0 ()\nvainfo: Supported profile and entrypoints\n      VAProfileNone                   : VAEntrypointVideoProc\n      VAProfileNone                   : VAEntrypointStats\n      VAProfileMPEG2Simple            : VAEntrypointVLD\n      VAProfileMPEG2Simple            : VAEntrypointEncSlice\n      VAProfileMPEG2Main              : VAEntrypointVLD\n      VAProfileMPEG2Main              : VAEntrypointEncSlice\n      VAProfileH264Main               : VAEntrypointVLD\n      VAProfileH264Main               : VAEntrypointEncSlice\n      VAProfileH264Main               : VAEntrypointFEI\n      VAProfileH264Main               : VAEntrypointEncSliceLP\n      VAProfileH264High               : VAEntrypointVLD\n      VAProfileH264High               : VAEntrypointEncSlice\n      VAProfileH264High               : VAEntrypointFEI\n      VAProfileH264High               : VAEntrypointEncSliceLP\n      VAProfileVC1Simple              : VAEntrypointVLD\n      VAProfileVC1Main                : VAEntrypointVLD\n      VAProfileVC1Advanced            : VAEntrypointVLD\n      VAProfileJPEGBaseline           : VAEntrypointVLD\n      VAProfileJPEGBaseline           : VAEntrypointEncPicture\n      VAProfileH264ConstrainedBaseline: VAEntrypointVLD\n      VAProfileH264ConstrainedBaseline: VAEntrypointEncSlice\n      VAProfileH264ConstrainedBaseline: VAEntrypointFEI\n      VAProfileH264ConstrainedBaseline: VAEntrypointEncSliceLP\n      VAProfileVP8Version0_3          : VAEntrypointVLD\n      VAProfileHEVCMain               : VAEntrypointVLD\n      VAProfileHEVCMain               : VAEntrypointEncSlice\n      VAProfileHEVCMain               : VAEntrypointFEI\n      VAProfileHEVCMain               : VAEntrypointEncSliceLP\n      VAProfileHEVCMain10             : VAEntrypointVLD\n      VAProfileHEVCMain10             : VAEntrypointEncSlice\n      VAProfileHEVCMain10             : VAEntrypointEncSliceLP\n      VAProfileVP9Profile0            : VAEntrypointVLD\n      VAProfileVP9Profile0            : VAEntrypointEncSliceLP\n      VAProfileVP9Profile1            : VAEntrypointVLD\n      VAProfileVP9Profile1            : VAEntrypointEncSliceLP\n      VAProfileVP9Profile2            : VAEntrypointVLD\n      VAProfileVP9Profile2            : VAEntrypointEncSliceLP\n      VAProfileVP9Profile3            : VAEntrypointVLD\n      VAProfileVP9Profile3            : VAEntrypointEncSliceLP\n      VAProfileHEVCMain12             : VAEntrypointVLD\n      VAProfileHEVCMain12             : VAEntrypointEncSlice\n      VAProfileHEVCMain422_10         : VAEntrypointVLD\n      VAProfileHEVCMain422_10         : VAEntrypointEncSlice\n      VAProfileHEVCMain422_12         : VAEntrypointVLD\n      VAProfileHEVCMain422_12         : VAEntrypointEncSlice\n      VAProfileHEVCMain444            : VAEntrypointVLD\n      VAProfileHEVCMain444            : VAEntrypointEncSliceLP\n      VAProfileHEVCMain444_10         : VAEntrypointVLD\n      VAProfileHEVCMain444_10         : VAEntrypointEncSliceLP\n      VAProfileHEVCMain444_12         : VAEntrypointVLD\n      VAProfileHEVCSccMain            : VAEntrypointVLD\n      VAProfileHEVCSccMain            : VAEntrypointEncSliceLP\n      VAProfileHEVCSccMain10          : VAEntrypointVLD\n      VAProfileHEVCSccMain10          : VAEntrypointEncSliceLP\n      VAProfileHEVCSccMain444         : VAEntrypointVLD\n      VAProfileHEVCSccMain444         : VAEntrypointEncSliceLP\n      VAProfileAV1Profile0            : VAEntrypointVLD\n      VAProfileHEVCSccMain444_10      : VAEntrypointVLD\n      VAProfileHEVCSccMain444_10      : VAEntrypointEncSliceLP\n</code></pre>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#kubernetes-gpu-support","title":"Kubernetes GPU support","text":"<p>Follow this comprehensive guide to setup Intel GPU acceleration on Kubernetes; start by installing the VA-API (Video Acceleration API) <code>non-free</code> drivers:</p> <pre><code># apt install intel-media-va-driver-non-free libva-drm2 libva-x11-2 -y\n</code></pre>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#node-feature-discovery","title":"Node Feature Discovery","text":"<p>Node Feature Discovery is a Kubernetes add-on that collects details about available hardware and exposes it as node labels. Initially, this node <code>octavo</code> exposed only the following labels:</p> <pre><code>$ kubectl get node octavo -o json | jq .metadata.labels\n{\n  \"beta.kubernetes.io/arch\": \"amd64\",\n  \"beta.kubernetes.io/os\": \"linux\",\n  \"kubernetes.io/arch\": \"amd64\",\n  \"kubernetes.io/hostname\": \"octavo\",\n  \"kubernetes.io/os\": \"linux\",\n  \"node-role.kubernetes.io/control-plane\": \"\"\n}\n</code></pre> <p>Install NFD with the official Helm chart, deploying it in its own dedicated namespace:</p> <pre><code>$ helm repo add node-feature-discovery \\\n  https://kubernetes-sigs.github.io/node-feature-discovery/charts\n\"node-feature-discovery\" has been added to your repositories\n\n$ helm upgrade -i \\\n  --create-namespace \\\n  -n node-feature-discovery \\\n  node-feature-discovery \\\n  node-feature-discovery/node-feature-discovery\nRelease \"node-feature-discovery\" does not exist. Installing it now.\nW0429 23:08:25.320035 1996504 warnings.go:70] spec.template.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution[0].preference.matchExpressions[0].key: node-role.kubernetes.io/master is use \"node-role.kubernetes.io/control-plane\" instead\nNAME: node-feature-discovery\nLAST DEPLOYED: Tue Apr 29 23:08:24 2025\nNAMESPACE: node-feature-discovery\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>After installing this many more labels are exposed, including lots of <code>feature.node.kubernetes.io</code> labels:</p> Node labels exposed after installing NFD <pre><code>$ kubectl get node octavo -o json | jq .metadata.labels\n{\n  \"beta.kubernetes.io/arch\": \"amd64\",\n  \"beta.kubernetes.io/os\": \"linux\",\n  \"feature.node.kubernetes.io/cpu-cpuid.ADX\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.AESNI\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.AVX\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.AVX2\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.AVXVNNI\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.BHI_CTRL\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.CETIBT\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.CETSS\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.CMPXCHG8\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.FLUSH_L1D\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.FMA3\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.FSRM\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.FXSR\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.FXSROPT\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.GFNI\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.HRESET\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.HYBRID_CPU\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.IA32_ARCH_CAP\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.IA32_CORE_CAP\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.IBPB\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.IDPRED_CTRL\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.LAHF\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.MD_CLEAR\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.MOVBE\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.MOVDIR64B\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.MOVDIRI\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.OSXSAVE\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.PSFD\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.RRSBA_CTRL\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.SERIALIZE\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.SHA\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.SPEC_CTRL_SSBD\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.STIBP\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.STOSB_SHORT\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.SYSCALL\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.SYSEE\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.VAES\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.VMX\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.VPCLMULQDQ\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.WAITPKG\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.X87\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.XGETBV1\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.XSAVE\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.XSAVEC\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.XSAVEOPT\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cpuid.XSAVES\": \"true\",\n  \"feature.node.kubernetes.io/cpu-cstate.enabled\": \"true\",\n  \"feature.node.kubernetes.io/cpu-hardware_multithreading\": \"true\",\n  \"feature.node.kubernetes.io/cpu-model.family\": \"6\",\n  \"feature.node.kubernetes.io/cpu-model.id\": \"186\",\n  \"feature.node.kubernetes.io/cpu-model.vendor_id\": \"Intel\",\n  \"feature.node.kubernetes.io/cpu-pstate.scaling_governor\": \"powersave\",\n  \"feature.node.kubernetes.io/cpu-pstate.status\": \"active\",\n  \"feature.node.kubernetes.io/cpu-pstate.turbo\": \"true\",\n  \"feature.node.kubernetes.io/kernel-config.NO_HZ\": \"true\",\n  \"feature.node.kubernetes.io/kernel-config.NO_HZ_FULL\": \"true\",\n  \"feature.node.kubernetes.io/kernel-version.full\": \"6.8.0-58-generic\",\n  \"feature.node.kubernetes.io/kernel-version.major\": \"6\",\n  \"feature.node.kubernetes.io/kernel-version.minor\": \"8\",\n  \"feature.node.kubernetes.io/kernel-version.revision\": \"0\",\n  \"feature.node.kubernetes.io/pci-0300_8086.present\": \"true\",\n  \"feature.node.kubernetes.io/pci-0300_8086.sriov.capable\": \"true\",\n  \"feature.node.kubernetes.io/storage-nonrotationaldisk\": \"true\",\n  \"feature.node.kubernetes.io/system-os_release.ID\": \"ubuntu\",\n  \"feature.node.kubernetes.io/system-os_release.VERSION_ID\": \"24.04\",\n  \"feature.node.kubernetes.io/system-os_release.VERSION_ID.major\": \"24\",\n  \"feature.node.kubernetes.io/system-os_release.VERSION_ID.minor\": \"04\",\n  \"kubernetes.io/arch\": \"amd64\",\n  \"kubernetes.io/hostname\": \"octavo\",\n  \"kubernetes.io/os\": \"linux\",\n  \"node-role.kubernetes.io/control-plane\": \"\"\n}\n</code></pre> <p>Additional detection rules are required to detect Intel GPU hardware. These can be downloaded from the intel/intel-device-plugins-for-kubernetes repository, in particular node-feature-rules.yaml is the file to be applied, albeit with a slight modification: remove or comment out the <code>extendedResources</code> section under the rule named <code>intel.sgx</code>:</p> node-feature-rules.yaml<pre><code>    - name: \"intel.sgx\"\n      labels:\n        \"intel.feature.node.kubernetes.io/sgx\": \"true\"\n      # extendedResources:\n      #   sgx.intel.com/epc: \"@cpu.security.sgx.epc\"\n</code></pre> <p>Without this change, the original manifest cannot be applied:</p> <pre><code>$ kubectl apply -f node-feature-rules.yaml \nError from server (BadRequest): error when creating \"node-feature-rules.yaml\": NodeFeatureRule in version \"v1alpha1\" cannot be handled as a NodeFeatureRule: strict decoding error: unknown field \"spec.rules[6].extendedResources\"\n</code></pre> <p>Apply the modified manifest:</p> <pre><code>$ kubectl apply -f node-feature-rules.yaml\nnodefeaturerule.nfd.k8s-sigs.io/intel-dp-devices created\n</code></pre> <p>Then inspect node labels again to check that there is a new set of <code>intel.feature.node.kubernetes.io</code> labels:</p> <pre><code>$ kubectl get node octavo -o json | jq .metadata.labels \\\n  | grep intel.feature.node.kubernetes.io\n  \"intel.feature.node.kubernetes.io/gpu\": \"true\",\n</code></pre> <p>At this point node labels should include additional (missing) <code>gpu.intel.com</code> labels, but these not showing up does not necessarily mean trouble. Moving on to the next section should prove whether the GPU becomes available to Kubernetes.</p>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#intel-device-plugin","title":"Intel Device Plugin","text":"<p>To install the Intel device plugin, first install Intel\u2019s device plugins operator:</p> <pre><code>$ helm repo add intel https://intel.github.io/helm-charts/\n\"intel\" has been added to your repositories\n\n$ helm upgrade -i \\\n  --create-namespace \\\n  -n intel-device-plugins-gpu \\\n  intel-device-plugins-operator \\\n  intel/intel-device-plugins-operator\nRelease \"intel-device-plugins-operator\" does not exist. Installing it now.\nNAME: intel-device-plugins-operator\nLAST DEPLOYED: Wed Apr 30 00:31:06 2025\nNAMESPACE: intel-device-plugins-gpu\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThank you for installing intel-device-plugins-operator.\n\nThe next step would be to install the device (plugin) specific chart.\n\nFriendly note about CRDs. Make sure to manually update CRDs if\nthey have changed. CRDs are not updated with helm by default.\n</code></pre>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#2026-update-disable-npu-controller","title":"2026 Update: disable NPU controller","text":"<p>Newer versions of the <code>intel-device-plugins-operator</code> by default attempt to initialize also the <code>v1.NpuDevicePlugin</code> even if there is no NPU in the system, as is the case in Intel NUCs:</p> <pre><code>I0209 18:45:01.738198       1 internal.go:571] \"Stopping and waiting for webhooks\" logger=\"intel-device-plugins-manager\"\nI0209 18:45:01.738279       1 server.go:249] \"Shutting down webhook server with timeout of 1 minute\" logger=\"controller-runtime.webhook\"\nI0209 18:45:01.738376       1 internal.go:574] \"Stopping and waiting for HTTP servers\" logger=\"intel-device-plugins-manager\"\nI0209 18:45:01.738395       1 server.go:68] \"shutting down server\" name=\"health probe\" addr=\"[::]:8081\"\nI0209 18:45:01.738411       1 server.go:254] \"Shutting down metrics server with timeout of 1 minute\" logger=\"controller-runtime.metrics\"\nI0209 18:45:01.738597       1 internal.go:578] \"Wait completed, proceeding to shutdown the manager\" logger=\"intel-device-plugins-manager\"\nE0209 18:45:01.738797       1 main.go:276] \"problem running manager\" err=\"failed to wait for npudeviceplugin caches to sync kind source: *v1.NpuDevicePlugin: timed out waiting for cache to be synced for Kind *v1.NpuDevicePlugin\" logger=\"setup\"\nE0209 18:45:01.738803       1 internal.go:517] \"error received after stop sequence was engaged\" err=\"leader election lost\" logger=\"intel-device-plugins-manager\"\n</code></pre> <p>This leads to the <code>inteldeviceplugins-controller-manager</code> being stuck in a crash loop:</p> <pre><code>$ kubectl get pods -n intel-device-plugins-gpu\nNAMESPACE                  NAME                                                     READY   STATUS             RESTARTS          AGE\n...\nintel-device-plugins-gpu   inteldeviceplugins-controller-manager-579649cd7d-8lnq7   0/1     CrashLoopBackOff   152 (4m35s ago)   18h\n</code></pre> <p>To avoid this issue, apply the following values to the Helm chart:</p> intel-operator-values.yaml<pre><code>manager:\n  devices:\n    gpu: true\n</code></pre> <pre><code>$ helm upgrade \\\n  -n intel-device-plugins-gpu  \\\n  intel-device-plugins-operator  \\\n  intel/intel-device-plugins-operator  \\\n  -f intel-operator-values.yaml \nRelease \"intel-device-plugins-operator\" has been upgraded. Happy Helming!\nNAME: intel-device-plugins-operator\nLAST DEPLOYED: Mon Feb  9 20:01:58 2026\nNAMESPACE: intel-device-plugins-gpu\nSTATUS: deployed\nREVISION: 3\nTEST SUITE: None\nNOTES:\nThank you for installing intel-device-plugins-operator.\n\nThe next step would be to install the device (plugin) specific chart.\n\nFriendly note about CRDs. Make sure to manually update CRDs if\nthey have changed. CRDs are not updated with helm by default.\n</code></pre>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#intel-gpu_1","title":"Intel GPU","text":"<p>With the operator running, use it to install Intel\u2019s GPU plugin, providing the following (minimal) values file:</p> intel-gpu-values.yaml<pre><code>name: gpudeviceplugin\n\nsharedDevNum: 1\nlogLevel: 2\nresourceManager: false\nenableMonitoring: true\nallocationPolicy: \"none\"\n\nnodeSelector:\n  intel.feature.node.kubernetes.io/gpu: 'true'\n\nnodeFeatureRule: true\n</code></pre> <pre><code>$ helm upgrade -i \\\n  --create-namespace \\\n  -n intel-device-plugins-gpu \\\n  intel-device-plugins-gpu \\\n  -f intel-gpu-values.yaml \\\n  intel/intel-device-plugins-gpu\nRelease \"intel-device-plugins-gpu\" does not exist. Installing it now.\nNAME: intel-device-plugins-gpu\nLAST DEPLOYED: Wed Apr 30 00:34:32 2025\nNAMESPACE: intel-device-plugins-gpu\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>With the plugin running, confirm that the GPU is actually found and made available as part of the node's <code>capacity</code>:</p> <pre><code>$ kubectl get node octavo -o json | jq .status.capacity\n{\n  \"cpu\": \"16\",\n  \"ephemeral-storage\": \"60Gi\",\n  \"gpu.intel.com/i915\": \"1\",\n  \"gpu.intel.com/i915_monitoring\": \"1\",\n  \"hugepages-1Gi\": \"0\",\n  \"hugepages-2Mi\": \"0\",\n  \"memory\": \"32405188Ki\",\n  \"pods\": \"110\"\n}\n\n$ klogs intel-device-plugins-gpu intel-gpu-plugin-gpudeviceplugin\nI0429 22:34:40.699725       1 gpu_plugin.go:799] GPU device plugin started with none preferred allocation policy\nI0429 22:34:40.716600       1 gpu_plugin.go:518] GPU (i915/xe) resource share count = 1\nI0429 22:34:40.718798       1 gpu_plugin.go:540] GPU scan update: 0-&gt;1 'i915_monitoring' resources found\nI0429 22:34:40.718820       1 gpu_plugin.go:540] GPU scan update: 0-&gt;1 'i915' resources found\nI0429 22:34:41.721633       1 server.go:285] Start server for i915 at: /var/lib/kubelet/device-plugins/gpu.intel.com-i915.sock\nI0429 22:34:41.727203       1 server.go:303] Device plugin for i915 registered\nI0429 22:34:41.817381       1 server.go:285] Start server for i915_monitoring at: /var/lib/kubelet/device-plugins/gpu.intel.com-i915_monitoring.sock\nI0429 22:34:41.920991       1 server.go:303] Device plugin for i915_monitoring registered\nI0430 17:06:14.422776       1 gpu_plugin.go:92] Select nonePolicy for GPU device allocation\nI0430 17:06:14.422815       1 gpu_plugin.go:138] Allocate deviceIds: [\"card0-0\"]\n</code></pre> <p>This confirms the GPU is now available for Kubernetes workloads, despite the missing <code>gpu.intel.com</code> labels.</p>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#jellyfin-gpu-support","title":"Jellyfin GPU support","text":"<p>To enable Jellyfin to use the GPU, several additional parameters need to be added to the <code>Deployment</code> above, based on Jonathan Gazeley's own Helm chart for Jellyfin, (concretely <code>statefulset.yaml</code>) and the <code>values.yaml</code> provided under the Jellyfin section in Intel GPU acceleration on Kubernetes. Most of these relate to permissions and for this it is most critical to determine the GID of the <code>render</code> group and add it to the pod's <code>securityContext.supplementalGroups</code>:</p> <pre><code>$ grep render /etc/group\nrender:x:993:\n</code></pre> Adding GPU support to Jellyfin deployment in <code>jellyfin.yaml</code> jellyfin.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jellyfin\n  namespace: media-center\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jellyfin\n  template:\n    metadata:\n      labels:\n        app: jellyfin\n    spec:\n      containers:\n      - name: jellyfin\n        image: jellyfin/jellyfin\n        ports:\n        - containerPort: 8096\n        securityContext:\n          privileged: true\n          allowPrivilegeEscalation: true\n          capabilities:\n            drop:\n              - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: false\n          runAsUser: 1000\n          runAsGroup: 1000\n        resources:\n          requests:\n            cpu: 1000m\n            memory: 1Gi\n            gpu.intel.com/i915: \"1\"\n          limits:\n            cpu: 3500m\n            memory: 6Gi\n            gpu.intel.com/i915: \"1\"\n        volumeMounts:\n        - name: dev-dri-renderd128\n          mountPath: /dev/dri/renderD128\n        - name: config\n          mountPath: /config\n        - name: family-videos\n          mountPath: /data/family-videos\n        - name: videos\n          mountPath: /data/videos\n        - name: tmp\n          mountPath: /tmp\n      securityContext:\n        runAsNonRoot: false\n        fsGroup: 1000\n        seccompProfile:\n          type: RuntimeDefault\n        runAsUser: 1000\n        runAsGroup: 1000\n        supplementalGroups:\n          - 993\n      volumes:\n      - name: dev-dri-renderd128\n        hostPath:\n          path: /dev/dri/renderD128\n      - name: config\n        persistentVolumeClaim:\n          claimName: config-pvc\n      - name: videos\n        persistentVolumeClaim:\n          claimName: public-video-pvc\n      - name: family-videos\n        persistentVolumeClaim:\n          claimName: ponder-video-pvc\n      - name: tmp\n        emptyDir:\n          medium: Memory\n</code></pre> <p>Applying this change will reconfigure only <code>deployment.apps/jellyfin</code>:</p> <pre><code>$ kubectl apply -f jellyfin.yaml\nnamespace/media-center unchanged\npersistentvolume/config-pv unchanged\npersistentvolumeclaim/config-pvc unchanged\npersistentvolume/public-video-pv unchanged\npersistentvolumeclaim/public-video-pvc unchanged\npersistentvolume/ponder-video-pv unchanged\npersistentvolumeclaim/ponder-video-pvc unchanged\ndeployment.apps/jellyfin configured\nservice/jellyfin-svc unchanged\ningress.networking.k8s.io/jellyfin-ingress unchanged\n</code></pre> <p>At this point the Playback Transcoding settings can be updated to enable hardware transoding with Video Acceleration API (VAAPI) and enable the  available codecs:</p> <p></p> <p>If video playback fails with client devices that require server-side transcoding, test GPU access from inside the Jellyfin pod.</p> <p>Another way to check whether Jellyfin has successfully initialized the GPU is to check the logs for lines mentioning it:</p> <pre><code>$ klogs media-center jellyfin\n...\n[19:35:53] [INF] [1] MediaBrowser.MediaEncoding.Encoder.MediaEncoder: VAAPI device /dev/dri/renderD128 is Intel GPU (iHD)\n[19:35:53] [INF] [1] MediaBrowser.MediaEncoding.Encoder.MediaEncoder: VAAPI device /dev/dri/renderD128 supports Vulkan DRM modifier\n[19:35:53] [INF] [1] MediaBrowser.MediaEncoding.Encoder.MediaEncoder: VAAPI device /dev/dri/renderD128 supports Vulkan DRM interop\n</code></pre>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#av1-gpu-transcoding","title":"AV1 GPU transcoding","text":"<p>With hardware acceleration enabled, playing the same AV1 video increases CPU load by a more steady 400% above the baseline, and raises core temperatures by 4-13 \u00baC:</p> <p></p> <p>Meanwhile the GPU load (as reported by <code>intel_gpu_top</code>) is most visible in the IRQ/s and VCS % (Video) metrics:</p> <p></p>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#conclusion","title":"Conclusion","text":"<p>While this has not been a very significant improvement by any of the metrics, it has been an interesting journey through the mechanisms to make Intel GPU available to pods in Kubernetes, which may be better used by different applications.</p>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#troubleshooting","title":"Troubleshooting","text":"<p>Such a deployment never works perfectly on the first try, so here are a few troubleshooting tips learned along the way.</p>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#no-thumbnails-for-family-videos","title":"No thumbnails for family videos","text":"<p>If thumbnails are not generated, check the logs to see why their generation is failing. A common reason is that <code>/tmp</code> is a read-only file system:</p> <pre><code>$ klogs media-center jellyfin\n...\n[07:36:51] [INF] [144] Emby.Server.Implementations.Library.LibraryManager: Validating media library\n[07:36:51] [ERR] [144] MediaBrowser.Providers.Folders.FolderMetadataService: Error in Dynamic Image Provider\nSystem.IO.IOException: Read-only file system : '/tmp/jellyfin'\n   at System.IO.FileSystem.CreateDirectory(String fullPath, UnixFileMode unixCreateMode)\n   at System.IO.Directory.CreateDirectory(String path)\n   at Emby.Server.Implementations.Images.BaseDynamicImageProvider`1.FetchToFileInternal(BaseItem item, IReadOnlyList`1 itemsWithImages, ImageType imageType, CancellationToken cancellationToken)\n   at Emby.Server.Implementations.Images.BaseDynamicImageProvider`1.FetchAsync(T item, MetadataRefreshOptions options, CancellationToken cancellationToken)\n   at MediaBrowser.Providers.Manager.MetadataService`2.RunCustomProvider(ICustomMetadataProvider`1 provider, TItemType item, String logName, MetadataRefreshOptions options, RefreshResult refreshResult, CancellationToken cancellationToken)\n</code></pre> <p>This is avoided by mounting <code>/tmp</code> as an ramdisk, using the <code>emptyDir</code> volume as seen in the above deployment. With Jellyfin having write access to <code>/tmp</code>, generation of thumbnails generates lines indicating images being created:</p> <pre><code>$ klogs media-center jellyfin\n...\n[18:01:23] [INF] [10] Jellyfin.Drawing.ImageProcessor: Creating image collage and saving to /tmp/jellyfin/757e22dcd1454925bbefe3fcc2fc997f.png\n[18:01:23] [INF] [10] Jellyfin.Drawing.ImageProcessor: Completed creation of image collage and saved to /tmp/jellyfin/757e22dcd1454925bbefe3fcc2fc997f.png\n[18:01:24] [INF] [51] MediaBrowser.MediaEncoding.Encoder.MediaEncoder: Starting /usr/lib/jellyfin-ffmpeg/ffprobe with args -analyzeduration 200M -probesize 1G -i file:\"/data/family-videos/Events/Switzerland/2013.mp4\" -threads 0 -v warning -print_format json -show_streams -show_chapters -show_format\n</code></pre> <p>Thumbnail generation may still fail for very long videos, because there is a default timeout of 10 seconds:</p> <pre><code>$ klogs media-center jellyfin\n...\nMediaBrowser.Common.FfmpegException: ffmpeg image extraction timed out for file:\"/data/family-videos/Locations/2022-06-27.mp4\" after 10000ms\n ---&gt; System.Threading.Tasks.TaskCanceledException: A task was canceled.\n   at System.Diagnostics.Process.WaitForExitAsync(CancellationToken cancellationToken)\n   at MediaBrowser.Common.Extensions.ProcessExtensions.WaitForExitAsync(Process process, TimeSpan timeout)\n   at MediaBrowser.MediaEncoding.Encoder.MediaEncoder.ExtractImageInternal(String inputPath, String container, MediaStream videoStream, Nullable`1 imageStreamIndex, Nullable`1 threedFormat, Nullable`1 offset, Boolean useIFrame, Nullable`1 targetFormat, Boolean isAudio, CancellationToken cancellationToken)\n   --- End of inner exception stack trace ---\n</code></pre> <p>Or possibly not fail, despite the timeout, since the videos that appeared to have been affected did have a thumbnail upon inspection in the library. In any case, this timeout can be adjusted in Jellyfin's <code>/config/system.xml</code> around line 180 under <code>&lt;TrickplayOptions&gt;</code>:</p> /config/system.xml<pre><code>  &lt;TrickplayOptions&gt;\n    &lt;EnableHwAcceleration&gt;false&lt;/EnableHwAcceleration&gt;\n    &lt;EnableHwEncoding&gt;false&lt;/EnableHwEncoding&gt;\n    &lt;EnableKeyFrameOnlyExtraction&gt;false&lt;/EnableKeyFrameOnlyExtraction&gt;\n    &lt;ScanBehavior&gt;NonBlocking&lt;/ScanBehavior&gt;\n    &lt;ProcessPriority&gt;BelowNormal&lt;/ProcessPriority&gt;\n    &lt;Interval&gt;100000&lt;/Interval&gt;\n    &lt;WidthResolutions&gt;\n      &lt;int&gt;320&lt;/int&gt;\n    &lt;/WidthResolutions&gt;\n    &lt;TileWidth&gt;10&lt;/TileWidth&gt;\n    &lt;TileHeight&gt;10&lt;/TileHeight&gt;\n    &lt;Qscale&gt;4&lt;/Qscale&gt;\n    &lt;JpegQuality&gt;90&lt;/JpegQuality&gt;\n    &lt;ProcessThreads&gt;1&lt;/ProcessThreads&gt;\n  &lt;/TrickplayOptions&gt;\n</code></pre>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#missing-gpuintelcom-labels","title":"Missing <code>gpu.intel.com</code> labels","text":"<p>The additional detection rules should also create some <code>gpu.intel.com</code> labels:</p> <pre><code>$ kubectl get node octavo -o json \\\n  | jq .metadata.labels \\\n  | grep gpu.intel.com\n  \"gpu.intel.com/device-id.0300-a720.count\": \"1\",\n  \"gpu.intel.com/device-id.0300-a720.present\": \"true\",\n</code></pre> <p>The <code>a720</code> device ID is found in <code>dmesg</code> logs:</p> <pre><code>00:02.0 VGA compatible controller [0300]: Intel Corporation Raptor Lake-P [UHD Graphics] [8086:a720] (rev 04)\n</code></pre> <p>The same procedure did produce the expected lines in <code>lexicon</code>:</p> <pre><code>$ kubectl get node lexicon -o json \\\n  | jq .metadata.labels \\\n  | grep gpu.intel.com\n  \"gpu.intel.com/device-id.0300-9a78.count\": \"1\",\n  \"gpu.intel.com/device-id.0300-9a78.present\": \"true\",\n</code></pre> <p>Again, the <code>9a78</code> device ID is found in <code>dmesg</code> logs:</p> <pre><code>00:02.0 VGA compatible controller [0300]: Intel Corporation Device [8086:9a78] (rev 01)\n</code></pre> <p>PCI ID <code>9a78</code> matches an entry for Intel\u00ae UHD Graphics / Tiger Lake in the GPUs with supported drivers table. PCI ID <code>a720</code> is actually missing from the table, but the matching GPU Intel\u00ae UHD Graphics / Raptor Lake-P  is there with PCI IDs <code>a721</code>.</p> <p>Possibly due to this mismatch, Node Feature Discovery does not seem to detect this GPU, even tough it should match the device <code>vendor</code> and <code>class</code>:</p> node-feature-rules.yaml<pre><code>    - name: \"intel.gpu\"\n      labels:\n        \"intel.feature.node.kubernetes.io/gpu\": \"true\"\n      matchFeatures:\n        - feature: pci.device\n          matchExpressions:\n            vendor: {op: In, value: [\"8086\"]}\n            class: {op: In, value: [\"0300\", \"0380\"]}\n</code></pre> <p>Even adding the specific <code>device</code> ID and more <code>class</code> values does not help:</p> node-feature-rules.yaml<pre><code>    - name: \"intel.gpu\"\n      labels:\n        \"intel.feature.node.kubernetes.io/gpu\": \"true\"\n      matchFeatures:\n        - feature: pci.device\n          matchExpressions:\n            vendor: {op: In, value: [\"8086\"]}\n            device: {op: In, value: [\"0720\"]}\n            class: {op: In, value: [\"0300\", \"0380\", \"0280\", \"1180\", \"0604\"]}\n</code></pre> <p>But maybe those <code>gpu.intel.com</code> labels are simply not added until the Intel Device Plugin is installed and running. This may be the very reason why those node labels did show up in <code>lexicon</code> during previous tests.</p>"},{"location":"blog/2025/04/29/jellyfin-on-kubernetes-with-intel-gpu/#gpu-access-from-the-pod","title":"GPU access from the pod","text":"<p>If video playback fails with client devices that require server-side transcoding, Jellyfin's own <code>ffmpeg</code> binary can be used to verify whether the GPU is in fact available to the Jellyfin pod. Test as <code>root</code> to confirm the GPU can be initialized:</p> <pre><code>root@octavo ~ # ffmpeg -v verbose \\\n  -init_hw_device vaapi=va \\\n  -init_hw_device vaapi=foo:/dev/dri/renderD128\nffmpeg version 6.1.1-3ubuntu5+esm2 Copyright (c) 2000-2023 the FFmpeg developers\n  built with gcc 13 (Ubuntu 13.2.0-23ubuntu4)\n  configuration: --prefix=/usr --extra-version=3ubuntu5+esm2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n  libavutil      58. 29.100 / 58. 29.100\n  libavcodec     60. 31.102 / 60. 31.102\n  libavformat    60. 16.100 / 60. 16.100\n  libavdevice    60.  3.100 / 60.  3.100\n  libavfilter     9. 12.100 /  9. 12.100\n  libswscale      7.  5.100 /  7.  5.100\n  libswresample   4. 12.100 /  4. 12.100\n  libpostproc    57.  3.100 / 57.  3.100\n[AVHWDeviceContext @ 0x5fc06da19640] Trying to use DRM render node for device 0.\n[AVHWDeviceContext @ 0x5fc06da19640] libva: VA-API version 1.20.0\n[AVHWDeviceContext @ 0x5fc06da19640] libva: Trying to open /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so\n[AVHWDeviceContext @ 0x5fc06da19640] libva: Found init function __vaDriverInit_1_20\n[AVHWDeviceContext @ 0x5fc06da19640] libva: va_openDriver() returns 0\n[AVHWDeviceContext @ 0x5fc06da19640] Initialised VAAPI connection: version 1.20\n[AVHWDeviceContext @ 0x5fc06da19640] VAAPI driver: Intel iHD driver for Intel(R) Gen Graphics - 24.1.0 ().\n[AVHWDeviceContext @ 0x5fc06da19640] Driver not found in known nonstandard list, using standard behaviour.\n[AVHWDeviceContext @ 0x5fc06da47400] libva: VA-API version 1.20.0\n[AVHWDeviceContext @ 0x5fc06da47400] libva: Trying to open /usr/lib/x86_64-linux-gnu/dri/iHD_drv_video.so\n[AVHWDeviceContext @ 0x5fc06da47400] libva: Found init function __vaDriverInit_1_20\n[AVHWDeviceContext @ 0x5fc06da47400] libva: va_openDriver() returns 0\n[AVHWDeviceContext @ 0x5fc06da47400] Initialised VAAPI connection: version 1.20\n[AVHWDeviceContext @ 0x5fc06da47400] VAAPI driver: Intel iHD driver for Intel(R) Gen Graphics - 24.1.0 ().\n[AVHWDeviceContext @ 0x5fc06da47400] Driver not found in known nonstandard list, using standard behaviour.\nHyper fast Audio and Video encoder\nusage: ffmpeg [options] [[infile options] -i infile]... {[outfile options] outfile}...\n\nUse -h to get full help or, even better, run 'man ffmpeg'\n</code></pre> <p>Running Jellyfin's <code>ffmpeg</code> with the same arguments from inside the pod, if it fails, can indicate the pod has not been correctly set up to have access to <code>/dev/dri/renderD128</code>.</p> <p>In this example, it was granted access to the wrong group <code>44(video)</code> instead of the required one <code>993(render)</code>:</p> <pre><code>$ kubectl exec --stdin --tty \\\n  $(kubectl get pods -n media-center | tail -1 | awk '{print $1}') \\\n  -n media-center -- /bin/bash\nI have no name!@jellyfin-798b466666-6l7bv:/$ id\nuid=1000 gid=1000 groups=1000,44(video)\n\nI have no name!@jellyfin-798b466666-6l7bv:/$ /usr/lib/jellyfin-ffmpeg/ffmpeg -v verbose \\\n  -init_hw_device vaapi=va \\\n  -init_hw_device vaapi=foo:/dev/dri/renderD128\nffmpeg version 7.0.2-Jellyfin Copyright (c) 2000-2024 the FFmpeg developers\n  built with gcc 12 (Debian 12.2.0-14)\n  configuration: --prefix=/usr/lib/jellyfin-ffmpeg --target-os=linux --extra-version=Jellyfin --disable-doc --disable-ffplay --disable-ptx-compression --disable-static --disable-libxcb --disable-sdl2 --disable-xlib --enable-lto=auto --enable-gpl --enable-version3 --enable-shared --enable-gmp --enable-gnutls --enable-chromaprint --enable-opencl --enable-libdrm --enable-libxml2 --enable-libass --enable-libfreetype --enable-libfribidi --enable-libfontconfig --enable-libharfbuzz --enable-libbluray --enable-libmp3lame --enable-libopus --enable-libtheora --enable-libvorbis --enable-libopenmpt --enable-libdav1d --enable-libsvtav1 --enable-libwebp --enable-libvpx --enable-libx264 --enable-libx265 --enable-libzvbi --enable-libzimg --enable-libfdk-aac --arch=amd64 --enable-libshaderc --enable-libplacebo --enable-vulkan --enable-vaapi --enable-amf --enable-libvpl --enable-ffnvcodec --enable-cuda --enable-cuda-llvm --enable-cuvid --enable-nvdec --enable-nvenc\n  libavutil      59.  8.100 / 59.  8.100\n  libavcodec     61.  3.100 / 61.  3.100\n  libavformat    61.  1.100 / 61.  1.100\n  libavdevice    61.  1.100 / 61.  1.100\n  libavfilter    10.  1.100 / 10.  1.100\n  libswscale      8.  1.100 /  8.  1.100\n  libswresample   5.  1.100 /  5.  1.100\n  libpostproc    58.  1.100 / 58.  1.100\n[AVHWDeviceContext @ 0x5f7c1a5a90c0] Cannot open DRM render node for device 0.\n[AVHWDeviceContext @ 0x5f7c1a5a90c0] Cannot open a VA display from DRM device (null).\nDevice creation failed: -542398533.\nFailed to set value 'vaapi=va' for option 'init_hw_device': Generic error in an external library\nError parsing global options: Generic error in an external library\n\nI have no name!@jellyfin-cff977878-tgm9h:/$ ls -l /dev/dri/*\ncrw-rw---- 1 root video 226,   0 Apr 30 19:17 /dev/dri/card0\ncrw-rw---- 1 root   993 226, 128 Apr 29 22:27 /dev/dri/renderD128\n\n/dev/dri/by-path:\ntotal 0\ncrw-rw---- 1 root video 226,   0 Apr 29 22:27 pci-0000:00:02.0-card\ncrw-rw---- 1 root   993 226, 128 Apr 29 22:27 pci-0000:00:02.0-render\n</code></pre> <p>This was fixed by adding the correct GID (<code>993</code>) in the pod's <code>securityContext.supplementalGroups</code>.</p>"},{"location":"blog/2025/05/10/in-place-upgrade-ubuntu-2204-to-2404/","title":"In-place upgrade Ubuntu 22.04 to 24.04","text":"<p>Upgrading from Ubuntu 22.04 to 24.04 in-place can be very convenient and a lot faster than installing Ubuntu 24.04 anew, but does it work well? That has been the question and doubt keeping me from trying again ever since one such upgrade went bad years ago.</p> <p>There is an old Intel NUC11PAHi7 (named <code>smart-computer</code> in an implosion of creative naming) kicking around since before this blog started, running Ubuntu Studio 22.04 with a very specific setup that was not well documented. Installing Ubuntu Studio 24.04 would have been a good change to document that process, but parts of it are likely obsolete and there was time pressure to upgrade to 24.04 before 22.04 reached EOL, so an in-place upgrade seemed worth a try.</p> <p>Starting the upgrade process is as simple as logging in with the first user and hitting the Upgrade button when prompted to upgrade, entering the password, and agreeing to proceed. A few warnings must be accepted (acknowledged) before proceeding:</p> <ol> <li>Third party sources disabled. This appears to disable just <code>google-chrome</code>.</li> <li>The upgrade process can take several hours to complete.<ol> <li>To ensure this is not interrupted, disable <code>crontab</code> entries to shut down.</li> </ol> </li> <li>Screen locking is disabled during the process.</li> <li>A long list of packages will be removed (277), installed (899) and upgraded (2661).</li> </ol> <p>After a while downloading packages (3882 MB) the installation becomes stuck when installing Thunderbird snap package:</p> <pre><code>Unpacking thunderbird-locale-en (2:1snap1-0ubuntu3) over (1:115.18.0+build1-0ubuntu0.22.04.1)...\nPreparing to unpack .../4-thunderbird_2%3a1snap1-0ubuntu3_amd64.deb ...\n=&gt; Installing the thunderbird snap\n==&gt; Checking connectivity with the snap store\n===&gt; Unable to contact the store, trying every minute for the next 30 minutes\n</code></pre> <p>There an issue preventing <code>snap</code> from reaching the store, despite the SNAP API being reachable:</p> <pre><code># snap refresh snap-store\nerror: cannot refresh \"snap-store\": Post \"https://api.snapcraft.io/v2/snaps/refresh\": dial tcp:\n       lookup api.snapcraft.io on 127.0.0.53:53: read udp 127.0.0.1:52705-&gt;127.0.0.53:53: read:\n       connection refused\n\n# curl -I https://api.snapcraft.io/v2/snaps/refresh\nHTTP/1.1 405 METHOD NOT ALLOWED\nserver: gunicorn\ndate: Sat, 10 May 2025 15:59:38 GMT\ncontent-type: text/html; charset=utf-8\nallow: OPTIONS, POST\ncontent-length: 153\nsnap-store-version: 69\nx-vcs-revision: 30d5a26fd6984c4f8f5b80e6c8a1d87dd1031179\nx-request-id: 00000000000000000000FFFFD9A239409A5E00000000000000000000FFFF0A8325F101BB681F77EA4AF02E3B\n</code></pre> <p>From very few, tangentially related forum threds, it seemed the only option to get snap unstuck as to restart the <code>snapd</code> service:</p> <pre><code># systemctl restart snapd.service \n\n# snap info snap-store\nname:    snap-store\nsummary: Snap Store is a graphical desktop application for discovering, installing and managing\n  snaps on Linux.\npublisher: Canonical\u2713\nstore-url: https://snapcraft.io/snap-store\ncontact:   https://bugs.launchpad.net/snap-store/\nlicense:   GPL-2.0+\ndescription: |\n  Snap Store showcases featured and popular applications with useful descriptions, ratings, reviews\n  and screenshots.\n\n  Applications can be found either through browsing categories or by searching.\n\n  Snap Store can also be used to switch channels, view and alter snap permissions and view and\n  submit reviews and ratings.\n\n  Snap Store is based on GNOME Software, optimized for the Snap experience.\nsnap-id: gjf3IPXoRiipCu9K0kVu52f0H56fIksg\nchannels:\n  2/stable:          0+git.90575829   2025-04-02 (1270) 11MB -\n  2/candidate:       \u2191                                       \n  2/beta:            \u2191                                       \n  2/edge:            0+git.506cb2c3   2025-04-24 (1279) 11MB -\n  latest/stable:     41.3-72-g80e7130 2024-09-22 (1216) 12MB -\n  latest/candidate:  \u2191                                       \n  latest/beta:       \u2191                                       \n  latest/edge:       0+git.506cb2c3   2025-04-24 (1279) 11MB -\n  preview/stable:    \u2013                                       \n  preview/candidate: 0.2.7-alpha      2023-02-02  (864) 10MB -\n  preview/beta:      \u2191                                       \n  preview/edge:      0.3.0-alpha      2023-08-14 (1017) 11MB -\n  1/stable:          41.3-72-g80e7130 2024-09-22 (1216) 12MB -\n  1/candidate:       \u2191                                       \n  1/beta:            \u2191                                       \n  1/edge:            41.3-72-g80e7130 2024-09-16 (1216) 12MB -\n</code></pre> <p>Once <code>snap</code> is able to reach the store again, the <code>thunderbird</code> app is finally upgraded and the upgrade process moves forward. Yet another prompt asks whether to Remove or Keep obsolete packages; I decided to keep obsolete packages; having already had quite a few packages removed. After reboot, there is just one package to update:</p> <code>apt dist-upgrade -y</code> <pre><code># apt dist-upgrade -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\nThe following packages were automatically installed and are no longer required:\n  acpi-support acpid agordejo blender-data cyclist digikam-data encfs ffmpeg2theora fonts-kacst\n  fonts-kacst-one fonts-khmeros-core fonts-lao fonts-lklug-sinhala fonts-sil-abyssinica fonts-sil-padauk\n  fonts-thai-tlwg fonts-tibetan-machine fonts-tlwg-garuda fonts-tlwg-garuda-ttf fonts-tlwg-kinnari\n  fonts-tlwg-kinnari-ttf fonts-tlwg-laksaman fonts-tlwg-laksaman-ttf fonts-tlwg-loma fonts-tlwg-loma-ttf\n  fonts-tlwg-mono fonts-tlwg-mono-ttf fonts-tlwg-norasi fonts-tlwg-norasi-ttf fonts-tlwg-purisa\n  fonts-tlwg-purisa-ttf fonts-tlwg-sawasdee fonts-tlwg-sawasdee-ttf fonts-tlwg-typewriter\n  fonts-tlwg-typewriter-ttf fonts-tlwg-typist fonts-tlwg-typist-ttf fonts-tlwg-typo fonts-tlwg-typo-ttf\n  fonts-tlwg-umpush fonts-tlwg-umpush-ttf fonts-tlwg-waree fonts-tlwg-waree-ttf foo-yc20 freeglut3 gamin\n  gcc-12-base:i386 gimp-data haveged irqbalance kdenlive-data kross ksystemlog lib2geom1.1.0 libabsl20210324\n  libamd2 libappimage0 libappstream4 libappstreamqt2 libarmadillo10 libart-2.0-2 libastro1 libatk1.0-data\n  libavcodec58 libavdevice58 libavfilter7 libavformat58 libavif13 libavutil56 libbabl-0.1-0\n  libblockdev-crypto2 libblockdev-fs2 libblockdev-loop2 libblockdev-part-err2 libblockdev-part2\n  libblockdev-swap2 libblockdev-utils2 libblockdev2 libboost-filesystem1.74.0 libboost-iostreams1.74.0\n  libboost-locale1.74.0 libboost-regex1.74.0 libboost-thread1.74.0 libbpf0 libcamd2 libcapi20-3t64\n  libcbor0.8 libccolamd2 libcfitsio9 libchamplain-0.12-0 libchamplain-gtk-0.12-0 libcholmod3\n  libclutter-1.0-0 libclutter-1.0-common libclutter-gtk-1.0-0 libcodec2-1.0 libcogl-common libcogl-pango20\n  libcogl-path20 libcogl20 libcolamd2 libcrypto++8t64 libcupsfilters1 libdav1d5 libdcmtk16 libdcmtk17t64\n  libdns-export1110 libdraco4 libdrm-nouveau2:i386 libembree4-4 libev4t64 libevent-pthreads-2.1-7t64\n  libfaudio0 libflac++6v5 libflac8 libflac8:i386 libfltk1.1 libfontembed1 libgamin0 libgav1-0 libgcab-1.0-0\n  libgegl-common libgeos3.10.2 libgit2-1.1 libglade2-0 libgnomecanvas2-0 libgnomecanvas2-common libgps28\n  libgraphicsmagick++-q16-12t64 libgssdp-1.2-0 libgtkmm-2.4-1t64 libgupnp-1.2-1 libgupnp-igd-1.0-4\n  libhavege2 libicu70 libicu70:i386 libilmbase25 libisc-export1105 libixml10 libjavascriptcoregtk-4.0-18\n  libjemalloc2 libkdecorations2private9 libkdynamicwallpaper1 libkf5akonadi-data libkf5akonadicontact-data\n  libkf5akonadicontact5abi1 libkf5akonadicore-bin libkf5akonadicore5abi2 libkf5akonadiprivate5abi2\n  libkf5akonadiwidgets5abi1 libkf5baloowidgets-data libkf5calendarcore5abi2 libkf5contacteditor5\n  libkf5grantleetheme-data libkf5grantleetheme-plugins libkf5grantleetheme5 libkf5jsapi5 libkf5krosscore5\n  libkf5krossui5 libkf5libkleo5abi1 libkf5mime-data libkf5mime5abi2 libkf5pimtextedit-data\n  libkf5pimtextedit5abi3 libkf5waylandserver5 libkpim5akonadi-data libkwaylandserver5 libkwinxrenderutils13\n  liblcms2-2:i386 libldap-2.5-0:i386 libllvm13 libllvm15t64 libllvm15t64:i386 liblog4cplus-2.0.5t64\n  liblua5.1-0 libmagick++-6.q16-8 libmagickcore-6.q16-6 libmagickcore-6.q16-6-extra libmagickwand-6.q16-6\n  libmanette-0.2-0 libmarblewidget-qt5-28 libmetis5 libmicrohttpd12t64 libmpdec3 libmujs1 libnetpbm10\n  libnetplan0 libnfs13 liboggkate1 libokular5core9 libopenal1:i386 libopencolorio1v5 libopencv-calib3d4.5d\n  libopencv-core4.5d libopencv-dnn4.5d libopencv-features2d4.5d libopencv-flann4.5d libopencv-imgproc4.5d\n  libopencv-ml4.5d libopencv-objdetect4.5d libopencv-video4.5d libopenexr25 libopenh264-6 libopenvdb10.0t64\n  libopts25 liborcus-0.17-0 liborcus-parser-0.17-0 libosdcpu3.5.0t64 libosdgpu3.5.0t64 libosmesa6\n  libparted-fs-resize0t64 libperl5.34 libphonon4qt5-data libplacebo192 libplist3 libpoppler118 libpostproc55\n  libproj22 libprotobuf-lite23 libprotobuf23 libpython3.10 libpython3.10-dev libpython3.10-minimal\n  libpython3.10-stdlib libqgpgme7 libqpdf28 libqt5networkauth5 libqtav1 libqtavwidgets1 libraw20 libre2-9\n  libshp2 libshp4 libsmbios-c2 libsnapd-glib1 libsnapd-qt1 libsndio7.0:i386 libsoup-2.4-1:i386 libspnav0\n  libsquish0 libsrt1.4-gnutls libstb0t64 libstk-4.6.1 libsuitesparseconfig5 libsuperlu5 libswresample3\n  libswscale5 libtesseract4 libtexlua53 libtexluajit2 libtiff-tools libtiff5 libtiff5:i386 libtinyxml2-9\n  libumfpack5 libunistring2 libunistring2:i386 libupnp13 libvkd3d-shader1 libvkd3d1 libvpx7 libvpx7:i386\n  libwebkit2gtk-4.0-37 libwebsockets16 libwxbase3.0-0v5 libwxgtk3.0-gtk3-0v5 libx264-163 libxkbregistry0\n  libxslt1.1:i386 libyaml-cpp0.7 libzxingcore1 linux-headers-5.15.0-138 linux-headers-5.15.0-138-generic\n  linux-headers-5.15.0-139 linux-headers-5.15.0-139-generic linux-lowlatency-headers-5.15.0-136\n  linux-lowlatency-headers-5.15.0-138 linuxaudio-new-session-manager marble-plugins marble-qt-data mediainfo\n  midisnoop mlocate muon opencv-data p7zip perl-modules-5.34 petri-foo python3-alsaaudio python3-cffi\n  python3-jack-client python3-lib2to3 python3-netifaces python3-pbr python3-pycparser python3-sip python3.10\n  python3.10-dev python3.10-minimal qwinff rtmpdump sntp synfig-examples ubuntu-advantage-tools\n  vkd3d-compiler vocproc xdg-dbus-proxy zita-njbridge\nUse 'apt autoremove' to remove them.\nThe following packages will be REMOVED:\n  linux-headers-5.15.0-136-lowlatency linux-headers-5.15.0-138-lowlatency linux-image-5.15.0-136-lowlatency\n  linux-image-5.15.0-138-lowlatency linux-modules-5.15.0-136-lowlatency linux-modules-5.15.0-138-lowlatency\nThe following NEW packages will be installed:\n  libgdal34t64 libgeos-c1t64 libgeos3.12.1t64 libmlt++7 libmlt7 libopencv-contrib406t64\n  libopencv-highgui406t64 libopencv-imgcodecs406t64 librttopo1 libspatialite8t64\nThe following packages will be upgraded:\n  krita\n1 upgraded, 10 newly installed, 6 to remove and 0 not upgraded.\nNeed to get 39.4 MB of archives.\nAfter this operation, 940 MB disk space will be freed.\nGet:1 http://archive.ubuntu.com/ubuntu noble/universe amd64 libgeos3.12.1t64 amd64 3.12.1-3build1 [849 kB]\nGet:2 http://archive.ubuntu.com/ubuntu noble/universe amd64 libgeos-c1t64 amd64 3.12.1-3build1 [94.5 kB]\nGet:3 http://archive.ubuntu.com/ubuntu noble/universe amd64 librttopo1 amd64 1.1.0-3build2 [191 kB]\nGet:4 http://archive.ubuntu.com/ubuntu noble/universe amd64 libspatialite8t64 amd64 5.1.0-3build1 [1,919 kB]\nGet:5 http://archive.ubuntu.com/ubuntu noble/universe amd64 libgdal34t64 amd64 3.8.4+dfsg-3ubuntu3 [8,346 kB]\nGet:6 http://archive.ubuntu.com/ubuntu noble/universe amd64 libopencv-imgcodecs406t64 amd64 4.6.0+dfsg-13.1ubuntu1 [128 kB]\nGet:7 http://archive.ubuntu.com/ubuntu noble/universe amd64 libopencv-highgui406t64 amd64 4.6.0+dfsg-13.1ubuntu1 [118 kB]\nGet:8 http://archive.ubuntu.com/ubuntu noble/universe amd64 libopencv-contrib406t64 amd64 4.6.0+dfsg-13.1ubuntu1 [3,968 kB]\nGet:9 http://archive.ubuntu.com/ubuntu noble/universe amd64 libmlt7 amd64 7.22.0-1build6 [1,603 kB]\nGet:10 http://archive.ubuntu.com/ubuntu noble/universe amd64 libmlt++7 amd64 7.22.0-1build6 [51.5 kB]\nGet:11 http://archive.ubuntu.com/ubuntu noble/universe amd64 krita amd64 1:5.2.2+dfsg-2build8 [22.1 MB]\nFetched 39.4 MB in 3s (12.1 MB/s)     \n(Reading database ... 657253 files and directories currently installed.)\nRemoving linux-headers-5.15.0-136-lowlatency (5.15.0-136.147) ...\nRemoving linux-headers-5.15.0-138-lowlatency (5.15.0-138.148) ...\nRemoving linux-modules-5.15.0-138-lowlatency (5.15.0-138.148) ...\nRemoving linux-image-5.15.0-136-lowlatency (5.15.0-136.147) ...\n/etc/kernel/postrm.d/initramfs-tools:\nupdate-initramfs: Deleting /boot/initrd.img-5.15.0-136-lowlatency\n/etc/kernel/postrm.d/zz-update-grub:\nSourcing file `/etc/default/grub'\nSourcing file `/etc/default/grub.d/ubuntustudio.cfg'\nGenerating grub configuration file ...\nFound linux image: /boot/vmlinuz-6.8.0-59-lowlatency\nFound initrd image: /boot/initrd.img-6.8.0-59-lowlatency\nFound linux image: /boot/vmlinuz-5.15.0-139-lowlatency\nFound initrd image: /boot/initrd.img-5.15.0-139-lowlatency\nFound linux image: /boot/vmlinuz-5.15.0-138-lowlatency\nFound initrd image: /boot/initrd.img-5.15.0-138-lowlatency\nFound memtest86+ 64bit EFI image: /boot/memtest86+x64.efi\nWarning: os-prober will not be executed to detect other bootable partitions.\nSystems on them will not be added to the GRUB boot configuration.\nCheck GRUB_DISABLE_OS_PROBER documentation entry.\nAdding boot menu entry for UEFI Firmware Settings ...\ndone\nRemoving linux-modules-5.15.0-136-lowlatency (5.15.0-136.147) ...\nRemoving linux-image-5.15.0-138-lowlatency (5.15.0-138.148) ...\n/etc/kernel/postrm.d/initramfs-tools:\nupdate-initramfs: Deleting /boot/initrd.img-5.15.0-138-lowlatency\n/etc/kernel/postrm.d/zz-update-grub:\nSourcing file `/etc/default/grub'\nSourcing file `/etc/default/grub.d/ubuntustudio.cfg'\nGenerating grub configuration file ...\nFound linux image: /boot/vmlinuz-6.8.0-59-lowlatency\nFound initrd image: /boot/initrd.img-6.8.0-59-lowlatency\nFound linux image: /boot/vmlinuz-5.15.0-139-lowlatency\nFound initrd image: /boot/initrd.img-5.15.0-139-lowlatency\nFound memtest86+ 64bit EFI image: /boot/memtest86+x64.efi\nWarning: os-prober will not be executed to detect other bootable partitions.\nSystems on them will not be added to the GRUB boot configuration.\nCheck GRUB_DISABLE_OS_PROBER documentation entry.\nAdding boot menu entry for UEFI Firmware Settings ...\ndone\nSelecting previously unselected package libgeos3.12.1t64:amd64.\n(Reading database ... 623715 files and directories currently installed.)\nPreparing to unpack .../00-libgeos3.12.1t64_3.12.1-3build1_amd64.deb ...\nUnpacking libgeos3.12.1t64:amd64 (3.12.1-3build1) ...\nSelecting previously unselected package libgeos-c1t64:amd64.\nPreparing to unpack .../01-libgeos-c1t64_3.12.1-3build1_amd64.deb ...\nUnpacking libgeos-c1t64:amd64 (3.12.1-3build1) ...\nSelecting previously unselected package librttopo1:amd64.\nPreparing to unpack .../02-librttopo1_1.1.0-3build2_amd64.deb ...\nUnpacking librttopo1:amd64 (1.1.0-3build2) ...\nSelecting previously unselected package libspatialite8t64:amd64.\nPreparing to unpack .../03-libspatialite8t64_5.1.0-3build1_amd64.deb ...\nUnpacking libspatialite8t64:amd64 (5.1.0-3build1) ...\nSelecting previously unselected package libgdal34t64:amd64.\nPreparing to unpack .../04-libgdal34t64_3.8.4+dfsg-3ubuntu3_amd64.deb ...\nUnpacking libgdal34t64:amd64 (3.8.4+dfsg-3ubuntu3) ...\nSelecting previously unselected package libopencv-imgcodecs406t64:amd64.\nPreparing to unpack .../05-libopencv-imgcodecs406t64_4.6.0+dfsg-13.1ubuntu1_amd64.deb ...\nUnpacking libopencv-imgcodecs406t64:amd64 (4.6.0+dfsg-13.1ubuntu1) ...\nSelecting previously unselected package libopencv-highgui406t64:amd64.\nPreparing to unpack .../06-libopencv-highgui406t64_4.6.0+dfsg-13.1ubuntu1_amd64.deb ...\nUnpacking libopencv-highgui406t64:amd64 (4.6.0+dfsg-13.1ubuntu1) ...\nSelecting previously unselected package libopencv-contrib406t64:amd64.\nPreparing to unpack .../07-libopencv-contrib406t64_4.6.0+dfsg-13.1ubuntu1_amd64.deb ...\nUnpacking libopencv-contrib406t64:amd64 (4.6.0+dfsg-13.1ubuntu1) ...\nSelecting previously unselected package libmlt7:amd64.\nPreparing to unpack .../08-libmlt7_7.22.0-1build6_amd64.deb ...\nUnpacking libmlt7:amd64 (7.22.0-1build6) ...\nSelecting previously unselected package libmlt++7:amd64.\nPreparing to unpack .../09-libmlt++7_7.22.0-1build6_amd64.deb ...\nUnpacking libmlt++7:amd64 (7.22.0-1build6) ...\nPreparing to unpack .../10-krita_1%3a5.2.2+dfsg-2build8_amd64.deb ...\nUnpacking krita (1:5.2.2+dfsg-2build8) over (1:5.0.2+dfsg-1build1) ...\nSetting up libgeos3.12.1t64:amd64 (3.12.1-3build1) ...\nSetting up libgeos-c1t64:amd64 (3.12.1-3build1) ...\nSetting up librttopo1:amd64 (1.1.0-3build2) ...\nSetting up libspatialite8t64:amd64 (5.1.0-3build1) ...\nSetting up libgdal34t64:amd64 (3.8.4+dfsg-3ubuntu3) ...\nSetting up libopencv-imgcodecs406t64:amd64 (4.6.0+dfsg-13.1ubuntu1) ...\nSetting up libopencv-highgui406t64:amd64 (4.6.0+dfsg-13.1ubuntu1) ...\nSetting up libopencv-contrib406t64:amd64 (4.6.0+dfsg-13.1ubuntu1) ...\nSetting up libmlt7:amd64 (7.22.0-1build6) ...\nSetting up libmlt++7:amd64 (7.22.0-1build6) ...\nSetting up krita (1:5.2.2+dfsg-2build8) ...\nProcessing triggers for mailcap (3.70+nmu1ubuntu1) ...\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.4) ...\n</code></pre> <p>After this, install packages from the list of Ubuntu Studio 24.04 essential packages:</p> <code>apt install ... -y</code> <pre><code># apt install gdebi-core wget gkrellm vim curl gkrellm-leds \\\n  gkrellm-xkb gkrellm-cpufreq geeqie playonlinux exfat-fuse \\\n  clementine id3v2 htop vnstat neofetch tigervnc-viewer sox \\\n  scummvm wine gamemode python-is-python3 exiv2 rename scrot \\\n  speedtest-cli xcalib python3-pip netcat-openbsd jstest-gtk \\\n  etherwake python3-selenium lm-sensors sysstat tor unrar \\\n  ttf-mscorefonts-installer winetricks icc-profiles ffmpeg \\\n  iotop-c xdotool redshift-gtk inxi vainfo vdpauinfo mpv \\\n  tigervnc-tools screen -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ngdebi-core is already the newest version (0.9.5.7+nmu7).\nwget is already the newest version (1.21.4-1ubuntu4.1).\ngkrellm is already the newest version (2.3.11-2build2).\nvim is already the newest version (2:9.1.0016-1ubuntu7.8).\ncurl is already the newest version (8.5.0-2ubuntu10.6).\ngkrellm-xkb is already the newest version (1.05-5.1build2).\ngkrellm-cpufreq is already the newest version (0.6.4-4).\ngeeqie is already the newest version (1:2.2-2ubuntu0.1).\nplayonlinux is already the newest version (4.3.4-3).\nexfat-fuse is already the newest version (1.4.0-2).\nclementine is already the newest version (1.4.0~rc1+git867-g9ef681b0e+dfsg-1ubuntu4).\nid3v2 is already the newest version (0.1.12+dfsg-7).\nhtop is already the newest version (3.3.0-4build1).\nvnstat is already the newest version (2.12-1).\nneofetch is already the newest version (7.1.0-4).\ntigervnc-viewer is already the newest version (1.13.1+dfsg-2build2).\nsox is already the newest version (14.4.2+git20190427-4build4).\nscummvm is already the newest version (2.8.0+dfsg-1build5).\nwine is already the newest version (9.0~repack-4build3).\ngamemode is already the newest version (1.8.1-2build1).\npython-is-python3 is already the newest version (3.11.4-1).\nexiv2 is already the newest version (0.27.6-1build1).\nrename is already the newest version (2.02-1).\nscrot is already the newest version (1.10-1build2).\nspeedtest-cli is already the newest version (2.1.3-2).\nxcalib is already the newest version (0.8.dfsg1-3).\npython3-pip is already the newest version (24.0+dfsg-1ubuntu1.1).\nnetcat-openbsd is already the newest version (1.226-1ubuntu2).\nnetcat-openbsd set to manually installed.\njstest-gtk is already the newest version (0.1.1~git20180602-2build2).\netherwake is already the newest version (1.09-4build1).\npython3-selenium is already the newest version (4.18.1+dfsg-1).\nlm-sensors is already the newest version (1:3.6.0-9build1).\nsysstat is already the newest version (12.6.1-2).\ntor is already the newest version (0.4.8.10-1build2).\nunrar is already the newest version (1:7.0.7-1build1).\nttf-mscorefonts-installer is already the newest version (3.8.1ubuntu1).\nwinetricks is already the newest version (20240105-2).\nicc-profiles is already the newest version (2.1-2).\nffmpeg is already the newest version (7:6.1.1-3ubuntu5+esm2).\niotop-c is already the newest version (1.26-1).\nxdotool is already the newest version (1:3.20160805.1-5build1).\nredshift-gtk is already the newest version (1.12-4.2ubuntu4).\ninxi is already the newest version (3.3.34-1-1).\nvainfo is already the newest version (2.12.0+ds1-1).\nvdpauinfo is already the newest version (1.5-2).\nmpv is already the newest version (0.37.0-1ubuntu4).\ntigervnc-tools is already the newest version (1.13.1+dfsg-2build2).\nThe following packages were automatically installed and are no longer required:\n  acpi-support acpid agordejo blender-data cyclist digikam-data encfs ffmpeg2theora fonts-kacst\n  fonts-kacst-one fonts-khmeros-core fonts-lao fonts-lklug-sinhala fonts-sil-abyssinica fonts-sil-padauk\n  fonts-thai-tlwg fonts-tibetan-machine fonts-tlwg-garuda fonts-tlwg-garuda-ttf fonts-tlwg-kinnari\n  fonts-tlwg-kinnari-ttf fonts-tlwg-laksaman fonts-tlwg-laksaman-ttf fonts-tlwg-loma fonts-tlwg-loma-ttf\n  fonts-tlwg-mono fonts-tlwg-mono-ttf fonts-tlwg-norasi fonts-tlwg-norasi-ttf fonts-tlwg-purisa\n  fonts-tlwg-purisa-ttf fonts-tlwg-sawasdee fonts-tlwg-sawasdee-ttf fonts-tlwg-typewriter\n  fonts-tlwg-typewriter-ttf fonts-tlwg-typist fonts-tlwg-typist-ttf fonts-tlwg-typo fonts-tlwg-typo-ttf\n  fonts-tlwg-umpush fonts-tlwg-umpush-ttf fonts-tlwg-waree fonts-tlwg-waree-ttf foo-yc20 freeglut3 gamin\n  gcc-12-base:i386 gimp-data haveged irqbalance kdenlive-data kross ksystemlog lib2geom1.1.0 libabsl20210324\n  libamd2 libappimage0 libappstream4 libappstreamqt2 libarmadillo10 libart-2.0-2 libastro1 libatk1.0-data\n  libavcodec58 libavdevice58 libavfilter7 libavformat58 libavif13 libavutil56 libbabl-0.1-0\n  libblockdev-crypto2 libblockdev-fs2 libblockdev-loop2 libblockdev-part-err2 libblockdev-part2\n  libblockdev-swap2 libblockdev-utils2 libblockdev2 libboost-filesystem1.74.0 libboost-iostreams1.74.0\n  libboost-locale1.74.0 libboost-regex1.74.0 libboost-thread1.74.0 libbpf0 libcamd2 libcapi20-3t64\n  libcbor0.8 libccolamd2 libcfitsio9 libchamplain-0.12-0 libchamplain-gtk-0.12-0 libcholmod3\n  libclutter-1.0-0 libclutter-1.0-common libclutter-gtk-1.0-0 libcodec2-1.0 libcogl-common libcogl-pango20\n  libcogl-path20 libcogl20 libcolamd2 libcrypto++8t64 libcupsfilters1 libdav1d5 libdcmtk16 libdcmtk17t64\n  libdns-export1110 libdraco4 libdrm-nouveau2:i386 libembree4-4 libev4t64 libevent-pthreads-2.1-7t64\n  libfaudio0 libflac++6v5 libflac8 libflac8:i386 libfltk1.1 libfontembed1 libgamin0 libgav1-0 libgcab-1.0-0\n  libgegl-common libgeos3.10.2 libgit2-1.1 libglade2-0 libgnomecanvas2-0 libgnomecanvas2-common libgps28\n  libgraphicsmagick++-q16-12t64 libgssdp-1.2-0 libgtkmm-2.4-1t64 libgupnp-1.2-1 libgupnp-igd-1.0-4\n  libhavege2 libicu70 libicu70:i386 libilmbase25 libisc-export1105 libixml10 libjavascriptcoregtk-4.0-18\n  libjemalloc2 libkdecorations2private9 libkdynamicwallpaper1 libkf5akonadi-data libkf5akonadicontact-data\n  libkf5akonadicontact5abi1 libkf5akonadicore-bin libkf5akonadicore5abi2 libkf5akonadiprivate5abi2\n  libkf5akonadiwidgets5abi1 libkf5baloowidgets-data libkf5calendarcore5abi2 libkf5contacteditor5\n  libkf5grantleetheme-data libkf5grantleetheme-plugins libkf5grantleetheme5 libkf5jsapi5 libkf5krosscore5\n  libkf5krossui5 libkf5libkleo5abi1 libkf5mime-data libkf5mime5abi2 libkf5pimtextedit-data\n  libkf5pimtextedit5abi3 libkf5waylandserver5 libkpim5akonadi-data libkwaylandserver5 libkwinxrenderutils13\n  liblcms2-2:i386 libldap-2.5-0:i386 libllvm13 libllvm15t64 libllvm15t64:i386 liblog4cplus-2.0.5t64\n  liblua5.1-0 libmagick++-6.q16-8 libmagickcore-6.q16-6 libmagickcore-6.q16-6-extra libmagickwand-6.q16-6\n  libmanette-0.2-0 libmarblewidget-qt5-28 libmetis5 libmicrohttpd12t64 libmpdec3 libmujs1 libnetpbm10\n  libnetplan0 libnfs13 liboggkate1 libokular5core9 libopenal1:i386 libopencolorio1v5 libopencv-calib3d4.5d\n  libopencv-core4.5d libopencv-dnn4.5d libopencv-features2d4.5d libopencv-flann4.5d libopencv-imgproc4.5d\n  libopencv-ml4.5d libopencv-objdetect4.5d libopencv-video4.5d libopenexr25 libopenh264-6 libopenvdb10.0t64\n  libopts25 liborcus-0.17-0 liborcus-parser-0.17-0 libosdcpu3.5.0t64 libosdgpu3.5.0t64 libosmesa6\n  libparted-fs-resize0t64 libperl5.34 libphonon4qt5-data libplacebo192 libplist3 libpoppler118 libpostproc55\n  libproj22 libprotobuf-lite23 libprotobuf23 libpython3.10 libpython3.10-dev libpython3.10-minimal\n  libpython3.10-stdlib libqgpgme7 libqpdf28 libqt5networkauth5 libqtav1 libqtavwidgets1 libraw20 libre2-9\n  libshp2 libshp4 libsmbios-c2 libsnapd-glib1 libsnapd-qt1 libsndio7.0:i386 libsoup-2.4-1:i386 libspnav0\n  libsquish0 libsrt1.4-gnutls libstb0t64 libstk-4.6.1 libsuitesparseconfig5 libsuperlu5 libswresample3\n  libswscale5 libtesseract4 libtexlua53 libtexluajit2 libtiff-tools libtiff5 libtiff5:i386 libtinyxml2-9\n  libumfpack5 libunistring2 libunistring2:i386 libupnp13 libvkd3d-shader1 libvkd3d1 libvpx7 libvpx7:i386\n  libwebkit2gtk-4.0-37 libwebsockets16 libwxbase3.0-0v5 libwxgtk3.0-gtk3-0v5 libx264-163 libxkbregistry0\n  libxslt1.1:i386 libyaml-cpp0.7 libzxingcore1 linux-headers-5.15.0-138 linux-headers-5.15.0-138-generic\n  linux-headers-5.15.0-139 linux-headers-5.15.0-139-generic linux-lowlatency-headers-5.15.0-136\n  linux-lowlatency-headers-5.15.0-138 linuxaudio-new-session-manager marble-plugins marble-qt-data mediainfo\n  midisnoop mlocate muon opencv-data p7zip perl-modules-5.34 petri-foo python3-alsaaudio python3-cffi\n  python3-jack-client python3-lib2to3 python3-netifaces python3-pbr python3-pycparser python3-sip python3.10\n  python3.10-dev python3.10-minimal qwinff rtmpdump sntp synfig-examples ubuntu-advantage-tools\n  vkd3d-compiler vocproc xdg-dbus-proxy zita-njbridge\nUse 'apt autoremove' to remove them.\nSuggested packages:\n  byobu | screenie | iselect\nThe following NEW packages will be installed:\n  gkrellm-leds screen\n0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 671 kB of archives.\nAfter this operation, 1,080 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu noble/main amd64 screen amd64 4.9.1-1build1 [655 kB]\nGet:2 http://archive.ubuntu.com/ubuntu noble/universe amd64 gkrellm-leds amd64 0.8.0-2build2 [16.0 kB]\nFetched 671 kB in 1s (705 kB/s)         \nSelecting previously unselected package screen.\n(Reading database ... 623901 files and directories currently installed.)\nPreparing to unpack .../screen_4.9.1-1build1_amd64.deb ...\nUnpacking screen (4.9.1-1build1) ...\nSelecting previously unselected package gkrellm-leds.\nPreparing to unpack .../gkrellm-leds_0.8.0-2build2_amd64.deb ...\nUnpacking gkrellm-leds (0.8.0-2build2) ...\nSetting up screen (4.9.1-1build1) ...\nSetting up gkrellm-leds (0.8.0-2build2) ...\nProcessing triggers for debianutils (5.17build1) ...\nProcessing triggers for install-info (7.1-3build2) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\n</code></pre> <p>Then reinstall GIMP since it was removed during the upgrade process:</p> <code>apt install gimp ... -y</code> <pre><code># apt install gimp gimp-gap gimp-gmic gimp-gutenprint \\\n      gimp-cbmplugs gimp-plugin-registry gimp-texturize -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages were automatically installed and are no longer required:\n  acpi-support acpid agordejo blender-data cyclist digikam-data encfs ffmpeg2theora fonts-kacst\n  fonts-kacst-one fonts-khmeros-core fonts-lao fonts-lklug-sinhala fonts-sil-abyssinica fonts-sil-padauk\n  fonts-thai-tlwg fonts-tibetan-machine fonts-tlwg-garuda fonts-tlwg-garuda-ttf fonts-tlwg-kinnari\n  fonts-tlwg-kinnari-ttf fonts-tlwg-laksaman fonts-tlwg-laksaman-ttf fonts-tlwg-loma fonts-tlwg-loma-ttf\n  fonts-tlwg-mono fonts-tlwg-mono-ttf fonts-tlwg-norasi fonts-tlwg-norasi-ttf fonts-tlwg-purisa\n  fonts-tlwg-purisa-ttf fonts-tlwg-sawasdee fonts-tlwg-sawasdee-ttf fonts-tlwg-typewriter\n  fonts-tlwg-typewriter-ttf fonts-tlwg-typist fonts-tlwg-typist-ttf fonts-tlwg-typo fonts-tlwg-typo-ttf\n  fonts-tlwg-umpush fonts-tlwg-umpush-ttf fonts-tlwg-waree fonts-tlwg-waree-ttf foo-yc20 freeglut3 gamin\n  gcc-12-base:i386 haveged irqbalance kdenlive-data kross ksystemlog lib2geom1.1.0 libabsl20210324 libamd2\n  libappimage0 libappstream4 libappstreamqt2 libarmadillo10 libart-2.0-2 libastro1 libatk1.0-data\n  libavcodec58 libavdevice58 libavfilter7 libavformat58 libavif13 libavutil56 libblockdev-crypto2\n  libblockdev-fs2 libblockdev-loop2 libblockdev-part-err2 libblockdev-part2 libblockdev-swap2\n  libblockdev-utils2 libblockdev2 libboost-filesystem1.74.0 libboost-iostreams1.74.0 libboost-locale1.74.0\n  libboost-regex1.74.0 libboost-thread1.74.0 libbpf0 libcamd2 libcapi20-3t64 libcbor0.8 libccolamd2\n  libcfitsio9 libchamplain-0.12-0 libchamplain-gtk-0.12-0 libcholmod3 libclutter-1.0-0 libclutter-1.0-common\n  libclutter-gtk-1.0-0 libcodec2-1.0 libcogl-common libcogl-pango20 libcogl-path20 libcogl20 libcolamd2\n  libcrypto++8t64 libcupsfilters1 libdav1d5 libdcmtk16 libdcmtk17t64 libdns-export1110 libdraco4\n  libdrm-nouveau2:i386 libembree4-4 libev4t64 libevent-pthreads-2.1-7t64 libfaudio0 libflac++6v5 libflac8\n  libflac8:i386 libfltk1.1 libfontembed1 libgamin0 libgav1-0 libgcab-1.0-0 libgeos3.10.2 libgit2-1.1\n  libglade2-0 libgnomecanvas2-0 libgnomecanvas2-common libgps28 libgssdp-1.2-0 libgtkmm-2.4-1t64\n  libgupnp-1.2-1 libgupnp-igd-1.0-4 libhavege2 libicu70 libicu70:i386 libilmbase25 libisc-export1105\n  libixml10 libjavascriptcoregtk-4.0-18 libjemalloc2 libkdecorations2private9 libkdynamicwallpaper1\n  libkf5akonadi-data libkf5akonadicontact-data libkf5akonadicontact5abi1 libkf5akonadicore-bin\n  libkf5akonadicore5abi2 libkf5akonadiprivate5abi2 libkf5akonadiwidgets5abi1 libkf5baloowidgets-data\n  libkf5calendarcore5abi2 libkf5contacteditor5 libkf5grantleetheme-data libkf5grantleetheme-plugins\n  libkf5grantleetheme5 libkf5jsapi5 libkf5krosscore5 libkf5krossui5 libkf5libkleo5abi1 libkf5mime-data\n  libkf5mime5abi2 libkf5pimtextedit-data libkf5pimtextedit5abi3 libkf5waylandserver5 libkpim5akonadi-data\n  libkwaylandserver5 libkwinxrenderutils13 liblcms2-2:i386 libldap-2.5-0:i386 libllvm13 libllvm15t64\n  libllvm15t64:i386 liblog4cplus-2.0.5t64 liblua5.1-0 libmagick++-6.q16-8 libmagickcore-6.q16-6\n  libmagickcore-6.q16-6-extra libmagickwand-6.q16-6 libmanette-0.2-0 libmarblewidget-qt5-28 libmetis5\n  libmicrohttpd12t64 libmpdec3 libmujs1 libnetpbm10 libnetplan0 libnfs13 liboggkate1 libokular5core9\n  libopenal1:i386 libopencolorio1v5 libopencv-calib3d4.5d libopencv-core4.5d libopencv-dnn4.5d\n  libopencv-features2d4.5d libopencv-flann4.5d libopencv-imgproc4.5d libopencv-ml4.5d\n  libopencv-objdetect4.5d libopencv-video4.5d libopenexr25 libopenh264-6 libopenvdb10.0t64 libopts25\n  liborcus-0.17-0 liborcus-parser-0.17-0 libosdcpu3.5.0t64 libosdgpu3.5.0t64 libosmesa6\n  libparted-fs-resize0t64 libperl5.34 libphonon4qt5-data libplacebo192 libplist3 libpoppler118 libpostproc55\n  libproj22 libprotobuf-lite23 libprotobuf23 libpython3.10 libpython3.10-dev libpython3.10-minimal\n  libpython3.10-stdlib libqgpgme7 libqpdf28 libqt5networkauth5 libqtav1 libqtavwidgets1 libraw20 libre2-9\n  libshp2 libshp4 libsmbios-c2 libsnapd-glib1 libsnapd-qt1 libsndio7.0:i386 libsoup-2.4-1:i386 libspnav0\n  libsquish0 libsrt1.4-gnutls libstb0t64 libstk-4.6.1 libsuitesparseconfig5 libsuperlu5 libswresample3\n  libswscale5 libtesseract4 libtexlua53 libtexluajit2 libtiff5 libtiff5:i386 libtinyxml2-9 libumfpack5\n  libunistring2 libunistring2:i386 libupnp13 libvkd3d-shader1 libvkd3d1 libvpx7 libvpx7:i386\n  libwebkit2gtk-4.0-37 libwebsockets16 libwxbase3.0-0v5 libwxgtk3.0-gtk3-0v5 libx264-163 libxkbregistry0\n  libxslt1.1:i386 libyaml-cpp0.7 libzxingcore1 linux-headers-5.15.0-138 linux-headers-5.15.0-138-generic\n  linux-headers-5.15.0-139 linux-headers-5.15.0-139-generic linux-lowlatency-headers-5.15.0-136\n  linux-lowlatency-headers-5.15.0-138 linuxaudio-new-session-manager marble-plugins marble-qt-data mediainfo\n  midisnoop mlocate muon opencv-data p7zip perl-modules-5.34 petri-foo python3-alsaaudio python3-cffi\n  python3-jack-client python3-lib2to3 python3-netifaces python3-pbr python3-pycparser python3-sip python3.10\n  python3.10-dev python3.10-minimal qwinff rtmpdump sntp synfig-examples ubuntu-advantage-tools\n  vkd3d-compiler vocproc xdg-dbus-proxy zita-njbridge\nUse 'apt autoremove' to remove them.\nThe following additional packages will be installed:\n  gutenprint-locales libamd3 libcamd3 libccolamd3 libcholmod5 libgegl-0.4-0t64 libgimp2.0t64 libgmic1\n  libgutenprint-common libgutenprint9 libgutenprintui2-2 libopencv-videoio406t64 libumfpack6\nSuggested packages:\n  gmic gutenprint-doc\nThe following NEW packages will be installed:\n  gimp gimp-cbmplugs gimp-gap gimp-gmic gimp-gutenprint gimp-plugin-registry gimp-texturize\n  gutenprint-locales libamd3 libcamd3 libccolamd3 libcholmod5 libgegl-0.4-0t64 libgimp2.0t64 libgmic1\n  libgutenprint-common libgutenprint9 libgutenprintui2-2 libopencv-videoio406t64 libumfpack6\n0 upgraded, 20 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 17.5 MB of archives.\nAfter this operation, 74.9 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu noble/universe amd64 libamd3 amd64 1:7.6.1+dfsg-1build1 [27.2 kB]\nGet:2 http://archive.ubuntu.com/ubuntu noble/universe amd64 libcamd3 amd64 1:7.6.1+dfsg-1build1 [23.8 kB]\nGet:3 http://archive.ubuntu.com/ubuntu noble/universe amd64 libccolamd3 amd64 1:7.6.1+dfsg-1build1 [25.9 kB]\nGet:4 http://archive.ubuntu.com/ubuntu noble/universe amd64 libcholmod5 amd64 1:7.6.1+dfsg-1build1 [667 kB]\nGet:5 http://archive.ubuntu.com/ubuntu noble/universe amd64 libumfpack6 amd64 1:7.6.1+dfsg-1build1 [268 kB]\nGet:6 http://archive.ubuntu.com/ubuntu noble/universe amd64 libgegl-0.4-0t64 amd64 1:0.4.48-2.4build2 [1,983 kB]\nGet:7 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 libgimp2.0t64 amd64 2.10.36-3ubuntu0.24.04.1 [898 kB]\nGet:8 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 gimp amd64 2.10.36-3ubuntu0.24.04.1 [4,680 kB]\nGet:9 http://archive.ubuntu.com/ubuntu noble/universe amd64 gimp-cbmplugs amd64 1.2.2-1.2build2 [31.5 kB]\nGet:10 http://archive.ubuntu.com/ubuntu noble/universe amd64 gimp-gap amd64 2.6.0+dfsg-7ubuntu3 [1,985 kB]\nGet:11 http://archive.ubuntu.com/ubuntu noble/universe amd64 libopencv-videoio406t64 amd64 4.6.0+dfsg-13.1ubuntu1 [199 kB]\nGet:12 http://archive.ubuntu.com/ubuntu noble/universe amd64 libgmic1 amd64 2.9.4-4build11 [3,617 kB]\nGet:13 http://archive.ubuntu.com/ubuntu noble/universe amd64 gimp-gmic amd64 2.9.4-4build11 [712 kB]\nGet:14 http://archive.ubuntu.com/ubuntu noble/main amd64 libgutenprint-common all 5.3.4.20220624T01008808d602-1build4 [618 kB]\nGet:15 http://archive.ubuntu.com/ubuntu noble/main amd64 libgutenprint9 amd64 5.3.4.20220624T01008808d602-1build4 [553 kB]\nGet:16 http://archive.ubuntu.com/ubuntu noble/universe amd64 libgutenprintui2-2 amd64 5.3.4.20220624T01008808d602-1build4 [91.4 kB]\nGet:17 http://archive.ubuntu.com/ubuntu noble/universe amd64 gimp-gutenprint amd64 5.3.4.20220624T01008808d602-1build4 [49.9 kB]\nGet:18 http://archive.ubuntu.com/ubuntu noble/universe amd64 gimp-plugin-registry amd64 9.20240404 [1,004 kB]\nGet:19 http://archive.ubuntu.com/ubuntu noble/universe amd64 gimp-texturize amd64 2.2-4build2 [19.1 kB]\nGet:20 http://archive.ubuntu.com/ubuntu noble/universe amd64 gutenprint-locales all 5.3.4.20220624T01008808d602-1build4 [7,308 B]\nFetched 17.5 MB in 3s (6,401 kB/s)              \nSelecting previously unselected package libamd3:amd64.\n(Reading database ... 624405 files and directories currently installed.)\nPreparing to unpack .../00-libamd3_1%3a7.6.1+dfsg-1build1_amd64.deb ...\nUnpacking libamd3:amd64 (1:7.6.1+dfsg-1build1) ...\nSelecting previously unselected package libcamd3:amd64.\nPreparing to unpack .../01-libcamd3_1%3a7.6.1+dfsg-1build1_amd64.deb ...\nUnpacking libcamd3:amd64 (1:7.6.1+dfsg-1build1) ...\nSelecting previously unselected package libccolamd3:amd64.\nPreparing to unpack .../02-libccolamd3_1%3a7.6.1+dfsg-1build1_amd64.deb ...\nUnpacking libccolamd3:amd64 (1:7.6.1+dfsg-1build1) ...\nSelecting previously unselected package libcholmod5:amd64.\nPreparing to unpack .../03-libcholmod5_1%3a7.6.1+dfsg-1build1_amd64.deb ...\nUnpacking libcholmod5:amd64 (1:7.6.1+dfsg-1build1) ...\nSelecting previously unselected package libumfpack6:amd64.\nPreparing to unpack .../04-libumfpack6_1%3a7.6.1+dfsg-1build1_amd64.deb ...\nUnpacking libumfpack6:amd64 (1:7.6.1+dfsg-1build1) ...\nSelecting previously unselected package libgegl-0.4-0t64:amd64.\nPreparing to unpack .../05-libgegl-0.4-0t64_1%3a0.4.48-2.4build2_amd64.deb ...\nUnpacking libgegl-0.4-0t64:amd64 (1:0.4.48-2.4build2) ...\nSelecting previously unselected package libgimp2.0t64:amd64.\nPreparing to unpack .../06-libgimp2.0t64_2.10.36-3ubuntu0.24.04.1_amd64.deb ...\nUnpacking libgimp2.0t64:amd64 (2.10.36-3ubuntu0.24.04.1) ...\nSelecting previously unselected package gimp.\nPreparing to unpack .../07-gimp_2.10.36-3ubuntu0.24.04.1_amd64.deb ...\nUnpacking gimp (2.10.36-3ubuntu0.24.04.1) ...\nSelecting previously unselected package gimp-cbmplugs.\nPreparing to unpack .../08-gimp-cbmplugs_1.2.2-1.2build2_amd64.deb ...\nUnpacking gimp-cbmplugs (1.2.2-1.2build2) ...\nSelecting previously unselected package gimp-gap.\nPreparing to unpack .../09-gimp-gap_2.6.0+dfsg-7ubuntu3_amd64.deb ...\nUnpacking gimp-gap (2.6.0+dfsg-7ubuntu3) ...\nSelecting previously unselected package libopencv-videoio406t64:amd64.\nPreparing to unpack .../10-libopencv-videoio406t64_4.6.0+dfsg-13.1ubuntu1_amd64.deb ...\nUnpacking libopencv-videoio406t64:amd64 (4.6.0+dfsg-13.1ubuntu1) ...\nSelecting previously unselected package libgmic1:amd64.\nPreparing to unpack .../11-libgmic1_2.9.4-4build11_amd64.deb ...\nUnpacking libgmic1:amd64 (2.9.4-4build11) ...\nSelecting previously unselected package gimp-gmic.\nPreparing to unpack .../12-gimp-gmic_2.9.4-4build11_amd64.deb ...\nUnpacking gimp-gmic (2.9.4-4build11) ...\nSelecting previously unselected package libgutenprint-common.\nPreparing to unpack .../13-libgutenprint-common_5.3.4.20220624T01008808d602-1build4_all.deb ...\nUnpacking libgutenprint-common (5.3.4.20220624T01008808d602-1build4) ...\nSelecting previously unselected package libgutenprint9.\nPreparing to unpack .../14-libgutenprint9_5.3.4.20220624T01008808d602-1build4_amd64.deb ...\nUnpacking libgutenprint9 (5.3.4.20220624T01008808d602-1build4) ...\nSelecting previously unselected package libgutenprintui2-2.\nPreparing to unpack .../15-libgutenprintui2-2_5.3.4.20220624T01008808d602-1build4_amd64.deb ...\nUnpacking libgutenprintui2-2 (5.3.4.20220624T01008808d602-1build4) ...\nSelecting previously unselected package gimp-gutenprint.\nPreparing to unpack .../16-gimp-gutenprint_5.3.4.20220624T01008808d602-1build4_amd64.deb ...\nUnpacking gimp-gutenprint (5.3.4.20220624T01008808d602-1build4) ...\nSelecting previously unselected package gimp-plugin-registry.\nPreparing to unpack .../17-gimp-plugin-registry_9.20240404_amd64.deb ...\nUnpacking gimp-plugin-registry (9.20240404) ...\nSelecting previously unselected package gimp-texturize.\nPreparing to unpack .../18-gimp-texturize_2.2-4build2_amd64.deb ...\nUnpacking gimp-texturize (2.2-4build2) ...\nSelecting previously unselected package gutenprint-locales.\nPreparing to unpack .../19-gutenprint-locales_5.3.4.20220624T01008808d602-1build4_all.deb ...\nUnpacking gutenprint-locales (5.3.4.20220624T01008808d602-1build4) ...\nSetting up libamd3:amd64 (1:7.6.1+dfsg-1build1) ...\nSetting up libgutenprint-common (5.3.4.20220624T01008808d602-1build4) ...\nSetting up libcamd3:amd64 (1:7.6.1+dfsg-1build1) ...\nSetting up gutenprint-locales (5.3.4.20220624T01008808d602-1build4) ...\nSetting up libopencv-videoio406t64:amd64 (4.6.0+dfsg-13.1ubuntu1) ...\nSetting up libccolamd3:amd64 (1:7.6.1+dfsg-1build1) ...\nSetting up libgutenprint9 (5.3.4.20220624T01008808d602-1build4) ...\nSetting up libgmic1:amd64 (2.9.4-4build11) ...\nSetting up libcholmod5:amd64 (1:7.6.1+dfsg-1build1) ...\nSetting up libumfpack6:amd64 (1:7.6.1+dfsg-1build1) ...\nSetting up libgutenprintui2-2 (5.3.4.20220624T01008808d602-1build4) ...\nSetting up libgegl-0.4-0t64:amd64 (1:0.4.48-2.4build2) ...\nSetting up libgimp2.0t64:amd64 (2.10.36-3ubuntu0.24.04.1) ...\nSetting up gimp (2.10.36-3ubuntu0.24.04.1) ...\nSetting up gimp-gap (2.6.0+dfsg-7ubuntu3) ...\nSetting up gimp-gmic (2.9.4-4build11) ...\nSetting up gimp-cbmplugs (1.2.2-1.2build2) ...\nSetting up gimp-gutenprint (5.3.4.20220624T01008808d602-1build4) ...\nSetting up gimp-plugin-registry (9.20240404) ...\nSetting up gimp-texturize (2.2-4build2) ...\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.4) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for mailcap (3.70+nmu1ubuntu1) ...\n</code></pre> <p>Also reinstall KDEnlive and Krita, which were also not included in the upgrade:</p> <code>apt install kdenlive krita krita-gmic -y</code> <p>``` console</p> <p>There may be more packages to install later, but so far the above seems to cover what was not automatically preserved during the upgrade process.</p>"},{"location":"blog/2025/05/10/in-place-upgrade-ubuntu-2204-to-2404/#apt-install-kdenlive-krita-krita-gmic-y","title":"apt install kdenlive krita krita-gmic -y","text":"<p>Reading package lists... Done Building dependency tree... Done Reading state information... Done krita is already the newest version (1:5.2.2+dfsg-2build8). krita set to manually installed. The following packages were automatically installed and are no longer required:   acpi-support acpid agordejo blender-data cyclist digikam-data encfs ffmpeg2theora fonts-kacst   fonts-kacst-one fonts-khmeros-core fonts-lao fonts-lklug-sinhala fonts-sil-abyssinica fonts-sil-padauk   fonts-thai-tlwg fonts-tibetan-machine fonts-tlwg-garuda fonts-tlwg-garuda-ttf fonts-tlwg-kinnari   fonts-tlwg-kinnari-ttf fonts-tlwg-laksaman fonts-tlwg-laksaman-ttf fonts-tlwg-loma fonts-tlwg-loma-ttf   fonts-tlwg-mono fonts-tlwg-mono-ttf fonts-tlwg-norasi fonts-tlwg-norasi-ttf fonts-tlwg-purisa   fonts-tlwg-purisa-ttf fonts-tlwg-sawasdee fonts-tlwg-sawasdee-ttf fonts-tlwg-typewriter   fonts-tlwg-typewriter-ttf fonts-tlwg-typist fonts-tlwg-typist-ttf fonts-tlwg-typo fonts-tlwg-typo-ttf   fonts-tlwg-umpush fonts-tlwg-umpush-ttf fonts-tlwg-waree fonts-tlwg-waree-ttf foo-yc20 freeglut3 gamin   gcc-12-base:i386 haveged irqbalance kross ksystemlog lib2geom1.1.0 libabsl20210324 libamd2 libappimage0   libappstream4 libappstreamqt2 libarmadillo10 libart-2.0-2 libastro1 libatk1.0-data libavcodec58   libavdevice58 libavfilter7 libavformat58 libavif13 libavutil56 libblockdev-crypto2 libblockdev-fs2   libblockdev-loop2 libblockdev-part-err2 libblockdev-part2 libblockdev-swap2 libblockdev-utils2   libblockdev2 libboost-filesystem1.74.0 libboost-iostreams1.74.0 libboost-locale1.74.0 libboost-regex1.74.0   libboost-thread1.74.0 libbpf0 libcamd2 libcapi20-3t64 libcbor0.8 libccolamd2 libcfitsio9   libchamplain-0.12-0 libchamplain-gtk-0.12-0 libcholmod3 libclutter-1.0-0 libclutter-1.0-common   libclutter-gtk-1.0-0 libcodec2-1.0 libcogl-common libcogl-pango20 libcogl-path20 libcogl20 libcolamd2   libcrypto++8t64 libcupsfilters1 libdav1d5 libdcmtk16 libdcmtk17t64 libdns-export1110 libdraco4   libdrm-nouveau2:i386 libembree4-4 libev4t64 libevent-pthreads-2.1-7t64 libfaudio0 libflac++6v5 libflac8   libflac8:i386 libfltk1.1 libfontembed1 libgamin0 libgav1-0 libgcab-1.0-0 libgeos3.10.2 libgit2-1.1   libglade2-0 libgnomecanvas2-0 libgnomecanvas2-common libgps28 libgssdp-1.2-0 libgtkmm-2.4-1t64   libgupnp-1.2-1 libgupnp-igd-1.0-4 libhavege2 libicu70 libicu70:i386 libilmbase25 libisc-export1105   libixml10 libjavascriptcoregtk-4.0-18 libjemalloc2 libkdecorations2private9 libkdynamicwallpaper1   libkf5akonadi-data libkf5akonadicontact-data libkf5akonadicontact5abi1 libkf5akonadicore-bin   libkf5akonadicore5abi2 libkf5akonadiprivate5abi2 libkf5akonadiwidgets5abi1 libkf5baloowidgets-data   libkf5calendarcore5abi2 libkf5contacteditor5 libkf5grantleetheme-data libkf5grantleetheme-plugins   libkf5grantleetheme5 libkf5jsapi5 libkf5krosscore5 libkf5krossui5 libkf5libkleo5abi1 libkf5mime-data   libkf5mime5abi2 libkf5pimtextedit-data libkf5pimtextedit5abi3 libkf5waylandserver5 libkpim5akonadi-data   libkwaylandserver5 libkwinxrenderutils13 liblcms2-2:i386 libldap-2.5-0:i386 libllvm13 libllvm15t64   libllvm15t64:i386 liblog4cplus-2.0.5t64 liblua5.1-0 libmagick++-6.q16-8 libmagickcore-6.q16-6   libmagickcore-6.q16-6-extra libmagickwand-6.q16-6 libmanette-0.2-0 libmarblewidget-qt5-28 libmetis5   libmicrohttpd12t64 libmpdec3 libmujs1 libnetpbm10 libnetplan0 libnfs13 liboggkate1 libokular5core9   libopenal1:i386 libopencolorio1v5 libopencv-calib3d4.5d libopencv-core4.5d libopencv-dnn4.5d   libopencv-features2d4.5d libopencv-flann4.5d libopencv-imgproc4.5d libopencv-ml4.5d   libopencv-objdetect4.5d libopencv-video4.5d libopenexr25 libopenh264-6 libopenvdb10.0t64 libopts25   liborcus-0.17-0 liborcus-parser-0.17-0 libosdcpu3.5.0t64 libosdgpu3.5.0t64 libosmesa6   libparted-fs-resize0t64 libperl5.34 libphonon4qt5-data libplacebo192 libplist3 libpoppler118 libpostproc55   libproj22 libprotobuf-lite23 libprotobuf23 libpython3.10 libpython3.10-dev libpython3.10-minimal   libpython3.10-stdlib libqgpgme7 libqpdf28 libqtav1 libqtavwidgets1 libraw20 libre2-9 libshp2 libshp4   libsmbios-c2 libsnapd-glib1 libsnapd-qt1 libsndio7.0:i386 libsoup-2.4-1:i386 libspnav0 libsquish0   libsrt1.4-gnutls libstb0t64 libstk-4.6.1 libsuitesparseconfig5 libsuperlu5 libswresample3 libswscale5   libtesseract4 libtexlua53 libtexluajit2 libtiff5 libtiff5:i386 libtinyxml2-9 libumfpack5 libunistring2   libunistring2:i386 libupnp13 libvkd3d-shader1 libvkd3d1 libvpx7 libvpx7:i386 libwebkit2gtk-4.0-37   libwebsockets16 libwxbase3.0-0v5 libwxgtk3.0-gtk3-0v5 libx264-163 libxkbregistry0 libxslt1.1:i386   libyaml-cpp0.7 libzxingcore1 linux-headers-5.15.0-138 linux-headers-5.15.0-138-generic   linux-headers-5.15.0-139 linux-headers-5.15.0-139-generic linux-lowlatency-headers-5.15.0-136   linux-lowlatency-headers-5.15.0-138 linuxaudio-new-session-manager marble-plugins marble-qt-data midisnoop   mlocate muon opencv-data p7zip perl-modules-5.34 petri-foo python3-alsaaudio python3-cffi   python3-jack-client python3-lib2to3 python3-netifaces python3-pbr python3-pycparser python3-sip python3.10   python3.10-dev python3.10-minimal qwinff rtmpdump sntp synfig-examples ubuntu-advantage-tools   vkd3d-compiler vocproc xdg-dbus-proxy zita-njbridge Use 'apt autoremove' to remove them. The following NEW packages will be installed:   kdenlive krita-gmic melt 0 upgraded, 3 newly installed, 0 to remove and 0 not upgraded. Need to get 0 B/3,769 kB of archives. After this operation, 11.3 MB of additional disk space will be used. Selecting previously unselected package melt. (Reading database ... 625685 files and directories currently installed.) Preparing to unpack .../melt_7.22.0-1build6_amd64.deb ... Unpacking melt (7.22.0-1build6) ... Selecting previously unselected package kdenlive. Preparing to unpack .../kdenlive_4%3a23.08.5-0ubuntu4_amd64.deb ... Unpacking kdenlive (4:23.08.5-0ubuntu4) ... Selecting previously unselected package krita-gmic. Preparing to unpack .../krita-gmic_2.9.4-4build11_amd64.deb ... Unpacking krita-gmic (2.9.4-4build11) ... Setting up melt (7.22.0-1build6) ... Setting up krita-gmic (2.9.4-4build11) ... Setting up kdenlive (4:23.08.5-0ubuntu4) ... Processing triggers for man-db (2.12.0-4build2) ... Processing triggers for mailcap (3.70+nmu1ubuntu1) ... Processing triggers for desktop-file-utils (0.27-2build1) ... ```</p>"},{"location":"blog/2025/05/11/streaming-gaming-pc-to-the-tv-with-moonlight-and-sunshine/","title":"Streaming gaming PC to the TV with Moonlight and Sunshine","text":"<p>Streaming a Gaming PC straight to an Android TV turned out to be easier than expected; it essentially was much like with Steam Link in the past, and now the WiFi network is a lot faster!</p>"},{"location":"blog/2025/05/11/streaming-gaming-pc-to-the-tv-with-moonlight-and-sunshine/#sunshine","title":"Sunshine","text":"<p>Sunshine  is a self-hosted game stream host for Moonlight; the \"server\" to run on the gaming PC.</p>"},{"location":"blog/2025/05/11/streaming-gaming-pc-to-the-tv-with-moonlight-and-sunshine/#installation","title":"Installation","text":"<p>Installing Sunshing is as simple as downloading the latest (not pre-release) Debian package for Ubuntu: from the releases repository, along with its only dependency (<code>miniupnpc</code>):</p> <pre><code>$ sudo apt install miniupnpc\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following package was automatically installed and is no longer required:\n  nvidia-firmware-550-550.120\nUse 'sudo apt autoremove' to remove it.\nThe following NEW packages will be installed:\n  miniupnpc\n0 upgraded, 1 newly installed, 0 to remove and 5 not upgraded.\nNeed to get 17.0 kB of archives.\nAfter this operation, 67.6 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu noble/universe amd64 miniupnpc amd64 2.2.6-1build2 [17.0 kB]\nFetched 17.0 kB in 0s (101 kB/s)     \nSelecting previously unselected package miniupnpc.\n(Reading database ... 492360 files and directories currently installed.)\nPreparing to unpack .../miniupnpc_2.2.6-1build2_amd64.deb ...\nUnpacking miniupnpc (2.2.6-1build2) ...\nSetting up miniupnpc (2.2.6-1build2) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\n\n$ wget \\\n  https://github.com/LizardByte/Sunshine/releases/download/v2025.122.141614/sunshine-ubuntu-24.04-amd64.deb\n\n$ sudo dpkg -i sunshine-ubuntu-24.04-amd64.deb \nSelecting previously unselected package sunshine.\n(Reading database ... 492370 files and directories currently installed.)\nPreparing to unpack sunshine-ubuntu-24.04-amd64.deb ...\nUnpacking sunshine (2025.122.141614) ...\nSetting up sunshine (2025.122.141614) ...\nNot in an rpm-ostree environment, proceeding with post install steps.\nSetting CAP_SYS_ADMIN capability on Sunshine binary.\n/usr/sbin/setcap cap_sys_admin+p /usr/bin/sunshine-v2025.122.141614\nCAP_SYS_ADMIN capability set on Sunshine binary.\nReloading udev rules.\nUdev rules reloaded successfully.\nProcessing triggers for desktop-file-utils (0.27-2build1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\n</code></pre> <p>There is also the option to install <code>sunshine.AppImage</code>.</p> <p>Either way, a bit of initial setup is required after installation; for X11 capture to work, you may need to disable the capabilities that were set for KMS capture:</p> <pre><code>$ sudo getcap  $(readlink -f $(which sunshine))\n/usr/bin/sunshine-v2025.122.141614 cap_sys_admin=p\n$ sudo setcap -r $(readlink -f $(which sunshine))\n$ sudo getcap  $(readlink -f $(which sunshine))\n\n$ systemctl --user start sunshine\n$ systemctl --user status sunshine\n\u25cf sunshine.service - Self-hosted game stream host for Moonlight\n     Loaded: loaded (/usr/lib/systemd/user/sunshine.service; disabled; preset: enabled)\n     Active: active (running) since Sun 2025-05-11 00:04:25 CEST; 8s ago\n    Process: 3266886 ExecStartPre=/bin/sleep 5 (code=exited, status=0/SUCCESS)\n   Main PID: 3270182 (sunshine)\n      Tasks: 18 (limit: 38298)\n     Memory: 118.1M (peak: 120.3M)\n        CPU: 715ms\n     CGroup: /user.slice/user-1000.slice/user@1000.service/app.slice/sunshine.service\n             \u2514\u25003270182 /usr/bin/sunshine\n\nMay 11 00:04:29 rapture sunshine[3270182]: [2025-05-11 00:04:29.975]: Info:\nMay 11 00:04:29 rapture sunshine[3270182]: [2025-05-11 00:04:29.975]: Info: // Ignore any errors mentioned above, they are not relevant. //\nMay 11 00:04:29 rapture sunshine[3270182]: [2025-05-11 00:04:29.975]: Info:\nMay 11 00:04:29 rapture sunshine[3270182]: [2025-05-11 00:04:29.975]: Info: Found H.264 encoder: h264_nvenc [nvenc]\nMay 11 00:04:29 rapture sunshine[3270182]: [2025-05-11 00:04:29.975]: Info: Found HEVC encoder: hevc_nvenc [nvenc]\nMay 11 00:04:30 rapture sunshine[3270182]: [2025-05-11 00:04:30.107]: Info: Open the Web UI to set your new username and password and getting started\nMay 11 00:04:30 rapture sunshine[3270182]: [2025-05-11 00:04:30.108]: Info: File /home/ponder/.config/sunshine/sunshine_state.json doesn't exist\nMay 11 00:04:30 rapture sunshine[3270182]: [2025-05-11 00:04:30.109]: Info: Adding avahi service rapture\nMay 11 00:04:30 rapture sunshine[3270182]: [2025-05-11 00:04:30.110]: Info: Configuration UI available at [https://localhost:47990]\nMay 11 00:04:31 rapture sunshine[3270182]: [2025-05-11 00:04:31.050]: Info: Avahi service rapture successfully established.\n\n$ systemctl --user enable sunshine\nCreated symlink /home/ponder/.config/systemd/user/xdg-desktop-autostart.target.wants/sunshine.service \u2192 /usr/lib/systemd/user/sunshine.service\n</code></pre>"},{"location":"blog/2025/05/11/streaming-gaming-pc-to-the-tv-with-moonlight-and-sunshine/#configuration","title":"Configuration","text":"<p>Sunshine is configured via the web ui, which is available on https://localhost:47990 by default.</p> <p>This is where clients are authorized when the try to connect (under the PIN tab) and applications are specified so they can be launched from the clients. A few applications are provided by default, including one to launch Steam Big Picture.</p>"},{"location":"blog/2025/05/11/streaming-gaming-pc-to-the-tv-with-moonlight-and-sunshine/#moonlight","title":"Moonlight","text":"<p>Moonlight is the streaming client to connect to the Sunshine server. The client should definitely be not on the same X11/Wayland session, that creates an infinite lopp situation.</p> <p>It can be used on a different Ubuntu PC, e.g. to play games on a light NUC PC one could connect from it (client) to the gaming PC (server):</p> <pre><code>$ snap search moonlight\nName         Version  Publisher   Notes  Summary\nmoonlight    6.1.0    maxiberta\u272a  -      Stream games and other applications from another PC running Sunshine or GeForce Experience\nheads-tails  1.0.13   technolog   -      Heads or Tails Multiplayer Crypto DexGames\n\n$ sudo snap install moonlight\nmoonlight 6.1.0 from Maximiliano Bertacchini (maxiberta\u272a) installed\n</code></pre> <p>For an Android TV, one can simply install the Android app: Moonlight Game Streaming.</p>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/","title":"Tracking progress with Ryot","text":"<p>Sometimes I wish for a centralized, automatically updated and moderately fancy-looking application to keep track of multiple activities; mostly around digital media.</p> <ul> <li>Audiobookshelf is pretty good but     separates podcasts from books and only shows yearly summary at the end of the year.     Audible does not offer even that, and no export options.</li> <li>Jellyfin (and previously     Plex) don't go beyond marking     things as \"done\".    Besides, movies and TV shows are not the kind of videos     I'm intersted in tracking progress with; video lectures are     (where was I with this Inkscape course?).</li> <li>Paper books are very nearly not even a thing anymore, but it would still be nice     to be able to track progress on them, as well as reading e-Books in     Komga.</li> <li>Video games are absurdly difficult to track progress for. Naturally grown from need,     a spreadsheet is works well enough to collect data across multiple platforms, but     it is limited, ugly and increasing slow as the library grows.<ul> <li>Steam shows only total and recent (last     2 weeks) gameplay, and probress is tracked in terms of achievements, not how     close you are to finish the main story. At least there is the option to query the     Steam Web API to periodically fetch gameplay     stats, so they can be kept at a higher resolution (daily, hourly, etc.).</li> <li>Nintendo Switch Parental Control (Android app).     shows only gameplay time per game (and per user) in the current month, after that     it shows only montly summaries. There is no option to export any of this.</li> <li>GOG requires installing their own (Windows-only)      Galaxy 2.0 client and the possiblity of exporting or even seeing your personal     gameplay stats appears to be not even a question.</li> </ul> </li> </ul> <p>Looking around for tracking applications in the awesome directory of awesome-selfhosted, two applications look promising and worth a try: Ryot and Yamtrack.</p>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#ryot","title":"Ryot","text":"<p>Ryot is a self hosted platform for tracking various facets of your life - media, fitness, etc. which seems to include everything that Yamtrack can track, plus other activities outside of media; it is focused in fitness but it could be used for other activities like studying, music practice, workshop time, other hobbies, sleep, etc.</p> <p>It supports Integration with Jellyfin, Plex, Audiobookshelf and many more, OpenID Connect, sending notifications to Discord and Ntfy. While it's not yet ready to easily track everything, it recently added a webhook endpoint that supports generic json which can be used to build integrations with other services that Ryot does not support natively by importing Generic Json generated from any non-supported systems (e.g. Steam API).</p>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#ryot-deployment","title":"Ryot deployment","text":"<p>Note</p> <p>This deployment has been updated to version 10 of Ryot; migration from v8 to v9 failed persistently. Version 10 requires the <code>MOVIES_AND_SHOWS_TMDB_ACCESS_TOKEN</code> to be a personal API Read Access Token from https://www.themoviedb.org/settings/api</p> <p>Ryot documentation starts off with an Installation based on <code>docker-compose</code>, on which the following basic deployment is based:</p> Basic Ryot deployment: <code>ryot.yaml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: ryot\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: postgres-pv\n  labels:\n    type: local\n  namespace: ryot\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 3Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/k8s/ryot/postgres\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pv-claim\n  namespace: ryot\nspec:\n  storageClassName: manual\n  volumeName: postgres-pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ryot\n  labels:\n    app: postgres\n  name: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      hostname: postgres\n      containers:\n      - image: postgres:16-alpine\n        env:\n        - name: \"POSTGRES_PASSWORD\"\n          value: \"28___________________________________ba\"\n        name: postgres\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgres-data\n      volumes:\n      - name: postgres-data\n        persistentVolumeClaim:\n          claimName: postgres-pv-claim\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: postgres\n  name: postgres-svc\n  namespace: ryot\nspec:\n  ports:\n  - port: 5432\n    protocol: TCP\n    targetPort: 5432\n    nodePort: 30543\n  selector:\n    app: postgres\n  type: NodePort\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: ryot\n  labels:\n    app: ryot\n  name: ryot\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ryot\n  template:\n    metadata:\n      labels:\n        app: ryot\n    spec:\n      containers:\n      - image: ghcr.io/ignisda/ryot:v10\n        env:\n        - name: \"DATABASE_URL\"\n          value: \"postgres://postgres:28___________________________________ba0@postgres-svc:5432/postgres\"\n        - name: \"MOVIES_AND_SHOWS_TMDB_ACCESS_TOKEN\"\n          value: \"ey_________________________________________________________________V4\"\n        - name: \"SERVER_ADMIN_ACCESS_TOKEN\"\n          value: \"28____________________________________a0\"\n        - name: \"TZ\"\n          value: \"Europe/Amsterdam\"\n        name: ryot\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: ryot\n  name: ryot-svc\n  namespace: ryot\nspec:\n  ports:\n  - port: 8000\n    protocol: TCP\n    targetPort: 8000\n    nodePort: 30380\n  selector:\n    app: ryot\n  type: NodePort\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ryot-ingress\n  namespace: ryot\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: ryot.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: ryot-svc\n                port:\n                  number: 8000\n  tls:\n    - secretName: tls-secret-cloudflare\n      hosts:\n        - ryot.very-very-dark-gray.top\n</code></pre> Ryot can be deployed with debug logs and full backtrace when needed. <p>To enable debug log with full backtrace, re-deploy with these environment variables:</p> <pre><code>        env:\n        - name: \"RUST_BACKTRACE\"\n          value: \"full\"\n        - name: \"RUST_LOG\"\n          value: \"ryot=debug\"\n</code></pre> <p>Before deploying the application, create a dedicated system user to own the local storage for the Postgres database:</p> <pre><code>root@octavo ~ # groupadd ryot -g 120\nroot@octavo ~ # useradd  ryot -u 120 -g 120 -s /usr/sbin/nologin\nroot@octavo ~ # mkdir -p /home/k8s/ryot/postgres\nroot@octavo ~ # chown -R ryot:ryot /home/k8s/ryot\nroot@octavo ~ # ls -hal /home/k8s/ryot\ntotal 0\ndrwxr-xr-x 1 ryot ryot  16 May 17 14:13 .\ndrwxr-xr-x 1 root root 330 May 17 14:13 ..\ndrwxr-xr-x 1 ryot ryot   0 May 17 14:13 postgres\n</code></pre> <p>Then deploy the application:</p> <pre><code>$ kubectl apply -f ryot.yaml\nnamespace/ryot created\npersistentvolume/postgres-pv created\npersistentvolumeclaim/postgres-pv-claim created\ndeployment.apps/postgres created\nservice/postgres-svc created\ndeployment.apps/ryot created\nservice/ryot-svc created\ningress.networking.k8s.io/ryot-ingress created\n\n$ kubectl get all -n ryot\nNAME                            READY   STATUS    RESTARTS   AGE\npod/postgres-5f649d875c-82fh4   1/1     Running   0          3s\npod/ryot-55c5845667-gplxd       1/1     Running   0          3s\n\nNAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\nservice/postgres-svc   NodePort   10.96.122.54   &lt;none&gt;        5432:30543/TCP   3s\nservice/ryot-svc       NodePort   10.98.120.72   &lt;none&gt;        8000:30380/TCP   3s\n\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/postgres   1/1     1            1           3s\ndeployment.apps/ryot       1/1     1            1           3s\n\nNAME                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/postgres-5f649d875c   1         1         1       3s\nreplicaset.apps/ryot-55c5845667       1         1         1       3s\n\n$ kubectl get ingress -n ryot\nNAME           CLASS   HOSTS                          ADDRESS         PORTS   AGE\nryot-ingress   nginx   ryot.very-very-dark-gray.top   192.168.0.171   80      59s\n</code></pre> <p>After a couple of minutes the application will be available at https://ryot.very-very-dark-gray.top/</p> <p>Warning</p> <p>By default, Ryot allows anyone to create a new account. Gating access behind Cloudflare Access is advisable, at least initially, to prevent strangers from creating accounts if they accidentally find the application.</p>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#ryot-setup","title":"Ryot setup","text":"<p>First, create a user to gain admin access to the application. Additional users can be created but will not have the Admin role, only the first user can add it.</p>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#import","title":"Import","text":"<p>The first step each user should probaly do is to import their progress so far in the supported systems. This is done under Settings &gt; Imports and Exports &gt; Import data using the relevant importers.</p>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#audiobookshelf","title":"Audiobookshelf","text":"<p>The Audiobookshelf importer supports importing all media that have a valid Audible ID or iTunes ID or ISBN. Unfortunately this means those audiobooks not available in Audible or iTunes, e.g. those from LibriVox, won't be imported.</p> <p>This will only import media that are already finished, neither books in progress nor  those that have not been started yet. Those will be added later, as progress is made, via the Audiobookshelf integration.</p> <ul> <li>Select a source: Audiobookshelf</li> <li>Instance URL: https://audiobookshelf.very-very-dark-gray.top or whatever the   ingress is for Audiobookshelf</li> <li>API Key: Create an API Key   to act on behalf of the user to track progress for. This is no longer the same as   the API token for the user, as described in the Audiobookshelf   authentication docs, which is now   used only in the Audiobookshelf Integration.</li> </ul> <p>The import will take a little while; in my case it took about 7 minutes to import 212 books.</p> <p></p> <p>Ryot will not import audiobooks that don't have ISBN or ASIN, a list of such titles can be found checking the result from the importer:</p> <p></p> <p>This can be mitigated by adding ISBN or ASIN to those titles in Audiobookshelf, e.g. libro.fm and other stores may list ISBN and/or ASIN for audiobooks purchased offline. Audiobooks from librivox.org appear to have no ISBN or ASIN.</p>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#jellyfin","title":"Jellyfin","text":"<p>The Jellyfin importer can import watched (i.e. not the entire) Movies and Shows libraries from Jellyfin, and it's very simple to setup:</p> <ul> <li>Select a source: Jellyfin</li> <li>Instance URL: https://jellyfin.very-very-dark-gray.top (without trailing <code>/</code>)</li> <li>or whatever the ingress is for     Jellyfin</li> <li>Username: the user to import progress for</li> <li>Password: their password</li> </ul> <p></p> Do not add a trailing <code>/</code> to the Instance URL. <p>Doing so, the import always fails immediately and seems to stay in the still running state, so the UI won't allow even looking at the logs. Inspecing the pod's logs in Kubernetes won't show much:</p> <pre><code>$ klogs ryot ryot\n[frontend] POST /settings/imports-and-exports.data?intent=deployImport 200 - - 14.458 ms\n[frontend] GET /settings/imports-and-exports.data 200 - - 36.602 ms\n[backend] \n[backend] thread 'main' panicked at /home/runner/work/ryot/ryot/crates/utils/external/src/lib.rs:93:14:\n[backend] called `Result::unwrap()` on an `Err` value: reqwest::Error { kind: Decode, source: Error(\"EOF while parsing a value\", line: 1, column: 0) }\n</code></pre> The Jellyfin API does allow fetching entirely libraries. <p>The Jellyfin API is actually reachable and does return 611 items, of which ~90% do have <code>ProviderIds</code>, which are required by Ryot in order to import items:</p> <pre><code>$ kubectl exec -it ryot-59cdcdc56d-k7bvn -n ryot -- \\\n  curl -H \"X-Emby-Token: a7____________________________ee\" \\\n  \"http://jellyfin.very-very-dark-gray.top/Items?IncludeItemTypes=Movie,Series&amp;Recursive=true&amp;Fields=ProviderIds\" \\\n  &gt; /tmp/items.json\n\n$ jq -r \".TotalRecordCount\" /tmp/items.json \n611\n\n$ jq -r \".Items[].ProviderIds | length\" /tmp/items.json | sort | uniq -c\n    60 0\n      6 1\n    196 2\n    306 3\n    43 4\n</code></pre>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#plex","title":"Plex","text":"<p>The Plex importer can import watched movies and shows from Plex but it may actually require having an active self-hosted Plex server, which is not running in the new server (octavo).</p> <p>No longer having my own self-hosted Plex Media Server running, it seems the process to obtain a <code>Plex-Token</code> as described here no longer works, and at this point I'm past done with Plex anyway.</p>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#integrations","title":"Integrations","text":""},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#audiobookshelf_1","title":"Audiobookshelf","text":"<p>The Audiobookshelf integration must be setup to keep syncing progress made on audiobooks. The setup is essentially the same as for the Audiobookshelf importer:</p> <ul> <li>Instance URL: https://audiobookshelf.very-very-dark-gray.top or whatever the   ingress is for Audiobookshelf</li> <li>API Key: the API token for the user, as described in the Audiobookshelf   authentication docs</li> </ul>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#jellyfin_1","title":"Jellyfin","text":"<p>The Jellyfin Sink integration automatically adds Jellyfin movies and shows (when they are played in Jellyfin). It only works for media that has a valid TMDb ID attached to their metadata. Create the integration and click on its eye icon to reveal its URL; this will be used when creating the webhook in Jellyfin.</p> <p>This integration requires the unofficial webhook plugin to be installed and active in Jellyfin, whiich a few steps in Jellyfin:</p> <ol> <li>Under Dashboard &gt; Plugins go to Manage Repositories</li> <li>Use the + New Repository button to add a new repository pointing to     https://raw.githubusercontent.com/shemanaev/jellyfin-plugin-repo/master/manifest.json</li> <li>Go back to Plugins and select the Available filter, then Search for <code>web</code></li> <li>Install the unofficial Webhooks plugin (not the official Webhook)</li> <li>Restart Jellyfin by restarting the deployment:     <pre><code>$ kubectl scale -n media-center deployment jellyfin --replicas=0\n$ kubectl scale -n media-center deployment jellyfin --replicas=1\n</code></pre></li> <li>Enable the Webhooks plugin once Jellyfin has restarted.</li> <li>Go to the newly available section Dashboard &gt; Webhooks and click on Add,     use the URL created by Ryot for the Jellyfin Sink integration and set the other      values as explained here</li> </ol> <p>The Jellyfin Push integration does not seem as interested, since it only marks items as watched in Jellyfin after they are marked as watched in Ryot, but the import won't import items that have not been  watched in Jellyfin in the first place.</p>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#komga","title":"Komga","text":"<p>There is no importer option for Komga so instead the Komga integration can be setup with</p> <ul> <li>Base URL: https://jellyfin.very-very-dark-gray.top/ or whatever the   ingress is for Jellyfin</li> <li>Username: the user to import progress for</li> <li>Password: their password</li> <li>Provider: Anlist</li> </ul> <p>This integration will not have anything visible effect until books are interacted with in Komga; only then will the integration add these under Book.</p>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#generic-json","title":"Generic JSON","text":"<p>For everything else, \"Generic Json\" can be used to import and sync progress from any other sources that Ryot. The format of the JSON file should be <code>CompleteExport</code> as described in the documentation for exporting.</p> <p>This should made it possible to import and sync video games, at least from Steam. However, having to implement this from scratch is not particularly appealing for now.</p>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#ryot-impressions","title":"Ryot impressions","text":"<p>Without spending many hours on it, all I seem to get out of Ryot is but a limited subset of what Audiobookshelf shows. While the app looks promising and pleasant to navigate, the funcionality achieved is too limited to justify regular use.</p>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#yamtrack","title":"Yamtrack","text":"<p>Yamtrack is a self hosted media tracker for movies, tv shows, anime and manga but, more interestingly to me, also books and video games. It even has integration with Jellyfin, to automatically track new media watched, which is nice, but what I'd really love to see is integration with Audiobookshelf and Komga, for books, and Steam for games.</p> <p>There is perhaps enough information available about the Yamtrack CSV import format that it may be not too hard to hack something together to import listening history and play time history using the available APIs:</p> <ul> <li>Audiobookshelf API     exposes listening sessions and libraries.</li> <li>Komga REST API     exposes progression and metadata for each book.</li> <li>Steam Web API     exposes recent (2-weeks) and total (forever) play time,     and the list of owned games. These can be used to track     progress and purchases of games.</li> </ul> <p>This may be worth the investment if Ryot turns out not to be the best option in the end. For now, lets just say Yamtrack is even easier to deploy, based on its own <code>docker-compose.yml</code></p> Basic Jellyfin deployment: <code>yamtrack.yaml</code> yamtrack.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: yamtrack\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: redis-pv\n  labels:\n    type: local\n  namespace: yamtrack\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 3Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/k8s/yamtrack/redis\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: redis-pv-claim\n  namespace: yamtrack\nspec:\n  storageClassName: manual\n  volumeName: redis-pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: yamtrack\n  labels:\n    app: redis\n  name: redis\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      hostname: redis\n      containers:\n      - image: redis:7-alpine\n        name: redis\n        volumeMounts:\n        - mountPath: /data\n          name: redis-data\n      volumes:\n      - name: redis-data\n        persistentVolumeClaim:\n          claimName: redis-pv-claim\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: redis\n  name: redis-svc\n  namespace: yamtrack\nspec:\n  ports:\n  - port: 6379\n    protocol: TCP\n    targetPort: 6379\n    nodePort: 30379\n  selector:\n    app: redis\n  type: NodePort\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: yamtrack-pv\n  labels:\n    type: local\n  namespace: yamtrack\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 3Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/k8s/yamtrack/db\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: yamtrack-pv-claim\n  namespace: yamtrack\nspec:\n  storageClassName: manual\n  volumeName: yamtrack-pv\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: yamtrack\n  labels:\n    app: yamtrack\n  name: yamtrack\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yamtrack\n  template:\n    metadata:\n      labels:\n        app: yamtrack\n    spec:\n      containers:\n      - image: ghcr.io/fuzzygrim/yamtrack\n        env:\n        - name: \"SECRET\"\n          value: \"____________________\"\n        - name: \"REDIS_URL\"\n          value: \"redis://redis-svc:6379\"\n        name: yamtrack\n        volumeMounts:\n          - name: yamtrack-data\n            mountPath: /yamtrack/db\n      volumes:\n      - name: yamtrack-data\n        persistentVolumeClaim:\n          claimName: yamtrack-pv-claim\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: yamtrack\n  name: yamtrack-svc\n  namespace: yamtrack\nspec:\n  ports:\n  - port: 8000\n    protocol: TCP\n    targetPort: 8000\n    nodePort: 30380\n  selector:\n    app: yamtrack\n  type: NodePort\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: yamtrack-ingress\n  namespace: yamtrack\n  annotations:\n    acme.cert-manager.io/http01-edit-in-place: \"true\"\n    cert-manager.io/issue-temporary-certificate: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: yamtrack.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: yamtrack-svc\n                port:\n                  number: 8000\n  tls:\n    - secretName: tls-secret-cloudflare\n      hosts:\n        - yamtrack.very-very-dark-gray.top\n</code></pre> <p>Prepare a local user and directory in the similar to the one for Ryot:</p> <pre><code>root@octavo ~ # groupadd yamtrack -g 120\nroot@octavo ~ # useradd  yamtrack -u 120 -g 120 -s /usr/sbin/nologin\nroot@octavo ~ # mkdir /home/k8s/yamtrack/redis /home/k8s/yamtrack/db\nroot@octavo ~ # chown -R yamtrack:yamtrack /home/k8s/yamtrack\nroot@octavo ~ # ls -hal /home/k8s/yamtrack\ntotal 0\ndrwxr-xr-x 1 yamtrack yamtrack  14 May 15 23:06 .\ndrwxr-xr-x 1 root     root     322 May 15 23:01 ..\ndrwxr-xr-x 1 yamtrack yamtrack   0 May 15 23:06 db\ndrwxr-xr-x 1 yamtrack yamtrack   0 May 15 23:06 redis\n</code></pre> <p>Then deploy the application:</p> <pre><code>$ kubectl apply -f yamtrack.yaml \nnamespace/yamtrack created\npersistentvolume/redis-pv created\npersistentvolumeclaim/redis-pv-claim created\ndeployment.apps/redis created\nservice/redis-svc created\npersistentvolume/yamtrack-pv created\npersistentvolumeclaim/yamtrack-pv-claim created\ndeployment.apps/yamtrack created\nservice/yamtrack-svc created\ningress.networking.k8s.io/yamtrack-ingress created\n\n$ kubectl get all -n yamtrack\nNAME                            READY   STATUS    RESTARTS   AGE\npod/redis-549b9c9b6f-2xq94      1/1     Running   0          98s\npod/yamtrack-668755cf84-xbt9v   1/1     Running   0          97s\n\nNAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nservice/redis-svc      NodePort   10.96.239.219   &lt;none&gt;        6379:30379/TCP   98s\nservice/yamtrack-svc   NodePort   10.111.29.62    &lt;none&gt;        8000:30380/TCP   98s\n\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/redis      1/1     1            1           98s\ndeployment.apps/yamtrack   1/1     1            1           98s\n\nNAME                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/redis-549b9c9b6f      1         1         1       98s\nreplicaset.apps/yamtrack-668755cf84   1         1         1       98s\n\n$ kubectl get ingress -n yamtrack\nNAME               CLASS   HOSTS                              ADDRESS         PORTS   AGE\nyamtrack-ingress   nginx   yamtrack.very-very-dark-gray.top   192.168.0.171   80      36s\n</code></pre> <p>After a couple of minutes the application will be available at https://yamtrack.very-very-dark-gray.top; otherwise inspect the logs to debug.</p> <code>$ klogs yamtrack yamtrack</code> <pre><code>$ klogs yamtrack yamtrack\nOperations to perform:\n  Apply all migrations: account, admin, app, auth, contenttypes, db, django_celery_beat, django_celery_results, events, lists, mfa, sessions, socialaccount, users\nRunning migrations:\n  Applying contenttypes.0001_initial... OK\n  Applying contenttypes.0002_remove_content_type_name... OK\n  Applying auth.0001_initial... OK\n  Applying auth.0002_alter_permission_name_max_length... OK\n  Applying auth.0003_alter_user_email_max_length... OK\n  Applying auth.0004_alter_user_username_opts... OK\n  Applying auth.0005_alter_user_last_login_null... OK\n  Applying auth.0006_require_contenttypes_0002... OK\n  Applying auth.0007_alter_validators_add_error_messages... OK\n  Applying auth.0008_alter_user_username_max_length... OK\n  Applying auth.0009_alter_user_last_name_max_length... OK\n  Applying auth.0010_alter_group_name_max_length... OK\n  Applying auth.0011_update_proxy_permissions... OK\n  Applying auth.0012_alter_user_first_name_max_length... OK\n  Applying users.0001_squashed_0008_user_game_layout_alter_user_last_search_type... OK\n  Applying account.0001_initial... OK\n  Applying account.0002_email_max_length... OK\n  Applying account.0003_alter_emailaddress_create_unique_verified_email... OK\n  Applying account.0004_alter_emailaddress_drop_unique_email... OK\n  Applying account.0005_emailaddress_idx_upper_email... OK\n  Applying account.0006_emailaddress_lower... OK\n  Applying account.0007_emailaddress_idx_email... OK\n  Applying account.0008_emailaddress_unique_primary_email_fixup... OK\n  Applying account.0009_emailaddress_unique_primary_email... OK\n  Applying admin.0001_initial... OK\n  Applying admin.0002_logentry_remove_auto_add... OK\n  Applying admin.0003_logentry_add_action_flag_choices... OK\n  Applying app.0001_squashed_0014_historicalanime_historicalepisode_historicalgame_and_more... OK\n  Applying app.0015_customlist_listitem_squashed_0021_alter_item_media_type... OK\n  Applying app.0022_item_source_squashed_0037_alter_item_media_type... OK\n  Applying app.0038_comic_historicalcomic_and_more... OK\n  Applying app.0039_anime_progress_changed_basicmedia_progress_changed_and_more... OK\n  Applying db.0001_initial... OK\n  Applying django_celery_beat.0001_initial... OK\n  Applying django_celery_beat.0002_auto_20161118_0346... OK\n  Applying django_celery_beat.0003_auto_20161209_0049... OK\n  Applying django_celery_beat.0004_auto_20170221_0000... OK\n  Applying django_celery_beat.0005_add_solarschedule_events_choices... OK\n  Applying django_celery_beat.0006_auto_20180322_0932... OK\n  Applying django_celery_beat.0007_auto_20180521_0826... OK\n  Applying django_celery_beat.0008_auto_20180914_1922... OK\n  Applying django_celery_beat.0006_auto_20180210_1226... OK\n  Applying django_celery_beat.0006_periodictask_priority... OK\n  Applying django_celery_beat.0009_periodictask_headers... OK\n  Applying django_celery_beat.0010_auto_20190429_0326... OK\n  Applying django_celery_beat.0011_auto_20190508_0153... OK\n  Applying django_celery_beat.0012_periodictask_expire_seconds... OK\n  Applying django_celery_beat.0013_auto_20200609_0727... OK\n  Applying django_celery_beat.0014_remove_clockedschedule_enabled... OK\n  Applying django_celery_beat.0015_edit_solarschedule_events_choices... OK\n  Applying django_celery_beat.0016_alter_crontabschedule_timezone... OK\n  Applying django_celery_beat.0017_alter_crontabschedule_month_of_year... OK\n  Applying django_celery_beat.0018_improve_crontab_helptext... OK\n  Applying django_celery_beat.0019_alter_periodictasks_options... OK\n  Applying django_celery_results.0001_initial... OK\n  Applying django_celery_results.0002_add_task_name_args_kwargs... OK\n  Applying django_celery_results.0003_auto_20181106_1101... OK\n  Applying django_celery_results.0004_auto_20190516_0412... OK\n  Applying django_celery_results.0005_taskresult_worker... OK\n  Applying django_celery_results.0006_taskresult_date_created... OK\n  Applying django_celery_results.0007_remove_taskresult_hidden... OK\n  Applying django_celery_results.0008_chordcounter... OK\n  Applying django_celery_results.0009_groupresult... OK\n  Applying django_celery_results.0010_remove_duplicate_indices... OK\n  Applying django_celery_results.0011_taskresult_periodic_task_name... OK\n  Applying django_celery_results.0012_taskresult_date_started... OK\n  Applying django_celery_results.0013_taskresult_django_cele_periodi_1993cf_idx... OK\n  Applying django_celery_results.0014_alter_taskresult_status... OK\n  Applying events.0001_initial... OK\n  Applying events.0002_alter_event_unique_together... OK\n  Applying events.0003_alter_event_unique_together_and_more... OK\n  Applying events.0004_fix_anime_episode_numbers... OK\n  Applying events.0005_alter_event_date... OK\n  Applying events.0006_alter_event_options_rename_date_event_datetime... OK\n  Applying events.0007_event_notification_sent... OK\n  Applying events.0008_delete_tv_events... OK\n  Applying events.0009_fix_movie_episode_number... OK\n  Applying events.0010_alter_event_options... OK\n  Applying events.0011_remove_event_unique_item_episode_and_more... OK\n  Applying events.0012_remove_event_unique_item_episode_and_more... OK\n  Applying events.0013_delete_single_anime_events... OK\n  Applying lists.0001_initial... OK\n  Applying lists.0002_alter_customlistitem_item_alter_customlist_items_and_more... OK\n  Applying lists.0003_alter_customlist_unique_together_and_more... OK\n  Applying mfa.0001_initial... OK\n  Applying mfa.0002_authenticator_timestamps... OK\n  Applying mfa.0003_authenticator_type_uniq... OK\n  Applying sessions.0001_initial... OK\n  Applying socialaccount.0001_initial... OK\n  Applying socialaccount.0002_token_max_lengths... OK\n  Applying socialaccount.0003_extra_data_default_dict... OK\n  Applying socialaccount.0004_app_provider_id_settings... OK\n  Applying socialaccount.0005_socialtoken_nullable_app... OK\n  Applying socialaccount.0006_alter_socialaccount_extra_data... OK\n  Applying users.0009_alter_user_options_squashed_0022_alter_user_last_search_type... OK\n  Applying users.0023_user_home_sort_user_home_sort_valid... OK\n  Applying users.0024_remove_user_lists_sort_valid_alter_user_lists_sort_and_more... OK\n  Applying users.0025_remove_user_last_search_type_valid_and_more... OK\n  Applying users.0026_user_anime_sort_user_book_sort_user_game_sort_and_more... OK\n  Applying users.0027_user_list_detail_sort_user_list_detail_sort_valid... OK\n  Applying users.0028_user_anime_status_user_book_status_user_game_status_and_more... OK\n  Applying users.0029_user_notification_urls... OK\n  Applying users.0030_user_notification_excluded_items... OK\n  Applying users.0031_remove_user_last_search_type_valid_and_more... OK\n  Applying users.0032_alter_user_token_and_generate_missing_user_tokens... OK\n  Applying users.0033_user_daily_digest_enabled_and_more... OK\nusermod: no changes\n2025-05-15 21:13:58,156 INFO Set uid to user 0 succeeded\n2025-05-15 21:13:58,168 INFO supervisord started with pid 1\n2025-05-15 21:13:59,170 INFO spawned: 'nginx' with pid 16\n2025-05-15 21:13:59,172 INFO spawned: 'gunicorn' with pid 17\n2025-05-15 21:13:59,175 INFO spawned: 'celery' with pid 18\n2025-05-15 21:13:59,176 INFO spawned: 'celery-beat' with pid 19\ncelery beat v5.5.2 (immunity) is starting.\n[2025-05-15 21:13:59 +0000] [17] [INFO] Starting gunicorn 23.0.0\n[2025-05-15 21:13:59 +0000] [17] [INFO] Listening at: http://127.0.0.1:8001 (17)\n[2025-05-15 21:13:59 +0000] [17] [INFO] Using worker: sync\n[2025-05-15 21:13:59 +0000] [37] [INFO] Booting worker with pid: 37\n__    -    ... __   -        _\nLocalTime -&gt; 2025-05-15 21:14:00\nConfiguration -&gt;\n    . broker -&gt; redis://redis-svc:6379//\n    . loader -&gt; celery.loaders.app.AppLoader\n    . scheduler -&gt; django_celery_beat.schedulers.DatabaseScheduler\n\n    . logfile -&gt; [stderr]@%INFO\n    . maxinterval -&gt; 5.00 seconds (5s)\n2025-05-15 21:14:00,475 INFO success: nginx entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2025-05-15 21:14:00,475 INFO success: gunicorn entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2025-05-15 21:14:00,475 INFO success: celery entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2025-05-15 21:14:00,475 INFO success: celery-beat entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n[2025-05-15 21:14:00 +0000] [19] [INFO] beat: Starting...\n[2025-05-15 21:14:00 +0000] [19] [INFO] DatabaseScheduler: Schedule changed.\n\n-------------- celery@yamtrack-668755cf84-xbt9v v5.5.2 (immunity)\n--- ***** ----- \n-- ******* ---- Linux-6.8.0-58-generic-x86_64-with 2025-05-15 21:14:00\n- *** --- * --- \n- ** ---------- [config]\n- ** ---------- .&gt; app:         yamtrack:0x7aefc7145df0\n- ** ---------- .&gt; transport:   redis://redis-svc:6379//\n- ** ---------- .&gt; results:     \n- *** --- * --- .&gt; concurrency: 1 (prefork)\n-- ******* ---- .&gt; task events: OFF (enable -E to monitor tasks in this worker)\n--- ***** ----- \n-------------- [queues]\n                .&gt; celery           exchange=celery(direct) key=celery\n\n\n[tasks]\n  . Import from AniList\n  . Import from Kitsu\n  . Import from MyAnimeList\n  . Import from SIMKL\n  . Import from Trakt\n  . Import from Yamtrack\n  . Reload calendar\n  . Send daily digest\n  . Send release notifications\n\n[2025-05-15 21:14:01 +0000] [18] [INFO] Connected to redis://redis-svc:6379//\n[2025-05-15 21:14:01 +0000] [18] [INFO] celery@yamtrack-668755cf84-xbt9v ready.\n</code></pre> Deploy Yamtrack without <code>securityContext</code> or it will fail: <pre><code>$ kubectl get all -n yamtrack\nNAME                            READY   STATUS    RESTARTS     AGE\npod/redis-6768c658df-ncnrk      1/1     Running   0            19s\npod/yamtrack-6d955f556b-hq5df   0/1     Error     1 (4s ago)   19s\n\nNAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nservice/redis-svc      NodePort   10.111.45.57    &lt;none&gt;        6379:30379/TCP   20s\nservice/yamtrack-svc   NodePort   10.105.251.86   &lt;none&gt;        8000:30380/TCP   20s\n\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/redis      1/1     1            1           20s\ndeployment.apps/yamtrack   0/1     1            0           20s\n\nNAME                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/redis-6768c658df      1         1         1       20s\nreplicaset.apps/yamtrack-6d955f556b   1         1         0       20s\n\n$ klogs yamtrack yamtrack\nOperations to perform:\n  Apply all migrations: account, admin, app, auth, contenttypes, db, django_celery_beat, django_celery_results, events, lists, mfa, sessions, socialaccount, users\nRunning migrations:\n  No migrations to apply.\ngroupmod: /etc/group.9: Permission denied\ngroupmod: cannot lock /etc/group; try again later.\n</code></pre>"},{"location":"blog/2025/05/18/tracking-progress-with-ryot/#videogame-progress-trackers","title":"Videogame progress trackers","text":"<p>Importing and tracking progress in video games can only be done manually, even though some gaming platforms provide APIs that some Home Assistant integrations use. [FEATURE REQUEST] track video games process on game platforms mentions of a few of those, and there is also the Steam API that can used to import libraries and track progress.</p> <p>There doesn't seem to be any self-hosted option to keep track of multi-platform video game libraries and game progress. The closest thing to it is the open-source project Playnite, sadly only available for Windows and not possible to run using Wine, PlayOnLinux or similar tools. The good news is, Playnite Will Eventually Make Its Way to Linux; the bad news is, not any time soon.</p> <p>In the meantime, there are several hosted services to do the above, although at most they can importing games from Steam or GOG and few other platforms, but not including itch.io, Nintendo or most other platforms. A quick review with the following show that at beast very basic functionality may be used without some sort of paid subscription:</p> <ul> <li>HowLongToBeat can be used for free. It is   kinda possible to track games in different states but maintaining a wishlist is not   a native, well-supported feature.</li> <li>Backloggd can be used for free (with ads) and support keeping   a wishlist alongside the collection, but the latter can only be seen in 3 different   sections (Played, Playing, Backlog) and sorting options are not remembered.</li> <li>INFINITE BACKLOG does support more detailled progress   tracking, plus importing games and progress from Steam, GOG and a few more. Both the   collection and wishlist can be sorted by release date and filtered by multiple fields.   Access to some features is gated behind Patreon, but the tiers are low enough that it   could justify a $1/mo. subscription.</li> <li>Keep Track of My Games gates most features behind   their own Fair Use Price.</li> <li>GG seems unfinished and inactive, with   elite features gated behind a $5/mo. subscriptions   (apparently on their own platform) and no updates since September 2022.</li> </ul> <p>Of all the above, INFINITE BACKLOG seems like the only potential winner.</p>"},{"location":"blog/2025/07/27/playing-steam-games-in-the-browser-with-self-hosted-headless-steam-service/","title":"Playing Steam games in the browser with self-hosted Headless Steam Service","text":"<p>Headless Steam is like a self-hosted GeForce NOW, which can be useful to play games in a browser while away on holidays. Although mainly intended to play Steam games, it also supports EmeDeck, Heroic and Lutris, all easy to install via Flatpak, and supports Intel GPU which is already setup for Jellyfin on Kubernetes with Intel GPU and not actually getting a lot of use; running games would probably be a better use of that Intel UHD GPU.</p>"},{"location":"blog/2025/07/27/playing-steam-games-in-the-browser-with-self-hosted-headless-steam-service/#installation","title":"Installation","text":"<p>Installing and running the Headless Steam service only requires a regular user to run steam as (e.g. <code>ponder</code> with UID 1000) and a few persistent directories:</p> <pre><code># mkdir -p /home/k8s/steam-headless/{home,.X11-unix,pulse}\n# chown -R 1000:1000 /home/k8s/steam-headless\n</code></pre>"},{"location":"blog/2025/07/27/playing-steam-games-in-the-browser-with-self-hosted-headless-steam-service/#headless-steam-deployment","title":"Headless Steam deployment","text":"<p>The following deployment is based on the provided docker-steam-headless/docs/k8s-files/statefulset.yaml, with the addition of a <code>Service</code> and an <code>Ingress</code> for remote access:</p> Steam Headless deployment: <code>steam-headless.yaml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: steam-headless\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: init-d-scripts\n  namespace: steam-headless\ndata:\n  add-vnc-passwd.sh: |\n    #!/bin/bash\n\n    # Define the file path\n    FILE_PATH=\"/usr/bin/start-x11vnc.sh\"\n\n    # Define the password part to insert\n    PASSWORD_PART=\"-passwd $USER_PASSWORD\"\n\n    # Check if the -passwd option is already in the script\n    if grep -q \"\\-passwd\" \"$FILE_PATH\"; then\n        echo \"VNC Password flag already exists.\"\n    else\n        echo \"Adding VNC password flag...\"\n        # Use sed to insert the password part before the '&amp;' at the end of the command line\n        sed -i \"/x11vnc.*\\&amp;/s/\\&amp;/ $PASSWORD_PART&amp;/\" \"$FILE_PATH\"\n        echo \"VNC Password flag added successfully.\"\n    fi\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: home-dir-pv\n  namespace: steam-headless\n  labels:\n    app: steam-headless\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 50Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: /home/k8s/steam-headless/home/default\n  persistentVolumeReclaimPolicy: Retain\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: home-dir-pvc\n  namespace: steam-headless\n  labels:\n    app: steam-headless\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 50Gi\n  volumeName: home-dir-pv\n  volumeMode: Filesystem\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: steam-headless\n  namespace: steam-headless\nspec:\n  serviceName: \"steam-headless\"\n  replicas: 1\n  selector:\n    matchLabels:\n      app: steam-headless\n  template:\n    metadata:\n      labels:\n        app: steam-headless\n    spec:\n      hostNetwork: true\n      securityContext:\n        fsGroup: 1000\n        fsGroupChangePolicy: \"OnRootMismatch\"\n      containers:\n      - name: steam-headless\n        securityContext:\n          privileged: true\n        image: josh5/steam-headless:latest\n        resources:\n          requests:\n            memory: \"8G\"\n            cpu: \"2\"\n            gpu.intel.com/i915: \"1\"\n          limits:\n            memory: \"16G\"\n            cpu: \"4\"\n            gpu.intel.com/i915: \"1\"\n        volumeMounts:\n        - name: dev-dri-renderd128\n          mountPath: /dev/dri/renderD128\n        - name: dshm\n          mountPath: /dev/shm\n        - name: writable-scripts\n          mountPath: /home/default/init.d/\n        - name: home-dir\n          mountPath: /home/default/\n        - name: input-devices\n          mountPath: /dev/input/\n        env:\n        - name: NAME\n          value: 'SteamHeadless'\n        - name: TZ\n          value: 'Europe/Madrid'\n        - name: USER_LOCALES\n          value: 'en_US.UTF-8 UTF-8'\n        - name: DISPLAY\n          value: ':55'\n        - name: DISPLAY_CDEPTH\n          value: '24'\n        - name: DISPLAY_REFRESH\n          value: '30'\n        - name: DISPLAY_SIZEH\n          value: '1080'\n        - name: DISPLAY_SIZEW\n          value: '2560'\n        - name: SHM_SIZE\n          value: '2G'\n        - name: DOCKER_RUNTIME\n          value: 'nvidia'\n        - name: PUID\n          value: '1000'\n        - name: PGID\n          value: '1000'\n        - name: UMASK\n          value: '000'\n        - name: USER_PASSWORD\n          value: '****************************'\n        - name: MODE\n          value: 'primary'\n        - name: WEB_UI_MODE\n          value: 'vnc'\n        - name: ENABLE_VNC_AUDIO\n          value: 'true'\n        - name: PORT_NOVNC_WEB\n          value: '8083'\n        - name: NEKO_NAT1TO1\n          value: ''\n        - name: 'ENABLE_STEAM'\n          value: 'true'\n        - name: ENABLE_SUNSHINE\n          value: 'true'\n        - name: SUNSHINE_USER\n          value: 'ponder'\n        - name: SUNSHINE_PASS\n          value: '****************************'\n        - name: ENABLE_EVDEV_INPUTS\n          value: 'true'\n      initContainers:\n      - name: init-scripts\n        image: busybox\n        command: ['sh', '-c', 'cp /config/add-vnc-passwd.sh /scripts/add-vnc-passwd.sh &amp;&amp; chmod +x /scripts/add-vnc-passwd.sh']\n        volumeMounts:\n        - name: init-d-scripts\n          mountPath: /config\n        - name: writable-scripts\n          mountPath: /scripts\n      volumes:\n      - name: init-d-scripts\n        configMap:\n          name: init-d-scripts\n          defaultMode: 0755\n      - name: writable-scripts\n        emptyDir: {}\n      - name: dev-dri-renderd128\n        hostPath:\n          path: /dev/dri/renderD128\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      - name: home-dir\n        persistentVolumeClaim:\n          claimName: home-dir-pvc\n      - name: input-devices\n        hostPath:\n          path: /dev/input/\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: steam-headless-svc\n  namespace: steam-headless\n  labels:\n    app: steam-headless\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 8083\n  selector:\n    app: steam-headless\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: steam-headless-ingress\n  namespace: steam-headless\n  labels:\n    app: steam-headless\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/websocket-services: steam-headless-svc\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: steam.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: steam-headless-svc\n                port:\n                  number: 80\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - steam.very-very-dark-gray.top\n</code></pre> <p>Note</p> <p>Default screen resolution is 1920x1080 and this should be changed by adding <code>DISPLAY_SIZEH</code> and <code>DISPLAY_SIZEW</code> to the <code>env</code> section, and refresh rate should also be adjusted by adding <code>DISPLAY_REFRESH</code>, but these settings appear to have no effect (no root cause found yet).</p> <p>Apply this deployment and check that the pod and ingress are up and running:</p> <pre><code>$ kubectl apply -f steam-headless.yaml \nnamespace/steam-headless created\npersistentvolume/home-pv created\npersistentvolumeclaim/home-pvc created\nstatefulset.apps/steam-headless created\nservice/steam-headless-svc created\ningress.networking.k8s.io/steam-headless-ingress created\n\n$ kubectl get all -n steam-headless \nNAME                   READY   STATUS    RESTARTS   AGE\npod/steam-headless-0   1/1     Running   0          89s\n\nNAME                         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\nservice/steam-headless-svc   ClusterIP   10.100.43.61   &lt;none&gt;        80/TCP    39m\n\nNAME                              READY   AGE\nstatefulset.apps/steam-headless   1/1     39m\n\n$ kubectl get ingress -n steam-headless \nNAME                     CLASS   HOSTS                           ADDRESS         PORTS     AGE\nsteam-headless-ingress   nginx   steam.very-very-dark-gray.top   192.168.0.171   80, 443   39m\n</code></pre> <p>Check the pod logs if it's not <code>Running</code> after a couple of minutes; if all goes well the pod should reach <code>Starting supervisord</code>:</p> <code>kubectl logs steam-headless-0 -n steam-headless</code> <pre><code>$ kubectl logs steam-headless-0 -n steam-headless \nBuild: [2025-07-26 03:44:23] [master] [23e5ec9fa4747ea05219b66ec938112c2a0fa110] [debian]\n\n[ /etc/cont-init.d/10-setup_user.sh: executing... ]\n**** Configure default user ****\n  - Setting default user uid=1000(default) gid=1000(default)\nusermod: no changes\n  - Adding default user to any additional required device groups\n  - Adding user 'default' to group: 'video'\n  - Adding user 'default' to group: 'audio'\n  - Adding user 'default' to group: 'input'\n  - Adding user 'default' to group: 'pulse'\n  - Adding user 'default' to group: 'polkitd' for device: /dev/input/event0\n  - Adding user 'default' to group: 'video' for device: /dev/dri/card0\n  - Adding user 'default' to group: 'user-gid-993' for device: /dev/dri/renderD128\n  - Setting umask to 000\n  - Create the user XDG_RUNTIME_DIR path '/tmp/.X11-unix/run'\n  - Setting ownership of all log files in '/home/default/.cache/log'\n  - Setting root password\n  - Setting user password\nDONE\n\n[ /etc/cont-init.d/11-setup_sysctl_values.sh: executing... ]\n**** Configure some system kernel parameters ****\n  - Setting the maximum number of memory map areas a process can create to 524288\nDONE\n\n[ /etc/cont-init.d/30-configure_dbus.sh: executing... ]\n**** Configure container dbus ****\n  - Container configured to run its own dbus\nDONE\n\n[ /etc/cont-init.d/30-configure_udev.sh: executing... ]\n**** Configure udevd ****\n  - Disable udevd - /run/udev does not exist\n  - Enable dumb-udev service\n  - Ensure the default user has permission to r/w on input devices\nDONE\n\n[ /etc/cont-init.d/40-setup_locale.sh: executing... ]\n**** Configure local ****\n  - Locales already set correctly to en_US.UTF-8 UTF-8\nDONE\n\n[ /etc/cont-init.d/50-configure_pulseaudio.sh: executing... ]\n**** Configure pulseaudio ****\n  - Enable pulseaudio service.\n  - Configure pulseaudio to pipe audio to a socket\nDONE\n\n[ /etc/cont-init.d/60-configure_gpu_driver.sh: executing... ]\n**** Found Intel device 'Intel Corporation Raptor Lake-P [UHD Graphics] (rev 04)' ****\n  - Enable i386 arch\n  - Install mesa vulkan drivers\n**** No AMD device found ****\n**** No NVIDIA device found ****\nDONE\n\n[ /etc/cont-init.d/70-configure_desktop.sh: executing... ]\n**** Configure Desktop ****\n  - Enable Desktop service.\n  - Ensure home directory template is owned by the default user.\n  - Installing default home directory template\nDONE\n\n[ /etc/cont-init.d/70-configure_xorg.sh: executing... ]\n**** Generate default xorg.conf ****\n  - Configure Xwrapper.config\n  - Configure container as primary the X server\n  - Enabling evdev input class on pointers, keyboards, touchpads, touch screens, etc.\n  - No monitors connected. Installing dummy xorg.conf\nDONE\n\n[ /etc/cont-init.d/80-configure_flatpak.sh: executing... ]\n**** Configure Flatpak ****\n  - Flatpak configured for running inside a Docker container\nDONE\n\n[ /etc/cont-init.d/90-configure_neko.sh: executing... ]\n**** Configure Neko ****\n  - Disable Neko server\nDONE\n\n[ /etc/cont-init.d/90-configure_steam.sh: executing... ]\n**** Configure Steam ****\n  - Enable Steam auto-start script\n  - Initializing Steam config\n  - Initializing Steam library\nDONE\n\n[ /etc/cont-init.d/90-configure_sunshine.sh: executing... ]\n**** Configure Sunshine ****\n  - Enable Sunshine server\nDONE\n\n[ /etc/cont-init.d/90-configure_vnc.sh: executing... ]\n**** Configure VNC ****\n  - Configure VNC service port '32036'\n  - Configure pulseaudio encoded stream port '32037'\n  - Enable VNC server\nDONE\n\n[ /etc/cont-init.d/95-setup_wol.sh: executing... ]\n**** Configure WoL Manager ****\n  - Disable WoL Manager service.\n\n**** Starting supervisord ****\n  - Logging all root services to '/var/log/supervisor/'\n  - Logging all user services to '/home/default/.cache/log/'\n\n2025-07-28 00:45:19,856 INFO Included extra file \"/etc/supervisor.d/dbus.ini\" during parsing\n2025-07-28 00:45:19,856 INFO Included extra file \"/etc/supervisor.d/desktop.ini\" during parsing\n2025-07-28 00:45:19,856 INFO Included extra file \"/etc/supervisor.d/neko.ini\" during parsing\n2025-07-28 00:45:19,856 INFO Included extra file \"/etc/supervisor.d/pulseaudio.ini\" during parsing\n2025-07-28 00:45:19,856 INFO Included extra file \"/etc/supervisor.d/steam.ini\" during parsing\n2025-07-28 00:45:19,857 INFO Included extra file \"/etc/supervisor.d/sunshine.ini\" during parsing\n2025-07-28 00:45:19,857 INFO Included extra file \"/etc/supervisor.d/udev.ini\" during parsing\n2025-07-28 00:45:19,857 INFO Included extra file \"/etc/supervisor.d/vnc-audio.ini\" during parsing\n2025-07-28 00:45:19,857 INFO Included extra file \"/etc/supervisor.d/vnc.ini\" during parsing\n2025-07-28 00:45:19,857 INFO Included extra file \"/etc/supervisor.d/wol-power-manager.ini\" during parsing\n2025-07-28 00:45:19,857 INFO Included extra file \"/etc/supervisor.d/xorg.ini\" during parsing\n2025-07-28 00:45:19,857 INFO Included extra file \"/etc/supervisor.d/xvfb.ini\" during parsing\n2025-07-28 00:45:19,857 INFO Set uid to user 0 succeeded\n2025-07-28 00:45:19,858 INFO RPC interface 'supervisor' initialized\n2025-07-28 00:45:19,859 CRIT Server 'unix_http_server' running without any HTTP authentication checking\n2025-07-28 00:45:19,859 INFO supervisord started with pid 1\n2025-07-28 00:45:20,860 INFO spawned: 'dbus' with pid 485\n2025-07-28 00:45:20,862 INFO spawned: 'udev' with pid 486\n2025-07-28 00:45:20,863 INFO spawned: 'xorg' with pid 487\n2025-07-28 00:45:20,864 INFO spawned: 'audiostream' with pid 488\n2025-07-28 00:45:20,865 INFO spawned: 'frontend' with pid 489\n2025-07-28 00:45:20,866 INFO spawned: 'pulseaudio' with pid 490\n2025-07-28 00:45:20,867 INFO spawned: 'x11vnc' with pid 491\n2025-07-28 00:45:20,867 INFO spawned: 'desktop' with pid 493\n2025-07-28 00:45:20,868 INFO spawned: 'sunshine' with pid 495\nPULSEAUDIO: Starting pulseaudio service\n2025-07-28 00:45:20,882 INFO reaped unknown pid 512 (exit status 0)\n2025-07-28 00:45:21,887 INFO success: dbus entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2025-07-28 00:45:21,887 INFO success: udev entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2025-07-28 00:45:21,887 INFO success: xorg entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2025-07-28 00:45:21,887 INFO success: audiostream entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2025-07-28 00:45:21,887 INFO success: frontend entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2025-07-28 00:45:21,887 INFO success: pulseaudio entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2025-07-28 00:45:21,887 INFO success: x11vnc entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2025-07-28 00:45:21,887 INFO success: desktop entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2025-07-28 00:45:21,887 INFO success: sunshine entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)\n2025-07-28 00:45:22,016 INFO reaped unknown pid 574 (exit status 1)\n2025-07-28 00:45:22,714 INFO reaped unknown pid 580 (exit status 0)\n2025-07-28 00:45:22,714 INFO reaped unknown pid 582 (exit status 0)\n2025-07-28 00:45:22,714 INFO reaped unknown pid 584 (exit status 0)\n2025-07-28 00:45:22,714 INFO reaped unknown pid 586 (exit status 0)\n2025-07-28 00:45:22,715 INFO reaped unknown pid 588 (exit status 0)\n2025-07-28 00:45:22,715 INFO reaped unknown pid 590 (exit status 0)\n2025-07-28 00:45:22,715 INFO reaped unknown pid 592 (exit status 0)\n2025-07-28 00:45:22,715 INFO reaped unknown pid 594 (exit status 0)\n2025-07-28 00:45:22,715 INFO reaped unknown pid 597 (exit status 0)\n2025-07-28 00:45:22,715 INFO reaped unknown pid 599 (exit status 0)\n2025-07-28 00:45:22,715 INFO reaped unknown pid 601 (exit status 0)\n2025-07-28 00:45:22,715 INFO reaped unknown pid 603 (exit status 0)\n2025-07-28 00:45:22,715 INFO reaped unknown pid 605 (exit status 0)\n2025-07-28 00:45:22,930 INFO reaped unknown pid 609 (exit status 0)\n2025-07-28 00:45:22,930 INFO reaped unknown pid 611 (exit status 0)\n2025-07-28 00:45:24,566 INFO reaped unknown pid 628 (exit status 0)\n2025-07-28 00:45:24,566 INFO reaped unknown pid 630 (exit status 0)\n2025-07-28 00:45:24,566 INFO reaped unknown pid 632 (exit status 0)\n2025-07-28 00:45:24,566 INFO reaped unknown pid 634 (exit status 0)\n2025-07-28 00:45:24,566 INFO reaped unknown pid 636 (exit status 0)\n2025-07-28 00:45:24,566 INFO reaped unknown pid 638 (exit status 0)\n2025-07-28 00:45:24,566 INFO reaped unknown pid 640 (exit status 0)\n</code></pre> <p>After a couple of minutes the web UI is ready to use at https://steam.very-very-dark-gray.top</p> <p></p> <p>To verify that the pod has full access to the GPU, open a Terminal and run <code>glxinfo | grep -i 'direct render'</code>; the onput should be <code>direct rendering: Yes</code>.</p>"},{"location":"blog/2025/07/27/playing-steam-games-in-the-browser-with-self-hosted-headless-steam-service/#authentication","title":"Authentication","text":"<p>Using a simple deployment, there is a big Connect button but no authentication at all, so this web UI is not ready to be deployed for remote use. Authentication must be added before exposing this externally; scale the <code>StatefulSet</code> down to zero replicas until authentication is added!</p> <pre><code>$ kubectl scale --replicas=0 sts steam-headless -n steam-headless\nstatefulset.apps/steam-headless scaled\n</code></pre> <p>VNC Password Authentication can be added, as a workaround, by adding the following script under <code>/home/default/init.d</code> (under <code>/home/k8s/steam-headless</code>):</p> <pre><code>#!/bin/bash\n\n# Define the file path\nFILE_PATH=\"/usr/bin/start-x11vnc.sh\"\n\n# Define the password part to insert\nPASSWORD_PART=\"-passwd $USER_PASSWORD\"\n\n# Check if the -passwd option is already in the script\nif grep -q \"\\-passwd\" \"$FILE_PATH\"; then\n    echo \"VNC Password flag already exists.\"\nelse\n    echo \"Adding VNC password flag...\"\n    # Use sed to insert the password part before the '&amp;' at the end of the command line\n    sed -i \"/x11vnc.*\\&amp;/s/\\&amp;/ $PASSWORD_PART&amp;/\" \"$FILE_PATH\"\n    echo \"VNC Password flag added successfully.\"\nfi\n</code></pre> <p>This relies on <code>USER_PASSWORD</code> environment variable existing, which should be in places from the deployment above. With this, clicking on the big Connect button prompts for the pasword. To make this script available, executable and writeable to the the <code>steam-headless</code> container, the deployment includes the script as a <code>ConfigMap</code> (which is always read-only) and copies it into the <code>steam-headless</code> container using an <code>init-container</code>.</p>"},{"location":"blog/2025/07/27/playing-steam-games-in-the-browser-with-self-hosted-headless-steam-service/#audio-over-ssl","title":"Audio over SSL","text":"<p>Bug #171: The audio web socket incorrectly uses SSL on reverse proxies makes audio not available in the current setup with NGinx reverse proxy. The only workaround available seems to be forwarding port <code>32041</code> to the host IP. The <code>nginx.ingress.kubernetes.io/websocket-services: steam-headless-svc</code> annotation on the <code>Ingress</code> does not seem to help.</p> <p>So far there have been no visible attempts from the browser to connect to port 32041, while audio is definitely not working for either Steam games or video playback on Firefox.</p>"},{"location":"blog/2025/09/25/ddns-updater-on-kubernetes/","title":"ddns-updater on Kubernetes","text":"<p>After years of enjoying a static IPv4 address for free, migrating to a new ISP required either paying a monthly fee for such a priviledge... or simply running a Dynamic DNS service to keep the relevant domains pointing to the correct IPv4 address as it updated.</p>"},{"location":"blog/2025/09/25/ddns-updater-on-kubernetes/#ddns-updater","title":"DDNS Updater","text":"<p>The service of choice was the Lightweight universal DDNS Updater program, available for Docker at qmcgaw/ddns-updater and fairly simple to run in Kubernetes.</p> <p>Create a <code>ddns-updater</code> directory and download the base manifest files:</p> <pre><code>$ curl -O https://raw.githubusercontent.com/qdm12/ddns-updater/master/k8s/base/deployment.yaml\n$ curl -O https://raw.githubusercontent.com/qdm12/ddns-updater/master/k8s/base/secret-config.yaml\n$ curl -O https://raw.githubusercontent.com/qdm12/ddns-updater/master/k8s/base/service.yaml\n$ curl -O https://raw.githubusercontent.com/qdm12/ddns-updater/master/k8s/base/kustomization.yaml\n</code></pre> <p>Then edit <code>secret-config.yaml</code> as explained in the DDNS Updater's <code>README</code> Configuration, following the example provided for the relevant DNS provider, e.g. Porkbun:</p> <pre><code>{\n  \"settings\": [\n    {\n      \"provider\": \"porkbun\",\n      \"domain\": \"very-very-dark-gray.top\",\n      \"api_key\": \"pk1_c16d...62c4\",\n      \"secret_api_key\": \"sk1_64f2...9589\",\n      \"ip_version\": \"ipv4\",\n      \"ipv6_suffix\": \"\"\n    }\n  ]\n}\n</code></pre> <p>The configuration can be added to the <code>CONFIG</code> variable in <code>secret-config.yaml</code></p> ddns-updater/secret-config.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: ddns-updater-config\ntype: Opaque\nstringData:\n  CONFIG: '{\"settings\":[{\"provider\":\"porkbun\",\"domain\":\"*.very-very-dark-gray.top\",\"api_key\":\"pk1_c16d...62c4\",\"secret_api_key\":\"sk1_64f2...9589\",\"ip_version\":\"ipv4\",\"ipv6_suffix\": \"\"}]}'\n</code></pre> <p>To make the service's web UI externally accessible, securely and resilient to DNS records being out of date after the IPv4 address changes, add the following <code>ingress.yaml</code> (based on <code>k8s/overlay/with-ingress-tls-cert-manager/ingress.yaml</code>) to enable access through a Cloudflare Tunnel (like the one previously setup to access Home Assistant):</p> ddns-updater/ingress.yaml<pre><code>---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ddns-updater-nginx\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.kubernetes.io/force-ssl-redirect: \"true\"\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: ddns-updater.very-very-dark-gray.top\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: ddns-updater-svc\n              port:\n                name: \"http\"\n  tls:\n    - hosts:\n        - ddns-updater.very-very-dark-gray.top\n      secretName: tls-secret\n</code></pre> <p>With all the above, the service can be started by applying the deployment:</p> <pre><code>$ kubectl apply -k .\nnamespace/ddns-updater created\nsecret/ddns-updater-config created\nservice/ddns-updater-svc created\ndeployment.apps/ddns-updater created\ningress.networking.k8s.io/ddns-updater-nginx created\n</code></pre> <p>Once the services have been running for a few minutes, the web UI will be accessible at https://ddns-updater.very-very-dark-gray.top/ for those users authorized to access it by the Cloudflare Tunnel policies.</p> <p>!!! note the above does not specify a <code>namespace</code> so everything is in the      <code>default</code> namespace.</p> <pre><code>$ kubectl get all \nNAME                                READY   STATUS      RESTARTS   AGE\npod/cm-acme-http-solver-zr4lm       1/1     Running     0          49m\npod/ddns-updater-5cbcfc865d-8jrqj   1/1     Running     0          49m\n\nNAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nservice/cm-acme-http-solver-drlkv   NodePort    10.110.215.98    &lt;none&gt;        8089:32080/TCP   49m\nservice/ddns-updater-svc            ClusterIP   10.104.110.181   &lt;none&gt;        80/TCP           49m\nservice/kubernetes                  ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP          152d\n\nNAME                           READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/ddns-updater   1/1     1            1           49m\n\nNAME                                      DESIRED   CURRENT   READY   AGE\nreplicaset.apps/ddns-updater-5cbcfc865d   1         1         1       49m\n\n$ kubectl get ingress\nNAME                        CLASS    HOSTS                                  ADDRESS         PORTS     AGE\ncm-acme-http-solver-fqqw4   &lt;none&gt;   ddns-updater.very-very-dark-gray.top   192.168.0.171   80        49m\nddns-updater-nginx          nginx    ddns-updater.very-very-dark-gray.top   192.168.0.171   80, 443   49m\n</code></pre>"},{"location":"blog/2025/09/25/ddns-updater-on-kubernetes/#why-not-ddclient","title":"(Why not) DDCLIENT","text":"<p>DDCLIENT seemd at first like the best option, but would not have worked in this case due to Issue #569: porkbun: Support Wildcard DNS entries.</p>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/","title":"Raspberry Pi 5 setup for a buddying coder","text":"<p>As a certain season approaches, I embarked in a little side quest: to setup a Raspberry Pi 5 for a good bro who is learning to code and wants to learn more about Linux as well. Since his goal is learning to code rather than (I hope) getting lost down the infinite rabbit holes of tinkering with the OS and tools, I set this up so he doesn't have to, but otherwise left it basic and uncomplicated.</p>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#hardware","title":"Hardware","text":"<p>As this setup has been working quite well for Alfred, the hardware chosen is a the same except for the case, due to short-term availability:</p> <ul> <li>Raspberry Pi 5 8GB</li> <li>Samsung 990 EVO Plus (2000 GB, M.2 2280)</li> <li>Argon NEO 5 M.2 NVME PCIE Case for Raspberry Pi 5</li> <li>Raspberry Pi Official 5 power supply, 27W USB-C</li> </ul> <p>Argon cases with NVMe extensions are tricky to assemble, one has to follow instructions carefully and make sure the PCIe cable is connected the right way and fully connected. This case actually came with two PCIe cables and a couple of spare screws, which is nice.</p> <p>See also this In-Depth Review Raspberry Pi 5 Argon NEO 5 Case.</p>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#install-to-nvme-ssd","title":"Install to NVMe SSD","text":"<p>To install Raspberry Pi Os directly on the NVMe drive, it goes first into an ICY BOX IB-1817M-C31 USB 3.1 NVMe enclosure, so the installation can be done on the SSD directl instead of using an Micro SD card.</p> <p>Only the best for my bro, so I installed Raspberry Pi OS Full which is a port of Debian Trixie with desktop environment and recommended applications and came out only last month (Release date: 1 Oct 2025).</p> <p>Tip</p> <p>Always download Raspberry Pi OS images from the official Raspberry Pi OS downloads and check its SHA256 against the profided integrity hash.</p> <p>Using <code>rpi-imager</code> there is no need to even download images manually, the tool does it automatically once the Raspberry Pi model (device) and OS version is chosen. The tool also allows setting a few basic parameter such as the password, WiFi network, etc.</p> <p></p> <p>Once the installation is done, the original partitions are replaced with just two (<code>bootfs</code>, <code>rootfs</code>):</p> <pre><code>[104897.486810]  sdf: sdf1 sdf2\n</code></pre> GPT would be necessary only if the disk is larger than 2TB. <p>Booting Pi from NVME greater than 2TB (GPT as opposed to MBR) includes manual instructions to install Raspberry Pi OS on a 4TB NVME using a GPT table. If you image a disk which is larger than 2TB with the Raspberry Pi tools or images, your disk will be limited to 2TB because they use MBR (Master Boot Record) instead of GPT (GUID partition table).</p>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#custom-partitions","title":"Custom partitions","text":"<p>It always felt odd, if not wrong, to run a Raspberry Pi with a large SSD disk without having most of the space allocated to a separate partition (e.g. <code>/home</code>), so I took the liberty of resizing the <code>rootfs</code> partition to mount most of the 2TB disk as <code>/home</code>.</p> <p>Note</p> <p>This may not be entirely useful when it comes to reinstalling the OS, since tools like <code>rpi-imager</code> will anyway wipe the entire device regardless of partitions, and in all honesty I am yet to reinstall the OS in any of my devices.</p> <p>This involved deleting the root partitiona and then recreating it with a different size, to then create a new primary partition taking the rest of the disk.</p> <p>For reference, see also:</p> <ul> <li>Resizing a Partition with fdisk</li> <li>How to Shrink an ext2/3/4 File system with resize2fs</li> </ul> <p>Since the disk has old-fashined MBR partitions, <code>fdisk</code> can be used. Note that this is done while the SSD is not yet installed in the Raspberry Pi, but on the USB adapter.</p> <p>The starting point is that <code>rpi-imager</code> has created only a 512MB boot partition and a 8.5GB root partition:</p> <pre><code># fdisk /dev/sdf \n\nWelcome to fdisk (util-linux 2.39.3).\nChanges will remain in memory only, until you decide to write them.\nBe careful before using the write command.\n\n\nCommand (m for help): p\nDisk /dev/sdf: 1.82 TiB, 2000398934016 bytes, 3907029168 sectors\nDisk model: SSD 990 EVO     \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisklabel type: dos\nDisk identifier: 0x796ebf23\n\nDevice     Boot   Start      End  Sectors  Size Id Type\n/dev/sdf1         16384  1064959  1048576  512M  c W95 FAT32 (LBA)\n/dev/sdf2       1064960 18792447 17727488  8.5G 83 Linux\n\nCommand (m for help): q\n</code></pre> <p>There is a small empty space before the first partition, which requires a bit of maneuviring around. Attempting to create new partitions with default values will first create a tiny 7MB partition at the start of the disk:</p> <pre><code>Command (m for help): n\nPartition type\n   p   primary (2 primary, 0 extended, 2 free)\n   e   extended (container for logical partitions)\nSelect (default p): p\nPartition number (3,4, default 3): \nFirst sector (2048-3907029167, default 2048): \nLast sector, +/-sectors or +/-size{K,M,G,T,P} (2048-16383, default 16383): \n\nCreated a new partition 3 of type 'Linux' and of size 7 MiB.\n\nCommand (m for help): n\nPartition type\n   p   primary (3 primary, 0 extended, 1 free)\n   e   extended (container for logical partitions)\nSelect (default e): p\n\nSelected partition 4\nFirst sector (18792448-3907029167, default 18792448): \nLast sector, +/-sectors or +/-size{K,M,G,T,P} (18792448-3907029167, default 3907029167): \n\nCreated a new partition 4 of type 'Linux' and of size 1.8 TiB.\n\nCommand (m for help): p\nDisk /dev/sdf: 1.82 TiB, 2000398934016 bytes, 3907029168 sectors\nDisk model: SSD 990 EVO     \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisklabel type: dos\nDisk identifier: 0x796ebf23\n\nDevice     Boot    Start        End    Sectors  Size Id Type\n/dev/sdf1          16384    1064959    1048576  512M  c W95 FAT32 (LBA)\n/dev/sdf2        1064960   18792447   17727488  8.5G 83 Linux\n/dev/sdf3           2048      16383      14336    7M 83 Linux\n/dev/sdf4       18792448 3907029167 3888236720  1.8T 83 Linux\n\nPartition table entries are not in disk order.\n\nCommand (m for help): q\n</code></pre> <p>To make things more interesting, consider that a root partition of 8.5 GB may not be enough. Despite having all user data in another patition, there are scenarios where a lot more than 10 GB of space is needed for system files, e.g. the Kubernetes cluster in <code>octavo</code> has 45GB under <code>/var/lib/containerd</code>. In case this Raspberry Pi may developt such needs, the root partition is resized to a generous 200MB; this involves the nerve-racking operation of deleting that partition, to the re-created starting on the same sector:</p> <ol> <li>Start from the original 2 partitions; delete partitons 3 and 4 is previously created.</li> <li>Create partitioan 3 (7MB) to fill the odd gap at the stat of the disk.</li> <li>Delete partition 2 (root).</li> <li>Create new partition 2 starting on the same sector (1064960, which should be the     default, thanks to partition 3 filling the gap) with the desired size (e.g. <code>+200G</code>).     Do not delete the <code>ext4</code> signature.</li> <li>Create new partition 4 taking the rest of the space (default start and end).</li> <li>Delete partition 3; probably not necessary, just to reduce visual clutter.</li> <li>Write changes to the partition table, once </li> </ol> <pre><code># fdisk /dev/sdf \n\nWelcome to fdisk (util-linux 2.39.3).\nChanges will remain in memory only, until you decide to write them.\nBe careful before using the write command.\n\n\nCommand (m for help): p\nDisk /dev/sdf: 1.82 TiB, 2000398934016 bytes, 3907029168 sectors\nDisk model: SSD 990 EVO     \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisklabel type: dos\nDisk identifier: 0x796ebf23\n\nDevice     Boot   Start      End  Sectors  Size Id Type\n/dev/sdf1         16384  1064959  1048576  512M  c W95 FAT32 (LBA)\n/dev/sdf2       1064960 18792447 17727488  8.5G 83 Linux\n\n\nCommand (m for help): n\nPartition type\n   p   primary (2 primary, 0 extended, 2 free)\n   e   extended (container for logical partitions)\nSelect (default p): p\nPartition number (3,4, default 3): \nFirst sector (2048-3907029167, default 2048): \nLast sector, +/-sectors or +/-size{K,M,G,T,P} (2048-16383, default 16383): \n\nCreated a new partition 3 of type 'Linux' and of size 7 MiB.\n\n\nCommand (m for help): d\nPartition number (1-3, default 3): 2\n\nPartition 2 has been deleted.\n\n\nCommand (m for help): n\nPartition type\n   p   primary (2 primary, 0 extended, 2 free)\n   e   extended (container for logical partitions)\nSelect (default p): p\nPartition number (2,4, default 2): \nFirst sector (1064960-3907029167, default 1064960): \nLast sector, +/-sectors or +/-size{K,M,G,T,P} (1064960-3907029167, default 3907029167): +200G\n\nCreated a new partition 2 of type 'Linux' and of size 200 GiB.\nPartition #2 contains a ext4 signature.\n\nDo you want to remove the signature? [Y]es/[N]o: n\n\n\nCommand (m for help): n\nPartition type\n   p   primary (3 primary, 0 extended, 1 free)\n   e   extended (container for logical partitions)\nSelect (default e): p\n\nSelected partition 4\nFirst sector (420495360-3907029167, default 420495360): \nLast sector, +/-sectors or +/-size{K,M,G,T,P} (420495360-3907029167, default 3907029167): \n\nCreated a new partition 4 of type 'Linux' and of size 1.6 TiB.\n\n\nCommand (m for help): p\nDisk /dev/sdf: 1.82 TiB, 2000398934016 bytes, 3907029168 sectors\nDisk model: SSD 990 EVO     \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisklabel type: dos\nDisk identifier: 0x796ebf23\n\nDevice     Boot     Start        End    Sectors  Size Id Type\n/dev/sdf1           16384    1064959    1048576  512M  c W95 FAT32 (LBA)\n/dev/sdf2         1064960  420495359  419430400  200G 83 Linux\n/dev/sdf3            2048      16383      14336    7M 83 Linux\n/dev/sdf4       420495360 3907029167 3486533808  1.6T 83 Linux\n\nPartition table entries are not in disk order.\n\n\nCommand (m for help): d\nPartition number (1-4, default 4): 3\n\nPartition 3 has been deleted.\n\n\nCommand (m for help): p\nDisk /dev/sdf: 1.82 TiB, 2000398934016 bytes, 3907029168 sectors\nDisk model: SSD 990 EVO     \nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisklabel type: dos\nDisk identifier: 0x796ebf23\n\nDevice     Boot     Start        End    Sectors  Size Id Type\n/dev/sdf1           16384    1064959    1048576  512M  c W95 FAT32 (LBA)\n/dev/sdf2         1064960  420495359  419430400  200G 83 Linux\n/dev/sdf4       420495360 3907029167 3486533808  1.6T 83 Linux\n\n\nCommand (m for help): w\nThe partition table has been altered.\nCalling ioctl() to re-read partition table.\nSyncing disks.\n</code></pre> <p>One the root partition has the desired size, it is possible to resize the Ext4 file system in it; but first one has to run a file system check:</p> <pre><code># resize2fs /dev/sdf2 200G\nresize2fs 1.47.0 (5-Feb-2023)\nPlease run 'e2fsck -f /dev/sdf2' first.\n\n# e2fsck -f /dev/sdf2\ne2fsck 1.47.0 (5-Feb-2023)\nPass 1: Checking inodes, blocks, and sizes\nPass 2: Checking directory structure\nPass 3: Checking directory connectivity\nPass 4: Checking reference counts\nPass 5: Checking group summary information\nrootfs: 183284/554880 files (0.1% non-contiguous), 1954143/2215936 blocks\n\n# resize2fs /dev/sdf2 200G\nresize2fs 1.47.0 (5-Feb-2023)\nResizing the filesystem on /dev/sdf2 to 52428800 (4k) blocks.\nThe filesystem on /dev/sdf2 is now 52428800 (4k) blocks long.\n\n# e2fsck /dev/sdf2\ne2fsck 1.47.0 (5-Feb-2023)\nrootfs: clean, 183284/13056000 files, 2741779/52428800 blocks\n# mkfs.ex /dev/sdf4\nmkfs.exfat  mkfs.ext2   mkfs.ext3   mkfs.ext4   \n# mkfs.ext4 /dev/sdf4\nmke2fs 1.47.0 (5-Feb-2023)\nCreating filesystem with 435816726 4k blocks and 108961792 inodes\nFilesystem UUID: 95b56139-4931-4dac-ab92-859e0d576453\nSuperblock backups stored on blocks: \n        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, \n        4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, \n        102400000, 214990848\n\nAllocating group tables: done                            \nWriting inode tables: done                            \nCreating journal (262144 blocks): done\nWriting superblocks and filesystem accounting information: done       \n</code></pre> <p>At this point the resized file system can be mounted, which will come in handy later.</p> <pre><code># mount /dev/sdf2 /mnt\n# ls /mnt/\nbin  boot  dev  etc  home  lib  lost+found  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var\n# ls /mnt/home/\npi  rpi-first-boot-wizard\n</code></pre>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#pre-boot-tweaks","title":"Pre-boot tweaks","text":"<p>The following adjustments can be make by editing files in the boot partition (<code>sdf1</code>) before booting the Raspberry Pi OS for the first time; mount it under <code>/mnt/boot</code> once the root file system is mounted on <code>/mnt</code>.</p> <pre><code># mount /dev/sdf1 /mnt/boot\n</code></pre> <p>Note</p> <p>This partition will be mounted as <code>/boot/firmware</code> after booting the Raspberry Pi OS for the first time.</p>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#pcie-gen-3","title":"PCIe Gen 3","text":"<p>NVMe SSD boot with the Raspberry Pi 5, at least with this Argon ONE case, turned out to be easy. When using a HAT+-compliant NVMe adapter, there is no need to enable the external PCIe port, it will be enabled automatically, but it is useful to force PCIe Gen 3 speeds using these options in <code>/mnt/boot/config.txt</code></p> <pre><code>[all]\ndtparam=pciex1\ndtparam=pciex1_gen=3\n</code></pre>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#enable-cgroups","title":"Enable CGroups","text":"<p>Troubleshooting Bootstrap for Kubernetes on the previous Raspberry Pi OS revealed that the required CGroups are not enabled by default. Since these may be needed by popular development tools, e.g. to run in Docker, enable these in <code>/mnt/boot/cmdline.txt</code></p> <pre><code>cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory console=serial0,115200 console=tty1 root=PARTUUID=796ebf23-02 rootfstype=ext4 fsck.repair=yes rootwait resize quiet splash plymouth.ignore-serial-consoles cfg80211.ieee80211_regdom=CH systemd.run=/boot/firstrun.sh systemd.run_success_action=reboot systemd.unit=kernel-command-line.target\n</code></pre>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#system-configuration","title":"System Configuration","text":""},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#wifi-reconfiguration","title":"WiFi (re)configuration","text":"<p>Just as it happened with Alfred, the WiFi connection was not established even though the system booted just fine the first time. Instead of going through the menus with <code>raspi-config</code>, access via SSH was gained by connecting the device to the LAN, which naturally worked like a charm:</p> <pre><code>pi@roach:~ $ ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n       valid_lft forever preferred_lft forever\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 88:a2:9e:10:1b:5d brd ff:ff:ff:ff:ff:ff\n    inet 192.168.0.70/24 brd 192.168.0.255 scope global dynamic noprefixroute eth0\n       valid_lft 86268sec preferred_lft 86268sec\n    inet6 fe80::6e5d:6e2d:f3cf:d040/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n3: wlan0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc fq_codel state DOWN group default qlen 1000\n    link/ether 88:a2:9e:10:1b:5e brd ff:ff:ff:ff:ff:ff\n</code></pre>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#additional-wifi-networks","title":"Additional WiFi networks","text":"<p>How to add a second WiFi network to your Raspberry Pi (updated for OS Bookworm) explains how the wpa_supplicant.conf file is no longer used to configure Wi-Fi connections and, instead, the current Raspberry Pi OS (based on Bookworm) uses NetworkManager to manage network connections, including Wi-Fi.</p> <p>There is a separate configuration file for each WiFi connection under <code>/etc/NetworkManager/system-connections</code>, all it takes to configure additional WiFi connections is to</p> <ol> <li>Copy a working file with a new name (e.g. the SSID name).</li> <li>Replace the SSID and password.</li> <li>Replace the <code>id</code> with a new unique value of choice.</li> <li>Replace the <code>uuid</code> with a new unique value from running <code>uuid</code>.</li> <li>Set the permissions to <code>600</code></li> </ol> <p>This can be done in advance of replacing WiFi access points, etc.</p> <p>WiFi connections are not effective immediately after rebooting, but after a couple of minutes there is a new IP address assigned to the wireless device (<code>wlan0</code>):</p> <pre><code>pi@roach:~ $ ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host noprefixroute \n       valid_lft forever preferred_lft forever\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 88:a2:9e:10:1b:5d brd ff:ff:ff:ff:ff:ff\n    inet 192.168.0.70/24 brd 192.168.0.255 scope global dynamic noprefixroute eth0\n       valid_lft 79805sec preferred_lft 79805sec\n    inet6 fe80::6e5d:6e2d:f3cf:d040/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n3: wlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 88:a2:9e:10:1b:5e brd ff:ff:ff:ff:ff:ff\n    inet 192.168.0.71/24 brd 192.168.0.255 scope global dynamic noprefixroute wlan0\n       valid_lft 84609sec preferred_lft 84609sec\n    inet6 fe80::8194:2d69:f7fb:c23e/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n</code></pre> <p>At this point the wired connecion to the LAN network can be removed.</p>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#disable-power-save-for-wifi","title":"Disable power save for WiFi","text":"<p>Raspberry Pi enables power save on WiFi by default:</p> <pre><code>[    5.598286] brcmfmac: F1 signature read @0x18000000=0x15264345\n[    5.600446] brcmfmac: brcmf_fw_alloc_request: using brcm/brcmfmac43455-sdio for chip BCM4345/6\n[    5.600754] usbcore: registered new interface driver brcmfmac\n[    5.790052] brcmfmac: brcmf_c_process_txcap_blob: no txcap_blob available (err=-2)\n[    5.790325] brcmfmac: brcmf_c_preinit_dcmds: Firmware: BCM4345/6 wl0: Aug 29 2023 01:47:08 version 7.45.265 (28bca26 CY) FWID 01-b677b91b\n[    6.423812] brcmfmac: brcmf_cfg80211_set_power_mgmt: power save enabled\n</code></pre> <p>This can lead to losing connectivity in certain conditions, so disable power save of WiFi to avoid having to deal with such problems like Reconnect on WiFi drop.</p> <pre><code>$ sudo /sbin/iw dev wlan0 set power_save off\n</code></pre> <pre><code>[ 1767.061929] brcmfmac: brcmf_cfg80211_set_power_mgmt: power save disabled\n</code></pre>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#fix-locales","title":"Fix locales","text":"<p>The above looks like rpi-imager took my PC\u2019s locale settings:</p> <pre><code>-bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n</code></pre> <p>The solution is to generate <code>en_US.UTF-8</code> and set it as the system default. To do this, run <code>raspi-config</code> and go into <code>5. Localisation Options</code> then <code>L1 Locale</code>, enable <code>en_US.UTF-8</code> and set it as the <code>Default locale for the system environment</code>.</p> <pre><code>$ sudo raspi-config\n...\nGenerating locales (this might take a while)...\n  en_GB.UTF-8... done\n  en_US.UTF-8... done\nGeneration complete.\n</code></pre>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#to-swap-or-not-to-swap","title":"To Swap Or Not To Swap","text":"<p>With 8 GB of RAM there should be no reason to need disk swap, never mind that it would be rather fast in this system. Besides, Kubernetes doesn't like swap and similar issues may be encountered by other tools. On the other hand, running a desktop environment and compilers could lead to actually neededing swap.</p> <p>Therefore, instead of directly disabling swap entirely, lets leave its default setup.</p> <p>Debian 13 (Trixie), on which Raspberry Pi OS is based, introduced a different mechanism for enabling and disabling the swap file. View which swap units are running:</p> <pre><code>pi@roach:~ $ systemctl list-units --quiet --type=swap\n  dev-zram0.swap loaded active active rpi-swap managed swap device (zram+file)\n</code></pre> <p>The default configuration is to automatically create swap when needed. To entirely disable it, create <code>/etc/rpi/swap.conf.d/99-disable-swap.conf</code> with the contents:</p> <pre><code>[Main]\nMechanism=none\n</code></pre> <p>Then stop the running swap unit:</p> <pre><code>pi@roach:~ $ sudo systemctl daemon-reload\npi@roach:~ $ systemctl list-units --quiet --type=swap\n  dev-zram0.swap loaded active active Compressed Swap on /dev/zram0\npi@roach:~ $ sudo systemctl stop dev-zram0.swap\npi@roach:~ $ systemctl list-units --quiet --type=swap\n</code></pre> <p>But for now, let leave it at its default configuration. </p>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#install-latest-updates","title":"Install latest updates","text":"<p>Before moving forward, make sure the system is up-to-date:</p> <pre><code>pi@roach:~ $ sudo apt update\nHit:1 http://deb.debian.org/debian trixie InRelease\nHit:2 http://deb.debian.org/debian trixie-updates InRelease\nHit:3 http://archive.raspberrypi.com/debian trixie InRelease\nHit:4 http://deb.debian.org/debian-security trixie-security InRelease\n129 packages can be upgraded. Run 'apt list --upgradable' to see them.\n\n$ sudo apt full-upgrade -y\nUpgrading:                      \n  agnostics         libssl3t64               raspi-utils          rpicam-apps-opencv-postprocess\n  bluez-firmware    libswresample5           raspi-utils-core     rpicam-apps-preview\n  chromium          libswscale8              raspi-utils-dt       rpinters\n  chromium-common   libtiff6                 raspi-utils-eeprom   userconf-pi\n  chromium-l10n     libvlc-bin               raspi-utils-otp      vlc\n  chromium-sandbox  libvlc5                  raspinfo             vlc-bin\n  ffmpeg            libvlccore9              rasputin             vlc-data\n  firefox           lpplug-bluetooth         rc-gui               vlc-l10n\n  ghostscript       lpplug-clock             rp-bookshelf         vlc-plugin-access-extra\n  gui-pkinst        lpplug-ejecter           rp-prefapps          vlc-plugin-base\n  gui-updater       lpplug-menu              rpcc                 vlc-plugin-notify\n  libavcodec61      lpplug-netman            rpd-applications     vlc-plugin-qt\n  libavdevice61     lpplug-power             rpd-common           vlc-plugin-samba\n  libavfilter10     lpplug-updater           rpd-developer        vlc-plugin-skins2\n  libavformat61     lpplug-volumepulse       rpd-graphics         vlc-plugin-video-output\n  libavutil59       lxpanel-pi               rpd-preferences      vlc-plugin-video-splitter\n  libcamera-ipa     mesa-libgallium          rpd-theme            vlc-plugin-visualization\n  libcamera-tools   mesa-vulkan-drivers      rpd-utilities        wayvnc\n  libcamera-v4l2    openjdk-21-jre           rpd-wayland-core     wf-panel-pi\n  libcamera0.5      openjdk-21-jre-headless  rpd-wayland-extras   wfplug-bluetooth\n  libdtovl0         openssl                  rpd-x-core           wfplug-clock\n  libegl-mesa0      openssl-provider-legacy  rpd-x-extras         wfplug-connect\n  libgbm1           pipanel                  rpi-chromium-mods    wfplug-ejecter\n  libgl1-mesa-dri   piwiz                    rpi-connect          wfplug-menu\n  libglx-mesa0      pplug-ejecter-data       rpi-eeprom           wfplug-netman\n  libgpiolib0       pplug-netman-schema      rpi-firefox-mods     wfplug-power\n  libgs-common      pplug-power-data         rpi-loop-utils       wfplug-updater\n  libgs10           pplug-updater-data       rpi-swap             wfplug-volumepulse\n  libgs10-common    pprompt                  rpi-userguide        xserver-common\n  libpisp-common    python3-libcamera        rpicam-apps          xserver-xorg-core\n  libpisp1          raindrop                 rpicam-apps-core\n  libpostproc58     raspberrypi-sys-mods     rpicam-apps-encoder\n  librpicam-app1    raspi-config             rpicam-apps-lite\n\nInstalling dependencies:\n  alacarte  gir1.2-gmenu-3.0  gnome-menus  libgnome-menu-3-0  qt6ct\n\nSummary:\n  Upgrading: 129, Installing: 5, Removing: 0, Not Upgrading: 0\n  Download size: 463 MB\n  Space needed: 27.6 MB / 191 GB available\n</code></pre>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#update-eeprom","title":"Update EEPROM","text":"<p>Then again, before moving forward, and especially because it was necessary to fix a Failed Argon installation on Alfred, use <code>raspi-config</code> to update the bootloader and set it to <code>Latest</code> (without resetting config). This updated the EEPROM from a May release to a November release, less than a week old:</p> <pre><code>pi@roach:~ $ sudo rpi-eeprom-update -l\n/usr/lib/firmware/raspberrypi/bootloader-2712/default/pieeprom-2025-05-08.bin\npi@roach:~ $ ls -l /usr/lib/firmware/raspberrypi/bootloader-2712/default/\ntotal 2152\n-rw-r--r-- 1 root root 2097152 Nov  6 13:30 pieeprom-2025-05-08.bin\n-rw-r--r-- 1 root root  102992 Nov  6 13:30 recovery.bin\npi@roach:~ $ sudo raspi-config\npi@roach:~ $ sudo rpi-eeprom-update -l\n/usr/lib/firmware/raspberrypi/bootloader-2712/latest/pieeprom-2025-11-05.bin\n</code></pre> <p>Reboot after updating the EEPROM; and definitely before moving forward.</p>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#argon-case-scripts","title":"Argon case scripts","text":"<p>The instructions in page 10 of the Argon NEO 5 M.2 NVME PCIE case manual to install the scripts for FAN control seem simple enough:</p> <p></p> <code>curl https://download.argon40.com/argon-eeprom.sh | bash</code> <pre><code>pi@roach:~ $ curl https://download.argon40.com/argon-eeprom.sh | bash\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                Dload  Upload   Total   Spent    Left  Speed\n100  1059  100  1059    0     0  16607      0 --:--:-- --:--:-- --:--:-- 16809\n*************\nArgon Setup  \n*************\nHit:1 http://deb.debian.org/debian trixie InRelease\nGet:2 http://deb.debian.org/debian trixie-updates InRelease [47.3 kB]\nHit:3 http://archive.raspberrypi.com/debian trixie InRelease\nHit:4 http://deb.debian.org/debian-security trixie-security InRelease\nFetched 47.3 kB in 0s (485 kB/s)\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nBOOTLOADER: up to date\n  CURRENT: Wed Nov  5 05:37:18 PM UTC 2025 (1762364238)\n  LATEST: Wed Nov  5 05:37:18 PM UTC 2025 (1762364238)\n  RELEASE: latest (/usr/lib/firmware/raspberrypi/bootloader-2712/latest)\n            Use raspi-config to change the release.\nUpdating bootloader EEPROM\nimage: /usr/lib/firmware/raspberrypi/bootloader-2712/latest/pieeprom-2025-11-05.bin\nconfig_src: blconfig device\nconfig: /tmp/tmp43cskz0p/boot.conf\n################################################################################\n[all]\nWAKE_ON_GPIO=0\nPOWER_OFF_ON_HALT=1\nPCIE_PROBE=1\nBOOT_UART=1\nBOOT_ORDER=0xf416\nNET_INSTALL_AT_POWER_ON=1\n\n################################################################################\n\n*** To cancel this update run 'sudo rpi-eeprom-update -r' ***\n\n*** CREATED UPDATE /tmp/tmp43cskz0p/pieeprom.upd  ***\n\n  CURRENT: Wed Nov  5 05:37:18 PM UTC 2025 (1762364238)\n  UPDATE: Wed Nov  5 05:37:18 PM UTC 2025 (1762364238)\n  BOOTFS: /boot/firmware\n'/tmp/tmp.rxzVUvjeh8' -&gt; '/boot/firmware/pieeprom.upd'\n\nUPDATING bootloader. This could take up to a minute. Please wait\n\n*** Do not disconnect the power until the update is complete ***\n\nIf a problem occurs then the Raspberry Pi Imager may be used to create\na bootloader rescue SD card image which restores the default bootloader image.\n\nflashrom -p linux_spi:dev=/dev/spidev10.0,spispeed=16000 -w /boot/firmware/pieeprom.upd\nVerifying update\nVERIFY: SUCCESS\nUPDATE SUCCESSFUL\n</code></pre> <p>After a required reboot, run the <code>argonneo5.sh</code> script too:</p> <code>curl https://download.argon40.com/argonneo5.sh | bash</code> <pre><code>pi@roach:~ $ curl https://download.argon40.com/argonneo5.sh | bash\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                Dload  Upload   Total   Spent    Left  Speed\n100  2171  100  2171    0     0  46338      0 --:--:-- --:--:-- --:--:-- 47195\n*************\nArgon Setup  \n*************\nHit:1 http://deb.debian.org/debian trixie InRelease\nHit:2 http://archive.raspberrypi.com/debian trixie InRelease\nHit:3 http://deb.debian.org/debian trixie-updates InRelease\nHit:4 http://deb.debian.org/debian-security trixie-security InRelease\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nCalculating upgrade... Done\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nBOOTLOADER: up to date\n  CURRENT: Wed Nov  5 05:37:18 PM UTC 2025 (1762364238)\n    LATEST: Wed Nov  5 05:37:18 PM UTC 2025 (1762364238)\n  RELEASE: latest (/usr/lib/firmware/raspberrypi/bootloader-2712/latest)\n            Use raspi-config to change the release.\nEEPROM settings up to date\n*********************\n  Setup Completed \n*********************\n\nPlease reboot for changes to take effect\n</code></pre> <p>After another required reboot, the firware is ready and there is nothing to do.</p>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#essential-software","title":"Essential Software","text":""},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#basic-packages","title":"Basic packages","text":"<p>Before moving forward, I like to install a few basics that make work easier, or on which subsequent packages depend:</p> <pre><code>$ sudo apt install -y bc git iotop-c netcat-openbsd rename speedtest-cli sysstat \\\n  vim python3-pip python3-influxdb python3-numpy python3-absl python3-unidecode\n</code></pre>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#fail2ban","title":"Fail2Ban","text":"<p>Just in case this system ever gets its port 22 exposed to the Internet, Fail2Ban scans log files and bans IP addresses conducting too many failed login attempts. It is a rather basic protection mechanism, and this system is not intended to have its SSH port open to the Internet, but it is so easy to install and enable that there is no excuse not to.</p> <pre><code>$ sudo apt-get install iptables fail2ban -y\n$ sudo systemctl enable --now fail2ban\n</code></pre> <p>The configuration in <code>/etc/fail2ban/jail.conf</code> can be spiced up to make a little more trigger-happy:</p> <pre><code># \"bantime\" is the number of seconds that a host is banned.\nbantime  = 12h\n# A host is banned if it has generated \"maxretry\" during the last \"findtime\"\n# seconds.\nfindtime  = 90m\n# \"maxretry\" is the number of failures before a host get banned.\nmaxretry = 3\n# \"bantime.increment\" allows to use database for searching of previously banned ip's to increase a \n# default ban time using special formula, default it is banTime * 1, 2, 4, 8, 16, 32...\nbantime.increment = true\n</code></pre> <p>Then restart the service to pick the changes up:</p> <pre><code>$ sudo systemctl restart fail2ban\n</code></pre>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#ssh-server","title":"SSH Server","text":"<p>As a general good practice, even if this system won't have its SSH port open to the Internet, and on top of having Fail2ban enabled, SSH authetication should be allowed only with What is SSH Public Key Authentication?. Once the system arrives as it final destination, its final owner should add their own public keys are added to <code>/home/pi/.ssh/authorized_keys</code> and disable password authentication by setting <code>PasswordAuthentication no</code> in <code>/etc/ssh/sshd_config</code>.</p> <p>This tutorial explains several concrete methods to the this on different platforms: Set up SSH public key authentication to connect to a remote system.</p>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#additional-software","title":"Additional Software","text":""},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#additional-debian-packages","title":"Additional Debian packages","text":"<p>The following packages are installed based on experience setting up desktop environments in other PCs, most recently Ubuntu Studio 24.04 for myself and a young artist.</p> <pre><code>$ sudo apt install -y gkrellm gkrellm-cpufreq geeqie id3v2 btop sox exiv2 rename \\\n  scrot python3-pip python3-selenium tor unrar ttf-mscorefonts-installer ffmpeg \\\n  iotop-c xdotool redshift-gtk inxi screen code\n</code></pre>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#visual-studio-code","title":"Visual Studio Code","text":"<p>Installing the <code>code</code> package is the canonical method to install VS Code on Raspberry Pi.</p> <p>VS Code is not officially supported on Raspberry Pi, so it pays to keep an eye on their community discussions for update and known issues and workarounds. One notorious known issue has been freezes on Raspberry Pi 4, for which the workaround is to disable use of the GPU by adding this flag in <code>/home/pi/.vscode/argv.json</code></p> <pre><code>{\n  \"disable-hardware-acceleration\": true\n}\n</code></pre> <p>This workaround may not be necessay for the more recent versions of VS Code, and the version installed at the time of writting is 1.105.1.</p>"},{"location":"blog/2025/11/04/raspberry-pi-5-setup-for-a-buddying-coder/#raspberry-pi-connect","title":"Raspberry Pi Connect","text":"<p>Raspberry Pi Connect provides secure access to your Raspberry Pi from anywhere in the world. While it is unsurprisingly a little laggy, it works well enough to use the desktop environment from any web browser anywhere without a complicated setup.</p> <p>Following the instructions is simple and quick enough to access the desktop environment within a couple of minutes, it can be enabled and disabled any time through SSH and it can also be added to (and remove from) a Raspberry Pi Connect account very easily.</p> <p>To connect, start the <code>rpi-connect</code> service and create a URL to sing in, then visit that URL with the browser with an active session on https://connect.raspberrypi.com/</p> <pre><code>pi@roach:~ $ rpi-connect on\n\u2713 Raspberry Pi Connect started\npi@roach:~ $ rpi-connect signin\nComplete sign in by visiting https://connect.raspberrypi.com/verify/XEKA-TPFN\n\n\u2713 Signed in\n</code></pre> <p>Once the device is signed in, it shows up at https://connect.raspberrypi.com/devices and it's just two clicks to open the desktop environment in a new browser window:</p> <p></p> <p></p> <p>Once this access is no longer necessary, the service can be disabled on the Raspberry Pi by simply running <code>rpi-connect off</code> and running <code>rpi-connect signout</code> before will deassociated from the previously used Raspberry Pi Connect account.</p> <pre><code>pi@roach:~ $ rpi-connect signout\n\u2713 Signed out\npi@roach:~ $ rpi-connect off\n\u2713 Raspberry Pi Connect stopped\n</code></pre>"},{"location":"blog/2025/12/14/fotocx-on-ubuntu-studio-2404/","title":"Fotocx on Ubuntu Studio 24.04","text":"<p>Fotocx is a free Linux program for editing photos or other images and managing a large collection. It is meant to manage very large photo collections of up to 1 million images on a strong PC, meaning a computer with 4+ CPU cores and 16+ GB memory. Rapture should be quite strong enough, with 8 CPU cores (16 threads) and 32 GB of RAM.</p> <p>Ubuntu 24.04 responsitories include an older version of this application, formerly named fotoxx, but the application is quite easy to build from source to run the latest version.</p>"},{"location":"blog/2025/12/14/fotocx-on-ubuntu-studio-2404/#installation","title":"Installation","text":"<p>The source code of Fotocx is not hosted on GitHub or similar; instead, it is distributed through the developer's website via tarball downloads. The source code can be browsed online in the Fossies archive at  https://fossies.org/linux/misc/fotocx-25.5-source.tar.gz/</p>"},{"location":"blog/2025/12/14/fotocx-on-ubuntu-studio-2404/#build-dependencies","title":"Build Dependencies","text":"<p>Several libraries are required to build Fotocx that may not be already installed:</p> <code>apt install build-essential pkg-config libgtk-3-dev libjpeg-dev libtiff-dev liblcms2-dev libchamplain-0.12-dev libgtk-3-dev libchamplain-gtk-0.12-dev libclutter-gtk-1.0-dev</code> <pre><code># apt install liblcms2-dev libchamplain-0.12-dev libclutter-gtk-1.0-dev libchamplain-gtk-0.12-dev\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  gir1.2-champlain-0.12 gir1.2-clutter-1.0 gir1.2-cogl-1.0 gir1.2-coglpango-1.0 gir1.2-gtkclutter-1.0\n  gir1.2-json-1.0 libchamplain-0.12-0 libclutter-1.0-0 libclutter-1.0-common libclutter-1.0-dev\n  libclutter-gtk-1.0-0 libcogl-common libcogl-dev libcogl-pango-dev libcogl-pango20 libcogl-path-dev\n  libcogl-path20 libcogl20 libdrm-dev libevdev-dev libgbm-dev libgudev-1.0-dev libinput-dev libjson-glib-dev\n  libmtdev-dev libnghttp2-dev libpciaccess-dev libpsl-dev libsoup-3.0-dev libsqlite3-dev\n  libsysprof-capture-4-dev libudev-dev libwacom-dev\nSuggested packages:\n  libchamplain-doc libclutter-1.0-doc libclutter-gtk-1.0-doc libcogl-doc libevdev-doc libjson-glib-doc\n  libnghttp2-doc libsoup-3.0-doc sqlite3-doc\nThe following NEW packages will be installed:\n  gir1.2-champlain-0.12 gir1.2-clutter-1.0 gir1.2-cogl-1.0 gir1.2-coglpango-1.0 gir1.2-gtkclutter-1.0\n  gir1.2-json-1.0 libchamplain-0.12-0 libchamplain-0.12-dev libclutter-1.0-0 libclutter-1.0-common\n  libclutter-1.0-dev libclutter-gtk-1.0-0 libclutter-gtk-1.0-dev libcogl-common libcogl-dev libcogl-pango-dev\n  libcogl-pango20 libcogl-path-dev libcogl-path20 libcogl20 libdrm-dev libevdev-dev libgbm-dev\n  libgudev-1.0-dev libinput-dev libjson-glib-dev liblcms2-dev libmtdev-dev libnghttp2-dev libpciaccess-dev\n  libpsl-dev libsoup-3.0-dev libsqlite3-dev libsysprof-capture-4-dev libudev-dev libwacom-dev\n  gir1.2-gtkchamplain-0.12 libchamplain-gtk-0.12-0 libchamplain-gtk-0.12-dev\n0 upgraded, 36 newly installed, 0 to remove and 0 not upgraded.\n</code></pre> <p>Building and installing Fotocx is then very easy and fast:</p> <code>make &amp;&amp; make install</code> <pre><code>$ wget https://kornelix.net/downloads/downloads/fotocx-25.5-source.tar.gz\n$ tar xfz fotocx-25.5-source.tar.gz\n$ cd fotocx/\n$ make\ng++ fotocx.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` -o fotocx.o \\\n\ng++ f.widgets.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.file.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.gallery.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.albums.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.select.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.meta.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.edit.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.repair.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.refine.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.effects.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.warp.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.combine.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.mashup.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.tools.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.batch.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ f.pixmap.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++ zfuncs.cc -fpermissive -Wno-write-strings  -Wno-deprecated-declarations  -Wno-class-memaccess  -fno-omit-frame-pointer  -Wall -g2 -rdynamic -O2 -Wno-format-truncation -Wno-stringop-truncation  -c `pkg-config --cflags gtk+-3.0 --cflags champlain-gtk-0.12` \\\n\ng++  fotocx.o f.widgets.o f.file.o f.gallery.o f.albums.o f.select.o f.meta.o f.edit.o f.repair.o f.refine.o f.effects.o f.warp.o f.combine.o f.mashup.o f.tools.o f.batch.o f.pixmap.o zfuncs.o `pkg-config --libs gtk+-3.0` -lrt -lpthread -ltiff -lpng -ljpeg -lclutter-1.0 -lclutter-gtk-1.0 -lchamplain-0.12 -lchamplain-gtk-0.12 -o fotocx \\\n\n# make install\nrm -f     /usr/local/bin/fotocx*\nrm -f     /usr/local/share/applications/fotocx*\nrm -f -R  /usr/local/share/doc/fotocx\nrm -f     /usr/local/share/man/man1/fotocx*\nrm -f -R  /usr/local/share/fotocx\nrm -f     /usr/local/share/icons/fotocx*\nrm -f     /usr/local/share/metainfo/kornelix.fotocx*\nmkdir -p  /usr/local/bin\nmkdir -p  /usr/local/share/applications\nmkdir -p  /usr/local/share/doc/fotocx\nmkdir -p  /usr/local/share/man/man1\nmkdir -p  /usr/local/share/fotocx\nmkdir -p  /usr/local/share/icons\nmkdir -p  /usr/local/share/metainfo\ncp -f fotocx /usr/local/bin\ncp -f fotocx.desktop /usr/local/share/applications\ncp -f -R  doc/* /usr/local/share/doc/fotocx\ngzip -fk -9 man/fotocx.man\nmv -f man/fotocx.man.gz /usr/local/share/man/man1/fotocx.1.gz\ncp -f -R  data /usr/local/share/fotocx\ncp -f -R  images /usr/local/share/fotocx\ncp -f fotocx.png /usr/local/share/icons\ncp -f metainfo/* /usr/local/share/metainfo\n</code></pre>"},{"location":"blog/2025/12/14/fotocx-on-ubuntu-studio-2404/#runtime-dependencies","title":"Runtime Dependencies","text":"<p>Fotocx requires several packages to run, while others are recommended. If any of the required packages are missing it will report these and refuse to run:</p> <pre><code>program            purpose          REQ   AVAIL   package (debian) \nexiftool           metadata         yes   yes     libimage-exiftool-perl \ndcraw              RAW files        yes   no      dcraw \naddr2line          crash report     yes   yes     binutils \nffmpeg             video files      no    yes     ffmpeg \ndjxl               .jxl files       no    no      libjxl-tools \ndwebp              .webp files      no    no      webp \nheif-convert       .heic files      no    no      libheif-examples \nheif-convert       .avif files      no    no      libheif-examples \nopj_decompress     .jp2 files       no    no      libopenjp2-tools \nvlc                video play       no    yes     vlc \n\nInstall following programs using your package manager: \ndcraw              RAW files        yes   no      dcraw \n</code></pre> <p>For future convenience, install all missing packages:</p> <code>apt install dcraw libjxl-tools webp libheif-examples libheif-examples libopenjp2-tools vlc</code> <pre><code># apt install dcraw libjxl-tools webp libheif-examples libheif-examples libopenjp2-tools vlc\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSuggested packages:\n  gphoto2\nThe following NEW packages will be installed:\n  dcraw libheif-examples libjxl-tools libopenjp2-tools libtcmalloc-minimal4t64 webp\n0 upgraded, 6 newly installed, 0 to remove and 0 not upgraded.\n</code></pre>"},{"location":"blog/2025/12/14/fotocx-on-ubuntu-studio-2404/#configuration","title":"Configuration","text":""},{"location":"blog/2025/12/14/fotocx-on-ubuntu-studio-2404/#index","title":"Index","text":"<p>When starting Fotocx for the first time, it presents a choice between building the index immediately or deferring it. If the index build is deferred, the application can be used but it will be slow and batch functions will not be available. Not intended to use the application right away, chose to build the index immediately.</p> <p>This operation, executed by <code>fotocx</code> and <code>perl</code>, was mostly CPU-bound, indexing photos out of a PCI v4 NVMe SSD, took 69 minutes for 130,999 files, including 118,641 photos, 9,670 RAW files and 2,688 videos, creating 58,983 titles and descriptions and 130,753 thumbnails, and producing a large number of warnings and errors about invalid files (e.g. <code>Unexpected end of file</code>).</p> <pre><code>index updates: 130999  title/desc updates: 58983 \nthumbnail updates: 130999, deletes: 0 \nwriting updated image index file \nall image files, including unmounted folders: 130999 \nafter removal of missing and blacklisted: 130999 \nImage files: 118641 (285.6 GB) + RAW files: 9670 (124.0 GB) + Video files: 2688 \nthumbnails found: 130341 \nimage files: 130999  index level: 2 \nindex completed, 4163.2 seconds \n\nFblock(0) \nsave_params()\nstart gallery: /home/ponder \nstart file: (null) \nstartup time: 4267.6 secs.\n</code></pre> <p> </p>"},{"location":"blog/2025/12/14/fotocx-on-ubuntu-studio-2404/#tutorial","title":"Tutorial","text":"<p>Once the indexing process finished, the UI defaults the Gallery to the <code>$HOME</code> folder, even though that is not where the photos are. Use the TOP button (left of @) to select the correct folder.</p> <p>For all this application may be praised in terms of performance, the UI is far from modern or intuitve. To get through the steep learning curve, the author provides several videos on their YouTube channel; one should start by watching the Fotocx Demo and Tutorial:</p> <p> </p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/","title":"Replacing Ingress-NGINX with Pomerium","text":"<p>The community Ingress-NGINX controller is scheduled for retirement in March 2026, so the time comes near to replace it with either a compatible Ingress-NGINX controller or migrate to the new Gateway API. Given the relatively short migration timeline, the former promises the least friction and complexity for self-hosted, single-node clusters.</p> <p>The clusters (<code>octavo</code> and <code>alfred</code>) are currently running Kubernetes version 1.32 which is will be the next one up to go End Of Life in Feb 28, 2026. Updating the cluster to Kubernetes version 1.34 would first require updating Ingress-NGINX to version 1.14 according to its Supported Versions table.</p> <p>Instead, it is possible to replace Ingress-NGINX entirely with Pomerium, which serves as a direct, secure-by-default alternative that combines standard reverse proxy functionality with integrated Zero Trust identity verification.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#installation","title":"Installation","text":"<p>Helm installation of Pomerium is no longer recommended for new deployments, instead use Manifests based deployment (using Kustomize) which ensures the most current Zero Trust architecture is used. This natively handles WebSockets and media streaming without the limitations found in standard NGINX setups.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#prerequisites","title":"Prerequisites","text":"<p>Besides Kubernetes v1.19.0 or higher,  Pomerium Prerequisites recomends a domain space and TLS certificates. Since the cluster is already running cert-manager v1.17.2 and every subdomain of <code>very-very-dark-gray.top</code> is already pointing to the public IP address, it is not necessary and generally discouraged to use <code>mkcert</code> to create locally trusted certificates.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#deploy-ingress-controller","title":"Deploy <code>ingress-controller</code>","text":"<p>The official documentation recommends to install Install Pomerium by deploying the (Kustomize-based) manifests directly from their GitHub repository:</p> <pre><code>$ kubectl apply -k github.com/pomerium/ingress-controller/config/default\\?ref=v0.31.0\n</code></pre> <p>To later make modifications to the deployment (add Pebble storage for persistance) the repository can be cloned locally and pinned to the latest state release:</p> <pre><code>$ cd ~/src\n$ git clone https://github.com/pomerium/ingress-controller.git\n$ cd ingress-controller/\n$ git checkout v0.31.3\n</code></pre> <p>Deploy Pomerium using its default configuration to start with:</p> <pre><code>$ cd config/default/\n$ kubectl apply -k .\n# Warning: 'commonLabels' is deprecated. Please use 'labels' instead. Run 'kustomize edit fix' to update your Kustomization automatically.\nnamespace/pomerium created\ncustomresourcedefinition.apiextensions.k8s.io/policyfilters.gateway.pomerium.io created\ncustomresourcedefinition.apiextensions.k8s.io/pomerium.ingress.pomerium.io created\nserviceaccount/pomerium-controller created\nserviceaccount/pomerium-gen-secrets created\nclusterrole.rbac.authorization.k8s.io/pomerium-controller created\nclusterrole.rbac.authorization.k8s.io/pomerium-gen-secrets created\nclusterrolebinding.rbac.authorization.k8s.io/pomerium-controller created\nclusterrolebinding.rbac.authorization.k8s.io/pomerium-gen-secrets created\nservice/pomerium-metrics created\nservice/pomerium-proxy created\ndeployment.apps/pomerium created\njob.batch/pomerium-gen-secrets created\ningressclass.networking.k8s.io/pomerium created\n</code></pre> <p>Pomerium pod is running but not yet ready; it appears to be unhealthy:</p> <pre><code>$ kubectl get pods -n pomerium\nNAME                         READY   STATUS      RESTARTS   AGE\npomerium-75d45cf7f8-c9tlx    0/1     Running     0          4m28s\npomerium-gen-secrets-8sx7m   0/1     Completed   0          4m28s\n\n$ kubectl describe pod pomerium-75d45cf7f8-c9tlx -n pomerium | tail\n\nEvents:\n  Type     Reason     Age                  From               Message\n  ----     ------     ----                 ----               -------\n  Normal   Scheduled  4m2s                 default-scheduler  Successfully assigned pomerium/pomerium-75d45cf7f8-c9tlx to octavo\n  Normal   Pulling    4m2s                 kubelet            Pulling image \"pomerium/ingress-controller:v0.31.3\"\n  Normal   Pulled     3m57s                kubelet            Successfully pulled image \"pomerium/ingress-controller:v0.31.3\" in 5.39s (5.39s including waiting). Image size: 101804642 bytes.\n  Normal   Created    3m57s                kubelet            Created container: pomerium\n  Normal   Started    3m57s                kubelet            Started container pomerium\n  Warning  Unhealthy  2s (x16 over 3m47s)  kubelet            Startup probe failed: Get \"http://10.244.0.167:28080/startupz\": dial tcp 10.244.0.167:28080: connect: connection refused\n</code></pre> <p>This <code>connection refused</code> on port 28080 during the startup probe indicates the Pomerium Ingress Controller is waiting for its mandatory global configuration before it starts its health check listeners. The Pomerium Ingress Controller is designed to fail its health probes until it has a valid Pomerium CRD applied, which is not ready yet.</p> <p>Until then, the controller is running in a loop waiting for the global settings that define its Identity Provider and internal secrets.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#pebble-storage","title":"Pebble Storage","text":"<p>System requirements for Pomerium include PostgreSQL 11 or higher, but this is not necessary for a single-node cluster. Instead, a self-healing file-based databroker that embeds Pebble (the storage engine behind CockroachDB) can be used to provide persistence across restarts without the complexity of managing a full PostgreSQL instance.</p> <p>Using a Manifest-based deployment, the standard Deployment can be modified into a StatefulSet to attach a <code>PersistentVolume</code> to the Pomerium Databroker. As has been done for previous deployments, the <code>PersistentVolume</code> will be using a <code>hostPath</code> pointing to a local directory owned by the user <code>pomerium</code> runs as (65532 as revealed by <code>ps aux | grep -i pomerium</code>).</p> <p>Create the local directory with the appropriate permissions: </p> <pre><code># mkdir -p /home/k8s/pomerium/data\n# chown -R 65532:65532 /home/k8s/pomerium/\n# chmod -R 750 /home/k8s/pomerium/\n# ls -laR /home/k8s/pomerium/\n/home/k8s/pomerium:\ntotal 0\ndrwxr-x--- 1 65532 root    8 Dec 18 23:06 .\ndrwxr-xr-x 1 root  root  478 Dec 18 23:06 ..\ndrwxr-x--- 1 65532 65532   0 Dec 18 23:06 data\n\n/home/k8s/pomerium/data:\ntotal 0\ndrwxr-x--- 1 65532 65532 0 Dec 18 23:06 .\ndrwxr-x--- 1 65532 root  8 Dec 18 23:06 ..\n</code></pre> <p>Create the Local Storage manifests and <code>kustomization.yaml</code> in a new <code>pomerium-overlay</code> directory and patch the Deployment to mount the volume:</p> <code>pomerium-overlay/pomerium-storage.yaml</code> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pomerium-pv-data\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 2Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /home/k8s/pomerium/data\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pomerium-pvc-data\n  namespace: pomerium\nspec:\n  storageClassName: manual\n  volumeName: pomerium-pv-data\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 2Gi\n</code></pre> <code>pomerium-overlay/kustomization.yaml</code> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - ../../../ingress-controller/config/default\n  - pomerium-storage.yaml\n\npatches:\n  - target:\n      kind: Deployment\n      name: pomerium\n    patch: |-\n      - op: add\n        path: /spec/template/spec/containers/0/volumeMounts/-\n        value:\n          name: pomerium-data\n          mountPath: /data\n      - op: add\n        path: /spec/template/spec/volumes/-\n        value: \n          name: pomerium-data\n          persistentVolumeClaim:\n            claimName: pomerium-pvc-data\n      - op: replace\n        path: /spec/strategy\n        value:\n          type: Recreate  # Terminates old pods before starting new ones\n      - op: add\n        path: /spec/template/spec/containers/0/lifecycle\n        value:\n          preStop:\n            exec:\n              # Gives the process 5 seconds to flush Pebble logs and release locks\n              command: [\"/bin/sh\", \"-c\", \"sleep 5\"]\n</code></pre> <p>To avoid file lock conflicts in the Pebble database across pods, lines 25-27 set the <code>Recreate</code> strategy so that the running pod are shut down before starting a new one. Otherwise, Kubernetes defaults to a <code>RollingUpdate</code> strategy which starts a new pod before the old one is terminated, leading to a deadlock situation where the old pod still has the lock on the database, which keeps the new pod stuck as  <code>ContainersNotReady</code>.</p> <p>Lines 29-34 force graceful shutdown to let the old pod flush Pebble logs and release locks.</p> <p>There are two different checks to make before applying this deployment:</p> <ol> <li> <p>Run <code>kubectl kustomize pomerium-overlay</code> to inspect that the generated deployment     manifest does replace the correct values.</p> <p>Tip</p> <p>This step is particularly useful to confirm the right amount of <code>../</code> has been added in front of <code>ingress-controller/config/default</code> depending on where files are.</p> </li> <li> <p>Run <code>kubectl apply -k pomerium-overlay --dry-run=client</code> to verify that <code>kubectl</code>     finds no errors in the generated manifest.</p> </li> </ol> <p>Once the above manifests are confirmed valid, patch the deployment with it:</p> <pre><code>$ kubectl apply -k pomerium-overlay \nnamespace/pomerium unchanged\ncustomresourcedefinition.apiextensions.k8s.io/policyfilters.gateway.pomerium.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/pomerium.ingress.pomerium.io unchanged\nserviceaccount/pomerium-controller unchanged\nserviceaccount/pomerium-gen-secrets unchanged\nclusterrole.rbac.authorization.k8s.io/pomerium-controller unchanged\nclusterrole.rbac.authorization.k8s.io/pomerium-gen-secrets unchanged\nclusterrolebinding.rbac.authorization.k8s.io/pomerium-controller unchanged\nclusterrolebinding.rbac.authorization.k8s.io/pomerium-gen-secrets unchanged\nservice/pomerium-metrics unchanged\nservice/pomerium-proxy unchanged\npersistentvolume/pomerium-pv-data created\npersistentvolumeclaim/pomerium-pvc-data created\ndeployment.apps/pomerium configured\njob.batch/pomerium-gen-secrets unchanged\ningressclass.networking.k8s.io/pomerium unchanged\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#global-pomerium-settings","title":"Global Pomerium Settings","text":""},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#pomerium-crd","title":"Pomerium CRD","text":"<p>Define the global settings in <code>pomerium-settings.yaml</code>, with <code>spec.storage.file.path</code> pointing to storage defined above (<code>/data/databroker</code>) and <code>spec.authenticate.url</code> pointing to the cluster's real domain to use the self-hosted identity-aware proxy, instead of the domain https://authenticate.pomerium.app which is the endpoint for Pomerium Zero (their SaaS control plane):</p> <code>pomerium-settings.yaml</code> <pre><code>apiVersion: ingress.pomerium.io/v1\nkind: Pomerium\nmetadata:\n  name: global\n  namespace: pomerium\nspec:\n  secrets: pomerium/bootstrap\n  authenticate:\n    url: https://authenticate.very-very-dark-gray.top:8443\n  storage:\n    file:\n      path: /data/databroker\n</code></pre> <p>Using a non-standard port like 8443 mostly works, if port 443 is taken.</p> <p>Applying the settings does not restart the <code>pomerium</code> pods, but it is now <code>READY</code>:</p> <pre><code>$ kubectl apply -f pomerium-settings.yaml\npomerium.ingress.pomerium.io/global created\n\n$ kubectl get pods -n pomerium\nNAME                         READY   STATUS      RESTARTS      AGE\npomerium-58bdd56668-6p5q5    1/1     Running     2 (16m ago)   36m\npomerium-gen-secrets-8sx7m   0/1     Completed   0             163m\n</code></pre> <p>The Pomerium Proxy service is now running in the cluster:</p> <pre><code>$ kubectl describe pomerium\nName:         global\nNamespace:    \nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  ingress.pomerium.io/v1\nKind:         Pomerium\nMetadata:\n  Creation Timestamp:  2025-12-18T23:53:05Z\n  Generation:          1\n  Resource Version:    51753377\n  UID:                 288ab14d-3cdc-490c-9650-cd09a463a72b\nSpec:\n  Authenticate:\n    URL:    https://authenticate.very-very-dark-gray.top\n  Secrets:  pomerium/bootstrap\n  Storage:\n    File:\n      Path:  /data/databroker\nStatus:\n  Settings Status:\n    Observed At:          2025-12-18T23:53:06Z\n    Observed Generation:  1\n    Reconciled:           true\nEvents:\n  Type     Reason      Age   From                                     Message\n  ----     ------      ----  ----                                     -------\n  Normal   Updated     17s   bootstrap pod/pomerium-58bdd56668-6p5q5  config updated\n</code></pre> <p>The <code>connection refused</code> on port 28080 errors are still present in the <code>Events</code> section, but they are now only old entries, and no more errors have been produced in the last 6 minutes:</p> <pre><code>$ kubectl describe pod pomerium-58bdd56668-6p5q5 -n pomerium | tail\n\nEvents:\n  Type     Reason     Age                  From               Message\n  ----     ------     ----                 ----               -------\n  Normal   Scheduled  31m                  default-scheduler  Successfully assigned pomerium/pomerium-58bdd56668-6p5q5 to octavo\n  Normal   Pulled     11m (x3 over 43m)    kubelet            Container image \"pomerium/ingress-controller:v0.31.3\" already present on machine\n  Normal   Created    11m (x3 over 43m)    kubelet            Created container: pomerium\n  Normal   Started    11m (x3 over 43m)    kubelet            Started container pomerium\n  Normal   Killing    11m (x2 over 33m)    kubelet            Container pomerium failed startup probe, will be restarted\n  Warning  Unhealthy   6m (x102 over 43m)  kubelet            Startup probe failed: Get \"http://10.244.0.169:28080/startupz\": dial tcp 10.244.0.169:28080: connect: connection refused\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#certifiate-for-the-auth-endpoint","title":"Certifiate for the auth endpoint","text":"<p>Without a dedicated certificate for the <code>authenticate.very-very-dark-gray.top</code> domain, users will see a security error when redirected here from other subdomains, because Pomerium\u2019s Authenticate service is a separate internal \"virtual route\" that requires its own certificate to be explicitly defined in your global settings.</p> <p>Generate a separate certificate for the authenticate domain and link it to the Pomerium global configuration CRD. First, create the certificate from <code>auth-certificate.yaml</code></p> <code>auth-certificate.yaml</code> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: pomerium-auth-cert\n  namespace: pomerium\nspec:\n  secretName: pomerium-auth-tls\n  issuerRef:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\n  commonName: authenticate.very-very-dark-gray.top\n  dnsNames:\n    - authenticate.very-very-dark-gray.top\n</code></pre> <p>Apply this manifest to crete the certificate:</p> <pre><code>$ kubectl apply -f auth-certificate.yaml\n</code></pre> <p>Wait for it to show <code>READY: True</code> via <code>kubectl get certificate -n pomerium</code> and then link the Certificate to Pomerium Settings by updating the Pomerium CRD to reference it:</p> <code>pomerium-settings.yaml</code> <pre><code>apiVersion: ingress.pomerium.io/v1\nkind: Pomerium\nmetadata:\n  name: global\n  namespace: pomerium\nspec:\n  secrets: pomerium/bootstrap\n  authenticate:\n    url: https://authenticate.very-very-dark-gray.top:8443\n  certificates:\n    - pomerium/pomerium-auth-tls\n  storage:\n    file:\n      path: /data/databroker\n</code></pre> <p>And apply the updated settings: </p> <pre><code>kubectl apply -f pomerium-settings.yaml\npomerium.ingress.pomerium.io/global configured\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#identity-provider-idp","title":"Identity Provider (IdP)","text":"<p>Before configuring the Ingress, store credentials for the chosen provider. In this case, start with Google to authenticate @gmail.com users, create an OAuth 2.0 Client ID for Web Server Applications, obtain the client ID and secret from and store them in <code>google-idp-secret.yaml</code>:</p> <code>google-idp-secret.yaml</code> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: idp-secret\n  namespace: pomerium\ntype: Opaque\nstringData:\n  client_id: \"your-google-client-id.apps.googleusercontent.com\"\n  client_secret: \"GOCSPX-your-google-client-secret\"\n</code></pre> <p>Note</p> <p>In the Google Cloud Console, the OAuth consent screen should be set to External (Google Auth Platform &gt; Audience) when using standard @gmail.com accounts. While the app status is Testing there is a 7-day token expiration and a 100-user limit. To increase or remove those limits, Publish app in the Console, although this is only necessary when requesting sensitive scopes beyond name, email address, and user profile.    </p> <p>Configure Pomerium to recognize Google as the provider and set a generous 30-day expiration for sessions:</p> <code>pomerium-settings.yaml</code> <pre><code>apiVersion: ingress.pomerium.io/v1\nkind: Pomerium\nmetadata:\n  name: global\n  namespace: pomerium\nspec:\n  cookie:\n    expire: 720h\n  secrets: pomerium/bootstrap\n  authenticate:\n    url: https://authenticate.very-very-dark-gray.top:8443\n  certificates:\n    - pomerium/pomerium-auth-tls\n  identityProvider:\n    provider: google\n    secret: pomerium/idp-secret\n  storage:\n    file:\n      path: /data/databroker\n</code></pre> <p>Apply the above in this order, so the secret is available for Pomerium:</p> <pre><code>$ kubectl apply -f google-idp-secret.yaml\nsecret/idp-secret created\n\n$ kubectl apply -f pomerium-settings.yaml\npomerium.ingress.pomerium.io/global configured\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#test-with-verify","title":"Test with Verify","text":"<p>To verify that the authentication and authorization flow works, use Pomerium's Test Service with an initial policy to allow only uses with @gmail.com addresses:</p> <code>verify-service.yaml</code> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  namespace: pomerium\n  name: verify\n  labels:\n    app: verify\n    service: verify\nspec:\n  ports:\n    - port: 8000\n      targetPort: 8000\n      name: http\n  selector:\n    app: pomerium-verify\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: pomerium\n  name: verify\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pomerium-verify\n  template:\n    metadata:\n      labels:\n        app: pomerium-verify\n    spec:\n      containers:\n        - image: docker.io/pomerium/verify\n          imagePullPolicy: IfNotPresent\n          name: httpbin\n          ports:\n            - containerPort: 8000\n              protocol: TCP\n              name: http\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  namespace: pomerium\n  name: verify\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.pomerium.io/pass_identity_headers: true\n    ingress.pomerium.io/policy: |\n      - allow:\n          or:\n            - email:\n                ends_with: \"@gmail.com\"\nspec:\n  ingressClassName: pomerium\n  rules:\n    - host: 'verify.very-very-dark-gray.top'\n      http:\n        paths:\n          - pathType: Prefix\n            path: /\n            backend:\n              service:\n                name: verify\n                port:\n                  number: 8000\n  tls:\n    - secretName: tls-secret-verify\n      hosts:\n        - verify.very-very-dark-gray.top\n</code></pre> <p>Note</p> <p>The <code>ingress.pomerium.io/allowed_domains</code> annotation and domain-based policies like <code>'domain: is: gmail.com'</code> fail with @gmail.com users because Pomerium's domain check looks for an OIDC claim called hd (Hosted Domain) and @gmail.com accounts do not have an hd claim, this is only present for Google Workspace/Enterprise accounts.</p> <p>Apply this manifest to start the verify service:</p> <pre><code>$ kubectl apply -f verify-service.yaml\nservice/verify created\ndeployment.apps/verify created\ningress.networking.k8s.io/verify created\n</code></pre> <p>Visiting https://verify.very-very-dark-gray.top:8443 now redirects to the Google login page, which redirects back to the application after the user logs in and agrees to disclose their identity to the application. When the user is authenticated with an email address that ends with <code>@gmail.com</code> the application grants access, although it shows an Identity verification failed due to other issues:</p> <p></p> about Identity verification failed error page. <p>When accessing Pomerium through a non-standard HTTPS port (e.g. 8443), this page shows an \"Identity verification failed\" error:</p> <p></p> <p>This is because the app is unable to fetch the JWKS file due to the application assuming it must be reachable on the standard HTTPS port 443. The file is available on port 8443 and this can be verified using <code>wget</code> from inside the <code>verify</code> pod:</p> <pre><code>$ kubectl exec -it deployment/verify -n pomerium -- wget -qO- \\\n  https://authenticate.very-very-dark-gray.top:8443/.well-known/pomerium/jwks.json\nwget: note: TLS certificate validation not implemented\n{\"keys\":[{\"use\":\"sig\",\"kty\":\"EC\",\"kid\":\"...\",\"crv\":\"P-256\",\"alg\":\"ES256\",\"x\":\"...\",\"y\":\"...\"}]\n</code></pre> <p>If the user is denied access, e.g. logged in with an email address on a different domain, then the result is a clear 403 Forbidden page:</p> <p></p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#additional-settings","title":"Additional Settings","text":"<p>To replace the existing Nginx-based <code>Ingress</code> for currently running applications, additional settings are necessary for specific purposes.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#access-control-for-users","title":"Access Control for Users","text":"<p>To restrict access to a concrete list of users by their email address, replace the above <code>allow</code> with a list of <code>'email: is: user@gmail.com'</code> conditions under the <code>or</code> condition:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: audiobookshelf-ingress\n  namespace: audiobookshelf\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.pomerium.io/pass_identity_headers: true\n    ingress.pomerium.io/policy: |\n      - allow:\n          or:\n            - email:\n                is: \"alice@gmail.com\"\n            - email:\n                is: \"bob@gmail.com\"\n</code></pre> <p>Warning</p> <p>The YAML parser will fail when multiple operator ending with a colon (<code>:</code>) are put on a single line (<code>email: is: \"bob@gmail.com\"</code>). The correct syntax is to either put each operator ending with a colon (<code>:</code>) on its own line (as seen above), or to wrap the entire condition in single quotes (<code>'email: is: \"bob@gmail.com\"'</code>).</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#api-access","title":"API access","text":"<p>Each service that exposes an API, authenticated or not, needs an additional <code>allow</code> policy to allow traffic directed to the API, so that it bypasses Pomerium authentication and relies instead on the API's own authentication. This policy should match only requests to the APIs used by applications other than web frontends, to keep Web frontends protected by Pomerium while allowing traffic to the APIs from mobile apps, etc.</p> <p>Finding the required <code>path</code> prefixes for each service is best done by consulting their API reference. In cases where that turns out to be not enough, Pomerium logs can be filtered for specific lines to find URL paths for those requests that are not allowed:</p> <pre><code>#!/bin/bash\n# Extrat URL paths for requests going through Pomerium\n\npod=$(kubectl get pods -n pomerium | grep \"^pomerium-.*Running\" | awk '{print $1}')\necho $pod\n\nkubectl get ingress -A | awk 'NR &gt; 1 {print $1, $2, $3, $4}' | \\\nwhile IFS=$' ' read -r namespace name class host; do\n  if [[ \"$class\" != \"pomerium\" ]]; then continue; fi\n  echo  \"requests to ${host} via ${name}: \"\n  kubectl logs \"${pod}\" -n pomerium \\\n  | grep \"authority.*${host}\" \\\n  | grep -v '\"response-code\":200' \\\n  | grep -oE '\"path\":\"[^\"]+\"' \\\n  | cut -f4 -d'\"' \\\n  | grep -oE '^/[^/]+' \\\n  | sort -u\ndone\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#audiobookshelf","title":"Audiobookshelf","text":"<p>The Audiobookshelf API reference turned out not so convenient to find all the URL paths needed for the  Audiobookshelf Android app, the above method of filtering Pomerium logs was needed to find the following prefixes:</p> <pre><code>metadata:\n  annotations:\n    ...\n    ingress.pomerium.io/policy: |\n      - allow:\n          or:\n            - http_path:\n                starts_with: \"/api\"\n            - http_path:\n                starts_with: \"/auth/refresh\"\n            - http_path:\n                starts_with: \"/audiobookshelf/socket.io\"\n            - http_path:\n                starts_with: \"/hls\"\n            - http_path:\n                starts_with: \"/login\"\n            - http_path:\n                starts_with: \"/public\"\n            - http_path:\n                starts_with: \"/socket.io\"\n            - http_path:\n                starts_with: \"/status\"\n      - allow:\n          or:\n            - email:\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#firefly-iii","title":"Firefly III","text":"<p>Firefly III includes a REST-based JSON API all under a single <code>/v1</code> prefix (full reference at https://api-docs.firefly-iii.org/). The application itself is not actually in use, but that single prefix is all that would be necessary to <code>allow</code> if it was to be used, e.g. from a Home Assistant integration.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#grafana","title":"Grafana","text":"<p>Grafana HTTP API reference suggests the entire API is under <code>/api</code>, including authentication.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#influxdb","title":"InfluxDB","text":"<p>InfluxDB Authentication being enabled for all exposed endpoints, similar rules are added to bypass Pomerium authentication for the relevant API paths. There is no policy to allow human users because this API is not meant to be accessed directly by users from browsers.</p> <p>All the legacy InfluxDB 1.x HTTP endpoints are under the following prefixes:</p> <pre><code>metadata:\n  annotations:\n    ...\n    ingress.pomerium.io/policy: |\n      - allow:\n          or:\n            - http_path:\n                starts_with: \"/debug\"\n            - http_path:\n                starts_with: \"/ping\"\n            - http_path:\n                starts_with: \"/query\"\n            - http_path:\n                starts_with: \"/write\"\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#komga","title":"Komga","text":"<p>The Komga eBook library exposes several API endpoints to make books accessible to eReaders and these all need to be allowed without enforcing the Gmail-based authentication:</p> <pre><code>metadata:\n  annotations:\n    ...\n    ingress.pomerium.io/policy: |\n      - allow:\n          or:\n            - http_path:\n                starts_with: \"/api\"\n            - http_path:\n                starts_with: \"/kobo\"\n            - http_path:\n                starts_with: \"/opds\"\n            - http_path:\n                starts_with: \"//sse\"\n      - allow:\n          or:\n            - email:\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#home-assistant","title":"Home Assistant","text":"<p>Home Assistant hosts a WebSocket API at <code>/api/websocket</code> and a RESTful API at <code>/api</code>, which needs to be allowed for the Home Assistant mobile app. In addition to that, Home Assistant Authentication API also needs to be allowed for the Home Assistant mobile app to be able to directly authenticate against the service.</p> <pre><code>metadata:\n  annotations:\n    ...\n    ingress.pomerium.io/policy: |\n      - allow:\n          or:\n            - http_path:\n                starts_with: \"/api\"\n            - http_path:\n                starts_with: \"/auth\"\n      - allow:\n          or:\n            - email:\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#jellyfin","title":"Jellyfin","text":"<p>To keep the Web UI locked by Pomerium while allowing traffic from mobile apps in, allow API paths to bypass authentication; the Jellyfin API uses a few path prefixes:</p> <p>Jellyfin API reference can be downloaded from https://api.jellyfin.org/ as a JSON file and API <code>path</code> prefixes can be extracted from the keys in its <code>paths[]</code> object. This way, the full list of request path prefixes can be extracted easily:</p> <pre><code>#!/bin/bash\n# Extrat request path prefixes from the Jellyfin API reference.\njson=$1\n\njq -r '.paths | keys[]' ${json} \\\n| grep -oE '^/[^/]+' \\\n| sort -u \\\n| while read dir; do\ncat &lt;&lt; ___EOF___\n                - http_path:\n                    starts_with: \"${dir}\"\n___EOF___\ndone\n</code></pre> <p>The list of prefixes may need to be updated when the API changes.</p> <pre><code>metadata:\n  annotations:\n    ...\n    ingress.pomerium.io/policy: |\n      - allow:\n          or:\n            - http_path:\n                starts_with: \"/api\"\n            - http_path:\n                starts_with: \"/Users/Public\"\n            - http_path:\n                starts_with: \"/System/Info/Public\"\n      - allow:\n          or:\n            - email:\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#navidrome","title":"Navidrome","text":"<p>To keep Navidrome Web UI behind Pomerium while allowing traffic from mobile apps (e.g. my favorite one, Ultrasonic), allow Subsonic API requests to bypass authentication; this requires a single path prefix:</p> <pre><code>metadata:\n  annotations:\n    ...\n    ingress.pomerium.io/policy: |\n      - allow:\n          or:\n            - http_path:\n                starts_with: \"/rest/\"\n      - allow:\n          or:\n            - email:\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#https-backends","title":"HTTPS backends","text":"<p>Some applications provide only an HTTPS endpoint and come with self-signed certificates, forcing reverse proxies to use that protocoal and skip TLS certificate validation. For these, Pomerium has policies equivalents to those NGinx provides for the same purpose:</p> <pre><code>metadata:\n  annotations:\n    ingress.pomerium.io/policy: |\n      ...\n    ingress.pomerium.io/secure_upstream: true # Support HTTPS backend\n    ingress.pomerium.io/tls_skip_verify: true # Skip verification of backend certs\n</code></pre> <p>Previously, these services used equivalent Nginx annotations to the same effect:</p> <pre><code>metadata:\n  annotations:\n    ingress.pomerium.io/policy: |\n      ...\n    nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"false\"\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#streaming","title":"Streaming","text":"<p>Applications that transfer large files, such as audio/video streaming or file-sharing applications, will require (or largely benefit from) higher CPU and Memory limits than the default. The following should be enough for 4K video and multiple concurrent streams; this can be added to the <code>kustomization.yaml</code> file used above to configure Pebble storage:</p> <code>pomerium-overlay/kustomization.yaml</code> <pre><code>  - target:\n      kind: Deployment\n      name: pomerium\n    patch: |-\n      - op: replace\n        path: /spec/template/spec/containers/0/resources\n        value:\n          requests:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n          limits:\n            cpu: \"1000m\"\n            memory: \"1Gi\"\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#websockets","title":"WebSockets","text":"<p>For the best experience with audio and video streaming applications, <code>Ingress</code> for streaming services should have the <code>timeout</code> disabled and explicitly allow \"upgrades\" like WebSockets:</p> <pre><code>metadata:\n  annotations:\n    ingress.pomerium.io/policy: |\n      ...\n    # Keeps the \"pipe\" open even during quiet parts of a stream.\n    ingress.pomerium.io/idle_timeout: 0s\n    # Prevents long-running streams (like a 2-hour movie) from timing out.\n    ingress.pomerium.io/timeout: 0s\n    # Allow requests to be upgraded to WebSockets\n    ingress.pomerium.io/allow_websockets: true\n</code></pre> <p>While NGINX handles WebSocket upgrades somewhat permissively, Pomerium (Envoy) is stricter regarding the <code>Connection</code> and <code>Upgrade</code> headers, especially when multiple proxies or non-standard ports are involved. Pomerium requires an explicit setting to allow Upgrades (like WebSockets) for specific routes; this is handled via the <code>allow_websockets</code> annotation.</p> <p>Enabling WebSockets is a recommended best practice for most modern media and streaming applications, and even some non-media applications, including:</p> <ul> <li>Audiobookshelf uses WebSockets to for almost all of its \"live\" functionality.</li> <li>Jellyfin uses WebSockets extensively for several core features.</li> <li>Headless Steam and other gaming streaming protocols that handle low-latency video     and input (Steam Remote Play, Moonlight/Sunshine, etc.) typically use WebSockets or     specialized protocols to exchange encryption keys and handle input (mouse/keyboard)     before the heavy UDP video stream begins.</li> <li>Home Assistant entire dashboard UI relies on a single persistent WebSocket.</li> <li>Navidrome and the Subsonic API are strictly HTTP-based and do not use WebSockets,     but enabling this future-proofs against potential future real-time extensions.</li> </ul>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#convert-ngnix-ingress-to-pomerium","title":"Convert Ngnix <code>Ingress</code> to Pomerium","text":"<p>Copy the current <code>Ingress</code> and make the following changes:</p> <ul> <li> <p>Change the <code>ingressClassName</code> to <code>pomerium</code>.</p> </li> <li> <p>Change the <code>name</code> so that both can temporarily coexist in the same namespace.</p> </li> <li> <p>Remove all (see note below) <code>annotations</code> except <code>cert-manager.io/cluster-issuer</code>.</p> </li> <li> <p>Add <code>ingress.pomerium.io/pass_identity_headers: true</code> to the <code>annotations</code> for     services that will be restricted and accessed by human users.</p> </li> <li> <p>Add <code>ingress.pomerium.io/preserve_host_header: true</code> to the <code>annotations</code> so that     Pomerium does not rewrite the <code>Host</code> header. This is required by certain applications     (e.g. Grafana) because their backend checks request headers to enforce the     same-origin policy.</p> <p>Many applications that rely on WebSockets will also need this, because they will reject the WebSocket handshake if the <code>Origin</code> sent by the browser does not match the <code>Host</code> header seen by the server.</p> <p>The UniFi Network application also requires this annotation, without it the login page fails with a generic error message asking to simply \"try again later\".</p> </li> <li> <p>Add <code>annotations</code> for applications that rely on HTTPS backends,     streaming or WebSockets.</p> </li> <li> <p>Add one of</p> <ul> <li> <p><code>ingress.pomerium.io/policy</code> to the <code>annotations</code> for services that require     access controls for human users or      API access.</p> </li> <li> <p><code>ingress.pomerium.io/allow_public_unauthenticated_access: true</code> to the     <code>annotations</code> for services do not require access to be controlled by Pomerium.</p> <p>Warning</p> <p>Use with caution, e.g. only with Cloudflare Tunnels, as it enables unrestricted Public Access.</p> </li> </ul> </li> </ul> Why remove all annotations other than <code>cert-manager.io/cluster-issuer</code>? <p>The following <code>annotations</code> not under <code>nginx.ingress.*</code> are unnecessary for Pomerium:</p> <ul> <li> <p><code>acme.cert-manager.io/http01-edit-in-place: true</code> is an NGINX-specific legacy     annotation that forces <code>cert-manager</code> to add the ACME challenge path directly to     the existing <code>Ingress</code> instead of creating a temporary challenge ingress.     Pomerium does not need this because the Pomerium Ingress Controller is designed     to watch for the temporary Ingresses created by <code>cert-manager</code> during the HTTP-01     challenge and automatically merges those challenge paths into its routing table.     Unlike early versions of NGINX, Pomerium's underlying Envoy engine can handle     dynamic routing updates (adding the <code>/.well-known/acme-challenge/</code> path) without     reloading or needing to edit the Ingress in place.</p> </li> <li> <p><code>cert-manager.io/issue-temporary-certificate: true</code> tells <code>cert-manager</code> to     issue a Fake Let's Encrypt or self-signed certificate immediately, so that the     <code>Ingress</code> has something to serve while waiting for the real Let's Encrypt     certificate. Pomerium does not need this because it has a built-in Fallback CA     to serve its own certificate (the \"Pomerium PSK CA\") when a referenced TLS secret     is missing or not yet ready. Once a new TLS secret is ready, Pomerium uses     Envoy's SDS (Secret Discovery Service) to hot-swap certificates; as soon as     <code>cert-manager</code> finishes the challenge and populates the secret, Pomerium detects     the update and swaps the internal certificate for the real Let's Encrypt     certificate without dropping active connections or requiring a temporary     placeholder.</p> </li> <li> <p><code>ingress.kubernetes.io/force-ssl-redirect: true</code> is unnecessary with Pomerium     because it automatically enforces HTTPS for all managed routes.</p> </li> </ul> <p>The following <code>annotations</code> under <code>nginx.ingress.*</code> are made unnecessary by Pomerium:</p> <ul> <li> <p><code>nginx.ingress.kubernetes.io/proxy-buffer-size: 16k</code> is not necessary because     Envoy (the engine inside Pomerium) defaults to 64 KB for maximum request headers     to prevent Header too large errors, especially when handling large cookies or     OIDC tokens.</p> </li> <li> <p><code>nginx.ingress.kubernetes.io/websocket-services</code> is entirely unnecessary because     Pomerium handles WebSockets automatically for all routes. Unlike NGINX, Pomerium     treats all HTTP traffic as potentially upgradable to WebSockets.</p> </li> <li> <p><code>nginx.ingress.kubernetes.io/whitelist-source-range: 10.244.0.0/16</code> is     unnecessary and generally unsupported by the Pomerium Ingress Controller.     NGINX relies on the assumption that if a request comes from 10.244.0.0/16 (the     internal pod network), it is \"safe\", while Pomerium requires that every request     be authenticated regardless of its origin.</p> </li> </ul>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#cloudflare-tunnels","title":"Cloudflare Tunnels","text":"<p>In addition to the above changes, for each <code>Ingress</code> behind a Cloudflare Tunnel, Edit the corresponding Connectors in https://one.dash.cloudflare.com/ (under Networks) to route traffic to port 443 to the <code>LoadBalancer</code> IP of Pomerium, instead of that of Nginx.</p> <p>Changing the <code>name</code> is not necessary because there is no point in having multiple <code>Ingress</code> in the same namespace; traffic is only ever routed to the one <code>LoadBalancer</code> IP set as the target for the corresponding Connector above. If the name is changed to be consistent with other Pomerium <code>Ingress</code> (e.g. <code>&lt;service-name&gt;-pomerium-ingress</code>), delete the old one since it won't be used:</p> <pre><code>$ kubectl delete ingress ddns-updater-nginx\ningress.networking.k8s.io \"ddns-updater-nginx\" deleted\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#update-kustomized-ingress","title":"Update Kustomized <code>Ingress</code>","text":"<p>Home Assistant was deployed in several clusters (<code>alfred</code> and <code>lexicon</code>, later replaced by <code>octavo</code>) with very similar manifests by using <code>kustomize</code>, so the above changes may need to be applied in different files depending on the specific structure of the manifests.</p> <p>For the current Home Assistant manifests, all the above changes need to be applied in the <code>base/ingress.yaml</code> file, so the same migration is applied to all Home Assistant  instances, while each cluster's <code>kustomization.yaml</code> needs only to reflect the change in the <code>name</code> of those <code>Ingress</code> that have their <code>ingressClassName</code> changed from <code>nginx</code> to <code>pomerium</code>.</p> <p>For now, all Home Assistant instances are accessed exclusively through Cloudflare Tunnels only, so they need only <code>ingress.pomerium.io/allow_public_unauthenticated_access: true</code> added to their <code>annotations</code>.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#kustomize-per-service-acls","title":"Kustomize per-service ACLs","text":"<p>Most of the Additional Settings above will be fixed for each application, with only sporadic updates, but the set of people to be added as authorized users is subject to change over time, so it would be convenient to have those centralized in a single file for all the applications.</p> <p>This can be done by leveraging <code>kustomize</code> to patch the <code>ingress.pomerium.io/policy</code> annotation for every application from a single file, so that authorizing a new user to access a number of applications can be quickly done by updating only that one file.</p> <p>The most reliable way to achieve a setup with a global policy (with 1 admin user who is authorized to access everything) and per-application overrides in Kustomize is to use a base resource for the global policy and a patch that targets each application to replace its policy to override the list of authorized users and/or API paths.</p> <p>The follow shows, as extreme examples, the least and most complex of the many applications currently running in <code>octavo</code>, along with a global policy that authorizes one <code>admin</code> user access to all applications:</p> <ul> <li><code>audiobookshelf</code> requires policy overrides for both API access and     human users, as well as <code>annotations</code> for      streaming and WebSockets.</li> <li><code>dashboard</code> needs only <code>annotations</code> for HTTPS backends.</li> <li><code>ddns-updater</code> needs none of that; in fact it needs not any access control because     it is only accessible through a Cloudflare Tunnel.</li> </ul> <p><code>audiobookshelf</code> is the only one that needs a patch on its <code>ingress.pomerium.io/policy</code> annotation, the others could be taken out of this <code>kustomize</code> setup entirely, but it also seems convenient to have all Pomerium <code>Ingress</code> manifests in a single place and it makes them also easier to update should they need a patch later.</p> <code>pomerium-ingress/kustomization.yaml</code> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - audiobookshelf.yaml\n  - dashboard.yaml\n  - ddns-updater.yaml \n\npatches:\n  # Global policy (targeting all Ingress)\n  - target:\n      kind: Ingress\n    patch: |-\n      apiVersion: networking.k8s.io/v1\n      kind: Ingress\n      metadata:\n        name: all-ingresses # Kustomize uses the target selector, this name is a placeholder\n        annotations:\n          ingress.pomerium.io/policy: |\n            - allow:\n                or:\n                  - email:\n                      is: \"admin-user@gmail.com\"\n\n  # Override for audiobookshelf (targeting only its Ingress)\n  # This patch must come after the global policy, to override it.\n  - target:\n      kind: Ingress\n      name: audiobookshelf-pomerium-ingress\n    patch: |-\n      - op: replace\n        path: \"/metadata/annotations/ingress.pomerium.io~1policy\"\n        value: |\n          - allow:\n              or:\n                - http_path:\n                    starts_with: \"/api\"\n                - http_path:\n                    starts_with: \"/login\"\n                - http_path:\n                    starts_with: \"/status\"\n          - allow:\n              or:\n                - email:\n                    is: \"admin-user@gmail.com\"\n                - email:\n                    is: \"alice@gmail.com\"\n                - email:\n                    is: \"bob@gmail.com\"\n</code></pre> <code>pomerium-ingress/audiobookshelf.yaml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: audiobookshelf-pomerium-ingress\n  namespace: audiobookshelf\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.pomerium.io/allow_websockets: true\n    ingress.pomerium.io/idle_timeout: 0s\n    ingress.pomerium.io/pass_identity_headers: true\n    ingress.pomerium.io/preserve_host_header: true\n    ingress.pomerium.io/timeout: 0s\nspec:\n  ingressClassName: pomerium\n  rules:\n    - host: audiobookshelf.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: audiobookshelf-svc\n                port:\n                  number: 13388\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - audiobookshelf.very-very-dark-gray.top\n</code></pre> <code>pomerium-ingress/dashboard.yaml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: dashboard-pomerium-ingress\n  namespace: kubernetes-dashboard\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.pomerium.io/pass_identity_headers: true\n    ingress.pomerium.io/secure_upstream: true\n    ingress.pomerium.io/tls_skip_verify: true\nspec:\n  ingressClassName: pomerium\n  rules:\n    - host: kubernetes.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard-kong-proxy\n                port:\n                  number: 443\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - kubernetes.very-very-dark-gray.top\n</code></pre> <code>pomerium-ingress/ddns-updater.yaml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ddns-updater-pomerium-ingress\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.pomerium.io/allow_public_unauthenticated_access: true\nspec:\n  ingressClassName: pomerium\n  rules:\n    - host: ddns-updater.very-very-dark-gray.top\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: ddns-updater-svc\n              port:\n                name: \"http\"\n  tls:\n    - hosts:\n        - ddns-updater.very-very-dark-gray.top\n      secretName: tls-secret\n</code></pre> <p>Applying the above setup will create (or update) all the Pomerium <code>Ingress</code> at once:</p> <pre><code>$ kubectl apply -k pomerium-ingress\ningress.networking.k8s.io/audiobookshelf-pomerium-ingress created\ningress.networking.k8s.io/dashboard-pomerium-ingress created\ningress.networking.k8s.io/ddns-updater-pomerium-ingress created\n</code></pre> <p>By having both the global policy and the overrides in the <code>patches</code> list, Kustomize applies them sequentially, so that each application-specific patch overrides the global  policy in the first patch. This is more reliable and easier to work with than <code>commonAnnotations</code> which is a \"global transformer\" and rather difficult to override because it applies to everything in the final build.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#final-traffic-migration","title":"Final Traffic Migration","text":"<p>Once all services have a Pomerium <code>Ingress</code> and are reachable on their assigned FQDN on port 8443, move traffic on port 443 over to the new Pomerium <code>Ingress</code>:</p> <ol> <li>Confirm the UniFi router and access points have their inform URL set to the      <code>LoadBalancer</code> IP of the UniFi Network application, which will not change.<ul> <li>UniFi access points accessible through SSH can be easily checked: the IP address     is in the <code>cfg/mgmt</code> file, under variables<code>mgmt.servers.1.url</code> and <code>mgmt_url</code>.</li> </ul> </li> <li>Log into the UniFi Network application at port 8443 on its <code>LoadBalancer</code> IP (e.g.     https://192.168.0.173:8443/)</li> <li>Update port forwarding rules on the router to point port 443 to the <code>LoadBalancer</code>     IP of Pomerium, and 8443 to Nginx, i.e. the opposite as the current setup.</li> <li>Remove <code>:8443</code> from the <code>authenticate.url</code> in <code>pomerium-settings.yaml</code> (and <code>apply</code>).</li> </ol>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#clean-up-retired-nginx-ingress","title":"Clean-up retired Nginx Ingress","text":"<p>Once all the traffic has been migrated and there is no need for Nginx ingresses, they can be deleted with this script:</p> delete-nginx-ingress.sh<pre><code>#!/bin/bash\n# Find and delete Nginx Ingress\n\nkubectl get ingress -A | awk 'NR &gt; 1 {print $1, $2, $3}' | \\\nwhile IFS=$' ' read -r namespace name class; do\n  if [[ \"$class\" != \"nginx\" ]]; then continue; fi\n  echo -e \"\\nRemoving ${name} from ${namespace}: \"\n  kubectl get ingress \"${name}\" -n \"${namespace}\"\n  kubectl delete ingress \"${name}\" -n \"${namespace}\"\ndone\n</code></pre> Result from running <code>delete-nginx-ingress.sh</code>. <pre><code>$ delete-nginx-ingress\n\nRemoving audiobookshelf-ingress from audiobookshelf: \nNAME                     CLASS   HOSTS                     ADDRESS         PORTS     AGE\naudiobookshelf-ingress   nginx   audiobookshelf.very-very-dark-gray.top   192.168.0.171   80, 443   239d\ningress.networking.k8s.io \"audiobookshelf-ingress\" deleted\n\nRemoving code-server-ingress from code-server: \nNAME                  CLASS   HOSTS                            ADDRESS         PORTS     AGE\ncode-server-ingress   nginx   code.very-very-dark-gray.top   192.168.0.171   80, 443   237d\ningress.networking.k8s.io \"code-server-ingress\" deleted\n\nRemoving firefly-iii-ingress from firefly-iii: \nNAME                  CLASS   HOSTS                           ADDRESS         PORTS     AGE\nfirefly-iii-ingress   nginx   firefly.very-very-dark-gray.top   192.168.0.171   80, 443   237d\ningress.networking.k8s.io \"firefly-iii-ingress\" deleted\n\nRemoving komga-ingress from komga: \nNAME            CLASS   HOSTS                  ADDRESS         PORTS     AGE\nkomga-ingress   nginx   komga.very-very-dark-gray.top   192.168.0.171   80, 443   239d\ningress.networking.k8s.io \"komga-ingress\" deleted\n\nRemoving jellyfin-ingress from media-center: \nNAME               CLASS   HOSTS                          ADDRESS         PORTS     AGE\njellyfin-ingress   nginx   jellyfin.very-very-dark-gray.top   192.168.0.171   80, 443   238d\ningress.networking.k8s.io \"jellyfin-ingress\" deleted\n\nRemoving grafana-ingress from monitoring: \nNAME              CLASS   HOSTS                   ADDRESS         PORTS     AGE\ngrafana-ingress   nginx   grafana.very-very-dark-gray.top   192.168.0.171   80, 443   240d\ningress.networking.k8s.io \"grafana-ingress\" deleted\n\nRemoving influxdb-ingress from monitoring: \nNAME               CLASS   HOSTS                         ADDRESS         PORTS     AGE\ninfluxdb-ingress   nginx   influxdb.very-very-dark-gray.top   192.168.0.171   80, 443   240d\ningress.networking.k8s.io \"influxdb-ingress\" deleted\n\nRemoving navidrome-ingress from navidrome: \nNAME                CLASS   HOSTS                              ADDRESS         PORTS     AGE\nnavidrome-ingress   nginx   navidrome.very-very-dark-gray.top   192.168.0.171   80, 443   239d\ningress.networking.k8s.io \"navidrome-ingress\" deleted\n\nRemoving ryot-ingress from ryot: \nNAME           CLASS   HOSTS                          ADDRESS         PORTS     AGE\nryot-ingress   nginx   ryot.very-very-dark-gray.top   192.168.0.171   80, 443   187d\ningress.networking.k8s.io \"ryot-ingress\" deleted\n\nRemoving steam-headless-ingress from steam-headless: \nNAME                     CLASS   HOSTS                          ADDRESS         PORTS     AGE\nsteam-headless-ingress   nginx   steam.very-very-dark-gray.top   192.168.0.171   80, 443   149d\ningress.networking.k8s.io \"steam-headless-ingress\" deleted\n\nRemoving unifi-ingress from unifi: \nNAME            CLASS   HOSTS                       ADDRESS         PORTS     AGE\nunifi-ingress   nginx   unifi.very-very-dark-gray.top   192.168.0.171   80, 443   236d\ningress.networking.k8s.io \"unifi-ingress\" deleted\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#tailscale","title":"Tailscale","text":"<p>The Tailscale Kubernetes operator does not depend on the NGINX Ingress Controller.</p> <p>Creating an Ingress with <code>ingressClassName: tailscale</code>, the operator provisions its own dedicated Tailscale proxy pods. Traffic from the tailnet goes directly to these Tailscale proxy pods, which then route it to the backend services. It does not pass through an NGINX ingress unless manually configured a \"funnel\" to do so. The Tailscale operator handles its own TLS certificate management (via Let's Encrypt and MagicDNS), independent of NGINX or cert-manager.</p> <p>Custom domains with Kubernetes Gateway API and Tailscale may be desirable in the future, but it requires Envoy Gateway (Helm) and ExternalDNS to be installed first, plus a few more requirement that seem less straight-forward. To be revisited.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#migrate-lets-encrypt-solvers","title":"Migrate Let's Encrypt solvers","text":"<p>Another use of the Ingress class <code>nginx</code> is made by the  Let's Encrypt HTTP-01 solvers to obtain and renew HTTPS certificates.</p> <p>Activating HTTP-01 solvers at this point will still use Ingress-NGINX, because <code>cert-manager</code> was originally setup to use its ingress class <code>nginx</code>:</p> <p><code>cert-manager-issuer.yaml</code></p> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n      - http01:\n          ingress:\n            class: nginx\n</code></pre> <p>If Ingress-NGINX is uninstalled at this point, new HTTP-01 solvers will fail, because they'll attempt to create a temporary Ingress with the <code>class: nginx</code> annotation, which no longer has an active controller to fulfill it.</p> <p>In other environments it may be possible to transition certificate renewals to Pomerium, but Pomerium running in Kubernetes is not able to route requests to port 80 to the HTTP-01 solvers, it can only redirect to port 443 instead.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#switch-to-dns-01-solvers","title":"Switch to DNS-01 solvers","text":"<p>The better alternative is to switch <code>cert-manager</code> to DNS-01 solvers, which do not require external requests to reach internal services, so it can be used to obtain certificates even for internal-only services that are not externally reachable.</p> <p>While CloudFlare DNS are supported by a native <code>cert-manager</code> driver, Porkbun is supported for DNS-01 challenges only via webhook solvers, so one must install an external webhook to bridge the Porkbun API with <code>cert-manager</code>. One such option is Porkbun Webhook for cert-manager; install this via Helm into the <code>cert-manager</code> namespace:</p> <pre><code>$ helm repo add cert-manager-webhook-porkbun \\\n  https://talinx.github.io/cert-manager-webhook-porkbun\n\"cert-manager-webhook-porkbun\" has been added to your repositories\n\n$ helm install cert-manager-webhook-porkbun \\\n  cert-manager-webhook-porkbun/cert-manager-webhook-porkbun \\\n  -n cert-manager \\\n  --set groupName=uu.am\nNAME: cert-manager-webhook-porkbun\nLAST DEPLOYED: Fri Jan 16 23:30:55 2026\nNAMESPACE: cert-manager\nSTATUS: deployed\nREVISION: 1\nNOTES:\nPorkbun cert-manager Webhook\nCreate an issuer and a certificate to issue a certificate for a domain.\n\n$ helm get values cert-manager-webhook-porkbun -n cert-manager\nUSER-SUPPLIED VALUES:\ngroupName: uu.am\n</code></pre> <p>Create a <code>ClusterRole</code> and <code>ClusterRoleBinding</code> to allow the <code>cert-manager</code> controller to access the Porkbun webhook's API:</p> <p><code>cert-manager-issuer-rbac.yaml</code></p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cert-manager-webhook-porkbun:domain-solver\nrules:\n  - apiGroups:\n      - uu.am          # must match webhook's groupName\n    resources:\n      - porkbun        # must match webhook's solverName\n    verbs:\n      - create\n      - get\n      - list\n      - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: cert-manager-webhook-porkbun:domain-solver\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cert-manager-webhook-porkbun:domain-solver\nsubjects:\n  - kind: ServiceAccount\n    name: cert-manager\n    namespace: cert-manager\n</code></pre> <pre><code>$ kubectl apply -f cert-manager-issuer-rbac.yaml \nWarning: resource clusterroles/cert-manager-webhook-porkbun:domain-solver is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\nclusterrole.rbac.authorization.k8s.io/cert-manager-webhook-porkbun:domain-solver configured\nWarning: resource clusterrolebindings/cert-manager-webhook-porkbun:domain-solver is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook-porkbun:domain-solver configured\n</code></pre> <p>While HTTP-01 solvers work independently of domain name, DNS-01 solvers are specific to each domain name if they are managed by different DNS providers. In this case, one domain is managed by Porkbun while another is managed by Cloudflare.</p> <p>For CloudFlare, to allow <code>cert-manager</code> to perform DNS-01 challenges, create a scoped API token in the Cloudflare Dashboard: </p> <ol> <li>Navigate to My Profile (top right corner) and select API Tokens.</li> <li>Click Create Token.</li> <li>Select the Edit zone DNS template.</li> <li>Configure the following Permissions:<ul> <li>Zone &gt; DNS &gt; Edit</li> <li>Zone &gt; Zone &gt; Read</li> </ul> </li> <li>Under Zone Resources, select Include and choose the specific zones this     token should manage (e.g., <code>very-very-dark-gray.top</code>).</li> <li>Click Continue to summary, then Create Token.</li> <li>Copy the token immediately; it will only be displayed once. </li> </ol> <p>Create also a porkbun API key, or re-use the one already created for <code>ddns-updater</code>, and store all the above in two Kubernetes secrets within the <code>cert-manager</code> namespace:</p> <p><code>cert-manager-issuer-dns-secrets.yaml</code></p> <pre><code>apiVersion: v1\nstringData:\n  PORKBUN_API_KEY: PORKBUN_API_KEY\n  PORKBUN_SECRET_API_KEY: PORKBUN_SECRET_API_KEY\nkind: Secret\nmetadata:\n  name: porkbun-secret\n  namespace: cert-manager\ntype: Opaque\n---\napiVersion: v1\nstringData:\n  CLOUDFLARE_API_TOKEN: CLOUDFLARE_API_TOKEN\nkind: Secret\nmetadata:\n  name: cloudflare-secret\n  namespace: cert-manager\ntype: Opaque\n</code></pre> <pre><code>$ kubectl apply -f cert-manager-issuer-dns-secrets.yaml \nsecret/porkbun-secret created\nsecret/cloudflare-secret created\n</code></pre> <p>Update <code>ClusterIssuer</code> to use the native CloudFlare and the Porkbun webhook solvers; these replaces the <code>http01</code> section completely (thus bypassing Pomerium redirects). The <code>ClusterIssuer</code> configuration needs two separate <code>dns01</code> solvers, one for each DNS provider:</p> <p><code>cert-manager-issuer-dns-secrets.yaml</code></p> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: root@uu.am\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n      - selector:\n          dnsZones:\n            - uu.am\n        dns01:\n          webhook:\n            groupName: uu.am\n            solverName: porkbun\n            config:\n              apiKey:\n                key: PORKBUN_API_KEY\n                name: porkbun-secret\n              secretApiKey:\n                key: PORKBUN_SECRET_API_KEY\n                name: porkbun-secret\n      - selector:\n          dnsZones:\n            - very-very-dark-gray.top\n        dns01:\n          cloudflare:\n            apiTokenSecretRef:\n              name: cloudflare-secret\n              key: CLOUDFLARE_API_TOKEN\n</code></pre> <pre><code>$ kubectl apply -f cert-manager-issuer.yaml \nclusterissuer.cert-manager.io/letsencrypt-prod configured\n</code></pre> <p>To test the new DNS solvers, add an <code>Ingress</code> under each domain for a simple service (e.g. <code>ddns-updater</code>) to trigger the creation of a certificate under each one:</p> <p><code>pomerium/pomerium-ingress/ddns-updater.yaml</code></p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ddns-updater-pomerium-ingress\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  ingressClassName: pomerium\n  rules:\n    - host: ddns-updater.uu.am\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: ddns-updater-svc\n              port:\n                name: \"http\"\n    - host: ddns-updater.very-very-dark-gray.top\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: ddns-updater-svc\n              port:\n                name: \"http\"\n  tls:\n    - hosts:\n        - ddns-updater.uu.am\n      secretName: tls-secret-uu-am\n    - hosts:\n        - ddns-updater.very-very-dark-gray.top\n      secretName: tls-secret-cloudflare\n</code></pre> <p>A few minutes after applying these to Pomerium, both certificates should be <code>READY</code>:</p> <pre><code>$ kubectl get certificate\nNAME                    READY   SECRET                  AGE\ntls-secret-cloudflare   True    tls-secret-cloudflare   14m\ntls-secret-uu-am        True    tls-secret-uu-am        14m\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#uninstall-ingress-nginx","title":"Uninstall Ingress-NGINX","text":"<p>After all the above migrations and consideration, the time finally comes to remove Ingress-NGINX entirely:</p> <pre><code>$ helm uninstall ingress-nginx -n ingress-nginx \nrelease \"ingress-nginx\" uninstalled\n\n$ kubectl get all -n ingress-nginx\nNAME                                           READY   STATUS        RESTARTS       AGE\npod/ingress-nginx-controller-b49d9c7b9-w26hb   1/1     Terminating   20 (17h ago)   258d\n</code></pre> <p>After a minute or so, there should be nothing left in the <code>ingress-nginx</code> namespace, and the <code>ingress-nginx-admission</code> should be no longer found as one of the active <code>validatingwebhookconfiguration</code>:</p> <pre><code>$ kubectl get all -n ingress-nginx\nNo resources found in ingress-nginx namespace.\n</code></pre> <pre><code>$ kubectl get validatingwebhookconfigurations \nNAME                                                  WEBHOOKS   AGE\ncert-manager-webhook                                  1          258d\ninteldeviceplugins-validating-webhook-configuration   7          255d\nmetallb-webhook-configuration                         6          258d\nprom-kube-prometheus-stack-admission                  2          19d\n</code></pre> <p>There are actually a few resources left in the namespace:</p> <pre><code>$ kubectl get \\\n  -n ingress-nginx \\\n  $(kubectl api-resources --namespaced=true --verbs=list -o name | tr '\\n' ',' | sed 's/,$//')\nNAME                         DATA   AGE\nconfigmap/kube-root-ca.crt   1      258d\n\nNAME                             TYPE     DATA   AGE\nsecret/ingress-nginx-admission   Opaque   3      258d\n\nNAME                     SECRETS   AGE\nserviceaccount/default   0         258d\n\nNAME                                             HOLDER                                     AGE\nlease.coordination.k8s.io/ingress-nginx-leader   ingress-nginx-controller-b49d9c7b9-w26hb   258d\n</code></pre> <p>These are all safe to remove:</p> <ul> <li><code>configmap/kube-root-ca.crt</code> is a standard Kubernetes resource automatically     injected into every namespace. it contains the public certificate for the cluster's     Certificate Authority (CA) so pods can verify the API server.</li> <li><code>secret/ingress-nginx-admission</code> is a certificate used by the Validating Admission     Webhook. It secured the communication that allowed the API server to ask NGINX     \"Is this new Ingress rule valid?\" before saving it. Since the webhook configuration     itself is gone, this certificate is orphaned and useless.</li> <li><code>serviceaccount/default</code> is the default identity for pods in this namespace.</li> <li><code>lease.coordination.k8s.io/ingress-nginx-leader</code> is a lock used by NGINX-ingress     for \"leader election\". It ensures that when running multiple replicas of the ingress     controller, only one acts as the \"leader\" to handle tasks like updating status     fields on Ingress objects. It is a small, lightweight object that has no purpose     without the NGINX pods. Helm often fails to delete these because they are created     dynamically by the application at runtime rather than being part of the manifest.</li> </ul> <p>Since all these resources are either orphaned (lease, secret) or boilerplate (configmap, serviceaccount), the cleanest and most efficient action is to delete the entire namespace:</p> <pre><code>$ kubectl delete namespace ingress-nginx\nnamespace \"ingress-nginx\" deleted\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#appendix-updating-pomerium","title":"Appendix: updating Pomerium","text":"<p>To update Pomerium, check the official documentation to Install Pomerium for the URL it recommends to deploy from, e.g.</p> <pre><code>$ kubectl apply -k github.com/pomerium/ingress-controller/config/default\\?ref=v0.32.0\n</code></pre> <p>Go back to the local copy of the <code>pomerium/ingress-controller</code> repository, pull updates and check that branch out:</p> <pre><code>$ git status\nHEAD detached at v0.31.3\nnothing to commit, working tree clean\n\n$ git pull\nremote: Enumerating objects: 70, done.\nremote: Counting objects: 100% (24/24), done.\nremote: Compressing objects: 100% (11/11), done.\nremote: Total 70 (delta 14), reused 13 (delta 13), pack-reused 46 (from 1)\nUnpacking objects: 100% (70/70), 151.32 KiB | 968.00 KiB/s, done.\nFrom https://github.com/pomerium/ingress-controller\n * [new branch]      0-32-0                -&gt; origin/0-32-0\n * [new branch]      cagocs/release-script -&gt; origin/cagocs/release-script\n   a93102d..338ba7f  main                  -&gt; origin/main\n * [new tag]         v0.32.0               -&gt; v0.32.0\nYou are not currently on a branch.\nPlease specify which branch you want to merge with.\nSee git-pull(1) for details.\n\n    git pull &lt;remote&gt; &lt;branch&gt;\n\n$ git checkout v0.32.0\nPrevious HEAD position was 14de0a0 prepare for v0.31.3 release (#1286)\nHEAD is now at d9e3e06 Customize ingress controller v0.32.0\n</code></pre> <p>Now go back to the <code>pomerium-overlay</code> directory created to deploy Pebble storage and apply it again; check first with <code>kubectl kustomize</code> that the new image will be pulled:</p> <pre><code>$ kubectl kustomize pomerium-overlay | grep 'image:'\n# Warning: 'commonLabels' is deprecated. Please use 'labels' instead. Run 'kustomize edit fix' to update your Kustomization automatically.\n        image: pomerium/ingress-controller:v0.32.0\n        image: pomerium/ingress-controller:main\n</code></pre> <p>After applying the change, Pomerium will be running the new version:</p> <pre><code>$ kubectl apply -k pomerium-overlay \n# Warning: 'commonLabels' is deprecated. Please use 'labels' instead. Run 'kustomize edit fix' to update your Kustomization automatically.\nnamespace/pomerium unchanged\ncustomresourcedefinition.apiextensions.k8s.io/policyfilters.gateway.pomerium.io unchanged\ncustomresourcedefinition.apiextensions.k8s.io/pomerium.ingress.pomerium.io configured\nserviceaccount/pomerium-controller unchanged\nserviceaccount/pomerium-gen-secrets unchanged\nclusterrole.rbac.authorization.k8s.io/pomerium-controller unchanged\nclusterrole.rbac.authorization.k8s.io/pomerium-gen-secrets unchanged\nclusterrolebinding.rbac.authorization.k8s.io/pomerium-controller unchanged\nclusterrolebinding.rbac.authorization.k8s.io/pomerium-gen-secrets unchanged\nservice/pomerium-metrics unchanged\nservice/pomerium-proxy unchanged\npersistentvolume/pomerium-pv-data unchanged\npersistentvolumeclaim/pomerium-pvc-data unchanged\ndeployment.apps/pomerium configured\njob.batch/pomerium-gen-secrets unchanged\ningressclass.networking.k8s.io/pomerium unchanged\n\n$ kubectl -n pomerium describe pod | grep 'Image:'\n    Image:         pomerium/ingress-controller:v0.32.0\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#appendix-alfred-migration","title":"Appendix: <code>alfred</code> migration","text":"<p>Same setup can be applied to <code>alfred</code>, which has all services behind either Tailscale or CloudFlare, with the following changes.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#alfred-pomerium-crd","title":"Alfred Pomerium CRD","text":"<p>While <code>octavo</code> was moved to a different domain, configured with ddns-updater to route external traffic directly through the router to Pomerium, <code>alfred</code> is behind a router that does not allow routing external traffic via port forwarding. This means, while Pomerium in <code>octavo</code> has <code>authenticate.url</code> pointing to that other domain, <code>alfred</code> needs Pomerium CRD to have <code>authenticate.url</code> pointing to the domain that is setup with CloudFlare.</p> <pre><code>$ kubectl apply -f pomerium-settings-alfred.yaml \npomerium.ingress.pomerium.io/global configured\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#alfred-certifiate-for-the-auth-endpoint","title":"Alfred Certifiate for the auth endpoint","text":"<p>Similarly, a dedicated certificate for the <code>authenticate.very-very-dark-gray.top</code> domain must be created, with the domain setup behind CloudFlare:</p> <pre><code>$ kubectl apply -f auth-certificate-alfred.yaml \ncertificate.cert-manager.io/pomerium-auth-cert created\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#alfred-cloudflare-hostnames","title":"Alfred CloudFlare hostnames","text":"<p>Because <code>alfred</code> is only reachable through the CloudFlare tunnel, additional hostnames have to be added in CloudFlare to make the <code>authenticate.very-very-dark-gray.top</code> route</p> <ul> <li>HTTP (port 80) requests to <code>/.well-known</code> to the <code>cm-acme-http-solver</code> service port,     typically http://localhost:32080</li> <li>HTTP (port 80) requests to the IP <code>LoadBalancer</code> IP address given to the     <code>pomerium-proxy</code> service.</li> </ul> <pre><code>$ kubectl get svc -n pomerium\nNAME                        TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                                    AGE\ncm-acme-http-solver-t6zdh   NodePort       10.101.209.98    &lt;none&gt;          8089:32080/TCP                             13m\npomerium-metrics            ClusterIP      10.104.197.212   &lt;none&gt;          9090/TCP                                   25m\npomerium-proxy              LoadBalancer   10.105.126.132   192.168.0.154   443:31936/TCP,443:31936/UDP,80:31514/TCP   25m\n</code></pre> <p>Without the route to port 80, the <code>pomerium-auth-tls</code> certificate won't become <code>READY</code>.</p> <pre><code>$ kubectl get certificate -n pomerium\nNAME                 READY   SECRET              AGE\npomerium-auth-cert   False   pomerium-auth-tls   7m25s\n</code></pre> <p>Once the setup is ready in CloudFlare, it may take some time for <code>alfred</code> to pick up the new DNS records.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#alfred-identity-provider-idp","title":"Alfred Identity Provider (IdP)","text":"<p>Again, <code>alfred</code> needs its own credentials for Google to authenticate @gmail.com users; create an OAuth 2.0 Client ID for Web Server Applications, obtain the client ID and secret from and store them in <code>google-idp-secret-alfred.yaml</code>:</p> <pre><code>$ kubectl apply -f google-idp-secret-alfred.yaml \nsecret/idp-secret created\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#alfred-test-with-verify","title":"Alfred test with Verify","text":"<p>At this point Pomerium should be ready to test with Verify, but first additional CloudFlare hostnames must be setup to route requests to the <code>verify.</code> subdomain on ports 80 and 443. Once those are ready, and the new certificate is <code>READY</code> too, applying the same <code>verify-service.yaml</code> manifest get the application ready at https://verify.very-very-dark-gray.top.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#alfred-additional-settings","title":"Alfred Additional Settings","text":"<p>Additional Settings can be copied directly for those services running in <code>alfred</code> based on their counterparts in <code>octavo</code>, simply copying the <code>pomerium-ingress</code> to Kustomize per-service ACLs as a new directory (e.g. <code>alfred-ingress</code>); removing the files for services not running, updating the FQDN for the services running in <code>alfred</code> behind CloudFlare tunnels, and applying the directory:</p> <pre><code>$ kubectl apply -k alfred-ingress\ningress.networking.k8s.io/audiobookshelf-pomerium-ingress created\ningress.networking.k8s.io/home-assistant-pomerium-ingress created\ningress.networking.k8s.io/dashboard-pomerium-ingress created\ningress.networking.k8s.io/grafana-pomerium-ingress created\n\n$ kubectl get ingress -A | grep pomerium\naudiobookshelf         audiobookshelf-pomerium-ingress          pomerium         arkham-library.very-very-dark-gray.top          192.168.0.154                               80, 443   62s\nhome-assistant         home-assistant-pomerium-ingress          pomerium         home-assistant-alfred.very-very-dark-gray.top   192.168.0.154                               80, 443   22s\nkubernetes-dashboard   dashboard-pomerium-ingress               pomerium         kubernetes-alfred.very-very-dark-gray.top       192.168.0.154                               80, 443   62s\nmonitoring             grafana-pomerium-ingress                 pomerium         bat-signal.very-very-dark-gray.top              192.168.0.154                               80, 443   62s\n</code></pre> <p>At this point the hostnames in CloudFlare can be updated to route requests to the LoadBalancer IP of the Pomerium service and that should complete the migration.</p>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#alfred-lets-encrypt-solvers","title":"Alfred Let's Encrypt solvers","text":"<p>Alfred <code>cert-manager</code> also must switch to DNS-01 solvers before uninstalling Ingress-NGINX. Since the setup is exactly the same as in Octavo, the same manifests can be applied without any changes.</p> <pre><code>$ helm repo add cert-manager-webhook-porkbun \\\n  https://talinx.github.io/cert-manager-webhook-porkbun\n\"cert-manager-webhook-porkbun\" has been added to your repositories\n\n$ helm install cert-manager-webhook-porkbun \\\n  cert-manager-webhook-porkbun/cert-manager-webhook-porkbun \\\n  -n cert-manager \\\n  --set groupName=uu.am\nNAME: cert-manager-webhook-porkbun\nLAST DEPLOYED: Sat Jan 17 21:27:45 2026\nNAMESPACE: cert-manager\nSTATUS: deployed\nREVISION: 1\nNOTES:\nPorkbun cert-manager Webhook\nCreate an issuer and a certificate to issue a certificate for a domain.\n\n$ kubectl apply -f cert-manager-issuer-dns-secrets.yaml \nsecret/porkbun-secret created\nsecret/cloudflare-secret created\n\n$ kubectl apply -f cert-manager-issuer-rbac.yaml \nWarning: resource clusterroles/cert-manager-webhook-porkbun:domain-solver is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\nclusterrole.rbac.authorization.k8s.io/cert-manager-webhook-porkbun:domain-solver configured\nWarning: resource clusterrolebindings/cert-manager-webhook-porkbun:domain-solver is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook-porkbun:domain-solver configured\n\n$ kubectl apply -f cert-manager-issuer.yaml \nWarning: resource clusterissuers/letsencrypt-prod is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\nclusterissuer.cert-manager.io/letsencrypt-prod configured\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#alfred-clean-up","title":"Alfred clean-up","text":"<p>Ingress-NGINX was installed in <code>alfred</code> using the <code>nginx-baremetal.yaml</code> manifest from kubernetes/ingress-nginx so the recommended method to uninstall it is to <code>delete</code> all those resources, since a few of them will not be deleted when deleting the <code>ingress-nginx</code> namespace:</p> <pre><code>$ kubectl delete -f nginx-baremetal.yaml\nnamespace \"ingress-nginx\" deleted\nserviceaccount \"ingress-nginx\" deleted\nserviceaccount \"ingress-nginx-admission\" deleted\nrole.rbac.authorization.k8s.io \"ingress-nginx\" deleted\nrole.rbac.authorization.k8s.io \"ingress-nginx-admission\" deleted\nclusterrole.rbac.authorization.k8s.io \"ingress-nginx\" deleted\nclusterrole.rbac.authorization.k8s.io \"ingress-nginx-admission\" deleted\nrolebinding.rbac.authorization.k8s.io \"ingress-nginx\" deleted\nrolebinding.rbac.authorization.k8s.io \"ingress-nginx-admission\" deleted\nclusterrolebinding.rbac.authorization.k8s.io \"ingress-nginx\" deleted\nclusterrolebinding.rbac.authorization.k8s.io \"ingress-nginx-admission\" deleted\nconfigmap \"ingress-nginx-controller\" deleted\nservice \"ingress-nginx-controller\" deleted\nservice \"ingress-nginx-controller-admission\" deleted\ndeployment.apps \"ingress-nginx-controller\" deleted\njob.batch \"ingress-nginx-admission-create\" deleted\njob.batch \"ingress-nginx-admission-patch\" deleted\ningressclass.networking.k8s.io \"nginx\" deleted\nvalidatingwebhookconfiguration.admissionregistration.k8s.io \"ingress-nginx-admission\" deleted\n</code></pre> <p>This takes a few minutes, after which the following test should find nothing left:</p> <pre><code>$ kubectl get \\\n  -n ingress-nginx \\\n  $(kubectl api-resources --namespaced=true --verbs=list -o name | tr '\\n' ',' | sed 's/,$//')\nNo resources found in ingress-nginx namespace.\n</code></pre> <p>Finally, the <code>ingress-nginx</code> namespace can be deleted:</p> <pre><code>$ kubectl delete namespace ingress-nginx\nnamespace \"ingress-nginx\" deleted\n</code></pre>"},{"location":"blog/2025/12/18/replacing-ingress-nginx-with-pomerium/#appendix-streaming-support","title":"Appendix: streaming support","text":"<p>Cloudflare Tunnels does not support or allow audio streaming on its Free plan, as it prohibits using its CDN (which Tunnels use by default) to serve a \"disproportionate percentage\" of audio files or other large media unless you are using a dedicated paid service like Cloudflare Stream or R2. Neither of those are actually suitable for self-hosted web-based audio streaming applications like Audiobookshelf or Navidrone; the closest match would be to offload storage of media files to Cloudflare R2 and then point those applications to that off-site storage. Not a great setup, specially since billing is based on the storage capacity rqeuired for the entire catalog, while only a portion of it is typically needed for listening.</p> <p>Discarding VPN-based solutions and Cloudflare-like tunnels, other solutions that offer improved security over exposing applications directly on the HTTPS port on a public IP address via Ingress-NGINX, could be one of the following Identity-Aware Proxies (IAP) to move from Passive Security (opening a port and hoping SSL/TLS is enough) to Active Zero-Trust Security, where access is denied until identity is verified at the network edge; users must first authenticate against a specialized \"gatekeeper\" service.</p> <ul> <li>Pomerium: A high-performance, Layer 7 reverse proxy that requires authentication     (via Google, GitHub, or OIDC) before a request ever reaches your Audiobookshelf     server. It treats your local app as if it were behind a corporate Zero-Trust wall.</li> <li>Authentik: A feature-rich identity provider that can act as a frontend for almost     any app. It allows you to enforce Multi-Factor Authentication (MFA) and specific     device posture checks before granting access to your audio library.</li> <li>Authelia: A lightweight alternative to Authentik that integrates directly with     existing reverse proxies like Nginx or Traefik to provide a unified login portal with     2FA.</li> </ul> <p>Pomerium Ingress Controller was chosen because it can entirely replace Ingress-NGINX.</p>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/","title":"Monitoring a Kubernetes cluster for vulnerabilities","text":"<p>Replacing Ingress-NGINX with Pomerium, prompted by the upcoming retirement in March 2026 of Ingress-NGINX controller, was a stark reminder the importance of keeping deployments updated and staying abrest of security issues, vulnerabilities and deprecations.</p> <p>Manually monitoring each application's repository for new releases, to then update each deployment manually, work well for a few deployments but does not scale well to dozens of deployments. The process should be automated to automatically update deployments, at last those with a good track record of hassle-free updates, so that manual updates are needed only for those prone to requiring more attention, intermediate backups, etc.</p>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#manual-monitoring-of-releases","title":"Manual monitoring of releases","text":"<p>Deployments that require special treatment during updates, and optionally those of special interest, can be monitored manually for better visibility.</p>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#watch-github-repositories","title":"Watch GitHub repositories","text":"<p>Source code repositories in GitHub can be Watched to reveive email notifcations when new release, security alerts and/or other new publications are available. For the purpose of keeping deployments up to date, keep an eye on releases and security alerts:</p> <p></p>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#newreleases","title":"<code>new(releases)</code>","text":"<p><code>new(releases)</code> is a simple service to get notifications about new version releases from many programming platforms (GitHub, GitLab, etc.) and docker registries (Docker Hub, Quay, etc.).  </p> <p>To strike a balance between time saving of managing dependencies and annoyances by notifications, emails can sent in batches (frequency can set per project) and releases can be filtered based on regular expressions (e.g. to ignore non-stable releases) The main page shows the latest releases with projects sorted to show first those with the most recent releases:</p> <p></p>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#manually-updating-a-deployment","title":"Manually updating a deployment","text":"<p>Once a new release for a Docker image is available, Kubernetes will automatically update the deployment the next time it is restarted; depending on how the container image version is specified, Kubernetes will pull the latest release that matches the tag specified in each <code>container.image</code> \u2014 this may be the latest stable, major or minor version.</p>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#deployment-restart","title":"Deployment restart","text":"<p>Applications with no special requirements for a gradeceful shutdown can be udpated by simply restarting the deployment, e.g. this will update Audiobookshelf in just a few seconds.</p> <pre><code>$ kubectl rollout restart deployment audiobookshelf -n audiobookshelf\ndeployment.apps/audiobookshelf restarted\n</code></pre> <p>Although restarting the deployment is itself a fast operation (the new pod is ready in about 10 seconds), it can disrupt the service for users in a few ways:</p> <ul> <li>If a user is listening to a book or podcast, their progress will no longer be saved     until they reload te client application (web or mobile).</li> <li>If the new version includes changes to user authentication, user will need to login     again. This can also interrupt tracking progress and streaming media.</li> </ul> <p>To avoid such service disruptions to its users, Audiobookshelf is restarted daily at 5am from a <code>crontab</code> (this is good enough because all the users are in the same time zone).</p>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#deployment-scale-downup","title":"Deployment scale down/up","text":"<p>Applications that require a relatively long time to shut down, such as the Minecraft Java server which needs to Save the World during shut down, this alternative script can be used to make sure the old pods hav enough time to gracefuly shutdown:</p> <pre><code>$ cat ~/bin/restart-audiobookshelf \n#!/bin/bash\nkubectl scale -n audiobookshelf deployment audiobookshelf --replicas=0\nsleep 60\nkubectl scale -n audiobookshelf deployment audiobookshelf --replicas=1\n</code></pre> <p>This approach is necessary when it is recommended to make a full backup before running the new version, e.g. this was neccessary to make full backes when updating UniFi from v9 to v10.</p> <p>Tip</p> <p>When there is no need to make a full backup before running the new version, the slow graceful shutdown is better accomplished by updating the deployment to set the <code>Recreate</code> strategy so that running pods are shut down before starting new ones, and ensure the old pod has enouth time to commit changes to local files by adding a <code>lifecycle.preStop</code> that waits for a specific amount of time, similarly to Pomerium's Pebble storage setup.</p>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#clean-up-discarded-replicasets","title":"Clean-up discarded <code>replicasets</code>","text":"<p>Restarting services with the above methods can lead to an accummulation of empty old <code>replicasets</code> that can be deleted once the new version is running. It is possible to reduce the history kept by the Deployment by setting <code>.spec.revisionHistoryLimit</code> in Deployment manifests (the default is typically 10)s, that <code>replicasets</code> with 0 replicas and no active pods are deleted through garbage collection once this limit is reached.</p> <p>Alternatively, these can be deleted as a one-off with this script:</p> <pre><code>#!/bin/bash\n#\n# Delete unused replicasets.\n\nkubectl get namespaces -o json \\\n| jq -r '.items[].metadata.name' \\\n| while read namespace; do\n  kubectl get replicasets -n $namespace -o json \\\n  | jq -r '.items[] | select(.status.replicas | contains(0))' \\\n  | jq -r '.metadata.name' \\\n  | while read replicaset; do\n    kubectl delete -n $namespace replicaset $replicaset --cascade=background; \n  done\ndone\n</code></pre>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#node-restart","title":"Node restart","text":"<p>Restarting the entire server (Kuberneters <code>node</code>) would also update deployments to the latest release available that matches their specified release tag. This can be done by  restarting the entire server, or as part of the process to  upgrade the kubernetes cluster to the next version.</p>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#automated-monitoring-of-vulnerabilities","title":"Automated monitoring of vulnerabilities","text":"<p>Presumably a good reason to keep a service updated to the latest version is to deploy fixes to bugs, including vulnerabilities. Whether that is actually true or not, that's another story, and to understand this it becomes necessary to have a good view of what vulnerabilities are actually present in the running versions of each service.</p>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#trivy","title":"Trivy","text":"<p>Trivy is an extremely fast open-source scanner that can scan the entire Kubernetes cluster or local images for vulnerabilities and misconfigurations.</p> <p>Debian/Ubuntu (Official) packages are easy to install as usual:</p> <pre><code># wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key \\\n  | gpg --dearmor |  tee /usr/share/keyrings/trivy.gpg &gt; /dev/null\n# echo \"deb [signed-by=/usr/share/keyrings/trivy.gpg] https://aquasecurity.github.io/trivy-repo/deb generic main\" \\\n  | tee -a /etc/apt/sources.list.d/trivy.list\ndeb [signed-by=/usr/share/keyrings/trivy.gpg] https://aquasecurity.github.io/trivy-repo/deb generic main\n# apt-get update\n# apt-get install trivy -y\n</code></pre> <p>The <code>trivy</code> CLI can be used to audit several types of targets, for Kubernetes it can be used directly on each Docker image, e.g.</p> <pre><code>$ trivy image yobasystems/alpine-mariadb:latest\n2025-12-24T17:25:29+01:00       INFO    [vuln] Vulnerability scanning is enabled\n2025-12-24T17:25:29+01:00       INFO    [secret] Secret scanning is enabled\n2025-12-24T17:25:29+01:00       INFO    [secret] If your scanning is slow, please try '--scanners vuln' to disable secret scanning\n2025-12-24T17:25:29+01:00       INFO    [secret] Please see https://trivy.dev/docs/v0.68/guide/scanner/secret#recommendation for faster secret detection\n2025-12-24T17:25:32+01:00       INFO    Detected OS     family=\"alpine\" version=\"3.23.0\"\n2025-12-24T17:25:32+01:00       WARN    This OS version is not on the EOL list  family=\"alpine\" version=\"3.23\"\n2025-12-24T17:25:32+01:00       INFO    [alpine] Detecting vulnerabilities...   os_version=\"3.23\" repository=\"3.23\" pkg_num=44\n2025-12-24T17:25:32+01:00       INFO    Number of language-specific files       num=0\n2025-12-24T17:25:32+01:00       WARN    Using severities from other vendors for some vulnerabilities. Read https://trivy.dev/docs/v0.68/guide/scanner/vulnerability#severity-selection for details.\n\nReport Summary\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Target                       \u2502  Type  \u2502 Vulnerabilities \u2502 Secrets \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 yobasystems/alpine-mariadb:latest (alpine 3.23.0) \u2502 alpine \u2502        1        \u2502    -    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nLegend:\n- '-': Not scanned\n- '0': Clean (no security findings detected)\n\n\nyobasystems/alpine-mariadb:latest (alpine 3.23.0)\n\nTotal: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 1, HIGH: 0, CRITICAL: 0)\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Library \u2502 Vulnerability  \u2502 Severity \u2502 Status \u2502 Installed Version \u2502 Fixed Version \u2502                           Title                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 c-ares  \u2502 CVE-2025-62408 \u2502 MEDIUM   \u2502 fixed  \u2502 1.34.5-r0         \u2502 1.34.6-r0     \u2502 c-ares: c-ares: Denial of Service due to query termination \u2502\n\u2502         \u2502                \u2502          \u2502        \u2502                   \u2502               \u2502 after maximum attempts...                                  \u2502\n\u2502         \u2502                \u2502          \u2502        \u2502                   \u2502               \u2502 https://avd.aquasec.com/nvd/cve-2025-62408                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#trivy-operator","title":"Trivy Operator","text":"<p>The Trivy Operator leverages Trivy to continuously scan the Kubernetes cluster for security issues. The scans are summarised in security reports as Kubernetes Custom Resource Definitions (CRD), which become accessible through the Kubernetes API. The Operator does this by watching Kubernetes for state changes and automatically triggering security scans in response. For example, a vulnerability scan is initiated when a new Pod is created. This way, users can find and view the risks that relate to different resources in a Kubernetes-native way.</p> <p>To install the operator using Helm, add the respository and then install the latest version in its own namespace:</p> <pre><code>$ helm repo add aqua https://aquasecurity.github.io/helm-charts/\n\"aqua\" has been added to your repositories\n\n$ helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"node-feature-discovery\" chart repository\n...Successfully got an update from the \"kubernetes-dashboard\" chart repository\n...Successfully got an update from the \"jetstack\" chart repository\n...Successfully got an update from the \"aqua\" chart repository\n...Successfully got an update from the \"ingress-nginx\" chart repository\n...Successfully got an update from the \"intel\" chart repository\n...Successfully got an update from the \"tailscale\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\n</code></pre> <p>The 0.31.0 is the latest version of the Helm chart.</p> <pre><code>$ helm install trivy-operator aqua/trivy-operator \\\n     --namespace trivy-system \\\n     --create-namespace \\\n     --version 0.31.0\nNAME: trivy-operator\nLAST DEPLOYED: Sun Dec 21 11:25:21 2025\nNAMESPACE: trivy-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nYou have installed Trivy Operator in the trivy-system namespace.\nIt is configured to discover Kubernetes workloads and resources in\nall namespace(s).\n\nInspect created VulnerabilityReports by:\n\n    kubectl get vulnerabilityreports --all-namespaces -o wide\n\nInspect created ConfigAuditReports by:\n\n    kubectl get configauditreports --all-namespaces -o wide\n\nInspect the work log of trivy-operator by:\n\n    kubectl logs -n trivy-system deployment/trivy-operator\n</code></pre> <p>Right after installing there are no vulnerabilities to report yet, the operator will need some time (several minutes) to scan the whole cluster:</p> <pre><code>$ kubectl get vulnerabilityreports -A -o wide\nNo resources found\n\n$ kubectl logs -n trivy-system deployment/trivy-operator\n2025/12/21 10:25:25 maxprocs: Leaving GOMAXPROCS=16: CPU quota undefined\n{\"level\":\"info\",\"ts\":\"2025-12-21T10:25:25Z\",\"logger\":\"main\",\"msg\":\"Starting operator\",\"buildInfo\":{\"Version\":\"0.29.0\",\"Commit\":\"c8b31d9428fe730da7f306e43abc45c3de904c94\",\"Date\":\"2025-09-23T06:46:35Z\",\"Executable\":\"\"}}\n{\"level\":\"info\",\"ts\":\"2025-12-21T10:25:25Z\",\"logger\":\"operator\",\"msg\":\"Resolved install mode\",\"install mode\":\"AllNamespaces\",\"operator namespace\":\"trivy-system\",\"target namespaces\":[],\"exclude namespaces\":\"\",\"target workloads\":[\"pod\",\"replicaset\",\"replicationcontroller\",\"statefulset\",\"daemonset\",\"cronjob\",\"job\"]}\n{\"level\":\"info\",\"ts\":\"2025-12-21T10:25:25Z\",\"logger\":\"operator\",\"msg\":\"Watching all namespaces\"}\n</code></pre> <p>Once the cluster has been scanned, <code>vulnerabilityreport</code> objects can be listed and inspected via the Kubernetes API:</p> <pre><code>$ kubectl get vulnerabilityreports -A -o wide\nNAMESPACE                  NAME                                                              REPOSITORY                      TAG        SCANNER   AGE     CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN\naudiobookshelf             replicaset-audiobookshelf-b49c49757-audiobookshelf                advplyr/audiobookshelf          latest     Trivy     31s     1          14     11       15    0\ncert-manager               replicaset-cert-manager-webhook-78cb4cf989-cert-manager-webhook   jetstack/cert-manager-webhook   v1.17.2    Trivy     46s     0          3      15       0     0\ndefault                    pod-command-demo-command-demo-container                           library/debian                  latest     Trivy     2m12s   0          0      10       50    0\nintel-device-plugins-gpu   daemonset-intel-gpu-plugin-gpudeviceplugin-intel-gpu-plugin       intel/intel-gpu-plugin          0.32.0     Trivy     2m13s   0          3      15       0     0\nkube-flannel               daemonset-kube-flannel-ds-install-cni                             flannel-io/flannel              v0.26.7    Trivy     112s    0          9      44       5     2\nkube-flannel               daemonset-kube-flannel-ds-kube-flannel                            flannel-io/flannel              v0.26.7    Trivy     2m7s    0          9      44       5     2\nkube-system                daemonset-kube-proxy-kube-proxy                                   kube-proxy                      v1.32.4    Trivy     2m13s   1          12     36       17    0\nkube-system                pod-etcd-octavo-etcd                                              etcd                            3.5.16-0   Trivy     104s    4          23     101      4     0\nkube-system                pod-kube-apiserver-octavo-kube-apiserver                          kube-apiserver                  v1.32.4    Trivy     2m14s   1          9      33       0     0\nkube-system                pod-kube-controller-manager-octavo-kube-controller-manager        kube-controller-manager         v1.32.4    Trivy     107s    1          12     33       0     0\nkube-system                pod-kube-scheduler-octavo-kube-scheduler                          kube-scheduler                  v1.32.4    Trivy     2m13s   1          8      32       0     0\nkube-system                replicaset-coredns-668d6bf9bc-coredns                             coredns/coredns                 v1.11.3    Trivy     16s     1          8      26       1     4\nkubernetes-dashboard       replicaset-656764b68d                                             kubernetesui/dashboard-api      1.12.0     Trivy     2m13s   0          4      20       0     0\nkubernetes-dashboard       replicaset-6c7c97c867                                             kubernetesui/dashboard-auth     1.2.4      Trivy     77s     0          6      21       0     0\nkubernetes-dashboard       replicaset-f59b77564                                              kubernetesui/dashboard-web      1.6.2      Trivy     54s     0          5      21       0     0\nkubernetes-dashboard       replicaset-kubernetes-dashboard-kong-79867c9c48-proxy             library/kong                    3.8        Trivy     59s     0          0      8        26    0\nmetallb-system             daemonset-speaker-speaker                                         metallb/speaker                 v0.14.9    Trivy     106s    0          11     55       0     0\nmonitoring                 replicaset-grafana-6fff9dbb6c-grafana                             grafana/grafana                 11.6.1     Trivy     37s     0          13     64       6     0\nmonitoring                 replicaset-influxdb-5974bf664f-influxdb                           library/influxdb                1.11.8     Trivy     67s     2          5      39       97    2\nnavidrome                  replicaset-navidrome-df8c9c769-navidrome                          deluan/navidrome                latest     Trivy     10s     0          0      3        7     0\nnode-feature-discovery     daemonset-node-feature-discovery-worker-worker                    nfd/node-feature-discovery      v0.17.3    Trivy     103s    0          30     100      6     0\nnode-feature-discovery     replicaset-node-feature-discovery-gc-5b65f7f5b6-gc                nfd/node-feature-discovery      v0.17.3    Trivy     43s     0          30     100      6     0\npomerium                   job-pomerium-gen-secrets-gen-secrets                              pomerium/ingress-controller     main       Trivy     2m12s   0          0      0        8     0\npomerium                   replicaset-pomerium-6b454bb8f9-pomerium                           pomerium/ingress-controller     v0.31.3    Trivy     73s     0          0      1        9     0\npomerium                   replicaset-verify-7889db6976-httpbin                              pomerium/verify                 latest     Trivy     78s     0          0      0        8     0\nryot                       replicaset-ryot-55c5845667-ryot                                   ignisda/ryot                    v8         Trivy     61s     2          19     90       92    1\ntailscale                  replicaset-operator-748ccd7c75-operator                           tailscale/k8s-operator          v1.82.0    Trivy     16s     0          5      24       5     0\ntailscale                  statefulset-7c5dc67989                                            tailscale/tailscale             v1.82.0    Trivy     2m13s   0          13     60       5     0\ntailscale                  statefulset-ts-home-assistant-tailscale-mdqlt-tailscale           tailscale/tailscale             v1.82.0    Trivy     2m14s   0          13     60       5     0\ntrivy-system               replicaset-trivy-operator-59489786c6-trivy-operator               aquasec/trivy-operator          0.29.0     Trivy     106s    0          6      21       5     0\n</code></pre> <p>Install <code>tree</code> to easily find the relevant reports for each deployement:</p> <pre><code>$ kubectl tree deployment unifi -n unifi\nNAMESPACE  NAME                                                       READY  REASON  STATUS   AGE \nunifi      Deployment/unifi                                           -              -        237d\nunifi      \u251c\u2500ReplicaSet/unifi-584f4847c7                              -              -        237d\nunifi      \u251c\u2500ReplicaSet/unifi-67679795dd                              -              -        19h \nunifi      \u2502 \u251c\u2500ConfigAuditReport/replicaset-unifi-67679795dd          -              -        19h \nunifi      \u2502 \u251c\u2500ExposedSecretReport/replicaset-unifi-67679795dd-unifi  -              -        19h \nunifi      \u2502 \u251c\u2500Pod/unifi-67679795dd-bw2vg                             True           Current  19h \nunifi      \u2502 \u251c\u2500SbomReport/replicaset-unifi-67679795dd-unifi           -              -        19h \nunifi      \u2502 \u2514\u2500VulnerabilityReport/replicaset-unifi-67679795dd-unifi  -              -        19h \nunifi      \u251c\u2500ReplicaSet/unifi-7d44b599dc                              -              -        97d \nunifi      \u2502 \u251c\u2500ConfigAuditReport/replicaset-unifi-7d44b599dc          -              -        3d7h\nunifi      \u2502 \u251c\u2500ExposedSecretReport/replicaset-unifi-7d44b599dc-unifi  -              -        3d6h\nunifi      \u2502 \u2514\u2500SbomReport/replicaset-unifi-7d44b599dc-unifi           -              -        3d6h\nunifi      \u251c\u2500ReplicaSet/unifi-8997b8865                               -              -        202d\nunifi      \u2514\u2500ReplicaSet/unifi-bff88d4df                               -              -        98d \n\n$ kubectl tree deployment home-assistant -n home-assistant\nNAMESPACE       NAME                                                                             READY  REASON  STATUS   AGE  \nhome-assistant  Deployment/home-assistant                                                        -              -        241d \nhome-assistant  \u2514\u2500ReplicaSet/home-assistant-77bf44c47b                                           -              -        241d \nhome-assistant    \u251c\u2500ConfigAuditReport/replicaset-home-assistant-77bf44c47b                       -              -        3d7h \nhome-assistant    \u251c\u2500ExposedSecretReport/replicaset-home-assistant-77bf44c47b-home-assistant-app  -              -        3d7h \nhome-assistant    \u251c\u2500Pod/home-assistant-77bf44c47b-trgrt                                          True           Current  144d \nhome-assistant    \u251c\u2500SbomReport/replicaset-home-assistant-77bf44c47b-home-assistant-app           -              -        3d7h \nhome-assistant    \u2514\u2500VulnerabilityReport/replicaset-home-assistant-77bf44c47b-home-assistant-app  -              -        6h49m\n\n$ kubectl tree deployment audiobookshelf -n audiobookshelf\nNAMESPACE       NAME                                                                        READY  REASON  STATUS   AGE \naudiobookshelf  Deployment/audiobookshelf                                                   -              -        239d\naudiobookshelf  \u2514\u2500ReplicaSet/audiobookshelf-c6d46b54f                                       -              -        13h \naudiobookshelf    \u251c\u2500ConfigAuditReport/replicaset-audiobookshelf-c6d46b54f                   -              -        13h \naudiobookshelf    \u251c\u2500ExposedSecretReport/replicaset-audiobookshelf-c6d46b54f-audiobookshelf  -              -        13h \naudiobookshelf    \u251c\u2500Pod/audiobookshelf-c6d46b54f-ngm8w                                      True           Current  13h \naudiobookshelf    \u251c\u2500SbomReport/replicaset-audiobookshelf-c6d46b54f-audiobookshelf           -              -        13h \naudiobookshelf    \u2514\u2500VulnerabilityReport/replicaset-audiobookshelf-c6d46b54f-audiobookshelf  -              -        13h \n</code></pre> <p>Use the <code>describe</code> command to inspect all details about each report. The output is often very long, so here is a trick to filter it down to show only <code>CRITICAL</code> vulnerabilties:</p> <pre><code>$ kubectl describe vulnerabilityreport \\\n    replicaset-audiobookshelf-c6d46b54f-audiobookshelf \\\n    -n audiobookshelf \\\n    | egrep -B11 -A1 CRITICAL | egrep --color '|.*Version.*|Title.*'\n    Title:               follow-redirects: Possible credential leak\n    Vulnerability ID:    CVE-2024-28849\n    Fixed Version:       2.5.4, 3.0.4, 4.0.4\n    Installed Version:   4.0.0\n    Last Modified Date:  2025-11-03T20:19:20Z\n    Links:\n    Package PURL:        pkg:npm/form-data@4.0.0\n    Primary Link:        https://avd.aquasec.com/nvd/cve-2025-7783\n    Published Date:      2025-07-18T17:15:44Z\n    Resource:            form-data\n    Score:               5.4\n    Severity:            CRITICAL\n    Target:              \n</code></pre>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#trivy-operator-dashboard","title":"Trivy Operator Dashboard","text":"<p>Trivy Operator Dashboard is a dedicate dashboard for the Trivy Pperator that comes with its own web UI (not based on Grafana). The recommended installation method is via Helm, with the following <code>trivy-operator-dashboard-values.yaml</code> to create a Pomerium-based <code>Ingress</code>:</p> prometheus/trivy-operator-dashboard-values.yaml<pre><code>ingress:\n  enabled: true\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.pomerium.io/pass_identity_headers: true\n    ingress.pomerium.io/secure_upstream: true\n    ingress.pomerium.io/tls_skip_verify: true\n    ingress.pomerium.io/policy: |\n      - allow:\n          or:\n            - email:\n                is: \"admin-user@gmail.com\"\n  className: pomerium\n  tls:\n    - secretName: tls-trivy-operator-dashboard\n      hosts:\n        - trivy-operator-dashboard.very-very-dark-gray.top\n  hosts:\n    - host: trivy-operator-dashboard.very-very-dark-gray.top\n      paths:\n        - path: /\n          pathType: Prefix\n</code></pre> <pre><code>$ helm install trivy-operator-dashboard \\\n  oci://ghcr.io/raoulx24/charts/trivy-operator-dashboard \\\n  --namespace monitoring \\\n  --version 1.7.2 \\\n  --values prometheus/trivy-operator-dashboard-values.yaml\nPulled: ghcr.io/raoulx24/charts/trivy-operator-dashboard:1.7.2\nDigest: sha256:a7831877b6f3d2124312cffbf5839b0d4f8159abf3215c29f837a17d6a2f65b9\nNAME: trivy-operator-dashboard\nLAST DEPLOYED: Sun Dec 21 19:02:34 2025\nNAMESPACE: monitoring\nSTATUS: deployed\nREVISION: 1\nNOTES:\n1. Get the application URL by running these commands:\n  https://trivy-operator-dashboard.very-very-dark-gray.top/\n</code></pre> <p>The dashboard is now available at https://trivy-operator-dashboard.very-very-dark-gray.top</p> <p></p>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#trivy-operator-dashboard-in-grafana","title":"Trivy Operator Dashboard in Grafana","text":"<p>Trivy Operator Dashboard in Grafana makes all those reports easier to find and provides better Big Picture views of the whole cluster, using Prometheus to scrape those reports and visualizing them on Grafana dashboards.</p> <p>Prometheus and Grafana can easily be installed through the kube-prometheus-stack Helm Chart, but Grafana is already installed so it is only necessary to install Prometheus. To omit Grafana it, create a custom <code>prometheus/values.yaml</code> file to disable the Grafana component:</p> prometheus/values.yaml<pre><code># Disable Grafana installation\ngrafana:\n  enabled: false\n\n# Ensure Prometheus can find the Trivy ServiceMonitors\nprometheus:\n  prometheusSpec:\n    serviceMonitorSelectorNilUsesHelmValues: false\n    serviceMonitorSelector: {}\n    serviceMonitorNamespaceSelector: {}\n</code></pre> <p>Add the repository:</p> <pre><code>$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n\"prometheus-community\" has been added to your repositories\n\n$ helm repo update\nHang tight while we grab the latest from your chart repositories...\n...\n...Successfully got an update from the \"prometheus-community\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\n</code></pre> <p>Install the stack in the existing <code>monitoring</code> namespace where Grafana is running, using the above <code>prometheus/values.yaml</code> to omit installing Grafana:</p> <pre><code>$ helm upgrade --install prom prometheus-community/kube-prometheus-stack \\\n  --namespace monitoring \\\n  --values prometheus/values.yaml\n\nRelease \"prom\" does not exist. Installing it now.\nNAME: prom\nLAST DEPLOYED: Sun Dec 21 13:32:07 2025\nNAMESPACE: monitoring\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nkube-prometheus-stack has been installed. Check its status by running:\n  kubectl --namespace monitoring get pods -l \"release=prom\"\n\nGet Grafana 'admin' user password by running:\n\n  kubectl --namespace monitoring get secrets prom-grafana -o jsonpath=\"{.data.admin-password}\" | base64 -d ; echo\n\nAccess Grafana local instance:\n\n  export POD_NAME=$(kubectl --namespace monitoring get pod -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=prom\" -oname)\n  kubectl --namespace monitoring port-forward $POD_NAME 3000\n\nGet your grafana admin user password by running:\n\n  kubectl get secret --namespace monitoring -l app.kubernetes.io/component=admin-secret -o jsonpath=\"{.items[0].data.admin-password}\" | base64 --decode ; echo\n\n\nVisit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create &amp; configure Alertmanager and Prometheus instances using the Operator.\n</code></pre> <p>The instructions referring to the Grafana dashboards can be ignored, they are only relevant when installing Grafana from this Helm chart.</p> <p>Then update the Trivy Operator to expoort its metrics to Prometheus, by upgrading the Helm chart with the following <code>trivy-values.yaml</code>:</p> prometheus/trivy-values.yaml<pre><code>serviceMonitor:\n  enabled: true\n</code></pre> <pre><code>$ helm upgrade \\\n  trivy-operator aqua/trivy-operator \\\n  -n trivy-system \\\n  --values prometheus/trivy-values.yaml\n\nRelease \"trivy-operator\" has been upgraded. Happy Helming!\nNAME: trivy-operator\nLAST DEPLOYED: Sun Dec 21 13:37:38 2025\nNAMESPACE: trivy-system\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\nNOTES:\nYou have installed Trivy Operator in the trivy-system namespace.\nIt is configured to discover Kubernetes workloads and resources in\nall namespace(s).\n\nInspect created VulnerabilityReports by:\n\n    kubectl get vulnerabilityreports --all-namespaces -o wide\n\nInspect created ConfigAuditReports by:\n\n    kubectl get configauditreports --all-namespaces -o wide\n\nInspect the work log of trivy-operator by:\n\n    kubectl logs -n trivy-system deployment/trivy-operator\n</code></pre> <p>This update to the Trivy operator Helm values leads to the creation of its <code>ServiceMonitor</code>:</p> <pre><code>$ kubectl describe servicemonitor trivy-operator -n trivy-system \nName:         trivy-operator\nNamespace:    trivy-system\nLabels:       app.kubernetes.io/instance=trivy-operator\n              app.kubernetes.io/managed-by=Helm\n              app.kubernetes.io/name=trivy-operator\n              app.kubernetes.io/version=0.29.0\n              helm.sh/chart=trivy-operator-0.31.0\nAnnotations:  meta.helm.sh/release-name: trivy-operator\n              meta.helm.sh/release-namespace: trivy-system\nAPI Version:  monitoring.coreos.com/v1\nKind:         ServiceMonitor\nMetadata:\n  Creation Timestamp:  2025-12-21T12:37:39Z\n  Generation:          1\n  Resource Version:    52316470\n  UID:                 610823f7-b6bd-459f-a6c7-8b5c18bf1ae4\nSpec:\n  Endpoints:\n    Honor Labels:  true\n    Port:          metrics\n    Scheme:        http\n  Selector:\n    Match Labels:\n      app.kubernetes.io/instance:  trivy-operator\n      app.kubernetes.io/name:      trivy-operator\nEvents:                            &lt;none&gt;\n</code></pre> <pre><code>$ kubectl get all -n monitoring\nNAME                                                         READY   STATUS    RESTARTS      AGE\npod/alertmanager-prom-kube-prometheus-stack-alertmanager-0   2/2     Running   0             7m7s\npod/grafana-6fff9dbb6c-v22hg                                 1/1     Running   19 (9d ago)   237d\npod/influxdb-5974bf664f-8r5mf                                1/1     Running   19 (9d ago)   237d\npod/prom-kube-prometheus-stack-operator-645fd684d6-n6qpf     1/1     Running   0             7m12s\npod/prom-kube-state-metrics-8576986c6b-xqcwl                 1/1     Running   0             7m12s\npod/prom-prometheus-node-exporter-r24zl                      1/1     Running   0             7m13s\npod/prometheus-prom-kube-prometheus-stack-prometheus-0       2/2     Running   0             7m7s\n\nNAME                                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\nservice/alertmanager-operated                     ClusterIP   None             &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   7m8s\nservice/grafana-svc                               NodePort    10.110.29.239    &lt;none&gt;        13000:30300/TCP              237d\nservice/influxdb-svc                              NodePort    10.110.65.108    &lt;none&gt;        18086:30086/TCP              237d\nservice/prom-kube-prometheus-stack-alertmanager   ClusterIP   10.100.199.60    &lt;none&gt;        9093/TCP,8080/TCP            7m13s\nservice/prom-kube-prometheus-stack-operator       ClusterIP   10.107.124.226   &lt;none&gt;        443/TCP                      7m13s\nservice/prom-kube-prometheus-stack-prometheus     ClusterIP   10.110.216.0     &lt;none&gt;        9090/TCP,8080/TCP            7m13s\nservice/prom-kube-state-metrics                   ClusterIP   10.96.142.254    &lt;none&gt;        8080/TCP                     7m13s\nservice/prom-prometheus-node-exporter             ClusterIP   10.97.182.188    &lt;none&gt;        9100/TCP                     7m13s\nservice/prometheus-operated                       ClusterIP   None             &lt;none&gt;        9090/TCP                     7m7s\n\nNAME                                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\ndaemonset.apps/prom-prometheus-node-exporter   1         1         1       1            1           kubernetes.io/os=linux   7m13s\n\nNAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana                               1/1     1            1           237d\ndeployment.apps/influxdb                              1/1     1            1           237d\ndeployment.apps/prom-kube-prometheus-stack-operator   1/1     1            1           7m13s\ndeployment.apps/prom-kube-state-metrics               1/1     1            1           7m13s\n\nNAME                                                             DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-6fff9dbb6c                               1         1         1       237d\nreplicaset.apps/influxdb-5974bf664f                              1         1         1       237d\nreplicaset.apps/prom-kube-prometheus-stack-operator-645fd684d6   1         1         1       7m13s\nreplicaset.apps/prom-kube-state-metrics-8576986c6b               1         1         1       7m13s\n\nNAME                                                                    READY   AGE\nstatefulset.apps/alertmanager-prom-kube-prometheus-stack-alertmanager   1/1     7m8s\nstatefulset.apps/prometheus-prom-kube-prometheus-stack-prometheus       1/1     7m7s\n</code></pre> <pre><code>$ kubectl get svc -n monitoring -o wide\nNAME                                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE     SELECTOR\nalertmanager-operated                     ClusterIP   None             &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   7m35s   app.kubernetes.io/name=alertmanager\ngrafana-svc                               NodePort    10.110.29.239    &lt;none&gt;        13000:30300/TCP              237d    app=grafana\ninfluxdb-svc                              NodePort    10.110.65.108    &lt;none&gt;        18086:30086/TCP              237d    app=influxdb\nprom-kube-prometheus-stack-alertmanager   ClusterIP   10.100.199.60    &lt;none&gt;        9093/TCP,8080/TCP            7m40s   alertmanager=prom-kube-prometheus-stack-alertmanager,app.kubernetes.io/name=alertmanager\nprom-kube-prometheus-stack-operator       ClusterIP   10.107.124.226   &lt;none&gt;        443/TCP                      7m40s   app=kube-prometheus-stack-operator,release=prom\nprom-kube-prometheus-stack-prometheus     ClusterIP   10.110.216.0     &lt;none&gt;        9090/TCP,8080/TCP            7m40s   app.kubernetes.io/name=prometheus,operator.prometheus.io/name=prom-kube-prometheus-stack-prometheus\nprom-kube-state-metrics                   ClusterIP   10.96.142.254    &lt;none&gt;        8080/TCP                     7m40s   app.kubernetes.io/instance=prom,app.kubernetes.io/name=kube-state-metrics\nprom-prometheus-node-exporter             ClusterIP   10.97.182.188    &lt;none&gt;        9100/TCP                     7m40s   app.kubernetes.io/instance=prom,app.kubernetes.io/name=prometheus-node-exporter\nprometheus-operated                       ClusterIP   None             &lt;none&gt;        9090/TCP                     7m34s   app.kubernetes.io/name=prometheus\n</code></pre>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#create-grafana-data-source","title":"Create Grafana Data source","text":"<p>Once Prometheus is running there are two ways to connec the existing grafana to it:</p> <ul> <li>If both services are running in the same cluster, an internal URL can be used to     connect based on the <code>prom-kube-prometheus-stack-prometheus</code> service at     http://prom-kube-prometheus-stack-prometheus.monitoring.svc:9090</li> <li> <p>If a public endpoint is otherwise needed to reach Prometheous, create an <code>Ingress</code>     for the Prometheus service:</p> Additional <code>Ingress</code> for the Prometheus service. pomerium/pomerium-ingress/monitoring.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: prometheus-pomerium-ingress\n  namespace: monitoring\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.pomerium.io/allow_public_unauthenticated_access: true\n    ingress.pomerium.io/allow_websockets: true\n    ingress.pomerium.io/pass_identity_headers: true\n    ingress.pomerium.io/preserve_host_header: true\nspec:\n  ingressClassName: pomerium\n  rules:\n    - host: prometheus.very-very-dark-gray.top\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: prom-kube-prometheus-stack-prometheus\n              port:\n                number: 9090\n  tls:\n    - hosts:\n        - prometheus.very-very-dark-gray.top\n      secretName: tls-secret\n</code></pre> </li> </ul>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#create-grafana-dashboards","title":"Create Grafana dashboards","text":"<p>Once the data source is connected, ready-made Grafana dashboards can be imported directly by ID into the existing Grafana instance.</p> <ul> <li>Trivy Operator Dashboard (ID: 21398) </li> <li>Trivy Operator - Vulnerabilities (ID: 16337) </li> </ul>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#automated-monitoring-of-releases","title":"Automated monitoring of releases","text":"<p>While automatically updating all deployments to the latest stable version of every Docker image may not be the best idea, at least monitoring the availability of new releases should be automated. Considering a small homelab server has accumulated 50 running containers in about three years, manually monitoring for new releases is already a bit much:</p> List of running Docker images as reported by <code>kubectl get pods</code> <pre><code>$ kubectl get pods -A -o jsonpath=\"{.items[*].spec.containers[*].image}\" \\\n    | tr -s '[[:space:]]' '\\n' | sort -u\ncodercom/code-server\ndebian\ndeluan/navidrome:latest\ndocker.io/grafana/grafana:12.3.1\ndocker.io/influxdb:1.11.8\ndocker.io/kubernetesui/dashboard-api:1.12.0\ndocker.io/kubernetesui/dashboard-auth:1.2.4\ndocker.io/kubernetesui/dashboard-metrics-scraper:1.2.2\ndocker.io/kubernetesui/dashboard-web:1.6.2\ndocker.io/mongo:8.0.0\ndocker.io/pomerium/verify\nfireflyiii/core\nghcr.io/advplyr/audiobookshelf:latest\nghcr.io/flannel-io/flannel:v0.26.7\nghcr.io/home-assistant/home-assistant:stable\nghcr.io/ignisda/ryot:v10\nghcr.io/raoulx24/trivy-operator-dashboard:1.7.1\ngotson/komga\nintel/intel-deviceplugin-operator:0.32.0\nintel/intel-gpu-plugin:0.32.0\njellyfin/jellyfin\nkong:3.8\nlscr.io/linuxserver/unifi-network-application:10.0.162\nmirror.gcr.io/aquasec/trivy-operator:0.29.0\npomerium/ingress-controller:main\npostgres:16-alpine\nqmcgaw/ddns-updater:latest\nquay.io/jetstack/cert-manager-cainjector:v1.17.2\nquay.io/jetstack/cert-manager-controller:v1.17.2\nquay.io/jetstack/cert-manager-webhook:v1.17.2\nquay.io/jetstack/version-checker:v0.10.0\nquay.io/metallb/controller:v0.14.9\nquay.io/metallb/speaker:v0.14.9\nquay.io/prometheus/alertmanager:v0.30.0\nquay.io/prometheus/node-exporter:v1.10.2\nquay.io/prometheus-operator/prometheus-config-reloader:v0.87.1\nquay.io/prometheus-operator/prometheus-operator:v0.87.1\nquay.io/prometheus/prometheus:v3.8.1\nregistry.k8s.io/coredns/coredns:v1.11.3\nregistry.k8s.io/etcd:3.5.16-0\nregistry.k8s.io/ingress-nginx/controller:v1.12.1@sha256:d2fbc4ec70d8aa2050dd91a91506e998765e86c96f32cffb56c503c9c34eed5b\nregistry.k8s.io/kube-apiserver:v1.32.4\nregistry.k8s.io/kube-controller-manager:v1.32.4\nregistry.k8s.io/kube-proxy:v1.32.4\nregistry.k8s.io/kube-scheduler:v1.32.4\nregistry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\nregistry.k8s.io/nfd/node-feature-discovery:v0.17.3\ntailscale/k8s-operator:v1.82.0\ntailscale/tailscale:v1.82.0\nyobasystems/alpine-mariadb:latest\n</code></pre>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#jetstack-version-checker","title":"Jetstack Version-Checker","text":"<p>Jetstack Version-Checker is a Kubernetes utility for observing the current versions of images running in the cluster, as well as the latest available upstream. Additionally, it monitors the Kubernetes cluster version against the latest available releases using official Kubernetes release channels. These checks get exposed as Prometheus metrics to be viewed on a dashboard.</p> <p>Install using Helm with the following <code>version-checker-values.yaml</code> to disable Grafana, because it's already running as part of monitoring with InfluxDB and Grafana, and enable the service metrics endpoint for Prometheous to scrape metrics:</p> prometheus/version-checker-values.yaml<pre><code># Disable Grafana installation\ndashboards:\n  enabled: false\n\n# Enable the ServiceMonitors for Prometheus to scrape metrics.\nserviceMonitor:\n  enabled: true\n</code></pre> <p>Add the Jetstack Helm repository and install the chart in the <code>monitoring</code> namespace:</p> <pre><code>$ helm repo add jetstack https://charts.jetstack.io\n\"jetstack\" already exists with the same configuration, skipping\n\n$ helm upgrade --install version-checker jetstack/version-checker \\\n  --namespace monitoring \\\n  --values prometheus/version-checker-values.yaml\nRelease \"version-checker\" does not exist. Installing it now.\nNAME: version-checker\nLAST DEPLOYED: Sun Dec 21 17:08:15 2025\nNAMESPACE: monitoring\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n\n$ kubectl describe servicemonitor version-checker -n monitoring \nName:         version-checker\nNamespace:    monitoring\nLabels:       app=version-checker\n              app.kubernetes.io/managed-by=Helm\nAnnotations:  meta.helm.sh/release-name: version-checker\n              meta.helm.sh/release-namespace: monitoring\nAPI Version:  monitoring.coreos.com/v1\nKind:         ServiceMonitor\nMetadata:\n  Creation Timestamp:  2025-12-21T16:08:18Z\n  Generation:          1\n  Resource Version:    52349035\n  UID:                 37dc3c0f-9bdd-4271-9ee5-810f53e3f3cb\nSpec:\n  Endpoints:\n    Path:  /metrics\n    Port:  web\n  Selector:\n    Match Labels:\n      App:  version-checker\nEvents:     &lt;none&gt;\n</code></pre> <p>Once Version-Checker is running, with its <code>ServiceMonitor</code> exposing metrics, there is (at least one) easy-to-install Grafana dashboard to conveniently display and explore these metrics: Version-Checker (ID: 22745) is the most current and recommended version-checker dashboard; it provides a clean table view of current image versions vs.latest upstream releases:</p> <p></p> <p>Supported Annotations are needed to make <code>version-checker</code>  match only specific version numbers, so that the \"Latest Version\" is picked among semantic version tags rather than versions that should not be running in production (<code>experimental</code>, <code>develop</code>, <code>unstable</code>, etc), in some cases even pinning to a specific major version other than the latest:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: unifi\n  namespace: unifi\nspec:\n  template:\n    metadata:\n      annotations:\n        enable.version-checker.io/unifi: \"true\"\n        match-regex.version-checker.io/unifi: \"^v?\\\\d+\\\\.\\\\d+\\\\.\\\\d+$\"\n        pin-major.version-checker.io/mongo: \"10\"\n        pin-minor.version-checker.io/mongo: \"0\"\n        resolve-sha-to-tags.version-checker.io/unifi: \"true\"\n        use-metadata.version-checker.io/unifi: \"false\"\n        use-sha.version-checker.io/unifi: \"false\"\n      labels:\n        app: unifi\n    spec:\n      containers:\n      - image: lscr.io/linuxserver/unifi-network-application:10.0.162\n</code></pre>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#appendix-install-krew-and-tree","title":"Appendix: install <code>krew</code> and <code>tree</code>","text":"<p>Install <code>krew</code> from Bash, append <code>export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"</code> to the <code>.bashrc</code> and reload it with <code>. .bashrc</code> to have <code>krew</code> installed and ready to use:</p> <pre><code>$ cd /tmp &amp;&amp; (\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\n  KREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\n  tar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n  ./\"${KREW}\" install krew\n)\n++ mktemp -d\n+ cd /tmp/tmp.e0UDytiaZ9\n++ uname\n++ tr '[:upper:]' '[:lower:]'\n+ OS=linux\n++ uname -m\n++ sed -e s/x86_64/amd64/ -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/'\n+ ARCH=amd64\n+ KREW=krew-linux_amd64\n+ curl -fsSLO https://github.com/kubernetes-sigs/krew/releases/latest/download/krew-linux_amd64.tar.gz\n+ tar zxvf krew-linux_amd64.tar.gz\n./._LICENSE\ntar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.provenance'\n./LICENSE\n./._krew-linux_amd64\ntar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.provenance'\n./krew-linux_amd64\n+ ./krew-linux_amd64 install krew\nAdding \"default\" plugin index from https://github.com/kubernetes-sigs/krew-index.git.\nUpdated the local copy of plugin index.\nInstalling plugin: krew\nInstalled plugin: krew\n\\\n | Use this plugin:\n |      kubectl krew\n | Documentation:\n |      https://krew.sigs.k8s.io/\n | Caveats:\n | \\\n |  | krew is now installed! To start using kubectl plugins, you need to add\n |  | krew's installation directory to your PATH:\n |  | \n |  |   * macOS/Linux:\n |  |     - Add the following to your ~/.bashrc or ~/.zshrc:\n |  |         export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n |  |     - Restart your shell.\n |  | \n |  |   * Windows: Add %USERPROFILE%\\.krew\\bin to your PATH environment variable\n |  | \n |  | To list krew commands and to get help, run:\n |  |   $ kubectl krew\n |  | For a full list of available plugins, run:\n |  |   $ kubectl krew search\n |  | \n |  | You can find documentation at\n |  |   https://krew.sigs.k8s.io/docs/user-guide/quickstart/.\n | /\n/\n\n$ echo 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' &gt;&gt; ~/.bashrc\n$ . .bashrc\n</code></pre> <p>Then use <code>krew</code> to isntall <code>kubectl tree</code></p> <pre><code>$ kubectl krew update\nUpdated the local copy of plugin index.\n\n$ kubectl krew install tree\nUpdated the local copy of plugin index.\nInstalling plugin: tree\nInstalled plugin: tree\n\\\n | Use this plugin:\n |      kubectl tree\n | Documentation:\n |      https://github.com/ahmetb/kubectl-tree\n | Caveats:\n | \\\n |  | krew is now installed! To start using kubectl plugins, you need to add\n |  | krew's installation directory to your PATH:\n |  | \n |  |   * macOS/Linux:\n |  |     - Add the following to your ~/.bashrc or ~/.zshrc:\n |  |         export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n |  |     - Restart your shell.\n |  | \n |  |   * Windows: Add %USERPROFILE%\\.krew\\bin to your PATH environment variable\n |  | \n |  | To list krew commands and to get help, run:\n |  |   $ kubectl krew\n |  | For a full list of available plugins, run:\n |  |   $ kubectl krew search\n |  | \n |  | You can find documentation at\n |  |   https://krew.sigs.k8s.io/docs/user-guide/quickstart/.\n | /\n/\n</code></pre> <p>Use the <code>kubectl tree</code> command on deployments to more easily find whether they are ready:</p> <pre><code>$ kubectl tree deployment unifi -n unifi\nNAMESPACE  NAME                            READY  REASON  STATUS   AGE \nunifi      Deployment/unifi                -              -        233d\nunifi      \u251c\u2500ReplicaSet/unifi-584f4847c7   -              -        233d\nunifi      \u251c\u2500ReplicaSet/unifi-7d44b599dc   -              -        94d \nunifi      \u2502 \u2514\u2500Pod/unifi-7d44b599dc-58z92  True           Current  94d \nunifi      \u251c\u2500ReplicaSet/unifi-8997b8865    -              -        199d\nunifi      \u2514\u2500ReplicaSet/unifi-bff88d4df    -              -        95d \n\n$ kubectl tree deployment home-assistant -n home-assistant\nNAMESPACE       NAME                                     READY  REASON  STATUS   AGE \nhome-assistant  Deployment/home-assistant                -              -        237d\nhome-assistant  \u2514\u2500ReplicaSet/home-assistant-77bf44c47b   -              -        237d\nhome-assistant    \u2514\u2500Pod/home-assistant-77bf44c47b-trgrt  True           Current  141d\n</code></pre>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#appendix-kube-bench","title":"Appendix: <code>kube-bench</code>","text":"<pre><code>$ wget https://github.com/aquasecurity/kube-bench/releases/download/v0.14.0/kube-bench_0.14.0_linux_amd64.deb\n$ sudo dpkg -i ./kube-bench_0.14.0_linux_amd64.deb \n\n\n$ sudo kube-bench run \n[INFO] 1 Control Plane Security Configuration\n[INFO] 1.1 Control Plane Node Configuration Files\n[PASS] 1.1.1 Ensure that the API server pod specification file permissions are set to 600 or more restrictive (Automated)\n[PASS] 1.1.2 Ensure that the API server pod specification file ownership is set to root:root (Automated)\n[PASS] 1.1.3 Ensure that the controller manager pod specification file permissions are set to 600 or more restrictive (Automated)\n[PASS] 1.1.4 Ensure that the controller manager pod specification file ownership is set to root:root (Automated)\n[PASS] 1.1.5 Ensure that the scheduler pod specification file permissions are set to 600 or more restrictive (Automated)\n[PASS] 1.1.6 Ensure that the scheduler pod specification file ownership is set to root:root (Automated)\n[PASS] 1.1.7 Ensure that the etcd pod specification file permissions are set to 600 or more restrictive (Automated)\n[PASS] 1.1.8 Ensure that the etcd pod specification file ownership is set to root:root (Automated)\n[WARN] 1.1.9 Ensure that the Container Network Interface file permissions are set to 600 or more restrictive (Manual)\n[PASS] 1.1.10 Ensure that the Container Network Interface file ownership is set to root:root (Manual)\n[PASS] 1.1.11 Ensure that the etcd data directory permissions are set to 700 or more restrictive (Automated)\n[FAIL] 1.1.12 Ensure that the etcd data directory ownership is set to etcd:etcd (Automated)\n[PASS] 1.1.13 Ensure that the default administrative credential file permissions are set to 600 (Automated)\n[PASS] 1.1.14 Ensure that the default administrative credential file ownership is set to root:root (Automated)\n[PASS] 1.1.15 Ensure that the scheduler.conf file permissions are set to 600 or more restrictive (Automated)\n[PASS] 1.1.16 Ensure that the scheduler.conf file ownership is set to root:root (Automated)\n[PASS] 1.1.17 Ensure that the controller-manager.conf file permissions are set to 600 or more restrictive (Automated)\n[PASS] 1.1.18 Ensure that the controller-manager.conf file ownership is set to root:root (Automated)\n[PASS] 1.1.19 Ensure that the Kubernetes PKI directory and file ownership is set to root:root (Automated)\n[PASS] 1.1.20 Ensure that the Kubernetes PKI certificate file permissions are set to 644 or more restrictive (Manual)\n[PASS] 1.1.21 Ensure that the Kubernetes PKI key file permissions are set to 600 (Manual)\n[INFO] 1.2 API Server\n[WARN] 1.2.1 Ensure that the --anonymous-auth argument is set to false (Manual)\n[PASS] 1.2.2 Ensure that the --token-auth-file parameter is not set (Automated)\n[WARN] 1.2.3 Ensure that the --DenyServiceExternalIPs is set (Manual)\n[PASS] 1.2.4 Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate (Automated)\n[FAIL] 1.2.5 Ensure that the --kubelet-certificate-authority argument is set as appropriate (Automated)\n[PASS] 1.2.6 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated)\n[PASS] 1.2.7 Ensure that the --authorization-mode argument includes Node (Automated)\n[PASS] 1.2.8 Ensure that the --authorization-mode argument includes RBAC (Automated)\n[WARN] 1.2.9 Ensure that the admission control plugin EventRateLimit is set (Manual)\n[PASS] 1.2.10 Ensure that the admission control plugin AlwaysAdmit is not set (Automated)\n[WARN] 1.2.11 Ensure that the admission control plugin AlwaysPullImages is set (Manual)\n[PASS] 1.2.12 Ensure that the admission control plugin ServiceAccount is set (Automated)\n[PASS] 1.2.13 Ensure that the admission control plugin NamespaceLifecycle is set (Automated)\n[PASS] 1.2.14 Ensure that the admission control plugin NodeRestriction is set (Automated)\n[FAIL] 1.2.15 Ensure that the --profiling argument is set to false (Automated)\n[FAIL] 1.2.16 Ensure that the --audit-log-path argument is set (Automated)\n[FAIL] 1.2.17 Ensure that the --audit-log-maxage argument is set to 30 or as appropriate (Automated)\n[FAIL] 1.2.18 Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate (Automated)\n[FAIL] 1.2.19 Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate (Automated)\n[WARN] 1.2.20 Ensure that the --request-timeout argument is set as appropriate (Manual)\n[PASS] 1.2.21 Ensure that the --service-account-lookup argument is set to true (Automated)\n[PASS] 1.2.22 Ensure that the --service-account-key-file argument is set as appropriate (Automated)\n[PASS] 1.2.23 Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate (Automated)\n[PASS] 1.2.24 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Automated)\n[PASS] 1.2.25 Ensure that the --client-ca-file argument is set as appropriate (Automated)\n[PASS] 1.2.26 Ensure that the --etcd-cafile argument is set as appropriate (Automated)\n[WARN] 1.2.27 Ensure that the --encryption-provider-config argument is set as appropriate (Manual)\n[WARN] 1.2.28 Ensure that encryption providers are appropriately configured (Manual)\n[WARN] 1.2.29 Ensure that the API Server only makes use of Strong Cryptographic Ciphers (Manual)\n[FAIL] 1.2.30 Ensure that the --service-account-extend-token-expiration parameter is set to false (Automated)\n[INFO] 1.3 Controller Manager\n[WARN] 1.3.1 Ensure that the --terminated-pod-gc-threshold argument is set as appropriate (Manual)\n[FAIL] 1.3.2 Ensure that the --profiling argument is set to false (Automated)\n[PASS] 1.3.3 Ensure that the --use-service-account-credentials argument is set to true (Automated)\n[PASS] 1.3.4 Ensure that the --service-account-private-key-file argument is set as appropriate (Automated)\n[PASS] 1.3.5 Ensure that the --root-ca-file argument is set as appropriate (Automated)\n[PASS] 1.3.6 Ensure that the RotateKubeletServerCertificate argument is set to true (Automated)\n[PASS] 1.3.7 Ensure that the --bind-address argument is set to 127.0.0.1 (Automated)\n[INFO] 1.4 Scheduler\n[FAIL] 1.4.1 Ensure that the --profiling argument is set to false (Automated)\n[PASS] 1.4.2 Ensure that the --bind-address argument is set to 127.0.0.1 (Automated)\n\n== Remediations master ==\n1.1.9 Run the below command (based on the file location on your system) on the control plane node.\nFor example, chmod 600 &lt;path/to/cni/files&gt;\n\n1.1.12 On the etcd server node, get the etcd data directory, passed as an argument --data-dir,\nfrom the command 'ps -ef | grep etcd'.\nRun the below command (based on the etcd data directory found above).\nFor example, chown etcd:etcd /var/lib/etcd\n\n1.2.1 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the below parameter.\n--anonymous-auth=false\n\n1.2.3 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and add the `DenyServiceExternalIPs` plugin\nto the enabled admission plugins, as such --enable-admission-plugin=DenyServiceExternalIPs.\n\n1.2.5 Follow the Kubernetes documentation and setup the TLS connection between\nthe apiserver and kubelets. Then, edit the API server pod specification file\n/etc/kubernetes/manifests/kube-apiserver.yaml on the control plane node and set the\n--kubelet-certificate-authority parameter to the path to the cert file for the certificate authority.\n--kubelet-certificate-authority=&lt;ca-string&gt;\n\n1.2.9 Follow the Kubernetes documentation and set the desired limits in a configuration file.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\nand set the below parameters.\n--enable-admission-plugins=...,EventRateLimit,...\n--admission-control-config-file=&lt;path/to/configuration/file&gt;\n\n1.2.11 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --enable-admission-plugins parameter to include\nAlwaysPullImages.\n--enable-admission-plugins=...,AlwaysPullImages,...\n\n1.2.15 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the below parameter.\n--profiling=false\n\n1.2.16 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --audit-log-path parameter to a suitable path and\nfile where you would like audit logs to be written, for example,\n--audit-log-path=/var/log/apiserver/audit.log\n\n1.2.17 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --audit-log-maxage parameter to 30\nor as an appropriate number of days, for example,\n--audit-log-maxage=30\n\n1.2.18 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --audit-log-maxbackup parameter to 10 or to an appropriate\nvalue. For example,\n--audit-log-maxbackup=10\n\n1.2.19 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --audit-log-maxsize parameter to an appropriate size in MB.\nFor example, to set it as 100 MB, --audit-log-maxsize=100\n\n1.2.20 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\nand set the below parameter as appropriate and if needed.\nFor example, --request-timeout=300s\n\n1.2.27 Follow the Kubernetes documentation and configure a EncryptionConfig file.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the --encryption-provider-config parameter to the path of that file.\nFor example, --encryption-provider-config=&lt;/path/to/EncryptionConfig/File&gt;\n\n1.2.28 Follow the Kubernetes documentation and configure a EncryptionConfig file.\nIn this file, choose aescbc, kms or secretbox as the encryption provider.\n\n1.2.29 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml\non the control plane node and set the below parameter.\n--tls-cipher-suites=TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256,\nTLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,\nTLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,\nTLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,\nTLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,\nTLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\n\n1.2.30 Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --service-account-extend-token-expiration parameter to false.\n`--service-account-extend-token-expiration=false`\nBy default, this parameter is set to true.\n\n1.3.1 Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml\non the control plane node and set the --terminated-pod-gc-threshold to an appropriate threshold,\nfor example, --terminated-pod-gc-threshold=10\n\n1.3.2 Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml\non the control plane node and set the below parameter.\n--profiling=false\n\n1.4.1 Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube-scheduler.yaml file\non the control plane node and set the below parameter.\n--profiling=false\n\n\n== Summary master ==\n40 checks PASS\n10 checks FAIL\n10 checks WARN\n0 checks INFO\n\n[INFO] 2 Etcd Node Configuration\n[INFO] 2 Etcd Node Configuration\n[PASS] 2.1 Ensure that the --cert-file and --key-file arguments are set as appropriate (Automated)\n[PASS] 2.2 Ensure that the --client-cert-auth argument is set to true (Automated)\n[PASS] 2.3 Ensure that the --auto-tls argument is not set to true (Automated)\n[PASS] 2.4 Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate (Automated)\n[PASS] 2.5 Ensure that the --peer-client-cert-auth argument is set to true (Automated)\n[PASS] 2.6 Ensure that the --peer-auto-tls argument is not set to true (Automated)\n[PASS] 2.7 Ensure that a unique Certificate Authority is used for etcd (Manual)\n\n== Summary etcd ==\n7 checks PASS\n0 checks FAIL\n0 checks WARN\n0 checks INFO\n\n[INFO] 3 Control Plane Configuration\n[INFO] 3.1 Authentication and Authorization\n[WARN] 3.1.1 Client certificate authentication should not be used for users (Manual)\n[WARN] 3.1.2 Service account token authentication should not be used for users (Manual)\n[WARN] 3.1.3 Bootstrap token authentication should not be used for users (Manual)\n[INFO] 3.2 Logging\n[WARN] 3.2.1 Ensure that a minimal audit policy is created (Manual)\n[WARN] 3.2.2 Ensure that the audit policy covers key security concerns (Manual)\n\n== Remediations controlplane ==\n3.1.1 Alternative mechanisms provided by Kubernetes such as the use of OIDC should be\nimplemented in place of client certificates.\n\n3.1.2 Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented\nin place of service account tokens.\n\n3.1.3 Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented\nin place of bootstrap tokens.\n\n3.2.1 Create an audit policy file for your cluster.\n\n3.2.2 Review the audit policy provided for the cluster and ensure that it covers\nat least the following areas,\n- Access to Secrets managed by the cluster. Care should be taken to only\n  log Metadata for requests to Secrets, ConfigMaps, and TokenReviews, in\n  order to avoid risk of logging sensitive data.\n- Modification of Pod and Deployment objects.\n- Use of `pods/exec`, `pods/portforward`, `pods/proxy` and `services/proxy`.\nFor most requests, minimally logging at the Metadata level is recommended\n(the most basic level of logging).\n\n\n== Summary controlplane ==\n0 checks PASS\n0 checks FAIL\n5 checks WARN\n0 checks INFO\n\n[INFO] 4 Worker Node Security Configuration\n[INFO] 4.1 Worker Node Configuration Files\n[FAIL] 4.1.1 Ensure that the kubelet service file permissions are set to 600 or more restrictive (Automated)\n[PASS] 4.1.2 Ensure that the kubelet service file ownership is set to root:root (Automated)\n[WARN] 4.1.3 If proxy kubeconfig file exists ensure permissions are set to 600 or more restrictive (Manual)\n[WARN] 4.1.4 If proxy kubeconfig file exists ensure ownership is set to root:root (Manual)\n[PASS] 4.1.5 Ensure that the --kubeconfig kubelet.conf file permissions are set to 600 or more restrictive (Automated)\n[PASS] 4.1.6 Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root (Automated)\n[PASS] 4.1.7 Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Manual)\n[PASS] 4.1.8 Ensure that the client certificate authorities file ownership is set to root:root (Manual)\n[FAIL] 4.1.9 If the kubelet config.yaml configuration file is being used validate permissions set to 600 or more restrictive (Automated)\n[PASS] 4.1.10 If the kubelet config.yaml configuration file is being used validate file ownership is set to root:root (Automated)\n[INFO] 4.2 Kubelet\n[PASS] 4.2.1 Ensure that the --anonymous-auth argument is set to false (Automated)\n[PASS] 4.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated)\n[PASS] 4.2.3 Ensure that the --client-ca-file argument is set as appropriate (Automated)\n[PASS] 4.2.4 Verify that if defined, the --read-only-port argument is set to 0 (Manual)\n[PASS] 4.2.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Manual)\n[PASS] 4.2.6 Ensure that the --make-iptables-util-chains argument is set to true (Automated)\n[PASS] 4.2.7 Ensure that the --hostname-override argument is not set (Manual)\n[PASS] 4.2.8 Ensure that the eventRecordQPS argument is set to a level which ensures appropriate event capture (Manual)\n[WARN] 4.2.9 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Manual)\n[PASS] 4.2.10 Ensure that the --rotate-certificates argument is not set to false (Automated)\n[PASS] 4.2.11 Verify that the RotateKubeletServerCertificate argument is set to true (Manual)\n[WARN] 4.2.12 Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers (Manual)\n[WARN] 4.2.13 Ensure that a limit is set on pod PIDs (Manual)\n[WARN] 4.2.14 Ensure that the --seccomp-default parameter is set to true (Manual)\n[WARN] 4.2.15 Ensure that the --IPAddressDeny is set to any (Manual)\n[INFO] 4.3 kube-proxy\n[PASS] 4.3.1 Ensure that the kube-proxy metrics service is bound to localhost (Automated)\n\n== Remediations node ==\n4.1.1 Run the below command (based on the file location on your system) on the each worker node.\nFor example, chmod 600 /lib/systemd/system/kubelet.service\n\n4.1.3 Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchmod 600 /etc/kubernetes/proxy.conf\n\n4.1.4 Run the below command (based on the file location on your system) on the each worker node.\nFor example, chown root:root /etc/kubernetes/proxy.conf\n\n4.1.9 Run the following command (using the config file location identified in the Audit step)\nchmod 600 /var/lib/kubelet/config.yaml\n\n4.2.9 If using a Kubelet config file, edit the file to set `tlsCertFile` to the location\nof the certificate file to use to identify this Kubelet, and `tlsPrivateKeyFile`\nto the location of the corresponding private key file.\nIf using command line arguments, edit the kubelet service file\n/lib/systemd/system/kubelet.service on each worker node and\nset the below parameters in KUBELET_CERTIFICATE_ARGS variable.\n--tls-cert-file=&lt;path/to/tls-certificate-file&gt;\n--tls-private-key-file=&lt;path/to/tls-key-file&gt;\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n4.2.12 If using a Kubelet config file, edit the file to set `tlsCipherSuites` to\nTLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\nor to a subset of these values.\nIf using executable arguments, edit the kubelet service file\n/lib/systemd/system/kubelet.service on each worker node and\nset the --tls-cipher-suites parameter as follows, or to a subset of these values.\n--tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n4.2.13 Decide on an appropriate level for this parameter and set it,\neither via the --pod-max-pids command line parameter or the PodPidsLimit configuration file setting.\n\n4.2.14 Set the parameter, either via the --seccomp-default command line parameter or the\nseccompDefault configuration file setting.\nBy default the seccomp profile is not enabled.\n\n4.2.15 Configuring the setting IPAddressDeny=any will deny service to any IP address not specified in the complimentary setting IPAddressAllow configuration parameter (\nIPAddressDeny=any\nIPAddressAllow={{ kubelet_secure_addresses }}\n*Note\n  kubelet_secure_addresses: \"localhost link-local {{ kube_pods_subnets |regex_replace(',', ' ') }} {{ kube_node_addresses }} {{ loadbalancer_apiserver.address | default('')\"\nBy default IPAddressDeny is not enabled.\n\n\n== Summary node ==\n17 checks PASS\n2 checks FAIL\n7 checks WARN\n0 checks INFO\n\n[INFO] 5 Kubernetes Policies\n[INFO] 5.1 RBAC and Service Accounts\n[WARN] 5.1.1 Ensure that the cluster-admin role is only used where required (Manual)\n[PASS] 5.1.2 Minimize access to secrets (Manual)\n[WARN] 5.1.3 Minimize wildcard use in Roles and ClusterRoles (Manual)\n[PASS] 5.1.4 Minimize access to create pods (Manual)\n[WARN] 5.1.5 Ensure that default service accounts are not actively used (Manual)\n[WARN] 5.1.6 Ensure that Service Account Tokens are only mounted where necessary (Manual)\n[WARN] 5.1.7 Avoid use of system:masters group (Manual)\n[WARN] 5.1.8 Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster (Manual)\n[WARN] 5.1.9 Minimize access to create persistent volumes (Manual)\n[WARN] 5.1.10 Minimize access to the proxy sub-resource of nodes (Manual)\n[WARN] 5.1.11 Minimize access to the approval sub-resource of certificatesigningrequests objects (Manual)\n[WARN] 5.1.12 Minimize access to webhook configuration objects (Manual)\n[WARN] 5.1.13 Minimize access to the service account token creation (Manual)\n[INFO] 5.2 Pod Security Standards\n[WARN] 5.2.1 Ensure that the cluster has at least one active policy control mechanism in place (Manual)\n[WARN] 5.2.2 Minimize the admission of privileged containers (Manual)\n[PASS] 5.2.3 Minimize the admission of containers wishing to share the host process ID namespace (Manual)\n[PASS] 5.2.4 Minimize the admission of containers wishing to share the host IPC namespace (Manual)\n[WARN] 5.2.5 Minimize the admission of containers wishing to share the host network namespace (Manual)\n[WARN] 5.2.6 Minimize the admission of containers with allowPrivilegeEscalation (Manual)\n[WARN] 5.2.7 Minimize the admission of root containers (Manual)\n[WARN] 5.2.8 Minimize the admission of containers with the NET_RAW capability (Manual)\n[WARN] 5.2.9 Minimize the admission of containers with added capabilities (Manual)\n[WARN] 5.2.10 Minimize the admission of containers with capabilities assigned (Manual)\n[WARN] 5.2.11 Minimize the admission of Windows HostProcess containers (Manual)\n[WARN] 5.2.12 Minimize the admission of HostPath volumes (Manual)\n[WARN] 5.2.13 Minimize the admission of containers which use HostPorts (Manual)\n[INFO] 5.3 Network Policies and CNI\n[WARN] 5.3.1 Ensure that the CNI in use supports NetworkPolicies (Manual)\n[WARN] 5.3.2 Ensure that all Namespaces have NetworkPolicies defined (Manual)\n[INFO] 5.4 Secrets Management\n[WARN] 5.4.1 Prefer using Secrets as files over Secrets as environment variables (Manual)\n[WARN] 5.4.2 Consider external secret storage (Manual)\n[INFO] 5.5 Extensible Admission Control\n[WARN] 5.5.1 Configure Image Provenance using ImagePolicyWebhook admission controller (Manual)\n[INFO] 5.6 General Policies\n[WARN] 5.6.1 Create administrative boundaries between resources using namespaces (Manual)\n[WARN] 5.6.2 Ensure that the seccomp profile is set to docker/default in your Pod definitions (Manual)\n[WARN] 5.6.3 Apply SecurityContext to your Pods and Containers (Manual)\n[WARN] 5.6.4 The default namespace should not be used (Manual)\n\n== Remediations policies ==\n5.1.1 Identify all clusterrolebindings to the cluster-admin role. Check if they are used and\nif they need this role or if they could use a role with fewer privileges.\nWhere possible, first bind users to a lower privileged role and then remove the\nclusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name]\nCondition: is_compliant is false if rolename is not cluster-admin and rolebinding is cluster-admin.\n\n5.1.3 Where possible replace any use of wildcards [\"*\"] in roles and clusterroles with specific\nobjects or actions.\nCondition: role_is_compliant is false if [\"*\"] is found in rules.\nCondition: clusterrole_is_compliant is false if [\"*\"] is found in rules.\n\n5.1.5 Create explicit service accounts wherever a Kubernetes workload requires specific access\nto the Kubernetes API server.\nModify the configuration of each default service account to include this value\n`automountServiceAccountToken: false`.\n\n5.1.6 Modify the definition of ServiceAccounts and Pods which do not need to mount service\naccount tokens to disable it, with `automountServiceAccountToken: false`.\nIf both the ServiceAccount and the Pod's .spec specify a value for automountServiceAccountToken, the Pod spec takes precedence.\nCondition: Pod is_compliant to true when\n  - ServiceAccount is automountServiceAccountToken: false and Pod is automountServiceAccountToken: false or notset\n  - ServiceAccount is automountServiceAccountToken: true notset and Pod is automountServiceAccountToken: false\n\n5.1.7 Remove the system:masters group from all users in the cluster.\n\n5.1.8 Where possible, remove the impersonate, bind and escalate rights from subjects.\n\n5.1.9 Where possible, remove create access to PersistentVolume objects in the cluster.\n\n5.1.10 Where possible, remove access to the proxy sub-resource of node objects.\n\n5.1.11 Where possible, remove access to the approval sub-resource of certificatesigningrequests objects.\n\n5.1.12 Where possible, remove access to the validatingwebhookconfigurations or mutatingwebhookconfigurations objects\n\n5.1.13 Where possible, remove access to the token sub-resource of serviceaccount objects.\n\n5.2.1 Ensure that either Pod Security Admission or an external policy control system is in place\nfor every namespace which contains user workloads.\n\n5.2.2 Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of privileged containers.\nAudit: the audit list all pods' containers to retrieve their .securityContext.privileged value.\nCondition: is_compliant is false if container's `.securityContext.privileged` is set to `true`.\nDefault: by default, there are no restrictions on the creation of privileged containers.\n\n5.2.5 Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of `hostNetwork` containers.\nAudit: the audit retrieves each Pod' spec.hostNetwork.\nCondition: is_compliant is false if Pod's spec.hostNetwork is set to `true`.\nDefault: by default, there are no restrictions on the creation of hostNetwork containers.\n\n5.2.6 Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers with `.securityContext.allowPrivilegeEscalation` set to `true`.\nAudit: the audit retrieves each Pod's container(s) `.securityContext.allowPrivilegeEscalation`.\nCondition: is_compliant is false if container's `.securityContext.allowPrivilegeEscalation` is set to `true`.\nDefault: If notset, privilege escalation is allowed (default to true). However if PSP/PSA is used with a `restricted` profile,\nprivilege escalation is explicitly disallowed unless configured otherwise.\n\n5.2.7 Create a policy for each namespace in the cluster, ensuring that either `MustRunAsNonRoot`\nor `MustRunAs` with the range of UIDs not including 0, is set.\n\n5.2.8 Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers with the `NET_RAW` capability.\n\n5.2.9 Ensure that `allowedCapabilities` is not present in policies for the cluster unless\nit is set to an empty array.\nAudit: the audit retrieves each Pod's container(s) added capabilities.\nCondition: is_compliant is false if added capabilities are added for a given container.\nDefault: Containers run with a default set of capabilities as assigned by the Container Runtime.\n\n5.2.10 Review the use of capabilites in applications running on your cluster. Where a namespace\ncontains applications which do not require any Linux capabities to operate consider adding\na PSP which forbids the admission of containers which do not drop all capabilities.\n\n5.2.11 Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers that have `.securityContext.windowsOptions.hostProcess` set to `true`.\n\n5.2.12 Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers with `hostPath` volumes.\n\n5.2.13 Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers which use `hostPort` sections.\n\n5.3.1 If the CNI plugin in use does not support network policies, consideration should be given to\nmaking use of a different plugin, or finding an alternate mechanism for restricting traffic\nin the Kubernetes cluster.\n\n5.3.2 Follow the documentation and create NetworkPolicy objects as you need them.\n\n5.4.1 If possible, rewrite application code to read Secrets from mounted secret files, rather than\nfrom environment variables.\n\n5.4.2 Refer to the Secrets management options offered by your cloud provider or a third-party\nsecrets management solution.\n\n5.5.1 Follow the Kubernetes documentation and setup image provenance.\n\n5.6.1 Follow the documentation and create namespaces for objects in your deployment as you need\nthem.\n\n5.6.2 Use `securityContext` to enable the docker/default seccomp profile in your pod definitions.\nAn example is as below:\n  securityContext:\n    seccompProfile:\n      type: RuntimeDefault\n\n5.6.3 Follow the Kubernetes documentation and apply SecurityContexts to your Pods. For a\nsuggested list of SecurityContexts, you may refer to the CIS Security Benchmark for Docker\nContainers.\n\n5.6.4 Ensure that namespaces are created to allow for appropriate segregation of Kubernetes\nresources and that all new resources are created in a specific namespace.\n\n\n== Summary policies ==\n4 checks PASS\n0 checks FAIL\n31 checks WARN\n0 checks INFO\n\n== Summary total ==\n68 checks PASS\n12 checks FAIL\n53 checks WARN\n0 checks INFO\n</code></pre>"},{"location":"blog/2025/12/21/monitoring-a-kubernetes-cluster-for-vulnerabilities/#appendix-more-grafana-dashboards-for-prometheous","title":"Appendix: more Grafana dashboards for Prometheous","text":"<p>Prometheus will be scrapping many more metrics about the Kubernetes cluster and its workloads and these can be explored using additional Grafana dashboards:</p> <ul> <li> <p>Kubernetes / Overview (ID: 21410)     is designed to be a one-stop solution for monitoring your Kubernetes cluster-     This dashboard provides a comprehensive view of the health and resource utilization     within a Kubernetes cluster. It is designed to assist cluster administrators and     DevOps engineers in monitoring and optimizing their deployments.</p> </li> <li> <p>Kubernetes / Views / Pods (ID: 15760)     is a modern 'Pods View' dashboard for your Kubernetes cluster(s). Made for     kube-prometheus-stack and take advantage of the latest Grafana features.</p> </li> <li> <p>Docker and system monitoring (ID: 893)     is a simple overview of the most important Docker host and container metrics.     This dashboard display Docker and system metric, the aim it\u2019s to have all the     metric on one dashboard. The first lines represents the system metric with gauge     and text/graph (easy to visualize and minimalist).</p> </li> <li> <p>Monitoring Golden Signals for Kubernetes (ID: 21073)     is a comprehensive Grafana dashboard that provides a detailed view of the key     metrics that can be categorized under the Four Golden Signals: Latency, Traffic,     Errors, and Saturation. Designed for quick issue identification, this dashboard     offers real-time insights into the performance and health of your Kubernetes     cluster. Ideal for DevOps teams and SREs, this dashboard is your go-to resource for     proactive Kubernetes cluster management.</p> </li> </ul>"},{"location":"blog/2026/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-octavo/","title":"Upgrading single-node Kubernetes cluster on Ubuntu Studio 24.04 (octavo)","text":"<p>Upgrading the single-node kubernetes cluster on <code>lexicon</code> went smoothly, so it's time to repeat the process on <code>octavo</code> and <code>alfred</code>, especially since the current version (1.32) will be the next one up to go End Of Life in Feb 28, 2026 and a preemptive Kubernetes Certificate Check reveals certificates are due to expire by Feb 22 on <code>alfred</code> and April 26 on <code>octavo</code>.</p> <p>Checking deployments before upgrading kubeadm clusters again found results mostly reassuring for version 1.34.</p>"},{"location":"blog/2026/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-octavo/#upgrade-to-133","title":"Upgrade to 1.33","text":"<p>The first upgrade would have required updating Ingress-NGINX to version 1.14 first, but this requirement has been eliminated by replacing Ingress-NGINX with Pomerium.</p> <pre><code># kubeadm version\nkubeadm version: &amp;version.Info{Major:\"1\", Minor:\"32\", GitVersion:\"v1.32.4\", GitCommit:\"59526cd4867447956156ae3a602fcbac10a2c335\", GitTreeState:\"clean\", BuildDate:\"2025-04-22T16:02:27Z\", GoVersion:\"go1.23.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\n# kubectl get nodes\nNAME     STATUS   ROLES           AGE    VERSION\noctavo   Ready    control-plane   235d   v1.32.4\n</code></pre> <p>Upgrade version 1.32.x to version 1.33.x starts by determining which version to upgrade to and updating the minor version in the repository configuration and then find the latest patch version, much like the previous Upgrade to 1.32:</p> <pre><code># curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key \\\n  | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# vi /etc/apt/sources.list.d/kubernetes.list\ndeb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /\n\n# apt update\n# apt-cache madison kubeadm\n   kubeadm | 1.33.7-1.1 | https://pkgs.k8s.io/core:/stable:/v1.33/deb  Packages\n   kubeadm | 1.33.6-1.1 | https://pkgs.k8s.io/core:/stable:/v1.33/deb  Packages\n   kubeadm | 1.33.5-1.1 | https://pkgs.k8s.io/core:/stable:/v1.33/deb  Packages\n   kubeadm | 1.33.4-1.1 | https://pkgs.k8s.io/core:/stable:/v1.33/deb  Packages\n   kubeadm | 1.33.3-1.1 | https://pkgs.k8s.io/core:/stable:/v1.33/deb  Packages\n   kubeadm | 1.33.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.33/deb  Packages\n   kubeadm | 1.33.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.33/deb  Packages\n   kubeadm | 1.33.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.33/deb  Packages\n</code></pre> <p>Now, before updating <code>kubeadm</code>, drain the node:</p> <code># kubectl drain --ignore-daemonsets --delete-emptydir-data octavo</code> <pre><code># kubectl drain --ignore-daemonsets --delete-emptydir-data octavo\nWarning: ignoring DaemonSet-managed Pods: intel-device-plugins-gpu/intel-gpu-plugin-gpudeviceplugin-wpsqp, kube-flannel/kube-flannel-ds-m8h8n, kube-system/kube-proxy-zlhbr, metallb-system/speaker-92c2g, monitoring/prom-prometheus-node-exporter-r24zl, node-feature-discovery/node-feature-discovery-worker-jjglh\nevicting pod audiobookshelf/audiobookshelf-56749d45d4-bs484\nevicting pod code-server/code-server-85785b5848-t5qvz\nevicting pod komga/komga-5984f79858-c7tb7\nevicting pod cert-manager/cert-manager-webhook-78cb4cf989-wb4wz\nevicting pod cert-manager/cert-manager-cainjector-666b8b6b66-fl6rp\nevicting pod default/command-demo\nevicting pod cert-manager/cert-manager-7d67448f59-c2fgn\nevicting pod node-feature-discovery/node-feature-discovery-master-767dcc6cb8-rsrcc\nevicting pod firefly-iii/firefly-iii-f865768bf-f8tp6\nevicting pod pomerium/pomerium-gen-secrets-8sx7m\nevicting pod monitoring/prometheus-prom-kube-prometheus-stack-prometheus-0\nevicting pod metallb-system/controller-bb5f47665-vt57w\nevicting pod monitoring/prom-kube-prometheus-stack-operator-645fd684d6-n6qpf\nevicting pod navidrome/navidrome-75945786b-82mst\nevicting pod kubernetes-dashboard/kubernetes-dashboard-api-64c997cbcc-cxbjt\nevicting pod pomerium/pomerium-765bfd4df6-d8ft2\nevicting pod ryot/postgres-84f94497fd-nxt9v\nevicting pod home-assistant/home-assistant-5db584bd48-lf9v9\nevicting pod kubernetes-dashboard/kubernetes-dashboard-metrics-scraper-76df4956c4-bx6wj\nevicting pod monitoring/version-checker-547c9c998c-hmcff\nevicting pod default/ddns-updater-75d5df4647-dztn4\nevicting pod kube-system/coredns-668d6bf9bc-zpwqm\nevicting pod kube-system/coredns-668d6bf9bc-7fcdw\nevicting pod node-feature-discovery/node-feature-discovery-gc-5b65f7f5b6-jwpt4\nevicting pod kubernetes-dashboard/kubernetes-dashboard-web-56df7655d9-jc8ss\nevicting pod ryot/ryot-7d64cb5797-5kj65\nevicting pod unifi/mongo-7878d99ff5-q8lb6\nevicting pod firefly-iii/firefly-iii-mysql-659f959f57-jxqcz\nevicting pod homepage/homepage-58c79b7856-csdcm\nevicting pod monitoring/prom-kube-state-metrics-8576986c6b-xqcwl\nevicting pod tailscale/ts-home-assistant-tailscale-mdqlt-0\nevicting pod tailscale/operator-68b5df646d-c6tm9\nevicting pod kubernetes-dashboard/kubernetes-dashboard-kong-79867c9c48-dwncj\nevicting pod tailscale/ts-kubernetes-dashboard-ingress-tailscale-jhb6z-0\nevicting pod kubernetes-dashboard/kubernetes-dashboard-auth-5cf6848ffd-5vcm7\nevicting pod monitoring/grafana-695b647cb4-9ql86\nevicting pod monitoring/influxdb-76d6df578-z6w8n\nevicting pod monitoring/alertmanager-prom-kube-prometheus-stack-alertmanager-0\nevicting pod trivy-system/trivy-operator-59489786c6-hgg2b\nevicting pod monitoring/trivy-operator-dashboard-66d4c85cd6-rs2dq\nevicting pod unifi/unifi-6548ccd8c8-xkf24\nevicting pod media-center/jellyfin-6bbd7f9798-zvxq9\nevicting pod intel-device-plugins-gpu/inteldeviceplugins-controller-manager-7fb7c8d6b9-phnvq\npod/pomerium-gen-secrets-8sx7m evicted\npod/controller-bb5f47665-vt57w evicted\nI0110 13:15:04.930345 2878823 request.go:729] Waited for 1.000804357s due to client-side throttling, not priority and fairness, request: POST:https://10.0.0.8:6443/api/v1/namespaces/kubernetes-dashboard/pods/kubernetes-dashboard-metrics-scraper-76df4956c4-bx6wj/eviction\npod/navidrome-75945786b-82mst evicted\npod/prometheus-prom-kube-prometheus-stack-prometheus-0 evicted\npod/pomerium-765bfd4df6-d8ft2 evicted\npod/kubernetes-dashboard-api-64c997cbcc-cxbjt evicted\npod/cert-manager-webhook-78cb4cf989-wb4wz evicted\npod/prom-kube-prometheus-stack-operator-645fd684d6-n6qpf evicted\npod/node-feature-discovery-master-767dcc6cb8-rsrcc evicted\npod/komga-5984f79858-c7tb7 evicted\npod/version-checker-547c9c998c-hmcff evicted\npod/command-demo evicted\npod/code-server-85785b5848-t5qvz evicted\npod/cert-manager-cainjector-666b8b6b66-fl6rp evicted\npod/cert-manager-7d67448f59-c2fgn evicted\npod/kubernetes-dashboard-metrics-scraper-76df4956c4-bx6wj evicted\npod/kubernetes-dashboard-web-56df7655d9-jc8ss evicted\npod/postgres-84f94497fd-nxt9v evicted\npod/node-feature-discovery-gc-5b65f7f5b6-jwpt4 evicted\npod/ryot-7d64cb5797-5kj65 evicted\npod/firefly-iii-mysql-659f959f57-jxqcz evicted\npod/mongo-7878d99ff5-q8lb6 evicted\npod/audiobookshelf-56749d45d4-bs484 evicted\npod/prom-kube-state-metrics-8576986c6b-xqcwl evicted\npod/homepage-58c79b7856-csdcm evicted\npod/ts-home-assistant-tailscale-mdqlt-0 evicted\npod/operator-68b5df646d-c6tm9 evicted\npod/coredns-668d6bf9bc-zpwqm evicted\npod/ddns-updater-75d5df4647-dztn4 evicted\npod/coredns-668d6bf9bc-7fcdw evicted\npod/kubernetes-dashboard-auth-5cf6848ffd-5vcm7 evicted\npod/ts-kubernetes-dashboard-ingress-tailscale-jhb6z-0 evicted\npod/influxdb-76d6df578-z6w8n evicted\npod/home-assistant-5db584bd48-lf9v9 evicted\npod/grafana-695b647cb4-9ql86 evicted\npod/trivy-operator-59489786c6-hgg2b evicted\npod/trivy-operator-dashboard-66d4c85cd6-rs2dq evicted\npod/alertmanager-prom-kube-prometheus-stack-alertmanager-0 evicted\npod/jellyfin-6bbd7f9798-zvxq9 evicted\npod/inteldeviceplugins-controller-manager-7fb7c8d6b9-phnvq evicted\nI0110 13:15:15.098024 2878823 request.go:729] Waited for 2.19577063s due to client-side throttling, not priority and fairness, request: GET:https://10.0.0.8:6443/api/v1/namespaces/firefly-iii/pods/firefly-iii-f865768bf-f8tp6\npod/kubernetes-dashboard-kong-79867c9c48-dwncj evicted\npod/unifi-6548ccd8c8-xkf24 evicted\npod/firefly-iii-f865768bf-f8tp6 evicted\nnode/octavo drained\n</code></pre> <p>The latest patch version is 1.33.7 and that is the one to upgrade control plane nodes:</p> <code># apt-get update &amp;&amp; apt-get install -y kubeadm='1.33.7-*' &amp;&amp; apt-mark hold kubeadm</code> <pre><code># apt-mark unhold kubeadm &amp;&amp; \\\n  apt-get update &amp;&amp; apt-get install -y kubeadm='1.33.7-*' &amp;&amp; \\\n  apt-mark hold kubeadm\nCanceled hold on kubeadm.\nFetched 6,581 B in 1s (7,931 B/s)\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.33.7-1.1' (isv:kubernetes:core:stable:v1.33:pkgs.k8s.io [amd64]) for 'kubeadm'\nThe following packages will be upgraded:\n  kubeadm\n1 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\nNeed to get 12.7 MB of archives.\nAfter this operation, 3,609 kB of additional disk space will be used.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.33/deb  kubeadm 1.33.7-1.1 [12.7 MB]\nFetched 12.7 MB in 0s (45.6 MB/s)\n(Reading database ... 168496 files and directories currently installed.)\nPreparing to unpack .../kubeadm_1.33.7-1.1_amd64.deb ...\nUnpacking kubeadm (1.33.7-1.1) over (1.32.4-1.1) ...\nSetting up kubeadm (1.33.7-1.1) ...\nScanning processes...                                                                                            \nScanning processor microcode...                                                                                  \nScanning linux images...                                                                                         \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubeadm set on hold.\n</code></pre> <p>Verify and apply the upgrade plan:</p> <code># kubeadm upgrade plan</code> <pre><code># kubeadm upgrade plan\n[preflight] Running pre-flight checks.\n[upgrade/config] Reading configuration from the \"kubeadm-config\" ConfigMap in namespace \"kube-system\"...\n[upgrade/config] Use 'kubeadm init phase upload-config --config your-config-file' to re-upload it.\n[upgrade] Running cluster health checks\nW0110 13:20:13.836378 2975827 health.go:135] The preflight check \"CreateJob\" was skipped because there are no schedulable Nodes in the cluster.\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: 1.32.4\n[upgrade/versions] kubeadm version: v1.33.7\nI0110 13:20:14.375889 2975827 version.go:261] remote version is much newer: v1.35.0; falling back to: stable-1.33\n[upgrade/versions] Target version: v1.33.7\n[upgrade/versions] Latest version in the v1.32 series: v1.32.11\n\nComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\nCOMPONENT   NODE      CURRENT   TARGET\nkubelet     octavo    v1.32.4   v1.32.11\n\nUpgrade to the latest version in the v1.32 series:\n\nCOMPONENT                 NODE      CURRENT    TARGET\nkube-apiserver            octavo    v1.32.4    v1.32.11\nkube-controller-manager   octavo    v1.32.4    v1.32.11\nkube-scheduler            octavo    v1.32.4    v1.32.11\nkube-proxy                          1.32.4     v1.32.11\nCoreDNS                             v1.11.3    v1.12.0\netcd                      octavo    3.5.16-0   3.5.24-0\n\nYou can now apply the upgrade by executing the following command:\n\n        kubeadm upgrade apply v1.32.11\n\n_____________________________________________________________________\n\nComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\nCOMPONENT   NODE      CURRENT   TARGET\nkubelet     octavo    v1.32.4   v1.33.7\n\nUpgrade to the latest stable version:\n\nCOMPONENT                 NODE      CURRENT    TARGET\nkube-apiserver            octavo    v1.32.4    v1.33.7\nkube-controller-manager   octavo    v1.32.4    v1.33.7\nkube-scheduler            octavo    v1.32.4    v1.33.7\nkube-proxy                          1.32.4     v1.33.7\nCoreDNS                             v1.11.3    v1.12.0\netcd                      octavo    3.5.16-0   3.5.24-0\n\nYou can now apply the upgrade by executing the following command:\n\n        kubeadm upgrade apply v1.33.7\n\n_____________________________________________________________________\n\n\nThe table below shows the current state of component configs as understood by this version of kubeadm.\nConfigs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or\nresetting to kubeadm defaults before a successful upgrade can be performed. The version to manually\nupgrade to is denoted in the \"PREFERRED VERSION\" column.\n\nAPI GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED\nkubeproxy.config.k8s.io   v1alpha1          v1alpha1            no\nkubelet.config.k8s.io     v1beta1           v1beta1             no\n_____________________________________________________________________\n</code></pre> <code># kubeadm upgrade apply v1.33.7</code> <pre><code># kubeadm upgrade apply v1.33.7\n[upgrade] Reading configuration from the \"kubeadm-config\" ConfigMap in namespace \"kube-system\"...\n[upgrade] Use 'kubeadm init phase upload-config --config your-config-file' to re-upload it.\n[upgrade/preflight] Running preflight checks\n[upgrade] Running cluster health checks\nW0110 13:21:59.695587 3005821 health.go:135] The preflight check \"CreateJob\" was skipped because there are no schedulable Nodes in the cluster.\n[upgrade/preflight] You have chosen to upgrade the cluster version to \"v1.33.7\"\n[upgrade/versions] Cluster version: v1.32.4\n[upgrade/versions] kubeadm version: v1.33.7\n[upgrade] Are you sure you want to proceed? [y/N]: y\n[upgrade/preflight] Pulling images required for setting up a Kubernetes cluster\n[upgrade/preflight] This might take a minute or two, depending on the speed of your internet connection\n[upgrade/preflight] You can also perform this action beforehand using 'kubeadm config images pull'\n[upgrade/control-plane] Upgrading your static Pod-hosted control plane to version \"v1.33.7\" (timeout: 5m0s)...\n[upgrade/staticpods] Writing new Static Pod manifests to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests1594195257\"\n[upgrade/staticpods] Preparing for \"etcd\" upgrade\n[upgrade/staticpods] Renewing etcd-server certificate\n[upgrade/staticpods] Renewing etcd-peer certificate\n[upgrade/staticpods] Renewing etcd-healthcheck-client certificate\n[upgrade/staticpods] Moving new manifest to \"/etc/kubernetes/manifests/etcd.yaml\" and backing up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2026-01-10-13-22-17/etcd.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=etcd\n[upgrade/staticpods] Component \"etcd\" upgraded successfully!\n[upgrade/etcd] Waiting for etcd to become available\n[upgrade/staticpods] Preparing for \"kube-apiserver\" upgrade\n[upgrade/staticpods] Renewing apiserver certificate\n[upgrade/staticpods] Renewing apiserver-kubelet-client certificate\n[upgrade/staticpods] Renewing front-proxy-client certificate\n[upgrade/staticpods] Renewing apiserver-etcd-client certificate\n[upgrade/staticpods] Moving new manifest to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" and backing up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2026-01-10-13-22-17/kube-apiserver.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=kube-apiserver\n[upgrade/staticpods] Component \"kube-apiserver\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-controller-manager\" upgrade\n[upgrade/staticpods] Renewing controller-manager.conf certificate\n[upgrade/staticpods] Moving new manifest to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" and backing up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2026-01-10-13-22-17/kube-controller-manager.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=kube-controller-manager\n[upgrade/staticpods] Component \"kube-controller-manager\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-scheduler\" upgrade\n[upgrade/staticpods] Renewing scheduler.conf certificate\n[upgrade/staticpods] Moving new manifest to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" and backing up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2026-01-10-13-22-17/kube-scheduler.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=kube-scheduler\n[upgrade/staticpods] Component \"kube-scheduler\" upgraded successfully!\n[upgrade/control-plane] The control plane instance for this node was successfully upgraded!\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upgrade/kubeconfig] The kubeconfig files for this node were successfully upgraded!\nW0110 13:25:05.926913 3005821 postupgrade.go:117] Using temporary directory /etc/kubernetes/tmp/kubeadm-kubelet-config1025000842 for kubelet config. To override it set the environment variable KUBEADM_UPGRADE_DRYRUN_DIR\n[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config1025000842/config.yaml\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[upgrade/kubelet-config] The kubelet configuration for this node was successfully upgraded!\n[upgrade/bootstrap-token] Configuring bootstrap token and cluster-info RBAC rules\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\n[upgrade] SUCCESS! A control plane node of your cluster was upgraded to \"v1.33.7\".\n\n[upgrade] Now please proceed with upgrading the rest of the nodes by following the right order.\n</code></pre> <p>Now that the control plane is updated, proceed to  upgrade kubelet and kubectl:</p> <code># apt-mark unhold kubelet kubectl &amp;&amp; apt-get update &amp;&amp; apt-get install -y kubelet='1.33.7-*' kubectl='1.33.7-*' &amp;&amp; apt-mark hold kubelet kubectl</code> <pre><code># apt-mark unhold kubelet kubectl &amp;&amp; \\\napt-get update &amp;&amp; apt-get install -y kubelet='1.33.7-*' kubectl='1.33.7-*' &amp;&amp; \\\napt-mark hold kubelet kubectl\nCanceled hold on kubelet.\nCanceled hold on kubectl.\nFetched 6,581 B in 1s (5,712 B/s)\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.33.7-1.1' (isv:kubernetes:core:stable:v1.33:pkgs.k8s.io [amd64]) for 'kubelet'\nSelected version '1.33.7-1.1' (isv:kubernetes:core:stable:v1.33:pkgs.k8s.io [amd64]) for 'kubectl'\nThe following package was automatically installed and is no longer required:\n  conntrack\nUse 'apt autoremove' to remove it.\nThe following packages will be upgraded:\n  kubectl kubelet\n2 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\nNeed to get 27.6 MB of archives.\nAfter this operation, 7,123 kB of additional disk space will be used.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.33/deb  kubectl 1.33.7-1.1 [11.7 MB]\nGet:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.33/deb  kubelet 1.33.7-1.1 [15.9 MB]\nFetched 27.6 MB in 0s (72.5 MB/s) \n(Reading database ... 168496 files and directories currently installed.)\nPreparing to unpack .../kubectl_1.33.7-1.1_amd64.deb ...\nUnpacking kubectl (1.33.7-1.1) over (1.32.4-1.1) ...\nPreparing to unpack .../kubelet_1.33.7-1.1_amd64.deb ...\nUnpacking kubelet (1.33.7-1.1) over (1.32.4-1.1) ...\nSetting up kubectl (1.33.7-1.1) ...\nSetting up kubelet (1.33.7-1.1) ...\nScanning processes...                                                                                            \nScanning candidates...                                                                                           \nScanning processor microcode...                                                                                  \nScanning linux images...                                                                                         \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nRestarting services...\nsystemctl restart kubelet.service\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubelet set on hold.\nkubectl set on hold.\n\n# systemctl daemon-reload\n# systemctl restart kubelet\n</code></pre> <p>At this point <code>kubectl</code> should confirm the cluster is fully upgraded:</p> <pre><code># kubectl  version --output=yaml\nclientVersion:\n  buildDate: \"2025-12-09T14:42:24Z\"\n  compiler: gc\n  gitCommit: a7245cdf3f69e11356c7e8f92b3e78ca4ee4e757\n  gitTreeState: clean\n  gitVersion: v1.33.7\n  goVersion: go1.24.11\n  major: \"1\"\n  minor: \"33\"\n  platform: linux/amd64\nkustomizeVersion: v5.6.0\nserverVersion:\n  buildDate: \"2025-12-09T14:35:23Z\"\n  compiler: gc\n  emulationMajor: \"1\"\n  emulationMinor: \"33\"\n  gitCommit: a7245cdf3f69e11356c7e8f92b3e78ca4ee4e757\n  gitTreeState: clean\n  gitVersion: v1.33.7\n  goVersion: go1.24.11\n  major: \"1\"\n  minCompatibilityMajor: \"1\"\n  minCompatibilityMinor: \"32\"\n  minor: \"33\"\n  platform: linux/amd64\n</code></pre> <p>Finally, bring the node back online by marking it schedulable:</p> <pre><code># kubectl uncordon octavo\nnode/octavo uncordoned\n</code></pre> <p>After a couple of minutes all services are back up.</p>"},{"location":"blog/2026/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-octavo/#update-flannel-before-134","title":"Update Flannel before 1.34","text":"<p>Upgrading networking components in Kubernetes requires careful sequencing. Flannel v0.27.0 is highly optimized for the 1.34 release cycle, and while Kubernetes 1.34 is designed for minimal disruption, the following risks apply to your upgrade strategies.</p> <p>Updating the CNI (Flannel) while still on Kubernetes 1.33 is generally considered the safest approach.</p> <ul> <li>Risks:<ul> <li>Configuration Drift: New default settings in v0.27.0 might not fully align with     older 1.33 kube-proxy behaviors, though this is rare for minor version bumps.</li> <li>DaemonSet Restart: Updating Flannel requires restarting its pods across all     nodes. If the update fails (e.g., image pull error), you risk losing pod-to-pod     connectivity across the entire cluster.</li> </ul> </li> <li>Benefits: Ensures the networking layer is ready for the new APIs and kernel optimizations that Kubernetes 1.34 utilizes.</li> </ul> <p>Recommended Upgrade Order:</p> <ol> <li>Backup your cluster configuration and etcd state.</li> <li>Upgrade Flannel to v0.27.x while on Kubernetes 1.33. Monitor for stability for 24 hours.</li> <li>Upgrade the Control Plane (API Server, Controller Manager) to Kubernetes 1.34.</li> <li>Upgrade Worker Nodes sequentially (Cordon, Drain, Upgrade Kubelet) to Kubernetes 1.34. </li> </ol> <p>Before upgrading Flannel, make a backup of its <code>configmap</code>:</p> <pre><code>$ kubectl get configmap -n kube-flannel kube-flannel-cfg -o yaml \\\n  &gt; flannel-config-backup.yaml\n</code></pre> <p>Then update Flannel by re-downloading its latest release of <code>kube-flannel.yml</code></p> <pre><code>$ wget -O kube-flannel.yml  \\\n  https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n</code></pre> <p>This is necessary because updating Flannel often involves more than just increasing its version numbers, as this <code>git diff</code> shows comparing the latest release from the one nearly a year ago:</p> <pre><code>$ git diff kube-flannel.yml\ndiff --git a/kube-flannel.yml b/kube-flannel.yml\nindex 3576fa3..9f16884 100644\n--- a/kube-flannel.yml\n+++ b/kube-flannel.yml\n@@ -143,7 +143,9 @@ spec:\n               fieldPath: metadata.namespace\n         - name: EVENT_QUEUE_DEPTH\n           value: \"5000\"\n-        image: ghcr.io/flannel-io/flannel:v0.26.7\n+        - name: CONT_WHEN_CACHE_NOT_READY\n+          value: \"false\"\n+        image: ghcr.io/flannel-io/flannel:v0.28.0\n         name: kube-flannel\n         resources:\n           requests:\n@@ -170,7 +172,7 @@ spec:\n         - /opt/cni/bin/flannel\n         command:\n         - cp\n-        image: ghcr.io/flannel-io/flannel-cni-plugin:v1.6.2-flannel1\n+        image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1\n         name: install-cni-plugin\n         volumeMounts:\n         - mountPath: /opt/cni/bin\n@@ -181,7 +183,7 @@ spec:\n         - /etc/cni/net.d/10-flannel.conflist\n         command:\n         - cp\n-        image: ghcr.io/flannel-io/flannel:v0.26.7\n+        image: ghcr.io/flannel-io/flannel:v0.28.0\n         name: install-cni\n         volumeMounts:\n         - mountPath: /etc/cni/net.d\n</code></pre> <p>The update itself is then as simpley as (re)applying the manifest:</p> <pre><code>$ kubectl apply -f kube-flannel.yml\nnamespace/kube-flannel unchanged\nserviceaccount/flannel unchanged\nclusterrole.rbac.authorization.k8s.io/flannel unchanged\nclusterrolebinding.rbac.authorization.k8s.io/flannel unchanged\nconfigmap/kube-flannel-cfg unchanged\ndaemonset.apps/kube-flannel-ds configured\n</code></pre>"},{"location":"blog/2026/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-octavo/#upgrade-to-134","title":"Upgrade to 1.34","text":"<p>Mostly the same as the upgrade to 1.33 with an important caveat.</p> <p>Upgrade version 1.33.x to version 1.34.x starts by determining which version to upgrade to and updating the minor version in the repository configuration and then find the latest patch version, much like the previous Upgrade to 1.33:</p> <pre><code># curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key \\\n  | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# vi /etc/apt/sources.list.d/kubernetes.list\ndeb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /\n\n# apt update\n# apt-cache madison kubeadm\n   kubeadm | 1.34.3-1.1 | https://pkgs.k8s.io/core:/stable:/v1.34/deb  Packages\n   kubeadm | 1.34.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.34/deb  Packages\n   kubeadm | 1.34.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.34/deb  Packages\n   kubeadm | 1.34.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.34/deb  Packages\n</code></pre> <p>Again, before updating <code>kubeadm</code>, drain the node:</p> <code># kubectl drain --ignore-daemonsets --delete-emptydir-data octavo</code> <pre><code># kubectl drain --ignore-daemonsets --delete-emptydir-data octavo\nnode/octavo cordoned\nWarning: ignoring DaemonSet-managed Pods: intel-device-plugins-gpu/intel-gpu-plugin-gpudeviceplugin-wpsqp, kube-flannel/kube-flannel-ds-ctb92, kube-system/kube-proxy-w725k, metallb-system/speaker-92c2g, monitoring/prom-prometheus-node-exporter-r24zl, node-feature-discovery/node-feature-discovery-worker-jjglh\nevicting pod cert-manager/cert-manager-cainjector-666b8b6b66-85xn6\nevicting pod code-server/code-server-85785b5848-hfbpf\nevicting pod unifi/unifi-6548ccd8c8-pjvcc\nevicting pod cert-manager/cert-manager-7d67448f59-sww8j\nevicting pod monitoring/influxdb-76d6df578-vdqck\nevicting pod kubernetes-dashboard/kubernetes-dashboard-api-64c997cbcc-lk2xt\nevicting pod node-feature-discovery/node-feature-discovery-master-767dcc6cb8-jg4sz\nevicting pod monitoring/prom-kube-prometheus-stack-operator-645fd684d6-w75qd\nevicting pod cert-manager/cert-manager-webhook-78cb4cf989-9zktk\nevicting pod metallb-system/controller-bb5f47665-mhtlg\nevicting pod homepage/homepage-58c79b7856-ldwrc\nevicting pod navidrome/navidrome-75945786b-6mj66\nevicting pod monitoring/alertmanager-prom-kube-prometheus-stack-alertmanager-0\nevicting pod media-center/jellyfin-6bbd7f9798-k4hj4\nevicting pod kubernetes-dashboard/kubernetes-dashboard-web-56df7655d9-z9bp2\nevicting pod monitoring/trivy-operator-dashboard-66d4c85cd6-fmz7d\nevicting pod kube-system/coredns-674b8bbfcf-ss8wf\nevicting pod tailscale/operator-68b5df646d-zs465\nevicting pod intel-device-plugins-gpu/inteldeviceplugins-controller-manager-7fb7c8d6b9-gfjwh\nevicting pod node-feature-discovery/node-feature-discovery-gc-5b65f7f5b6-ld7mh\nevicting pod firefly-iii/firefly-iii-mysql-659f959f57-6hr8p\nevicting pod ryot/postgres-84f94497fd-ql4pf\nevicting pod default/ddns-updater-75d5df4647-rxfzw\nevicting pod komga/komga-5984f79858-fzw5l\nevicting pod monitoring/version-checker-547c9c998c-hvv7l\nevicting pod ryot/ryot-848d9f4b6b-zqmfx\nevicting pod monitoring/prom-kube-state-metrics-8576986c6b-9wvfh\nevicting pod home-assistant/home-assistant-5db584bd48-c4xwz\nevicting pod kube-system/coredns-674b8bbfcf-9j8qs\nevicting pod tailscale/ts-kubernetes-dashboard-ingress-tailscale-jhb6z-0\nevicting pod kubernetes-dashboard/kubernetes-dashboard-auth-5cf6848ffd-r2zmq\nevicting pod kubernetes-dashboard/kubernetes-dashboard-kong-79867c9c48-5h28m\nevicting pod kubernetes-dashboard/kubernetes-dashboard-metrics-scraper-76df4956c4-5bkdg\nevicting pod tailscale/ts-home-assistant-tailscale-mdqlt-0\nevicting pod firefly-iii/firefly-iii-f865768bf-vhqk6\nevicting pod monitoring/prometheus-prom-kube-prometheus-stack-prometheus-0\nevicting pod trivy-system/trivy-operator-59489786c6-dzlqj\nevicting pod audiobookshelf/audiobookshelf-5798dfc5d5-wm6ts\nevicting pod unifi/mongo-7878d99ff5-6q6z6\nevicting pod monitoring/grafana-695b647cb4-jk7kr\nevicting pod pomerium/pomerium-765bfd4df6-5nn2r\nI0111 22:55:16.851803 1528671 request.go:752] \"Waited before sending request\" delay=\"1.000274541s\" reason=\"client-side throttling, not priority and fairness\" verb=\"POST\" URL=\"https://10.0.0.8:6443/api/v1/namespaces/trivy-system/pods/trivy-operator-59489786c6-dzlqj/eviction\"\npod/homepage-58c79b7856-ldwrc evicted\npod/cert-manager-cainjector-666b8b6b66-85xn6 evicted\npod/ts-home-assistant-tailscale-mdqlt-0 evicted\npod/prometheus-prom-kube-prometheus-stack-prometheus-0 evicted\npod/kubernetes-dashboard-auth-5cf6848ffd-r2zmq evicted\npod/prom-kube-state-metrics-8576986c6b-9wvfh evicted\npod/kubernetes-dashboard-api-64c997cbcc-lk2xt evicted\npod/audiobookshelf-5798dfc5d5-wm6ts evicted\npod/ryot-848d9f4b6b-zqmfx evicted\npod/version-checker-547c9c998c-hvv7l evicted\npod/inteldeviceplugins-controller-manager-7fb7c8d6b9-gfjwh evicted\npod/postgres-84f94497fd-ql4pf evicted\npod/ts-kubernetes-dashboard-ingress-tailscale-jhb6z-0 evicted\npod/jellyfin-6bbd7f9798-k4hj4 evicted\npod/alertmanager-prom-kube-prometheus-stack-alertmanager-0 evicted\npod/trivy-operator-59489786c6-dzlqj evicted\npod/komga-5984f79858-fzw5l evicted\npod/code-server-85785b5848-hfbpf evicted\npod/node-feature-discovery-master-767dcc6cb8-jg4sz evicted\npod/prom-kube-prometheus-stack-operator-645fd684d6-w75qd evicted\npod/cert-manager-7d67448f59-sww8j evicted\npod/cert-manager-webhook-78cb4cf989-9zktk evicted\npod/trivy-operator-dashboard-66d4c85cd6-fmz7d evicted\npod/coredns-674b8bbfcf-9j8qs evicted\npod/influxdb-76d6df578-vdqck evicted\npod/kubernetes-dashboard-web-56df7655d9-z9bp2 evicted\npod/controller-bb5f47665-mhtlg evicted\npod/node-feature-discovery-gc-5b65f7f5b6-ld7mh evicted\npod/ddns-updater-75d5df4647-rxfzw evicted\npod/unifi-6548ccd8c8-pjvcc evicted\npod/operator-68b5df646d-zs465 evicted\npod/navidrome-75945786b-6mj66 evicted\npod/kubernetes-dashboard-metrics-scraper-76df4956c4-5bkdg evicted\npod/firefly-iii-mysql-659f959f57-6hr8p evicted\npod/grafana-695b647cb4-jk7kr evicted\npod/pomerium-765bfd4df6-5nn2r evicted\npod/mongo-7878d99ff5-6q6z6 evicted\npod/coredns-674b8bbfcf-ss8wf evicted\npod/home-assistant-5db584bd48-c4xwz evicted\npod/kubernetes-dashboard-kong-79867c9c48-5h28m evicted\npod/firefly-iii-f865768bf-vhqk6 evicted\nnode/octavo drained\n</code></pre> <p>Again, before updating <code>kubeadm</code>, drain the node:</p> <code># apt-get update &amp;&amp; apt-get install kubeadm='1.34.3-*' &amp;&amp; apt-mark hold kubeadm</code> <pre><code># apt-get update &amp;&amp; \\\n  apt-get install kubeadm='1.34.3-*' &amp;&amp; \\\n  apt-mark hold kubeadm\nFetched 6,581 B in 1s (7,701 B/s)\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.34.3-1.1' (isv:kubernetes:core:stable:v1.34:pkgs.k8s.io [amd64]) for 'kubeadm'\nThe following package was automatically installed and is no longer required:\n  conntrack\nUse 'apt autoremove' to remove it.\nThe following held packages will be changed:\n  kubeadm\nThe following packages will be upgraded:\n  kubeadm\n1 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\nNeed to get 12.5 MB of archives.\nAfter this operation, 524 kB disk space will be freed.\nDo you want to continue? [Y/n]  \nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.34/deb  kubeadm 1.34.3-1.1 [12.5 MB]\nFetched 12.5 MB in 1s (23.5 MB/s)\n(Reading database ... 168512 files and directories currently installed.)\nPreparing to unpack .../kubeadm_1.34.3-1.1_amd64.deb ...\nUnpacking kubeadm (1.34.3-1.1) over (1.33.7-1.1) ...\nSetting up kubeadm (1.34.3-1.1) ...\nScanning processes...                                                                \nScanning processor microcode...                                                      \nScanning linux images...                                                             \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nNo services need to be restarted.\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubeadm set on hold.\n</code></pre> <p>Verify and apply the upgrade plan:</p> <code># kubeadm upgrade plan</code> <pre><code># kubeadm upgrade plan\n[preflight] Running pre-flight checks.\n[upgrade/config] Reading configuration from the \"kubeadm-config\" ConfigMap in namespace \"kube-system\"...\n[upgrade/config] Use 'kubeadm init phase upload-config kubeadm --config your-config-file' to re-upload it.\n[upgrade] Running cluster health checks\nW0111 22:56:39.632491 1563261 health.go:134] The preflight check \"CreateJob\" was skipped because there are no schedulable Nodes in the cluster.\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: 1.33.7\n[upgrade/versions] kubeadm version: v1.34.3\nI0111 22:56:40.075055 1563261 version.go:260] remote version is much newer: v1.35.0; falling back to: stable-1.34\n[upgrade/versions] Target version: v1.34.3\n[upgrade/versions] Latest version in the v1.33 series: v1.33.7\n\nComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\nCOMPONENT   NODE      CURRENT   TARGET\nkubelet     octavo    v1.33.7   v1.34.3\n\nUpgrade to the latest stable version:\n\nCOMPONENT                 NODE      CURRENT    TARGET\nkube-apiserver            octavo    v1.33.7    v1.34.3\nkube-controller-manager   octavo    v1.33.7    v1.34.3\nkube-scheduler            octavo    v1.33.7    v1.34.3\nkube-proxy                          1.33.7     v1.34.3\nCoreDNS                             v1.12.0    v1.12.1\netcd                      octavo    3.5.24-0   3.6.5-0\n\nYou can now apply the upgrade by executing the following command:\n\n        kubeadm upgrade apply v1.34.3\n\n_____________________________________________________________________\n\n\nThe table below shows the current state of component configs as understood by this version of kubeadm.\nConfigs that have a \"yes\" mark in the \"MANUAL UPGRADE REQUIRED\" column require manual config upgrade or\nresetting to kubeadm defaults before a successful upgrade can be performed. The version to manually\nupgrade to is denoted in the \"PREFERRED VERSION\" column.\n\nAPI GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED\nkubeproxy.config.k8s.io   v1alpha1          v1alpha1            no\nkubelet.config.k8s.io     v1beta1           v1beta1             no\n_____________________________________________________________________\n</code></pre> <code># kubeadm upgrade apply v1.33.7</code> <pre><code># kubeadm upgrade apply v1.34.3\n[upgrade] Reading configuration from the \"kubeadm-config\" ConfigMap in namespace \"kube-system\"...\n[upgrade] Use 'kubeadm init phase upload-config kubeadm --config your-config-file' to re-upload it.\n[upgrade/preflight] Running preflight checks\n[upgrade] Running cluster health checks\nW0111 22:56:49.519658 1565941 health.go:134] The preflight check \"CreateJob\" was skipped because there are no schedulable Nodes in the cluster.\n[upgrade/preflight] You have chosen to upgrade the cluster version to \"v1.34.3\"\n[upgrade/versions] Cluster version: v1.33.7\n[upgrade/versions] kubeadm version: v1.34.3\n[upgrade] Are you sure you want to proceed? [y/N]: y\n[upgrade/preflight] Pulling images required for setting up a Kubernetes cluster\n[upgrade/preflight] This might take a minute or two, depending on the speed of your internet connection\n[upgrade/preflight] You can also perform this action beforehand using 'kubeadm config images pull'\n[upgrade/control-plane] Upgrading your static Pod-hosted control plane to version \"v1.34.3\" (timeout: 5m0s)...\n[upgrade/staticpods] Writing new Static Pod manifests to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests1221527384\"\n[upgrade/staticpods] Preparing for \"etcd\" upgrade\n[upgrade/staticpods] Renewing etcd-server certificate\n[upgrade/staticpods] Renewing etcd-peer certificate\n[upgrade/staticpods] Renewing etcd-healthcheck-client certificate\n[upgrade/staticpods] Moving new manifest to \"/etc/kubernetes/manifests/etcd.yaml\" and backing up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2026-01-11-22-57-01/etcd.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=etcd\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [Get \"https://10.0.0.8:6443/api/v1/namespaces/kube-system/pods?labelSelector=component%3Detcd\": dial tcp 10.0.0.8:6443: connect: connection refused]\n[apiclient] Error getting Pods with label selector \"component=etcd\" [pods is forbidden: User \"kubernetes-admin\" cannot list resource \"pods\" in API group \"\" in the namespace \"kube-system\"]\n[upgrade/staticpods] Component \"etcd\" upgraded successfully!\n[upgrade/etcd] Waiting for etcd to become available\n[upgrade/staticpods] Preparing for \"kube-apiserver\" upgrade\n[upgrade/staticpods] Renewing apiserver certificate\n[upgrade/staticpods] Renewing apiserver-kubelet-client certificate\n[upgrade/staticpods] Renewing front-proxy-client certificate\n[upgrade/staticpods] Renewing apiserver-etcd-client certificate\n[upgrade/staticpods] Moving new manifest to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" and backing up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2026-01-11-22-57-01/kube-apiserver.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=kube-apiserver\n[upgrade/staticpods] Component \"kube-apiserver\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-controller-manager\" upgrade\n[upgrade/staticpods] Renewing controller-manager.conf certificate\n[upgrade/staticpods] Moving new manifest to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\" and backing up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2026-01-11-22-57-01/kube-controller-manager.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=kube-controller-manager\n[upgrade/staticpods] Component \"kube-controller-manager\" upgraded successfully!\n[upgrade/staticpods] Preparing for \"kube-scheduler\" upgrade\n[upgrade/staticpods] Renewing scheduler.conf certificate\n[upgrade/staticpods] Moving new manifest to \"/etc/kubernetes/manifests/kube-scheduler.yaml\" and backing up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2026-01-11-22-57-01/kube-scheduler.yaml\"\n[upgrade/staticpods] Waiting for the kubelet to restart the component\n[upgrade/staticpods] This can take up to 5m0s\n[apiclient] Found 1 Pods for label selector component=kube-scheduler\n[upgrade/staticpods] Component \"kube-scheduler\" upgraded successfully!\n[upgrade/control-plane] The control plane instance for this node was successfully upgraded!\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upgrade/kubeconfig] The kubeconfig files for this node were successfully upgraded!\nW0111 23:00:28.196018 1565941 postupgrade.go:116] Using temporary directory /etc/kubernetes/tmp/kubeadm-kubelet-config1928247334 for kubelet config. To override it set the environment variable KUBEADM_UPGRADE_DRYRUN_DIR\n[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config1928247334/config.yaml\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/instance-config.yaml\"\n[patches] Applied patch of type \"application/strategic-merge-patch+json\" to target \"kubeletconfiguration\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[upgrade/kubelet-config] The kubelet configuration for this node was successfully upgraded!\n[upgrade/bootstrap-token] Configuring bootstrap token and cluster-info RBAC rules\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\n[upgrade] SUCCESS! A control plane node of your cluster was upgraded to \"v1.34.3\".\n\n[upgrade] Now please proceed with upgrading the rest of the nodes by following the right order.\n</code></pre> <p>Now that the control plane is updated, proceed to  upgrade kubelet and kubectl:</p> <code># apt-mark unhold kubelet kubectl &amp;&amp; apt-get update &amp;&amp; apt-get install -y kubelet='1.33.7-*' kubectl='1.33.7-*' &amp;&amp; apt-mark hold kubelet kubectl</code> <pre><code># apt-mark unhold kubelet kubectl &amp;&amp; \\\n  apt-get update &amp;&amp; \\\n  apt-get install kubelet='1.34.3-*' kubectl='1.34.3-*' &amp;&amp; \\\n  apt-mark hold kubelet kubectl\nCanceled hold on kubelet.\nCanceled hold on kubectl.\nFetched 6,581 B in 1s (5,888 B/s)\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nSelected version '1.34.3-1.1' (isv:kubernetes:core:stable:v1.34:pkgs.k8s.io [amd64]) for 'kubelet'\nSelected version '1.34.3-1.1' (isv:kubernetes:core:stable:v1.34:pkgs.k8s.io [amd64]) for 'kubectl'\nThe following package was automatically installed and is no longer required:\n  conntrack\nUse 'apt autoremove' to remove it.\nThe following packages will be upgraded:\n  kubectl kubelet\n2 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\nNeed to get 24.7 MB of archives.\nAfter this operation, 22.1 MB disk space will be freed.\nGet:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.34/deb  kubectl 1.34.3-1.1 [11.7 MB]\nGet:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.34/deb  kubelet 1.34.3-1.1 [13.0 MB]\nFetched 24.7 MB in 1s (40.2 MB/s) \n(Reading database ... 168512 files and directories currently installed.)\nPreparing to unpack .../kubectl_1.34.3-1.1_amd64.deb ...\nUnpacking kubectl (1.34.3-1.1) over (1.33.7-1.1) ...\nPreparing to unpack .../kubelet_1.34.3-1.1_amd64.deb ...\nUnpacking kubelet (1.34.3-1.1) over (1.33.7-1.1) ...\nSetting up kubectl (1.34.3-1.1) ...\nSetting up kubelet (1.34.3-1.1) ...\nScanning processes...                                                                \nScanning candidates...                                                               \nScanning processor microcode...                                                      \nScanning linux images...                                                             \n\nRunning kernel seems to be up-to-date.\n\nThe processor microcode seems to be up-to-date.\n\nRestarting services...\nsystemctl restart kubelet.service\n\nNo containers need to be restarted.\n\nNo user sessions are running outdated binaries.\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\nkubelet set on hold.\nkubectl set on hold.\n\n# systemctl daemon-reload\n# systemctl restart kubelet\n</code></pre> <p>At this point <code>kubectl</code> should confirm the cluster is fully upgraded:</p> <pre><code># kubectl  version --output=yaml\nclientVersion:\n  buildDate: \"2025-12-09T15:06:39Z\"\n  compiler: gc\n  gitCommit: df11db1c0f08fab3c0baee1e5ce6efbf816af7f1\n  gitTreeState: clean\n  gitVersion: v1.34.3\n  goVersion: go1.24.11\n  major: \"1\"\n  minor: \"34\"\n  platform: linux/amd64\nkustomizeVersion: v5.7.1\nserverVersion:\n  buildDate: \"2025-12-09T14:59:13Z\"\n  compiler: gc\n  emulationMajor: \"1\"\n  emulationMinor: \"34\"\n  gitCommit: df11db1c0f08fab3c0baee1e5ce6efbf816af7f1\n  gitTreeState: clean\n  gitVersion: v1.34.3\n  goVersion: go1.24.11\n  major: \"1\"\n  minCompatibilityMajor: \"1\"\n  minCompatibilityMinor: \"33\"\n  minor: \"34\"\n  platform: linux/amd64\n</code></pre> <p>Finally, bring the node back online by marking it schedulable:</p> <pre><code># kubectl uncordon octavo\nnode/octavo uncordoned\n</code></pre> <p>After a couple of minutes most services are back up...</p>"},{"location":"blog/2026/01/10/upgrading-single-node-kubernetes-cluster-on-ubuntu-studio-2404-octavo/#fix-firefly-iii","title":"Fix Firefly-III","text":"<p>Once all servicers should be back up, it is always good to check that they are actually back up.</p> <p>This time, just one was not yet ready:</p> <pre><code>$ kubectl get all -A\nNAMESPACE                  NAME                                                         READY   STATUS             RESTARTS         AGE\naudiobookshelf             pod/audiobookshelf-5798dfc5d5-nhrdm                          1/1     Running            0                8m8s\ncert-manager               pod/cert-manager-7d67448f59-5llzl                            1/1     Running            0                7m56s\ncert-manager               pod/cert-manager-cainjector-666b8b6b66-x4dck                 1/1     Running            0                8m8s\ncert-manager               pod/cert-manager-webhook-78cb4cf989-kb24w                    1/1     Running            0                7m56s\ncode-server                pod/code-server-85785b5848-9ln8r                             1/1     Running            0                8m7s\ndefault                    pod/ddns-updater-75d5df4647-bnstp                            1/1     Running            0                7m56s\nfirefly-iii                pod/firefly-iii-f865768bf-kgjq7                              1/1     Running            0                8m7s\nfirefly-iii                pod/firefly-iii-mysql-659f959f57-ph92z                       0/1     ImagePullBackOff   0                7m56s\nhome-assistant             pod/home-assistant-5db584bd48-4mvwf                          0/1     Running            0                7m56s\nhomepage                   pod/homepage-58c79b7856-9h59f                                1/1     Running            0                8m8s\n...\n\nNAMESPACE                  NAME                                                    READY   UP-TO-DATE   AVAILABLE   AGE\naudiobookshelf             deployment.apps/audiobookshelf                          1/1     1            1           258d\ncert-manager               deployment.apps/cert-manager                            1/1     1            1           260d\ncert-manager               deployment.apps/cert-manager-cainjector                 1/1     1            1           260d\ncert-manager               deployment.apps/cert-manager-webhook                    1/1     1            1           260d\ncode-server                deployment.apps/code-server                             1/1     1            1           255d\ndefault                    deployment.apps/ddns-updater                            1/1     1            1           108d\nfirefly-iii                deployment.apps/firefly-iii                             1/1     1            1           255d\nfirefly-iii                deployment.apps/firefly-iii-mysql                       0/1     1            0           255d\nhome-assistant             deployment.apps/home-assistant                          0/1     1            0           259d\nhomepage                   deployment.apps/homepage                                1/1     1            1           10d\n\nNAMESPACE                  NAME                                                               DESIRED   CURRENT   READY   AGE\naudiobookshelf             replicaset.apps/audiobookshelf-5798dfc5d5                          1         1         1       18h\ncert-manager               replicaset.apps/cert-manager-7d67448f59                            1         1         1       260d\ncert-manager               replicaset.apps/cert-manager-cainjector-666b8b6b66                 1         1         1       260d\ncert-manager               replicaset.apps/cert-manager-webhook-78cb4cf989                    1         1         1       260d\ncode-server                replicaset.apps/code-server-85785b5848                             1         1         1       7d\ndefault                    replicaset.apps/ddns-updater-75d5df4647                            1         1         1       7d\nfirefly-iii                replicaset.apps/firefly-iii-f865768bf                              1         1         1       7d\nfirefly-iii                replicaset.apps/firefly-iii-mysql-659f959f57                       1         1         0       7d\nhome-assistant             replicaset.apps/home-assistant-5db584bd48                          1         1         0       7d\nhomepage                   replicaset.apps/homepage-58c79b7856                                1         1         1       9d\n</code></pre> <p>At this point it seems clear something is not going well with the pods running image <code>firefly-iii-mysql</code>:</p> <pre><code>$ kubectl get all -n firefly-iii \nNAME                                     READY   STATUS             RESTARTS   AGE\npod/firefly-iii-f865768bf-kgjq7          1/1     Running            0          11m\npod/firefly-iii-mysql-659f959f57-ph92z   0/1     ImagePullBackOff   0          10m\n\nNAME                            TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nservice/firefly-iii-mysql-svc   NodePort   10.105.106.204   &lt;none&gt;        3306:30306/TCP   255d\nservice/firefly-iii-svc         NodePort   10.98.129.183    &lt;none&gt;        8080:30080/TCP   255d\n\nNAME                                READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/firefly-iii         1/1     1            1           255d\ndeployment.apps/firefly-iii-mysql   0/1     1            0           255d\n\nNAME                                           DESIRED   CURRENT   READY   AGE\nreplicaset.apps/firefly-iii-f865768bf          1         1         1       7d\nreplicaset.apps/firefly-iii-mysql-659f959f57   1         1         0       7d\n</code></pre> <p>The <code>ImagePullBackOff</code> status typically means the required image is not available and there is no reason to believe that this will resolve it self with tiem. Inspect the pod's logs:</p> <pre><code>$ kubectl describe pod firefly-iii-mysql-659f959f57-ph92z -n firefly-iii \nName:             firefly-iii-mysql-659f959f57-ph92z\nNamespace:        firefly-iii\n...\n\nEvents:\n  Type     Reason            Age                  From               Message\n  ----     ------            ----                 ----               -------\n  ...      ...               ...                  ...                ...\n  Normal   Pulling           86s (x5 over 4m29s)  kubelet            Pulling image \"yobasystems/alpine-mariadb:latest\"\n  Warning  Failed            85s (x5 over 4m23s)  kubelet            Failed to pull image \"yobasystems/alpine-mariadb:latest\": rpc error: code = NotFound desc = failed to pull and unpack image \"docker.io/yobasystems/alpine-mariadb:latest\": failed to resolve image: docker.io/yobasystems/alpine-mariadb:latest: not found\n  Warning  Failed            85s (x5 over 4m23s)  kubelet            Error: ErrImagePull\n  Normal   BackOff           7s (x15 over 4m22s)  kubelet            Back-off pulling image \"yobasystems/alpine-mariadb:latest\"\n  Warning  Failed            7s (x15 over 4m22s)  kubelet            Error: ImagePullBackOff\n</code></pre> <p>Kubernetes is reporting that image tag \"yobasystems/alpine-mariadb:latest\" is not available and indeed it is not!</p> <p>A thorough search in https://hub.docker.com/r/yobasystems/alpine-mariadb/tags shows there is no such tag as <code>latest</code> but there are tags that only contain the CPU architecture they are built for. The obious chocice is to switch to Linux as much as you can (e.g. <code>amd64</code>), so the manifest in <code>firefly-iii.yaml</code> must be updated to pull that particular image tag:</p> <pre><code>$ git diff firefly-iii.yaml \ndiff --git a/firefly-iii.yaml b/firefly-iii.yaml\nindex e6a6f0f..9f9d608 100644\n--- a/firefly-iii.yaml\n+++ b/firefly-iii.yaml\n@@ -64,7 +64,7 @@ spec:\n         tier: mysql\n     spec:\n       containers:\n-      - image: yobasystems/alpine-mariadb:latest\n+      - image: yobasystems/alpine-mariadb:amd64\n         imagePullPolicy: Always\n         name: mysql\n         env:\n</code></pre> <p>Before applying the update, make a backup copy of the storage usef ror the MySQL databse:</p> <pre><code># cp -a /home/k8s/firefly-iii/mysql /home/k8s/firefly-iii/mysql-2026-01-11-latest\n# du -s /home/k8s/firefly-iii/mysql /home/k8s/firefly-iii/mysql-2026-01-11-latest\n220972  /home/k8s/firefly-iii/mysql\n220972  /home/k8s/firefly-iii/mysql-2026-01-11-latest\n</code></pre> <p>Applying the updated manifest gets the pod to correctly find the requested image and starts the container, so the service is back up again:</p> <pre><code>$ kubectl apply -f firefly-iii.yaml \nnamespace/firefly-iii unchanged\npersistentvolume/firefly-iii-pv-mysql unchanged\npersistentvolumeclaim/firefly-iii-pvc-mysql unchanged\ndeployment.apps/firefly-iii-mysql configured\nservice/firefly-iii-mysql-svc unchanged\npersistentvolume/firefly-iii-pv-upload unchanged\npersistentvolumeclaim/firefly-iii-pvc-upload unchanged\nservice/firefly-iii-svc unchanged\ndeployment.apps/firefly-iii unchanged\n\n$ kubectl get all -n firefly-iii\nNAME                                    READY   STATUS              RESTARTS   AGE\npod/firefly-iii-f865768bf-kgjq7         1/1     Running             0          15m\npod/firefly-iii-mysql-b8549c77c-4t8mk   0/1     ContainerCreating   0          6s\n\nNAME                            TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nservice/firefly-iii-mysql-svc   NodePort   10.105.106.204   &lt;none&gt;        3306:30306/TCP   255d\nservice/firefly-iii-svc         NodePort   10.98.129.183    &lt;none&gt;        8080:30080/TCP   255d\n\nNAME                                READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/firefly-iii         1/1     1            1           255d\ndeployment.apps/firefly-iii-mysql   0/1     1            0           255d\n\nNAME                                           DESIRED   CURRENT   READY   AGE\nreplicaset.apps/firefly-iii-f865768bf          1         1         1       7d\nreplicaset.apps/firefly-iii-mysql-659f959f57   0         0         0       7d\nreplicaset.apps/firefly-iii-mysql-b8549c77c    1         1         0       6s\n</code></pre> <p>Inspecting the pod's events shows the image has been successfully pulled:</p> <pre><code>$ kubectl describe pod firefly-iii-mysql-b8549c77c-4t8mk -n firefly-iii \nName:             firefly-iii-mysql-b8549c77c-4t8mk\nNamespace:        firefly-iii\n...\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  19s   default-scheduler  Successfully assigned firefly-iii/firefly-iii-mysql-b8549c77c-4t8mk to octavo\n  Normal  Pulling    19s   kubelet            Pulling image \"yobasystems/alpine-mariadb:amd64\"\n  Normal  Pulled     14s   kubelet            Successfully pulled image \"yobasystems/alpine-mariadb:amd64\" in 4.924s (4.924s including waiting). Image size: 85805520 bytes.\n  Normal  Created    14s   kubelet            Created container: mysql\n  Normal  Started    14s   kubelet            Started container mysql\n</code></pre> <p>All this was necessary only because the <code>latest</code> tag was temporarily yobasystems/alpine-mariadb tags, once the <code>latest</code> tag reinstantiated the original manifest would work again.</p>"},{"location":"blog/2026/01/24/migrating-nfs-volumes-to-the-nfs-csi-driver-for-kubernetes/","title":"Migrating NFS volumes to the NFS CSI driver for Kubernetes","text":"<p>NFS volumes have been mounted the lazy way as <code>hostPath</code> volumes, with the entire NFS volume being mounted by the host OS. While this works well enough in a single-node cluster, it wouldn't work well in a multi-node cluster and is just not the proper way to mount NFs volumes in Kubernetes.</p> <p>For a better, safer and more efficient setup, NFS volumes will now be mounted using the NFS CSI driver for Kubernetes.</p>"},{"location":"blog/2026/01/24/migrating-nfs-volumes-to-the-nfs-csi-driver-for-kubernetes/#enable-nfsv41-support","title":"Enable NFSv4.1 support","text":"<p>It is highly recommended that NFS servers support NFSv4.1 for optimal performance and security. In a Synology NAS, this setting is in the Control Panel under File Services &gt; NFS.</p>"},{"location":"blog/2026/01/24/migrating-nfs-volumes-to-the-nfs-csi-driver-for-kubernetes/#install-the-nfs-csi-driver","title":"Install the NFS CSI driver","text":"<p>Install the NFS CSI driver with Helm with this <code>nfs-csi-values.yaml</code> for control plane scheduling and modern NFS protocol support:</p> <p><code>nfs-csi-values.yaml</code></p> <pre><code>controller:\n  replicas: 1\n  runOnControlPlane: true\n  dnsPolicy: ClusterFirstWithHostNet\n\nnode:\n  dnsPolicy: ClusterFirstWithHostNet\n\nexternalSnapshotter:\n  enabled: true\n</code></pre> <p>The use of <code>dnsPolicy: ClusterFirstWithHostNet</code> is required when the NFS server is specified as a hostname, and <code>externalSnapshotter</code> is recommended for backups (later).</p> Do not specify <code>replicas: 2</code> in a single-node cluster. <p>Attempting to run with <code>replicas: 2</code> in a single-node customer fails because the second pod stays in a <code>CrashLoopBackOff</code> loop:</p> <pre><code>$ kubectl --namespace=kube-system get pods --selector=\"app.kubernetes.io/instance=csi-driver-nfs\" --watch\nNAME                                READY   STATUS     RESTARTS     AGE\ncsi-nfs-controller-5d6c68d9-8lxvq   5/5     Running    0            19s\ncsi-nfs-controller-5d6c68d9-bnnhl   4/5     NotReady   1 (3s ago)   18s\ncsi-nfs-node-kq85t                  3/3     Running    0            19s\ncsi-nfs-controller-5d6c68d9-bnnhl   4/5     CrashLoopBackOff   1 (2s ago)   18s\ncsi-nfs-controller-5d6c68d9-bnnhl   4/5     NotReady           2 (15s ago)   31s\ncsi-nfs-controller-5d6c68d9-bnnhl   4/5     CrashLoopBackOff   2 (15s ago)   46s\ncsi-nfs-controller-5d6c68d9-bnnhl   4/5     NotReady           3 (30s ago)   61s\ncsi-nfs-controller-5d6c68d9-bnnhl   4/5     CrashLoopBackOff   3 (15s ago)   75s\ncsi-nfs-controller-5d6c68d9-bnnhl   4/5     NotReady           4 (50s ago)   110s\ncsi-nfs-controller-5d6c68d9-bnnhl   4/5     CrashLoopBackOff   4 (15s ago)   2m5s\n\n$ kubectl -n kube-system describe pod csi-nfs-controller-5d6c68d9-bnnhl\n...\n  Warning  BackOff    9s (x15 over 3m5s)    kubelet            Back-off restarting failed container liveness-probe in pod csi-nfs-controller-5d6c68d9-bnnhl_kube-system(30fea10c-5b29-4873-91d4-eed1ae8e3b45)\n</code></pre> <p>In a single-node cluster, setting <code>controller.replicas: 2</code> results in a <code>CrashLoopBackOff</code> because both controller pods attempt to bind to the same host port for health checks or metrics while running on the same physical host. Also, the NFS CSI controller uses leader election. While the second pod waits to become the leader, it may fail its liveness probe if the probe is incorrectly configured to check for an active \"leader\" state rather than just \"running\" status.</p> <p>The best practice for a single-node cluster that may later be upgraded to a high availability multi-node cluster is to start with <code>replicas: 1</code> and configure pod anti-affinity so that, when a new node is added later, when scaling the deployment up to <code>replicas: 2</code> Kubernetes will automatically place them on different nodes:</p> <p><code>nfs-csi-values.yaml</code></p> <pre><code>controller:\n  replicas: 1\n  runOnControlPlane: true\n  dnsPolicy: ClusterFirstWithHostNet\n\n  # Prepare for future expansion with anti-affinity\n  affinity:\n    podAntiAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n            - key: app\n              operator: In\n              values:\n              - csi-nfs-controller\n          topologyKey: \"kubernetes.io/hostname\"\n\nnode:\n  dnsPolicy: ClusterFirstWithHostNet\n\nexternalSnapshotter:\n  enabled: true\n</code></pre> <p>Once a new node is added to the cluster, update <code>nfs-csi-values.yaml</code> to set <code>replicas: 2</code> and upgrade the deployment:</p> <pre><code>$ helm upgrade csi-driver-nfs csi-driver-nfs/csi-driver-nfs \\\n    -n kube-system -f nfs-csi-values.yaml\n</code></pre> <p>Install the latest version of the NFS CSI driver in the <code>kube-system</code> namespace using the above <code>nfs-csi-values.yaml</code> and check that its opds are all running:</p> <pre><code>$ helm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts\n\n$ helm install csi-driver-nfs csi-driver-nfs/csi-driver-nfs \\\n    --namespace kube-system \\\n    --version 4.12.1 \\\n    -f nfs-csi-values.yaml\nNAME: csi-driver-nfs\nLAST DEPLOYED: Sat Jan 24 10:35:53 2026\nNAMESPACE: kube-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThe CSI NFS Driver is getting deployed to your cluster.\n\nTo check CSI NFS Driver pods status, please run:\n\n  kubectl --namespace=kube-system get pods --selector=\"app.kubernetes.io/instance=csi-driver-nfs\" --watch\n\n$ kubectl --namespace=kube-system get pods --selector=\"app.kubernetes.io/instance=csi-driver-nfs\" --watch\nNAME                                READY   STATUS    RESTARTS   AGE\ncsi-nfs-controller-5d6c68d9-8lxvq   5/5     Running   0          18s\ncsi-nfs-node-kq85t                  3/3     Running   0          18s\n</code></pre> <p>Warning</p> <p>Before continuing to the next saction, ake sure to install the <code>nfs-common</code> package  in the host OS. Otherwise, pods scheduled to nodes lacking <code>nfs-common</code> will fail  to start, typically showing <code>mount.nfs: command not found</code> or similar errors.</p>"},{"location":"blog/2026/01/24/migrating-nfs-volumes-to-the-nfs-csi-driver-for-kubernetes/#migrate-hostpath-to-nfs-csi","title":"Migrate <code>hostPath</code> to NFS CSI","text":"<p>The Navidrome deployment is used here to illustrate the migration.</p> <p>To replace each <code>hostPath</code> configuration with an NFS CSI mount that targets a specific subdirectory, start by replacing the <code>hostPath</code> element with a <code>csi</code> defining the NFS server and base path and the <code>subDir</code> property in  in the <code>PersistentVolume</code>:</p> <p><code>navidrome.yaml</code> (<code>PersistentVolume</code> <code>navidrome-pv-music</code>)</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: navidrome-pv-music\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 100Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  csi:\n    driver: nfs.csi.k8s.io\n    volumeHandle: luggage-music-nfs-octavo\n    volumeAttributes:\n      server: luggage\n      share: /volume1/NetBackup\n      subDir: public/audio/Music\n  mountOptions:\n    - nfsvers=4.1\n    - hard\n</code></pre> <p>Changing <code>accessModes</code> to <code>ReadWriteMany</code> allows multiple nodes to access the volume simultaneously and adding <code>subDir: public/audio/Music</code> make the CSI driver handles the subdirectory mount so that pods are guaranteed to have no access to other parts of the NFS volume. CSI drivers often require explicit <code>mountOptions</code> like <code>nfsvers=4.1</code> or <code>hard</code> to be defined in the <code>PersistentVolume</code> <code>spec</code> to ensure consistent behavior across nodes.</p> <p>The <code>PersistentVolumeClaim</code> needs only to change <code>accessModes</code> to <code>ReadWriteMany</code>:</p> <p><code>navidrome.yaml</code> (<code>PersistentVolumeClaim</code> <code>navidrome-pvc-music</code>)</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: navidrome-pvc-music\n  namespace: navidrome\nspec:\n  storageClassName: manual\n  volumeName: navidrome-pv-music\n  accessModes:\n    - ReadWriteMany\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> <p>No changes are required to the pods or deployment, all the differences are abstracted inside the <code>PersistentVolume</code>. What is required at this point is to delete the existing <code>PersistentVolume</code>, so that a new one may be created with the same name. This is required to change the storage backend without losing data or creating naming conflicts; with this \"delete and recreate\" sequence:</p> <ol> <li> <p>Pre-requisite: set <code>persistentVolumeReclaimPolicy: Retain</code> if not already set, so     that deleting the <code>PersistentVolume</code> resource does not trigger the deletion of the     actual data. This was already set in the original     Navidrome deployment.</p> </li> <li> <p>Scale deployments down to <code>replicas: 0</code> too stop all the pods using the volume, to     release the mount:     </p><pre><code>$ kubectl scale -n navidrome deployment navidrome --replicas=0\ndeployment.apps/navidrome scaled\n</code></pre><p></p> </li> <li> <p>Delete the <code>PersistentVolumeClaim</code> first, then the <code>PersistentVolume</code>:     </p><pre><code>$ kubectl -n navidrome delete persistentvolumeclaim navidrome-pvc-music \npersistentvolumeclaim \"navidrome-pvc-music\" deleted from navidrome namespace\n\n$ kubectl delete persistentvolume navidrome-pv-music\npersistentvolume \"navidrome-pv-music\" deleted\n</code></pre><p></p> </li> <li> <p>Recreate the <code>PersistentVolumeClaim</code> and <code>PersistentVolume</code> by reapplying the     updated manifest in <code>navidrome.yaml</code>. Because the old resources are gone, Kubernetes     will accept the new configuration as a fresh creation.     </p><pre><code>$ kubectl apply -f navidrome.yaml \nnamespace/navidrome unchanged\npersistentvolume/navidrome-pv-data unchanged\npersistentvolume/navidrome-pv-music created\npersistentvolumeclaim/navidrome-pvc-data unchanged\npersistentvolumeclaim/navidrome-pvc-music created\ndeployment.apps/navidrome configured\nservice/navidrome-svc unchanged\n</code></pre><p></p> </li> </ol> <p>After a couple of minutes the deployment should be back up, because applying the manifest scales it back up to <code>replicas: 1</code></p> <pre><code>$ kubectl -n navidrome get all\nNAME                             READY   STATUS    RESTARTS   AGE\npod/navidrome-64fb55cd88-m6hll   1/1     Running   0          2m17s\n\nNAME                    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nservice/navidrome-svc   NodePort   10.110.51.110   &lt;none&gt;        4533:30533/TCP   270d\n\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/navidrome   1/1     1            1           270d\n\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/navidrome-64fb55cd88   1         1         1       3d14h\n</code></pre> <p>Accessing Navidrome on its web UI and playing music confirms the application still works.</p>"},{"location":"blog/2026/01/24/migrating-nfs-volumes-to-the-nfs-csi-driver-for-kubernetes/#repeat-for-all-deployments","title":"Repeat for all deployments","text":"<p>Find other deployments using <code>hostPath</code> mounts on the host-mounted NFS directory and  repeat the above steps to migrate them:</p> <pre><code>$ grep -rn /home/nas . | cut -f1 -d: | sort -u\n./audiobookshelf.yaml\n./jellyfin.yaml\n./komga.yaml\n</code></pre> <p>This reveals only a few applications are using the NFS volume, namely Audiobookshelf, Jellyfin and Komga.</p>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/","title":"Migrating Kubernetes volumes to Longhorn","text":"<p>Migrating NFS volumes to the NFS CSI driver was an easy step forward preparing the single-node Kubernetes cluster to be upgraded to an Active-Active High Availability cluster. The next step in that direction is to migrate volumes currently implemented (the lazy way) with <code>hostPath</code> pointed to local NVMe SSD storage to a distributed file system, while still leveraging the SSDs speed as high-availability distributed volumes replicated across every node's local NVMe SSDs.</p>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#motivation","title":"Motivation","text":"<p>While NFS volumes are the best choice for bulk data (media, backups, etc.) these are too slow for mission-critical configuration files and databases. Longhorn is better for these because it can mirror data across all nodes, if one fails the pod instantly restarts on another node with its data intact, and whenever adding a new node Longhorn automatically rebalances replicas to utilize the new storage. Moving forward, Longhorn can also use the Synology NAS as a \"Backup Target\" to make additional backups.</p>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#prepare-host-os","title":"Prepare Host OS","text":"<p>Run these commands on all current and future nodes to install Longhorn\u2019s required dependencies</p> <pre><code># apt install -y open-iscsi util-linux dmsetup\n\n# modprobe iscsi_tcp\n# modprobe dm_crypt\n# echo \"iscsi_tcp\" | tee -a /etc/modules-load.d/longhorn.conf\niscsi_tcp\n# echo \"dm_crypt\" | tee -a /etc/modules-load.d/longhorn.conf\ndm_crypt\n\n# systemctl enable --now iscsid\nSynchronizing state of iscsid.service with SysV service script with /usr/lib/systemd/systemd-sysv-install.\nExecuting: /usr/lib/systemd/systemd-sysv-install enable iscsid\nCreated symlink /etc/systemd/system/sysinit.target.wants/iscsid.service \u2192 /usr/lib/systemd/system/iscsid.service.\n\n# systemctl status iscsid\n\u25cf iscsid.service - iSCSI initiator daemon (iscsid)\n     Loaded: loaded (/usr/lib/systemd/system/iscsid.service; enabled; preset: enabled)\n     Active: active (running) since Sat 2026-01-24 15:34:28 CET; 19s ago\nTriggeredBy: \u25cf iscsid.socket\n       Docs: man:iscsid(8)\n    Process: 1256258 ExecStartPre=/usr/lib/open-iscsi/startup-checks.sh (code=exited, status=0/SUCCESS)\n    Process: 1256279 ExecStart=/usr/sbin/iscsid (code=exited, status=0/SUCCESS)\n   Main PID: 1256334 (iscsid)\n      Tasks: 2 (limit: 37734)\n     Memory: 3.6M (peak: 3.9M)\n        CPU: 27ms\n     CGroup: /system.slice/iscsid.service\n             \u251c\u25001256333 /usr/sbin/iscsid\n             \u2514\u25001256334 /usr/sbin/iscsid\n\nJan 24 15:34:28 octavo systemd[1]: Starting iscsid.service - iSCSI initiator daemon (iscsid)...\nJan 24 15:34:28 octavo iscsid[1256279]: iSCSI logger with pid=1256333 started!\nJan 24 15:34:28 octavo systemd[1]: Started iscsid.service - iSCSI initiator daemon (iscsid).\nJan 24 15:34:29 octavo iscsid[1256333]: iSCSI daemon with pid=1256334 started!\n</code></pre> Additional dependencies for optional features. <ul> <li><code>cryptsetup</code> and <code>dm_crypt</code> would be required to enable Volume Encryption.     <code>cryptsetup</code> uses the <code>dm_crypt</code> kernel module to manage LUKS2-formatted     encrypted volumes (alss used previously to     encrypt external SSD), ensuring that     data is secured at rest before being written to the physical SSDs</li> <li><code>jq</code> is not used by the Longhorn storage engine itself during runtime, but it is     useful to parse JSON output and would be needed for running Longhorn Environment     Check scripts to verify nodes before installation.</li> <li><code>nfs-common</code> will be needed to make backups to NFS volumes; already installed for     Migrating NFS volumes to the NFS CSI driver</li> </ul>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#migration-from-btrfs-to-xfs","title":"Migration from Btrfs to XFS","text":"<p>Longhorn Installation Requirements includes a host filesystem that supports the <code>file extents</code> feature to store the data, <code>ext4</code> and <code>XFS</code> being the only ones supported; this presents a challenge because both local disks in <code>octavo</code> are formatted as <code>Btrfs</code> and all workloads depend on at least one of them.</p> <p>While there are adjustments that can be made to <code>Btrfs</code> volumes to minimize issues with Longhorn, ultimately <code>Btrfs</code> being unsupported means a kernel update or a specific IO pattern could still lead to volumes becoming stuck in Read-Only mode. Alternatives to Longhorn such as Ceph (via Rook Operator), GlusterFS (via various CSI drivers) or OpenEBS LocalPV also have enough incompatibilities with <code>Btrfs</code> volumes that would not serve the purpose.</p>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#remove-use-of-the-sata-ssd","title":"Remove use of the SATA SSD","text":"<p>Warning</p> <p>Stop all <code>crontab</code> jobs and other scripts that may be syncing files to or from any file sytem in the NVMe or SATA SSDS. Make sure that scripts running periodically (e.g. those listed by <code>crontab -l</code>) will not run or write data under <code>/home/ssd</code> since that would write data to the NVMe SSD and may accidentally fill it up.</p> <p>The only use of the SATA SSD is for media files that are replicated from the NAs, so there is nothing in the SATA SSD that needs to be copied outside of it. It is enough to migrate the one volume using the SATA SSD to a NFS CSI mount and the disk is ready to format.</p> <p>Create a new XFS file system on the SATA SSD and mount it in a temporary directory:</p> <pre><code># umount /home/ssd \n# mkfs.xfs -f /dev/sda\nmeta-data=/dev/sda               isize=512    agcount=4, agsize=244188662 blks\n         =                       sectsz=4096  attr=2, projid32bit=1\n         =                       crc=1        finobt=1, sparse=1, rmapbt=1\n         =                       reflink=1    bigtime=1 inobtcount=1 nrext64=0\ndata     =                       bsize=4096   blocks=976754646, imaxpct=5\n         =                       sunit=0      swidth=0 blks\nnaming   =version 2              bsize=4096   ascii-ci=0, ftype=1\nlog      =internal log           bsize=4096   blocks=476930, version=2\n         =                       sectsz=4096  sunit=1 blks, lazy-count=1\nrealtime =none                   extsz=4096   blocks=0, rtextents=0\nDiscarding blocks...Done.\n\n# blkid /dev/sda\n/dev/sda: UUID=\"76347c52-c635-49b3-baa3-4ea98e41b4a4\" BLOCK_SIZE=\"4096\" TYPE=\"xfs\"\n\n# mkdir -p /mnt/sata_temp\n# mount /dev/sda /mnt/sata_temp\n</code></pre> <p>Warning</p> <p>Remove the line in <code>/etc/fstab</code> to mount the old file system on <code>/home/ssd</code> or else the system will no longer boot.</p>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#copy-nvme-to-sata-ssd","title":"Copy NVMe to SATA SSD","text":"<p>Since the <code>/home</code> partitiion is critical for all workloads, the migration must be performed with the cluster services stopped to ensure data consistency. To minimize down-time for the affected services, at least the largest part of the data can be copied while workloads are running because it is mostly read-only:</p> <pre><code># time rsync --bwlimit=500000 -aHAXv /home/depot /mnt/sata_temp/\n\nsent 921,117,494,040 bytes  received 741,595 bytes  364,438,471.07 bytes/sec\ntotal size is 920,889,625,124  speedup is 1.00\n\nreal    42m7.041s\nuser    7m56.286s\nsys     22m42.006s\n</code></pre> <p>Warning</p> <p>Writting to the problematic Crucial MX500 SSD requires hard-limiting the bandwidth to 500 MB/s with <code>--bwlimit=500000</code></p> <p>This takes 42 minutes to copy over 858 GB of <code>Podcasts</code>; that's 42 minutes less of down-time with all workloads down for the complete migration. Additional time can be saved if other directories are found to be unnecessary to move; as it happens 240 GB were found left back under <code>/home/k8s/photos</code> with no workload using them and additional 40 GB were found under <code>/home/k8s/minecraft-server-backups</code> that were also out of use.</p> <p>Once most of the data has been transfered, all Kubernetes workloads and services must be stopped to finalize the transfer. It is actually necessary to first drain the node, to make sure all pods are stopped, otherwise pods will continue running and potentially writting to the files in the current <code>/home</code> partition.</p> <pre><code># kubectl drain --ignore-daemonsets --delete-emptydir-data --force octavo\n# systemctl stop kubelet\n# systemctl stop containerd\n</code></pre> <p>Once all the processes are stopped, <code>lsof</code> should report that not a single process is using files under <code>/home</code> even though it may not yet be possible to unmount it.</p> <pre><code># time lsof +D /home \n\nreal    0m8.257s\nuser    0m1.813s\nsys     0m7.057s\n\n# umount /home\numount: /home: target is busy.\n</code></pre> <p><code>/home</code> cannot be unmounted yet because the NFS volume from the Synology NAS is still mounted as <code>/home/nas</code> and before that one can be unmounted it is necessary to stop the Continuous Monitoring service:</p> <pre><code># systemctl stop conmon\n# umount /home/nas\n# umount /home\n</code></pre> <p>Note</p> <p>Add <code>-x</code> to the <code>rsync</code> command flags to avoid copying files from other file systems.</p> <pre><code># mount /home\n# time rsync -aHAXvx /home/ /mnt/sata_temp/\n\nreal    5m29.869s\nuser    0m15.302s\nsys     0m59.440s\n</code></pre> <p>At this point the <code>/home</code> partition should never be used again.</p> <p>To make sure no files are written to it; unmount it, disable its entry in <code>/etc/fstab</code> and reboot.</p> <p>Do not try to reformat the NVMe partition without rebooting. It won't work.</p>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#copy-sata-to-nvme-ssd","title":"Copy SATA to NVMe SSD","text":"<p>After rebooting without mounting the (old) <code>/home</code> partition, create a new XFS file system in the NVME partition and take note of its new block ID:</p> <pre><code># time mkfs.xfs -f /dev/nvme0n1p5\nmeta-data=/dev/nvme0n1p5         isize=512    agcount=4, agsize=232323200 blks\n         =                       sectsz=512   attr=2, projid32bit=1\n         =                       crc=1        finobt=1, sparse=1, rmapbt=1\n         =                       reflink=1    bigtime=1 inobtcount=1 nrext64=0\ndata     =                       bsize=4096   blocks=929292800, imaxpct=5\n         =                       sunit=0      swidth=0 blks\nnaming   =version 2              bsize=4096   ascii-ci=0, ftype=1\nlog      =internal log           bsize=4096   blocks=453756, version=2\n         =                       sectsz=512   sunit=0 blks, lazy-count=1\nrealtime =none                   extsz=4096   blocks=0, rtextents=0\nDiscarding blocks...Done.\n\nreal    0m21.188s\nuser    0m0.010s\nsys     0m0.103s\n\n# blkid /dev/nvme0n1p5\n/dev/nvme0n1p5: UUID=\"1a2f94cc-315f-4bf1-8b59-0575b49fe098\" BLOCK_SIZE=\"512\" TYPE=\"xfs\" PARTUUID=\"df113399-0802-42bf-b170-1a9e62b79220\"\n\n# blkid /dev/sda\n/dev/sda: UUID=\"76347c52-c635-49b3-baa3-4ea98e41b4a4\" BLOCK_SIZE=\"4096\" TYPE=\"xfs\"\n</code></pre> <p>Update the <code>/etc/fstab</code> entries for <code>/home</code> and <code>/home/ssd</code> with their new Block ID (and <code>xfs</code> instead of <code>btrfs</code>), then mount only <code>/home</code> using its <code>/etc/fstab</code> entry, mount the SATA SSD in the temporary directory (so it's not under <code>/home</code>) and copy all its data back:</p> <pre><code># mount /home\n# mount /dev/sda /mnt/sata_temp\n# time rsync -aHAXvx /mnt/sata_temp/ /home/\n\nsent 1,033,492,365,270 bytes  received 24,759,287 bytes  361,306,458.51 bytes/sec\ntotal size is 1,041,827,909,959  speedup is 1.01\n\nreal    47m39.689s\nuser    12m36.162s\nsys     29m17.498s\n\n#  df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/nvme0n1p2   60G   14G   46G  23% /\n/dev/nvme0n1p4   60G   36G   25G  59% /var/lib\n/dev/nvme0n1p1  1.1G  6.2M  1.1G   1% /boot/efi\n/dev/nvme0n1p5  3.5T  1.1T  2.5T  30% /home\n/dev/sda        3.7T  1.1T  2.7T  28% /mnt/sata_temp\n</code></pre> <p>Once all data has been copied back to the NVMe, now on a new XFS file system, and all entries <code>/etc/fstab</code> have been updated with the new file systems' UUIDs, mount the SATA SSD and the NAS back on their original mount points:</p> <pre><code># umount /mnt/sata_temp\n# mount /home/ssd\n# mount /home/nas\n# df -h\nFilesystem                  Size  Used Avail Use% Mounted on\n/dev/nvme0n1p2               60G   14G   46G  23% /\n/dev/nvme0n1p4               60G   36G   25G  59% /var/lib\n/dev/nvme0n1p1              1.1G  6.2M  1.1G   1% /boot/efi\n/dev/nvme0n1p5              3.5T  1.1T  2.5T  30% /home\nluggage:/volume1/NetBackup   21T   15T  6.0T  72% /home/nas\n/dev/sda                    3.7T  1.1T  2.7T  28% /home/ssd\n</code></pre> <p>Finally the cluster workloads can be restored by uncordoning the node:</p> <pre><code># kubectl uncordon octavo\nnode/octavo uncordoned\n</code></pre>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#install-longhorn","title":"Install Longhorn","text":"<p>To keep track of deployment values, create <code>longhorn-values.yaml</code> with the following values:</p> <p><code>longhorn-values.yaml</code></p> <pre><code>defaultSettings:\n  allowVolumeCreationWithDegradedAvailability: \"true\"\n  createDefaultDiskLabeledNodes: \"true\"\n  defaultDataPath: /home/longhorn\n  deletingConfirmationFlag: \"true\"\nmetrics:\n  serviceMonitor:\n    enabled: \"true\"\n</code></pre> <p>Before installing the Helm chart, create a <code>longhorn</code> directory under each partition with fast local (SSD) storage:</p> <pre><code># mkdir /home/longhorn /home/ssd/longhorn\n\n# ls -lad /home/longhorn/ /home/ssd/longhorn/\ndrwxr-xr-x 2 root root 6 Jan 25 13:54 /home/longhorn/\ndrwxr-xr-x 2 root root 6 Jan 25 16:00 /home/ssd/longhorn/\n</code></pre> <p>Install with Helm using the latest of Chart versions available in artifacthub.io/longhorn:</p> <pre><code>$ helm repo add longhorn https://charts.longhorn.io\n\"longhorn\" has been added to your repositories\n\n$ helm repo update\nfrom your chart repositories...\n...Successfully got an update from the \"longhorn\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\n\n$ helm upgrade --install longhorn longhorn/longhorn \\\n  --values longhorn-values.yaml \\\n  --namespace longhorn-system \\\n  --create-namespace \\\n  --version 1.10.1\nRelease \"longhorn\" does not exist. Installing it now.\nI0125 14:20:26.605248  523341 warnings.go:107] \"Warning: unrecognized format \\\"int64\\\"\"\nI0125 14:20:26.606712  523341 warnings.go:107] \"Warning: unrecognized format \\\"int64\\\"\"\nI0125 14:20:26.610027  523341 warnings.go:107] \"Warning: unrecognized format \\\"int64\\\"\"\nI0125 14:20:26.610080  523341 warnings.go:107] \"Warning: unrecognized format \\\"int64\\\"\"\nNAME: longhorn\nLAST DEPLOYED: Sun Jan 25 14:20:26 2026\nNAMESPACE: longhorn-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nLonghorn is now installed on the cluster!\n\nPlease wait a few minutes for other Longhorn components such as CSI deployments, Engine Images, and Instance Managers to be initialized.\n\nVisit our documentation at https://longhorn.io/docs/\n</code></pre> <p>After a few minutes all the pods and services are up and running:</p> <code>kubectl get all -n longhorn-system</code> <pre><code>$ kubectl get all -n longhorn-system\nNAME                                                    READY   STATUS    RESTARTS   AGE\npod/csi-attacher-5857549d6f-57tz9                       1/1     Running   0          4m4s\npod/csi-attacher-5857549d6f-cd5l8                       1/1     Running   0          4m4s\npod/csi-attacher-5857549d6f-nh7dn                       1/1     Running   0          4m4s\npod/csi-provisioner-57f9d44448-6g5kf                    1/1     Running   0          4m3s\npod/csi-provisioner-57f9d44448-b2p59                    1/1     Running   0          4m4s\npod/csi-provisioner-57f9d44448-zp4wd                    1/1     Running   0          4m3s\npod/csi-resizer-547f8b9dc8-2nw8d                        1/1     Running   0          4m3s\npod/csi-resizer-547f8b9dc8-sq7kx                        1/1     Running   0          4m4s\npod/csi-resizer-547f8b9dc8-t9mff                        1/1     Running   0          4m4s\npod/csi-snapshotter-8558df8679-2s7vw                    1/1     Running   0          4m3s\npod/csi-snapshotter-8558df8679-7cfmt                    1/1     Running   0          4m3s\npod/csi-snapshotter-8558df8679-9st8q                    1/1     Running   0          4m3s\npod/engine-image-ei-3154f3aa-4p885                      1/1     Running   0          4m9s\npod/instance-manager-106c7c23639743eccb1b438e18f1bc72   1/1     Running   0          4m9s\npod/longhorn-csi-plugin-js5zc                           3/3     Running   0          4m3s\npod/longhorn-driver-deployer-676f7f6c5c-v4kvh           1/1     Running   0          4m17s\npod/longhorn-manager-7446v                              2/2     Running   0          4m17s\npod/longhorn-ui-7c54575f4d-8ccrr                        1/1     Running   0          4m17s\npod/longhorn-ui-7c54575f4d-pfghw                        1/1     Running   0          4m18s\n\nNAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/longhorn-admission-webhook   ClusterIP   10.104.236.167   &lt;none&gt;        9502/TCP   4m19s\nservice/longhorn-backend             ClusterIP   10.97.214.18     &lt;none&gt;        9500/TCP   4m19s\nservice/longhorn-frontend            ClusterIP   10.108.65.154    &lt;none&gt;        80/TCP     4m19s\nservice/longhorn-recovery-backend    ClusterIP   10.101.82.196    &lt;none&gt;        9503/TCP   4m19s\n\nNAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.apps/engine-image-ei-3154f3aa   1         1         1       1            1           &lt;none&gt;          4m9s\ndaemonset.apps/longhorn-csi-plugin        1         1         1       1            1           &lt;none&gt;          4m4s\ndaemonset.apps/longhorn-manager           1         1         1       1            1           &lt;none&gt;          4m19s\n\nNAME                                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/csi-attacher               3/3     3            3           4m4s\ndeployment.apps/csi-provisioner            3/3     3            3           4m4s\ndeployment.apps/csi-resizer                3/3     3            3           4m4s\ndeployment.apps/csi-snapshotter            3/3     3            3           4m4s\ndeployment.apps/longhorn-driver-deployer   1/1     1            1           4m19s\ndeployment.apps/longhorn-ui                2/2     2            2           4m19s\n\nNAME                                                  DESIRED   CURRENT   READY   AGE\nreplicaset.apps/csi-attacher-5857549d6f               3         3         3       4m4s\nreplicaset.apps/csi-provisioner-57f9d44448            3         3         3       4m4s\nreplicaset.apps/csi-resizer-547f8b9dc8                3         3         3       4m4s\nreplicaset.apps/csi-snapshotter-8558df8679            3         3         3       4m4s\nreplicaset.apps/longhorn-driver-deployer-676f7f6c5c   1         1         1       4m19s\nreplicaset.apps/longhorn-ui-7c54575f4d                2         2         2       4m19s\n</code></pre>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#disable-multipath","title":"Disable Multipath","text":"<p>Longhorn can run into issues when Multipath is enabled, which is visible in the output from <code>lsblk</code> on a Longhorn block device, e.g.</p> <pre><code># lsblk /dev/longhorn/pvc-7ae11ade-d6c8-4296-ac67-58953e3dddc2NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS\nsda 8:0 0 50G 0 disk\n \u2514\u2500mpatha 252:0 0 50G 0 mpath \n</code></pre> <p>This can cause a mount to fail despite the device showing as Healthy in the Longhorn UI. When Longhorn attaches the volume, <code>multipathd</code> immediately \"claims\" the device (<code>sda</code>) and creates a virtual device (<code>/dev/mapper/mpatha</code>). Because <code>multipathd</code> has an exclusive lock on the block device, the Longhorn CSI driver (and the <code>mount</code> command) gets an <code>\"Already mounted or mount point busy\"</code> error when it tries to open <code>/dev/longhorn/pvc-7ae11ade...</code> Longhorn shows the volume as Healthy because the iSCSI connection is technically up and the data is replicated; the \"Health\" check doesn't know the host OS has \"hijacked\" the local block device.</p> <p>To avoid such problems disable and remove <code>multipathd</code>:</p> <pre><code># systemctl disable --now multipathd\n# apt-get remove multipath-tools -y\n</code></pre>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#longhorn-ui-ingress","title":"Longhorn UI Ingress","text":"<p>To make the Longhorn UI, create an <code>Ingress</code> to access it over HTTPS:</p> <p><code>pomerium/pomerium-ingress/longhorn.yaml</code></p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: longhorn-pomerium-ingress\n  namespace: longhorn-system\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.pomerium.io/pass_identity_headers: true\n    ingress.pomerium.io/preserve_host_header: true\nspec:\n  ingressClassName: pomerium\n  rules:\n    - host: longhorn.very-very-dark-gray.top\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: longhorn-frontend\n              port:\n                name: \"http\"\n  tls:\n    - hosts:\n        - longhorn.very-very-dark-gray.top\n      secretName: tls-secret\n</code></pre> <pre><code>$ kubectl apply -k pomerium/pomerium-ingress/\ningress.networking.k8s.io/audiobookshelf-pomerium-ingress unchanged\ningress.networking.k8s.io/code-server-pomerium-ingress unchanged\ningress.networking.k8s.io/firefly-iii-pomerium-ingress unchanged\ningress.networking.k8s.io/home-assistant-pomerium-ingress unchanged\ningress.networking.k8s.io/homepage-pomerium-ingress unchanged\ningress.networking.k8s.io/komga-pomerium-ingress unchanged\ningress.networking.k8s.io/dashboard-pomerium-ingress unchanged\ningress.networking.k8s.io/longhorn-pomerium-ingress created\ningress.networking.k8s.io/jellyfin-pomerium-ingress unchanged\ningress.networking.k8s.io/grafana-pomerium-ingress unchanged\ningress.networking.k8s.io/influxdb-pomerium-ingress unchanged\ningress.networking.k8s.io/prometheus-pomerium-ingress unchanged\ningress.networking.k8s.io/navidrome-pomerium-ingress unchanged\ningress.networking.k8s.io/ryot-pomerium-ingress unchanged\ningress.networking.k8s.io/steam-headless-pomerium-ingress unchanged\ningress.networking.k8s.io/unifi-network-app-pomerium-ingress unchanged\ningress.networking.k8s.io/ddns-updater-pomerium-ingress unchanged\n</code></pre> <p>After a little over a minute the DNS challenge is completed and the Longhorn UI is live at longhorn.very-very-dark-gray.top</p> <pre><code>$ kubectl get challenge -n longhorn-system --watch\nNAME                                STATE     DOMAIN                             AGE\ntls-secret-1-2700837206-765358469   pending   longhorn.very-very-dark-gray.top   7s\ntls-secret-1-2700837206-765358469   valid     longhorn.very-very-dark-gray.top   79s\n\n$ kubectl get certificate -n longhorn-system\nNAME         READY   SECRET       AGE\ntls-secret   True    tls-secret   89s\n</code></pre> <p>At first the UI will only show that is 1 Node and it is Disabled:</p> <p></p> <p>To make the node available, add to it the <code>create-default-disk</code> label:</p> <p></p><pre><code>$ kubectl label node octavo node.longhorn.io/create-default-disk=true\nnode/octavo labeled\n</code></pre> Once the node is labeled it becomes Schedulable and the next steps can be taken.<p></p> <p></p>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#configure-disks","title":"Configure disks","text":"<p>One the node is Schedulable go to the Nodes tab and use the drop-down menu on the right end of the node's entry to Edit node and disks. Add the SATA SSD, add a +New Disk Tag to each disk to reflect their hardware interface (and bandwidth) and set their Storage Reserved to the recommended 50 Gi.</p> <p></p> <p>Storage metrics represent aggregate values across all disks configured on all nodes:</p> <ul> <li>Reserved storage is the space Longhorn will not use for volume replicas.     It is set aside for the Host OS, other applications, and to prevent the disk from     reaching 100% capacity.</li> <li>Used storage is the actual physical space currently occupied by Longhorn     data and other system files.</li> <li>Schedulable storage is the amount of new volume capacity Longhorn can     allocate to pods.</li> </ul>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#targeted-storageclasses","title":"Targeted <code>StorageClass</code>es","text":"<p>To enable the automatic creation of volumes in each disk, create a <code>StorageClass</code> for each type of disk (NVMe, SATA) with this manifest:</p> <p><code>longhorn-storage.yaml</code></p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: longhorn-nvme\nprovisioner: driver.longhorn.io\nallowVolumeExpansion: true\nparameters:\n  numberOfReplicas: \"1\" # Increase to 2 after adding a second node\n  diskSelector: \"nvme\"\n  dataLocality: \"best-effort\"\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: longhorn-sata\nprovisioner: driver.longhorn.io\nallowVolumeExpansion: true\nparameters:\n  numberOfReplicas: \"1\"\n  diskSelector: \"sata\"\n  dataLocality: \"best-effort\"\n</code></pre> <p>Longhorn supports supports online volume expansion, allowing the volume to grow without downtime, only if the <code>StorageClass</code> has <code>allowVolumeExpansion: true</code> which cannot be added later without deleting and recreating the <code>StorageClass</code> (existing volumes are not deleted).</p> <pre><code>$ kubectl apply -f longhorn-storage.yaml\nstorageclass.storage.k8s.io/longhorn-nvme created\nstorageclass.storage.k8s.io/longhorn-sata created\n</code></pre>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#replication-vs-bandwidth","title":"Replication Vs. Bandwidth","text":"<p>When creating new PVC using the <code>longhorn-nvme</code> class to use the fastest SSD, <code>accessModes</code> should almost always be set to <code>ReadWriteOnce</code>.</p> <p>RWO volumes (<code>ReadWriteOnce</code>) are mounted directly as block devices via iSCSI. This provides the highest performance for database-like workloads (e.g., Postgres, Redis, or application caches) because there is no network filesystem overhead. Most Kubernetes deployments (even those with multiple replicas) do not require multiple pods to write to the same volume simultaneously. Instead, each pod typically manages its own data. In a multi-node cluster, if one node fails, Kubernetes will move the pod to the other node. Longhorn will then detach the RWO volume from the old node and attach it to the new one.</p> <p>RWX volumes (<code>ReadWriteMany</code>) should only be used when an application specifically requires multiple pods (possibly on multiple nodes) to  read and write to the exact same files at the same time.  Longhorn implements RWX by spinning up a \"Share Manager\" pod that acts as an NFS server for that specific volume (this requires the <code>nfs-common</code>).</p> <p>When scaling a single-node cluster to two nodes and setting <code>numberOfReplicas: 2</code>, an RWO volume will still be fully distributed and synced across both nodes' NVMe SSDs. The \"Once\" in <code>ReadWriteOnce</code> refers only to how many nodes can mount the volume at one time, not how many nodes store the data. Even with RWO, data is redundant and safe on both nodes.</p> <p>However, in a two-node Longhorn cluster with two replicas, each pod will not use exclusively local PCIe NVMe bandwidth for its storage operations. While each pod can be guaranteed to have a local copy of its data, the synchronous replication requirement of a distributed system introduces network latency. </p> <p>By default, Longhorn may schedule a pod on a node that does not contain a local replica of its volume. To force each pod to prioritize its local disk, enable Data Locality in the <code>StorageClass</code> or volume settings: </p> <ul> <li><code>strict-local</code>: Enforces that a healthy replica must exist on the same node as the     pod. This provides the lowest possible latency for reads but prevents the pod from     starting if the local disk is unavailable. </li> <li><code>best-effort</code>: Longhorn attempts to keep one replica on the same node as the pod.     If it can't, the pod will still run but will access its data over the network from     the other node. This is very nearly always as fast as <code>strict-local</code> but with the     advantage that pods can move across nodes more easily and quickly, which maximizes     uptime without compromising performance.</li> </ul> <p>Even if a pod is reading directly from its local NVMe SSD, write operations are synchronous. When a pod writes data, the Longhorn Engine (running on the same node as the pod) must successfully write that data to both the local replica and the remote replica on the second node before the write is considered \"complete\".</p> <p>This means the write bandwidth and latency are capped by the network speed and the overhead of the iSCSI/Longhorn protocol, rather than the raw PCIe NVMe bandwidth. While the pod will benefit from NVMe speeds for local reads, the overall performance is lower than a raw local NVMe SSD: reads will have near-local speeds but writes is limited by network latency.</p> <p>Ultimately, if an application requires raw PCIe NVMe bandwidth and doesn't need Longhorn\u2019s high availability features, then a <code>hostPath</code> or Local Path Provisioner class may be best for that specific workload. However, for most general-purpose applications, the performance trade-off of Longhorn is acceptable for the benefit of having a fully synced, distributed cluster.</p>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#active-active-vs-active-passive","title":"Active-Active Vs. Active-Passive","text":"<p>For a single-pod deployment like code-server, when scaling up to 2 replicas on 2 nodes, with Longhorn replicating its RWO volume with data locality set to <code>best-effort</code> to keep both volumes in sync, the result is an active-passive cluster, with one pod being active while the other stays in stand-by to take over only when the first one goes down.</p> <p>This setup will not work for an active-active setup because of two reasons:</p> <ol> <li> <p>A <code>ReadWriteOnce</code> (RWO) volume is physically locked to a single node at a time.     If Pod-A is running one node and has the volume mounted, Pod-B on the other node     will be unable to start. It will stay in a <code>ContainerCreating</code> or     <code>MatchNodeSelector</code> state because Longhorn cannot attach an RWO volume to two nodes     simultaneously. While Longhorn replicates the data to both nodes' NVMe SSDs in the     background, only one engine can be the Primary (the writer) at any given moment.</p> <p>With <code>dataLocality: best-effort</code> and <code>numberOfReplicas: 2</code> every byte Pod-A writes to the one NVMe is synchronously sent over the network to the other NVMe, so that both disks are always bit-for-bit identical. If one node crashes, Kubernetes detects the node failure. It then schedules a new Pod on the other node. Longhorn \"promotes\" the second replica to be the new Primary, and the Pod starts. This is an Active-Passive High Availability setup, with redundancy, but not both pods responding to requests at the same time.</p> </li> <li> <p>Even when using RWX (which allows both pods to run), applications like     <code>code-server</code> are not stateless; <code>code-server</code> (and its underlying VS Code     engine) uses SQLite databases for extensions and settings. SQLite does not     support multiple processes writing to the same file over a network (NFS/RWX).     Attempting to run multiple instances on a single such database would lead to     database corruption or immediate \"Locked\" errors. Moreover, if Pomerium proxy sends     a request for \"Save File\" to Pod-B, but the \"Open Editor\" session was handled by     Pod-A, the session state would be inconsistent.</p> </li> </ol> <p>RWO volumes (best compromise for speed and replication) support Disaster Recovery (Active-Passive) setups, ensuring data is never lost a node dies. However, it cannot be Active-Active; for that, an application must be specifically designed to store its state in an external database (like Postgres) rather than a local RWO/RWX volume.</p> <p>Most of the currently running applications are designed as monolithic services that rely on a single local database (SQLite) or a local file system (e.g. Home Assistant), making them Active-Passive by nature. However, several can be adapted for Active-Active  High Availability with specific configurations: Pomerium and Homepage are stateless, Grafana can have its SQLite database replaced by Postgres or MySQL, but ony InfluxDB 3.0 supports clustering.</p>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#from-hostpath-to-longhorn","title":"From <code>hostPath</code> to Longhorn","text":"<p>At this point data can be migrated from the old <code>hostPath</code> volume to the new Longhorn volumes in three steps for each application (Deployment):</p> <ol> <li>Create a new <code>PersistentVolumeClaim</code> using <code>storageClassName: longhorn-nvme</code>.</li> <li>Scale the deployment down.</li> <li>Copy data using a simple pod to run the <code>cp</code> command e.g. using <code>busybox</code>.</li> <li>Update the Deployment to mount the new <code>PersistentVolumeClaim</code>.</li> <li>Scale the deployment back up.</li> </ol> <p>To copy the data the same simple pod can be run by adjusting just the highlighted values:</p> <p><code>longhorn-migrator.yaml</code></p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hostpath-to-longhorn-migrator\n  namespace: code-server  # Set to each application's namespace\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure  # Restart only upon failure.\n      containers:\n      - name: worker\n        image: ubuntu:22.04\n        command: [\"/bin/sh\", \"-c\"]\n        args:\n          - |\n            apt-get update &amp;&amp; apt-get install -y rsync\n            rsync -uva /old/ /new/\n        volumeMounts:\n        - name: old-data\n          mountPath: /old  # Point to existing hostPath subdirectory\n          readOnly: true\n        - name: new-data\n          mountPath: /new  # Point to new Longhorn PVC\n      volumes:\n      - name: old-data\n        hostPath:\n          path: /home/k8s/code-server\n      - name: new-data\n        persistentVolumeClaim:\n          claimName: code-server-pvc-lh  # The new PVC created for each application.\n</code></pre>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#example-migration","title":"Example migration","text":"<p>To illustrate the migration process with a simple case, start by migrating VS Code Server.</p> <ol> <li> <p>Update the <code>code-server.yaml</code> manifest to add a new <code>PersistentVolumeClaim</code> using     <code>storageClassName: longhorn-nvme</code> and apply the mani fest to create the PVC:</p> <p><code>code-server.yaml</code></p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: code-server-pvc-lh\n  namespace: code-server\nspec:\n  storageClassName: longhorn-nvme\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n</code></pre> <p>Warning</p> <p>The <code>accessModes</code> value cannot be easly done later, e.g. when adding a node; see Replication Vs. Bandwidth.</p> <p>Confirm the PVC is created after applying the <code>code-server.yaml</code> manifest:</p> <pre><code>$ kubectl apply -f code-server.yaml \nnamespace/code-server unchanged\nservice/code-server unchanged\npersistentvolume/code-server-pv unchanged\npersistentvolumeclaim/code-server-pv-claim unchanged\npersistentvolumeclaim/code-server-pvc-lh created\ndeployment.apps/code-server unchanged\n\n$ kubectl -n code-server get pvc\nNAME                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    VOLUMEATTRIBUTESCLASS   AGE\ncode-server-pv-claim   Bound    code-server-pv                             10Gi       RWO            manual          &lt;unset&gt;                 269d\ncode-server-pvc-lh     Bound    pvc-f549ff26-5424-4b30-b0a3-245a845888f7   5Gi        RWO            longhorn-nvme   &lt;unset&gt;                 68s\n</code></pre> </li> <li> <p>Scale the deployment down.</p> <pre><code>$ kubectl -n code-server scale deployment code-server --replicas=0\ndeployment.apps/code-server scaled\n</code></pre> </li> <li> <p>Copy data using a simple pod to run the <code>cp</code> command, e.g. using <code>busybox</code>.</p> <p><code>longhorn-migrator.yaml</code></p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hostpath-to-longhorn-migrator\n  namespace: code-server  # Set to each application's namespace\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure  # Restart only upon failure.\n      containers:\n      - name: worker\n        image: ubuntu:22.04\n        command: [\"/bin/sh\", \"-c\"]\n        args:\n          - |\n            apt-get update &amp;&amp; apt-get install -y rsync\n            rsync -uva /old/ /new/\n        volumeMounts:\n        - name: old-data\n          mountPath: /old  # Point to existing hostPath subdirectory\n          readOnly: true\n        - name: new-data\n          mountPath: /new  # Point to new Longhorn PVC\n      volumes:\n      - name: old-data\n        hostPath:\n          path: /home/k8s/code-server\n      - name: new-data\n        persistentVolumeClaim:\n          claimName: code-server-pvc-lh  # The new PVC created for each application.\n</code></pre> <pre><code>$ kubectl apply -f longhorn-migrator.yaml \njob.batch/hostpath-to-longhorn-migrator created\n\n$ kubectl -n code-server get jobs --watch\nNAME                            STATUS    COMPLETIONS   DURATION   AGE\nhostpath-to-longhorn-migrator   Running   0/1           12s        12s\nhostpath-to-longhorn-migrator   Running   0/1           18s        18s\nhostpath-to-longhorn-migrator   SuccessCriteriaMet   0/1           19s        19s\nhostpath-to-longhorn-migrator   Complete             1/1           19s        19s\n\n$ kubectl -n code-server delete job hostpath-to-longhorn-migrator \njob.batch \"hostpath-to-longhorn-migrator\" deleted from code-server namespace\n</code></pre> </li> <li> <p>Update the Deployment to mount the new <code>PersistentVolumeClaim</code>.</p> <p><code>code-server.yaml</code></p> <pre><code>volumes:\n  - name: code-server-storage\n    persistentVolumeClaim:\n      claimName: code-server-pvc-lh\n</code></pre> <p>At this point the old <code>PersistentVolumeClaim</code> and <code>PersistentVolume</code> using <code>hostPath</code> can be removed. Apply the <code>code-server.yaml</code> manifest again:</p> <pre><code>$ kubectl apply -f code-server.yaml \nnamespace/code-server unchanged\nservice/code-server unchanged\npersistentvolumeclaim/code-server-pvc-lh unchanged\ndeployment.apps/code-server configured\n</code></pre> </li> <li> <p>Scale the deployment back up.</p> <pre><code>$ kubectl -n code-server scale deployment code-server --replicas=1\ndeployment.apps/code-server scaled\n\n$ kubectl -n code-server get pods --watch\nNAME                           READY   STATUS              RESTARTS   AGE\ncode-server-67c85cf5d7-gwpnk   0/1     ContainerCreating   0          1s\ncode-server-67c85cf5d7-gwpnk   1/1     Running             0          12s\n</code></pre> </li> <li> <p>Clean-up the old <code>PersistentVolumeClaim</code> and <code>PersistentVolume</code> based on <code>hostPath</code>:</p> <pre><code>$ kubectl -n code-server delete pvc code-server-pv-claim\npersistentvolumeclaim \"code-server-pv-claim\" deleted from code-server namespace\n\n$ kubectl delete pv code-server-pv\npersistentvolume \"code-server-pv\" deleted\n</code></pre> </li> </ol>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#nas-to-longhorn-sync","title":"NAS-to-Longhorn sync","text":"<p>Some applications work better when files are in a \"local\" file system, e.g. Audiobookshelf detects new books when added to a local (<code>hostPath</code>) volume, but when added to a NFS volume the library must be manually re-scanned; while this is not too bad for audiobooks when added at a rate of a few per month , it becomes a problem  with podcasts since the aggregate release rate of episodes soon amounts to a few every day.</p> <p>To keep such apps running off of \"local\" Longhorn volumes while using the NAS NFS volume as the canonical repository, run a sidecar pod that continuously syncs content from the NAS to the Longhorn volume. To avoid constantly scanning the content of files in the NAS, the pod should scan the NFS volume for metadata updates. Here is the sidecar pod added to Komga:</p> <p>Kubernetes deployment: <code>komga.yaml</code></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: komga\n  name: komga\n  namespace: komga\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: komga\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: komga\n    spec:\n      containers:\n        - image: alpine:latest\n          imagePullPolicy: Always\n          name: sync-nvme-from-nas\n          command: [\"/bin/sh\"]\n          args:\n            - \"-c\"\n            - |\n              apk add --no-cache rsync\n              SOURCE=\"/nas-source\"\n              TARGET=\"/data-target\"\n              LAST_FINGERPRINT=\"\"\n              echo \"Starting Smart-Sync Poller...\"\n              while true; do\n                CURRENT_FINGERPRINT=$(ls -Rl --full-time $SOURCE | md5sum)\n                if [ \"$CURRENT_FINGERPRINT\" != \"$LAST_FINGERPRINT\" ]; then\n                  echo \"Change detected on Synology NAS. Synchronizing to NVMe...\"\n                  rsync -au --delete --inplace \"$SOURCE/\" \"$TARGET/\"\n                  LAST_FINGERPRINT=$CURRENT_FINGERPRINT\n                  echo \"Sync complete. Waiting for next change...\"\n                fi\n                sleep 60\n              done\n          volumeMounts:\n          - name: komga-ebooks-nfs\n            mountPath: /nas-source\n            readOnly: true\n          - name: komga-books\n            mountPath: /data-target\n          securityContext:\n            allowPrivilegeEscalation: false\n            runAsUser: 118\n            runAsGroup: 118\n        - image: gotson/komga\n          imagePullPolicy: Always\n          name: komga\n</code></pre> <p>After applying this change to the Komga deployment, the pod is now running two containers: the application <code>komga</code> and the sidecar <code>sync-nvme-from-nas</code>. Dropping new files in the <code>ebooks</code> directory in the NAS, or deleting them, is detected and synced to the Longhorn volume:</p> <pre><code>$ kubectl logs -n komga -f deployment/komga -c sync-nvme-from-nas -f\n(1/6) Installing acl-libs (2.3.2-r1)\n(2/6) Installing lz4-libs (1.10.0-r0)\n(3/6) Installing popt (1.19-r4)\n(4/6) Installing libxxhash (0.8.3-r0)\n(5/6) Installing zstd-libs (1.5.7-r2)\n(6/6) Installing rsync (3.4.1-r1)\nExecuting busybox-1.37.0-r30.trigger\nOK: 9602 KiB in 22 packages\nStarting Smart-Sync Poller...\nChange detected on Synology NAS. Synchronizing to NVMe...\nsending incremental file list\n./\n\nsent 573,984 bytes  received 1,172 bytes  383,437.33 bytes/sec\ntotal size is 13,228,874,013  speedup is 23,000.50\nSync complete. Waiting for next change...\n\nChange detected on Synology NAS. Synchronizing to NVMe...\nsending incremental file list\nManuals/books/\nManuals/books/FUJIFILM X-T5 Owner's Manual.pdf\n\nsent 8,633,357 bytes  received 1,165 bytes  5,756,348.00 bytes/sec\ntotal size is 13,228,874,013  speedup is 1,532.09\nSync complete. Waiting for next change...\n</code></pre>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#longhorn-backups","title":"Longhorn Backups","text":"<p>Once pods are migrated to Longhorn volumes, setting up backups to the Synology NAS is easy.</p> <p>First, create the necessary directories in the NAS:</p> <pre><code># mkdir /home/nas/backups/longhorn\n</code></pre> <p>Then, Edit the default target under Backup and Restore &gt; Backup Targets and set the URL to <code>nfs://192.168.0.4:/volume1/NetBackup/backups/longhorn</code> after having creating the directories in the NAS.</p>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#recurrent-jobs","title":"Recurrent jobs","text":"<p>To set up a global backup system that covers volumes across all namespaces, leverage Longhorn\u2019s Recurring Job Groups. These allow defining the schedule once and then applying it to any PVC simply by adding a label.</p> <p>Create the Global Recurring Jobs with the following <code>longhorn-backups.yaml</code> manifest to create the jobs in the <code>longhorn-system</code> namespace. By adding them to the default group, they become available to any volume in the cluster.</p> <p><code>longhorn-backups.yaml</code></p> <pre><code>apiVersion: longhorn.io/v1beta2\nkind: RecurringJob\nmetadata:\n  name: global-daily-backup\n  namespace: longhorn-system\nspec:\n  cron: \"0 2 * * *\"        # 2:00 AM daily\n  task: \"backup\"           \n  groups:\n  - default                # Group name used for assignment\n  retain: 7                # Keeps 1 week of dailies\n  concurrency: 2           # Allows 2 volumes to backup simultaneously\n---\napiVersion: longhorn.io/v1beta2\nkind: RecurringJob\nmetadata:\n  name: global-weekly-backup\n  namespace: longhorn-system\nspec:\n  cron: \"0 3 * * 0\"        # 3:00 AM every Sunday\n  task: \"backup\"\n  groups:\n  - default\n  retain: 4                # Keeps 1 month of weeklies\n  concurrency: 1\n</code></pre> <pre><code>$ kubectl apply -f longhorn-backups.yaml\nrecurringjob.longhorn.io/global-daily-backup created\nrecurringjob.longhorn.io/global-weekly-backup created\n</code></pre> <p>In Longhorn, jobs do not target specific namespaces; instead, Volumes (the underlying objects of PVCs) \"subscribe\" to Groups. When a volume has a label matching a group name defined in a <code>RecurringJob</code>, Longhorn automatically includes that volume in the schedule. Because Longhorn volumes are cluster-scoped, this works regardless of which namespace the PVC resides in.</p> <p>Add the following label to the manifest of each PVC to be backed up. Longhorn will automatically propagate this label to the underlying volume.</p> <p><code>code-server.yaml</code></p> <pre><code>metadata:\n  name: code-server-pvc-lh\n  namespace: code-server\n  labels:\n    # This enables all jobs in the 'default' group for this volume\n    recurring-job-group.longhorn.io/default: \"enabled\"\n</code></pre> <p>Backups will be found under Backup and Restore &gt; Backups after the first 2:00 AM run, identified by their source Volume Name and Timestamp, regardless of their original Kubernetes namespace.</p>"},{"location":"blog/2026/01/25/migrating-kubernetes-volumes-to-longhorn/#adding-future-nodes","title":"Adding future nodes","text":"<p>To add nodes in the future (TBC):</p> <ol> <li>Create an <code>XFS</code> (or <code>ext4</code>) file system and mount it on <code>/home</code>.</li> <li>Create the <code>/home/longhorn</code> directory.</li> <li>Join the node to the cluster.</li> <li>Label the node with <code>node.longhorn.io/create-default-disk=true</code>.<ul> <li>Longhorn will automatically detect <code>/home/longhorn</code> on the new node.</li> </ul> </li> <li>Update the <code>longhorn-nvme</code> <code>StorageClass</code>. or individual volumes in the relevant     deployments, to <code>numberOfReplicas: 2</code> so that Longhorn syncs data across nodes.</li> </ol>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/","title":"Upgrading a single-node Kubernetes cluster to Zero Downtime Maintenance","text":"<p>Ever since the new, more powerful <code>octavo</code> replaced the good old <code>lexicon</code> server as the single-node Kubernetes cluster to serve all the local self-hosting needs, the latter has not found any use. Since it would be a waste to let it sit in a box unused, it will be setup to join the (for now) single-node cluster that is <code>octavo</code> to enable Zero Downtime Maintenance.</p>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#the-pln","title":"The PLN","text":"<p>The first step will be to have <code>lexicon</code> join the cluster as a worker node, while keeping <code>octavo</code> as the control plane. Chances are another Intel NUC will join the cluster later with an intermediate CPU (Intel Core i5) which will bring the total size of the cluster to the ideal 3: a control plane and two wokers for an Active-Passive High Availability setup.</p> <p>Nodes with more capable CPU and GPU can be setup to run the more CPU/GPU instensive workloads, such as Jellyfin (heavy media transcoding), by using Node Labels and Taints to ensure high-demand tasks stay on the more capable nodes.</p> <p>All nodes have direct access to a NAS where all files are available, both NUCs have NVMe SSDs for the operating system and Kubernetes local persistent volumns, and <code>octavo</code> has an additional 4TB SATA SSD, a capability unlikely to be added to other nodes in the near future. Keeping Jellyfin running in <code>octavo</code> should let it stay using the media files out of the 4TB SATA SSD rather than using the NAS, while most of audio-only media can be replicated on the NVMe SSDs.</p> <p>To handle the diverse storage landscape, two <code>StorageClasses</code> will be defined, one for the NVMe SSD and another one for the bigger but slower SATA SSD. For apps that benefit from faster SSD, <code>nodeAffinity</code> can force them onto <code>octavo</code>, while <code>lexicon</code> is kept  exclusively for overflow or light tasks, with a taint to <code>lexicon</code> so that only specifically tolerated pods will schedule on it.</p>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#potential-expansions","title":"Potential expansions","text":"<p>There is a distinct possibility that a second high-perf NUC (with an Intel Core i5 CPU) may be added later. In that case, <code>lexicon</code> could be dedicated (mostly) only to run the control panel, leaving the high-perf NUCs run all the workloads. Pod Anti-Affinity could then be used to ensure that if one of then high-perf nodes goes down, a redundant copy of critical apps are already running on the second high-perf NUC.</p> <p>This would then leave <code>lexicon</code> as the third Quorum/Control Plane \"witness\" node, which would be useful to avoid \"split-brain\" issues during network partitions. For a 3-node Kubernetes cluster to be truly stable (HA), it needs a Quorum (a majority vote) to make decisions. A \"witness\" node provides this third vote without needing to be powerful.</p> Alternative devices considered (and discarded). <p>There is Raspberry Pi 4 currently available but in has only 2GB or RAM, which is not enough (too tight) becuase the Kubernetes Control Plane typically takes up to 1.8 GB. Instead, an old (2016) retired ASUS Chromebox-M014U with a Core i3-4010U CPU, 4 GB of RAM and 16 GB SSD, already running GalliumOS 3.1, could be enough to run the Control Plane node, but considering the latest release of GalliumOS 3.1 was released on 2019-12-22, it doesn't seem fit for this purpose.</p> <p>If there is a critical hardware failure on <code>lexicon</code>, the cluster degrades but stays online, management continues without Fault Tolerance. Applications already running on worker nodes are unaffected by a control plane failure and will continue to run normally, but if a worker node also fails while the control plane is degraded (only 1 node left), then Kubernetes cannot \"self-heal\" by rescheduling those pods to <code>octavo</code> because the \"brain\" (API server) is inaccessible.</p> <p>In such event, replacing a failed hardware \"witness\" is a standard maintenance task:</p> <ol> <li>Remove the failed node from the <code>etcd</code> member list using <code>etcdctl member remove &lt;ID&gt;</code> from one of the healthy NUCs.</li> <li>Delete the dead node object from Kubernetes using <code>kubectl delete node &lt;chromebox-name&gt;</code></li> <li>Bring in a replacement and join it as a new control plane node using <code>kubeadm join --control-plane</code></li> </ol>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#reinstall-lexicon","title":"Reinstall <code>lexicon</code>","text":"<p><code>lexicon</code> was running Ubuntu Server 22.04 and was upgraded to 24.04 using the <code>do-release-upgrade</code> tool but, although the upgrade went well, it did not fix an old issue that caused the NIC to slowly become more and more overloaded and slow:</p> <p></p> <p>Installing Ubuntu Server 24.04 went smoothly and without any problems, the NUC booted from the USB stick and secure boot, enabled by default, never presented any problem.</p> <p>Once the intaller boots, the installation steps are:</p> <ol> <li>Choose language and keyboard layout.</li> <li>Choose Ubuntu Server (default, not (minimized)).<ul> <li>Checked the option to Search for third-party drivers.</li> </ul> </li> <li>Networking: DHCP on wired network.<ul> <li>The <code>enp89s0</code> interface is the NUC's integrated 2.5Gbps NIC (Intel I226-V).</li> </ul> </li> <li>Pick a local Ubuntu mirror to install packages from.</li> <li>Setup a Custom storage layout as follows<ol> <li>Select the disk (Samsung SSD 970 EVO PLUS 2TB) to Use As Boot Device.     This automatically creates a 1GB partition for <code>/boot/efi</code> (formatted as <code>fat32</code>).</li> <li>Create a 30G partition to mount as <code>/</code> (formatted as <code>ext4</code>).</li> <li>Create a 30G partition to reverse for a future OS.</li> <li>Create a 60G partition to mount as <code>/var/lib</code> (formatted as <code>xfs</code>).</li> <li>Create a partition with the remaining space (1.7T) to mount as <code>/home</code>     (formatted as <code>xfs</code>).</li> </ol> </li> <li>Confirm partitions &amp; changes.</li> <li>Set up a Profile: username (<code>ponder</code>), hostname (<code>lexicon</code>) and password.</li> <li>Skip Upgrade to Ubuntu Pro (to be done later).</li> <li>Install OpenSSH server and allow password authentication (for now).</li> <li>A selection of snap packages is available at this point, none were selected.</li> <li>Confirm all previous choices and start to install software.</li> <li>Once the installation is complete, remove the UBS stick and hit Enter to reboot.</li> </ol> <p>After the first reboot, the server is setup with the same steps as done for <code>octavo</code>:</p> <ul> <li> <p>Disable swap.</p> </li> <li> <p>Tweak Bash prompt.</p> </li> <li> <p>Tweak OpenSSH server to set the (only) <code>enp89s0</code> interface (Intel 2.5GB NIC)     with the <code>.6</code>addresses with this Netplan configuration:</p> /etc/netplan/50-cloud-init.yaml<pre><code># Dual static IP on LAN, nothing else.\nnetwork:\n  version: 2\n  renderer: networkd\n  ethernets:\n    enp89s0:\n      dhcp4: no\n      dhcp6: no\n      # Ser IP address &amp; subnet mask\n      addresses: [ 10.0.0.6/24, 192.168.0.6/24 ]\n      # Set default gateway\n      routes:\n      - to: default\n        via: 192.168.0.1  # UniFi router gateway\n      # Set DNS name servers\n      nameservers:\n        addresses: [ 77.109.128.2, 213.144.129.20 ]\n</code></pre> </li> <li> <p>Set correct timezone.</p> </li> <li> <p>Update system packages.</p> </li> <li> <p>Upgrade to Ubuntu Pro.</p> </li> <li> <p>Stop <code>apparmor</code> spew in the logs.</p> </li> <li> <p>Mount the NAS NFS.</p> </li> <li> <p>Install Continuous Monitoring.</p> </li> <li> <p>Setup Remote Access:     Cloudflare Tunnel     and     Tailscale.</p> </li> </ul>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#update-to-hwe-614-kernel","title":"Update to HWE 6.14 kernel","text":"<p>While the Intel 11th Gen hardware does not require the absolute latest kernel, having both nodes on the same major kernel branch (e.g. 6.14) simplifies troubleshooting and ensures CNI and networking features behave identically across the cluster.</p> <p>Once the system is upgraded to Ubuntu 24.04, switching the kernel is as easy as <code>apt install linux-generic-hwe-24.04</code> and it even becomes the new default kernel in GRUB:</p> <code># apt install linux-generic-hwe-24.04 -y</code> <pre><code># apt install linux-generic-hwe-24.04 -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  libdebuginfod-common libdebuginfod1t64 linux-headers-6.14.0-37-generic linux-headers-generic-hwe-24.04\n  linux-hwe-6.14-headers-6.14.0-37 linux-hwe-6.14-tools-6.14.0-37 linux-image-6.14.0-37-generic\n  linux-image-generic-hwe-24.04 linux-modules-6.14.0-37-generic linux-modules-extra-6.14.0-37-generic\n  linux-tools-6.14.0-37-generic\nSuggested packages:\n  linux-hwe-6.14-tools\nThe following NEW packages will be installed:\n  libdebuginfod-common libdebuginfod1t64 linux-generic-hwe-24.04 linux-headers-6.14.0-37-generic\n  linux-headers-generic-hwe-24.04 linux-hwe-6.14-headers-6.14.0-37 linux-hwe-6.14-tools-6.14.0-37\n  linux-image-6.14.0-37-generic linux-image-generic-hwe-24.04 linux-modules-6.14.0-37-generic\n  linux-modules-extra-6.14.0-37-generic linux-tools-6.14.0-37-generic\n0 upgraded, 12 newly installed, 0 to remove and 6 not upgraded.\nNeed to get 0 B/199 MB of archives.\nAfter this operation, 320 MB of additional disk space will be used.\nPreconfiguring packages ...\nSelecting previously unselected package libdebuginfod-common.\n(Reading database ... 87562 files and directories currently installed.)\nPreparing to unpack .../00-libdebuginfod-common_0.190-1.1ubuntu0.1_all.deb ...\nUnpacking libdebuginfod-common (0.190-1.1ubuntu0.1) ...\nSelecting previously unselected package libdebuginfod1t64:amd64.\nPreparing to unpack .../01-libdebuginfod1t64_0.190-1.1ubuntu0.1_amd64.deb ...\nUnpacking libdebuginfod1t64:amd64 (0.190-1.1ubuntu0.1) ...\nSelecting previously unselected package linux-modules-6.14.0-37-generic.\nPreparing to unpack .../02-linux-modules-6.14.0-37-generic_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-modules-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-image-6.14.0-37-generic.\nPreparing to unpack .../03-linux-image-6.14.0-37-generic_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-image-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-modules-extra-6.14.0-37-generic.\nPreparing to unpack .../04-linux-modules-extra-6.14.0-37-generic_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-modules-extra-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-image-generic-hwe-24.04.\nPreparing to unpack .../05-linux-image-generic-hwe-24.04_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-image-generic-hwe-24.04 (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-hwe-6.14-headers-6.14.0-37.\nPreparing to unpack .../06-linux-hwe-6.14-headers-6.14.0-37_6.14.0-37.37~24.04.1_all.deb ...\nUnpacking linux-hwe-6.14-headers-6.14.0-37 (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-headers-6.14.0-37-generic.\nPreparing to unpack .../07-linux-headers-6.14.0-37-generic_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-headers-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-headers-generic-hwe-24.04.\nPreparing to unpack .../08-linux-headers-generic-hwe-24.04_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-headers-generic-hwe-24.04 (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-generic-hwe-24.04.\nPreparing to unpack .../09-linux-generic-hwe-24.04_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-generic-hwe-24.04 (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-hwe-6.14-tools-6.14.0-37.\nPreparing to unpack .../10-linux-hwe-6.14-tools-6.14.0-37_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-hwe-6.14-tools-6.14.0-37 (6.14.0-37.37~24.04.1) ...\nSelecting previously unselected package linux-tools-6.14.0-37-generic.\nPreparing to unpack .../11-linux-tools-6.14.0-37-generic_6.14.0-37.37~24.04.1_amd64.deb ...\nUnpacking linux-tools-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSetting up libdebuginfod-common (0.190-1.1ubuntu0.1) ...\nSetting up linux-hwe-6.14-headers-6.14.0-37 (6.14.0-37.37~24.04.1) ...\nSetting up linux-modules-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSetting up linux-headers-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSetting up libdebuginfod1t64:amd64 (0.190-1.1ubuntu0.1) ...\nSetting up linux-image-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nI: /boot/vmlinuz is now a symlink to vmlinuz-6.14.0-37-generic\nI: /boot/initrd.img is now a symlink to initrd.img-6.14.0-37-generic\nSetting up linux-modules-extra-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nSetting up linux-headers-generic-hwe-24.04 (6.14.0-37.37~24.04.1) ...\nSetting up linux-hwe-6.14-tools-6.14.0-37 (6.14.0-37.37~24.04.1) ...\nSetting up linux-image-generic-hwe-24.04 (6.14.0-37.37~24.04.1) ...\nSetting up linux-generic-hwe-24.04 (6.14.0-37.37~24.04.1) ...\nSetting up linux-tools-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.6) ...\nProcessing triggers for linux-image-6.14.0-37-generic (6.14.0-37.37~24.04.1) ...\n/etc/kernel/postinst.d/initramfs-tools:\nupdate-initramfs: Generating /boot/initrd.img-6.14.0-37-generic\n/etc/kernel/postinst.d/zz-update-grub:\nSourcing file `/etc/default/grub'\nGenerating grub configuration file ...\nFound linux image: /boot/vmlinuz-6.14.0-37-generic\nFound initrd image: /boot/initrd.img-6.14.0-37-generic\nFound linux image: /boot/vmlinuz-6.8.0-94-generic\nFound initrd image: /boot/initrd.img-6.8.0-94-generic\nWarning: os-prober will not be executed to detect other bootable partitions.\nSystems on them will not be added to the GRUB boot configuration.\nCheck GRUB_DISABLE_OS_PROBER documentation entry.\nAdding boot menu entry for UEFI Firmware Settings ...\ndone\nScanning processes...                                                                                        \nScanning candidates...                                                                                       \nScanning processor microcode...                                                                              \nScanning linux images...                                                                                     \n\nPending kernel upgrade!\nRunning kernel version:\n  6.8.0-94-generic\nDiagnostics:\n  The currently running kernel version is not the expected kernel version 6.14.0-37-generic.\n\nRestarting the system to load the new kernel will not be handled automatically, so you should consider\nrebooting.\n\nThe processor microcode seems to be up-to-date.\n\nRestarting services...\n\nService restarts being deferred:\n/etc/needrestart/restart.d/dbus.service\nsystemctl restart getty@tty1.service\nsystemctl restart systemd-logind.service\nsystemctl restart unattended-upgrades.service\nsystemctl restart wpa_supplicant.service\n\nNo containers need to be restarted.\n\nUser sessions running outdated binaries:\nroot @ session #1: sshd[913]\nroot @ session #4: sshd[1153]\nroot @ user manager service: systemd[918]\n\nNo VM guests are running outdated hypervisor (qemu) binaries on this host.\n</code></pre> <p>Check what the new default kernel is to make sure it is the newer one:</p> <pre><code># awk -F\"'\" '/menuentry / &amp;&amp; /with Linux/ {print i++ \" : \" $2}' \\\n  /boot/grub/grub.cfg\n0 : Ubuntu, with Linux 6.14.0-37-generic\n1 : Ubuntu, with Linux 6.14.0-37-generic (recovery mode)\n2 : Ubuntu, with Linux 6.8.0-90-generic\n3 : Ubuntu, with Linux 6.8.0-90-generic (recovery mode)\n</code></pre> <p>Reboot the server to load the new kernel now.</p>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes on Raspberry Pi 5 (<code>alfred</code>) showed quite a few new hurdles caused by newer versions of Kubernetes (v1.32.2) and a few components, but now those have been deprecated, so that following the installation process from <code>octavo</code> is good enough of a guide.</p> <p>Storage Requirements are satisfaied in the same way by having similar partitions setup.</p> <p>Install Helm (via <code>apt</code>) and then install Kubernetes (also via <code>apt</code>); install Kubernetes version v1.34.3 which is the one running at the moment in the cluster:</p> <pre><code># kubectl version --output=yaml\nclientVersion:\n  buildDate: \"2025-12-09T15:06:39Z\"\n  compiler: gc\n  gitCommit: df11db1c0f08fab3c0baee1e5ce6efbf816af7f1\n  gitTreeState: clean\n  gitVersion: v1.34.3\n  goVersion: go1.24.11\n  major: \"1\"\n  minor: \"34\"\n  platform: linux/amd64\nkustomizeVersion: v5.7.1\n\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n</code></pre> <p>Finally, install container runtime and Kubernetes is ready to be initialized.</p>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#bootstrap-with-kubeadm","title":"Bootstrap with <code>kubeadm</code>","text":"<p>Adding Linux worker nodes is the next big step towards upgrading the single-node cluster to a multi-node cluster, so here is where the setup of this node (<code>lexicon</code>) diverges from that of the first node (<code>octavo</code>).</p>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#join-as-worker-node","title":"Join as worker node","text":"<p>Having initialized the cluster originally in <code>octavo</code> with <code>kubeadm init</code>, the first step is to obtain a fresh token to join the customer (this token expires in 24 hours):</p> <pre><code>$ kubeadm token create --print-join-command\nkubeadm join 10.0.0.8:6443 \\\n  --token ivpct4.7piqcgw68ng77kn5 \\\n  --discovery-token-ca-cert-hash \\\n  sha256:18d968e92516e1a2808166d90a7d7c8b6f7b37cbac6328c49793863f9ae2b982 \n</code></pre> <p>Then run that command on the new node:</p> <pre><code># kubeadm join 10.0.0.8:6443 \\\n  --token a0vi2t.rhxs1cc2rkicpeu0 \\\n  --discovery-token-ca-cert-hash \\\n  sha256:18d968e92516e1a2808166d90a7d7c8b6f7b37cbac6328c49793863f9ae2b982\n[preflight] Running pre-flight checks\n[preflight] Reading configuration from the \"kubeadm-config\" ConfigMap in namespace \"kube-system\"...\n[preflight] Use 'kubeadm init phase upload-config kubeadm --config your-config-file' to re-upload it.\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/instance-config.yaml\"\n[patches] Applied patch of type \"application/strategic-merge-patch+json\" to target \"kubeletconfiguration\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Starting the kubelet\n[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s\n[kubelet-check] The kubelet is healthy after 504.313399ms\n[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap\n\nThis node has joined the cluster:\n* Certificate signing request was sent to apiserver and a response was received.\n* The Kubelet was informed of the new secure connection details.\n\nRun 'kubectl get nodes' on the control-plane to see this node join the cluster.\n</code></pre> <p>After a few seconds the new node is ready:</p> <pre><code>$ kubectl get nodes\nNAME      STATUS   ROLES           AGE    VERSION\nlexicon   Ready    &lt;none&gt;          42s    v1.34.3\noctavo    Ready    control-plane   281d   v1.34.3\n</code></pre>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#temporary-cordon","title":"Temporary <code>cordon</code>","text":"<p>Even if the new node is ready to run pods, it is not yet ready to satisfy all their requirements yet; even though all the <code>hostPath</code> volumes have been migrated to Longhorn, they are not yet replicated to the new node.</p> <p>To avoid having pods scheduled in the new node before everything is ready, temporarily cordon it:</p> <pre><code>$ kubectl cordon lexicon\nnode/lexicon cordoned\n\n$ kubectl get nodes -o wide\nNAME      STATUS                     ROLES           AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nlexicon   Ready,SchedulingDisabled   &lt;none&gt;          19h    v1.34.3   192.168.0.6   &lt;none&gt;        Ubuntu 24.04.3 LTS   6.14.0-37-generic   containerd://2.2.1\noctavo    Ready                      control-plane   282d   v1.34.3   192.168.0.8   &lt;none&gt;        Ubuntu 24.04.3 LTS   6.14.0-37-generic   containerd://2.2.1\n</code></pre>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#setup-kubectl-access","title":"Setup <code>kubectl</code> access","text":"<p>To run <code>kubectl</code> as a non-root user, copy the Kubernetes config file under the <code>~/.kube</code> directory from the currento node to the new one:</p> <pre><code>$ scp -r .kube/ lexicon:\n</code></pre> <p>And with that all the <code>kubectl</code> commands work:</p> <pre><code>$ kubectl get nodes\nNAME      STATUS   ROLES           AGE     VERSION\nlexicon   Ready    &lt;none&gt;          4m53s   v1.34.3\noctavo    Ready    control-plane   281d    v1.34.3\n\n$ kubectl cluster-info\nKubernetes control plane is running at https://10.0.0.8:6443\nCoreDNS is running at https://10.0.0.8:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#default-pod-distribution","title":"Default pod distribution","text":"<p>After joining the cluster as a worker node, infrastructure components will behave differently depending on how they were deployed (<code>DaemonSets</code> vs. <code>Deployments</code>).</p> <p>Resources deployed as a <code>DaemonSet</code> will immediately spawn a pod on the new node, so these are the components that will run on <code>lexicon</code> automatically:</p> <ul> <li>Flannel: <code>kube-flannel</code> pod will start to establish the pod network on <code>lexicon</code>.</li> <li>MetalLB: The speaker pods will start and <code>lexicon</code> will be able to respond to ARP     requests for <code>LoadBalancer</code> IPs (once it settles).</li> <li>Longhorn: the <code>longhorn-manager</code> and <code>csi-plugin</code> pods will start and Longhorn     will detect the new 2TB SSD if the Node Labeling Job running or after the node is     manually labeled.</li> <li>Intel Device Plugin: this is an operator-managed <code>DaemonSet</code>, so it will     detect the GPU and make it available for hardware transcoding (e.g., in Jellyfin).</li> <li>Prometheus: a <code>DaemonSet</code> running as part of the     Trivy Operator Dashboard in Grafana.</li> </ul> <code>kubectl get daemonsets.apps -A</code> <pre><code>$ kubectl get daemonsets.apps -A\nNAMESPACE                  NAME                               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                               AGE\nintel-device-plugins-gpu   intel-gpu-plugin-gpudeviceplugin   1         1         1       1            1           intel.feature.node.kubernetes.io/gpu=true   277d\nkube-flannel               kube-flannel-ds                    1         1         1       1            1           &lt;none&gt;                                      281d\nkube-system                csi-nfs-node                       1         1         1       1            1           kubernetes.io/os=linux                      8d\nkube-system                kube-proxy                         1         1         1       1            1           kubernetes.io/os=linux                      281d\nlonghorn-system            engine-image-ei-ff1cedad           1         1         1       1            1           &lt;none&gt;                                      2d20h\nlonghorn-system            longhorn-csi-plugin                1         1         1       1            1           &lt;none&gt;                                      2d20h\nlonghorn-system            longhorn-manager                   1         1         1       1            1           &lt;none&gt;                                      7d5h\nmetallb-system             metallb-speaker                    1         1         1       1            1           kubernetes.io/os=linux                      44h\nmonitoring                 prom-prometheus-node-exporter      1         1         1       1            1           kubernetes.io/os=linux                      42d\nnode-feature-discovery     node-feature-discovery-worker      1         1         1       1            1           &lt;none&gt;                                      277d\n</code></pre> <p>Resources deployed as <code>Deployments</code> (single-replica so far) or <code>StatefulSets</code> will stay running only on on <code>octavo</code>. Most of these will need to be scaled up to 2+ replicas before a pod is started on <code>lexicon</code>, with a few notable exceptions:</p> <ul> <li>CoreDNS: Has 2 replicas; Kubernetes will likely move one to <code>lexicon</code> to balance     the load.</li> <li>Longhorn UI/Controller: These central \"brains\" stay on <code>octavo</code>.</li> </ul> <code>kubectl get deployments.apps -A; kubectl get statefulsets.apps -A</code> <pre><code>$ kubectl get deployments.apps -A \nNAMESPACE                  NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE\naudiobookshelf             audiobookshelf                          1/1     1            1           279d\ncert-manager               cert-manager                            1/1     1            1           281d\ncert-manager               cert-manager-cainjector                 1/1     1            1           281d\ncert-manager               cert-manager-webhook                    1/1     1            1           281d\ncert-manager               cert-manager-webhook-porkbun            1/1     1            1           15d\ncode-server                code-server                             1/1     1            1           276d\ndefault                    ddns-updater                            1/1     1            1           129d\nfirefly-iii                firefly-iii                             1/1     1            1           276d\nfirefly-iii                firefly-iii-mysql                       1/1     1            1           276d\nhome-assistant             home-assistant                          1/1     1            1           280d\nhomepage                   homepage                                1/1     1            1           31d\nintel-device-plugins-gpu   inteldeviceplugins-controller-manager   1/1     1            1           277d\nkomga                      komga                                   1/1     1            1           279d\nkube-system                coredns                                 2/2     2            2           281d\nkube-system                csi-nfs-controller                      1/1     1            1           8d\nkube-system                headlamp                                1/1     1            1           22h\nkube-system                snapshot-controller                     1/1     1            1           8d\nkubernetes-dashboard       kubernetes-dashboard-api                1/1     1            1           281d\nkubernetes-dashboard       kubernetes-dashboard-auth               1/1     1            1           281d\nkubernetes-dashboard       kubernetes-dashboard-kong               1/1     1            1           281d\nkubernetes-dashboard       kubernetes-dashboard-metrics-scraper    1/1     1            1           281d\nkubernetes-dashboard       kubernetes-dashboard-web                1/1     1            1           281d\nlonghorn-system            csi-attacher                            3/3     3            3           2d22h\nlonghorn-system            csi-provisioner                         3/3     3            3           2d22h\nlonghorn-system            csi-resizer                             3/3     3            3           2d22h\nlonghorn-system            csi-snapshotter                         3/3     3            3           2d22h\nlonghorn-system            longhorn-driver-deployer                1/1     1            1           7d8h\nlonghorn-system            longhorn-ui                             2/2     2            2           7d8h\nmedia-center               jellyfin                                1/1     1            1           278d\nmetallb-system             metallb-controller                      1/1     1            1           46h\nmonitoring                 grafana                                 1/1     1            1           280d\nmonitoring                 influxdb                                1/1     1            1           280d\nmonitoring                 prom-kube-prometheus-stack-operator     1/1     1            1           42d\nmonitoring                 prom-kube-state-metrics                 1/1     1            1           42d\nmonitoring                 trivy-operator-dashboard                1/1     1            1           42d\nmonitoring                 version-checker                         1/1     1            1           42d\nnavidrome                  navidrome                               1/1     1            1           279d\nnode-feature-discovery     node-feature-discovery-gc               1/1     1            1           278d\nnode-feature-discovery     node-feature-discovery-master           1/1     1            1           278d\npomerium                   pomerium                                1/1     1            1           45d\nryot                       postgres                                1/1     1            1           227d\nryot                       ryot                                    1/1     1            1           227d\ntailscale                  operator                                1/1     1            1           281d\ntrivy-system               trivy-operator                          1/1     1            1           42d\nunifi                      mongo                                   1/1     1            1           276d\nunifi                      unifi                                   1/1     1            1           276d\n\n$ kubectl get statefulsets.apps -A \nNAMESPACE        NAME                                                   READY   AGE\nmonitoring       alertmanager-prom-kube-prometheus-stack-alertmanager   1/1     42d\nmonitoring       prometheus-prom-kube-prometheus-stack-prometheus       1/1     42d\nsteam-headless   steam-headless                                         0/0     188d\ntailscale        ts-home-assistant-tailscale-mdqlt                      1/1     280d\ntailscale        ts-kubernetes-dashboard-ingress-tailscale-jhb6z        1/1     281d\n</code></pre>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#longhorn","title":"Longhorn","text":"<p>Longhorn needs to know the new node is ready for data; to activate Longhorn storage on the new node it need to be labeled:</p> <pre><code># kubectl label node lexicon node.longhorn.io/create-default-disk=true\nnode/lexicon labeled\n</code></pre> <p>Once the node is labeled, the Longhorn UI will immediately show that the node now has one disk (<code>/home/longhorn</code>). The new disk will not automatically get any Disk Tag so the <code>nvme</code> must be manually added so that the disk fits the <code>longhorn-nmve</code> storage class.</p> <p>Once the node is ready and the 2TB SSD disk is ready and tagged, existing volumes can be rescaled up by increasint the value of <code>numberOfReplicas</code> in the relevant volumes or even on the <code>longhorn-nmve</code> storage class:</p> <pre><code>$ kubectl patch storageclass longhorn-nvme --type merge \\\n  -p '{\"parameters\":{\"numberOfReplicas\":\"2\"}}'\n</code></pre> <p>This can also be done by updating y reapplying the <code>longhorn-storage.yaml</code> manifest created when  migratng <code>hostPaht</code> volumes to Longhorn:</p> <p><code>longhorn-storage.yaml</code></p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: longhorn-nvme\nprovisioner: driver.longhorn.io\nallowVolumeExpansion: true\nparameters:\n  numberOfReplicas: \"2\"\n  diskSelector: \"nvme\"\n  dataLocality: \"best-effort\"\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: longhorn-sata\nprovisioner: driver.longhorn.io\nallowVolumeExpansion: true\nparameters:\n  numberOfReplicas: \"1\"\n  diskSelector: \"sata\"\n  dataLocality: \"best-effort\"\n</code></pre> <p>However, one does not simply modify a <code>StorageClass</code>:</p> <pre><code>$ kubectl apply -f longhorn-storage.yaml\nThe StorageClass \"longhorn-nvme\" is invalid: parameters: Forbidden: updates to parameters are forbidden.\n</code></pre> <p>Instead, the <code>StorageClass</code> must be deleted, then recreated anew; this does not affect existing volumes, only those that are created later:</p> <pre><code>$ kubectl delete storageclass longhorn-nvme\nstorageclass.storage.k8s.io \"longhorn-nvme\" deleted\n\n$ kubectl apply -f longhorn-storage.yaml \nstorageclass.storage.k8s.io/longhorn-nvme created\nstorageclass.storage.k8s.io/longhorn-sata unchanged\n</code></pre> <p>Since this does not affect existing volumes, Longhorn will not immediately start replicating the volumes from <code>octavo</code> to <code>lexicon</code>; this needs to be done manually for each existing volume by <code>patch</code>ing it; with the caveat that volumes on the SATA SSD are not to be replicated:</p> <pre><code>$ kubectl get volumes.longhorn.io -n longhorn-system -o json \\\n| jq -r '.items[] | select(.spec.numberOfReplicas==1 and (.spec.diskSelector | contains([\"nvme\"]))) | .metadata.name' \\\n| xargs -I {} kubectl -n longhorn-system patch volumes.longhorn.io {} \\\n  --type merge -p '{\"spec\":{\"numberOfReplicas\":2}}'\nvolume.longhorn.io/pvc-0c892178-0451-4043-be76-9e2e33464631 patched\nvolume.longhorn.io/pvc-1083fedd-27e9-4a58-8a8f-6b8553d62034 patched\nvolume.longhorn.io/pvc-17d73e68-4c5a-4f34-b5dc-89936202d8d7 patched\nvolume.longhorn.io/pvc-1d7dc891-4694-4744-be75-6ab12c11aea9 patched\nvolume.longhorn.io/pvc-2610bf0b-0c90-4ecb-956c-355d8619dbe4 patched\nvolume.longhorn.io/pvc-2b72de1d-93d8-492e-aed1-1708a35ce5b4 patched\nvolume.longhorn.io/pvc-2e7cd0bd-6efe-4e8b-ae81-a025b447a7f9 patched\nvolume.longhorn.io/pvc-43916c81-6e07-4eae-92e4-e37b816c407c patched\nvolume.longhorn.io/pvc-495872f3-cf95-4268-b3ea-2f4d51d33399 patched\nvolume.longhorn.io/pvc-5c724ff5-e7dd-483b-9450-0b8e299c49ca patched\nvolume.longhorn.io/pvc-5dd1e736-b80b-43a8-9570-95a3637cff4d patched\nvolume.longhorn.io/pvc-6250f010-4a0b-4ac5-83a6-07cf71d95b33 patched\nvolume.longhorn.io/pvc-726c1b2d-c0c0-4232-85f5-b3119558d0d1 patched\nvolume.longhorn.io/pvc-72741e77-c05c-4b84-b74d-bddcf32a2236 patched\nvolume.longhorn.io/pvc-7ae11ade-d6c8-4296-ac67-58953e3dddc2 patched\nvolume.longhorn.io/pvc-aea4a7e9-baf0-4fbc-ba5f-7007a66fcef6 patched\nvolume.longhorn.io/pvc-e3f04fc3-9126-43d6-82c0-c427a730b338 patched\nvolume.longhorn.io/pvc-fd8bb4e8-39a4-4b7f-a3c5-668333e4d64a patched\n</code></pre> <p>Doing this will result in all those volumes going from Healthy to Degraded. Because <code>lexicon</code> was previously cordoned to avoid pods running on it before the volumes were ready, this also means the Kubernetes scheduler (and by extension the Longhorn scheduler) is forbidden from starting the Replica Instance Manager pods; so now is the time to uncordon <code>lexicon</code>.</p> <pre><code>$ kubectl uncordon lexicon \nnode/lexicon already uncordoned\n</code></pre> <p>This launches Longhorn into an I/O frenzy to replicate all those volumes and keeps the 2.5 Gbps NICs at their maximum throughput for 8 minutes to replicate about 250 GB first, then stays at nearly the maximum for over an hour to finish replicating the last volume, much larger than the rest, until eventually all volumes are Healthy again.</p>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#intel-gpu","title":"Intel GPU","text":"<p>Support for the Intel GPU works automatically when joining the node:</p> <pre><code>$ kubectl describe node lexicon\nName:               lexicon\nRoles:              &lt;none&gt;\nLabels:             beta.kubernetes.io/arch=amd64\n                    ...\n                    gpu.intel.com/device-id.0300-9a78.count=1\n                    gpu.intel.com/device-id.0300-9a78.present=true\n                    intel.feature.node.kubernetes.io/gpu=true\n...\nCapacity:\n  cpu:                            4\n  ephemeral-storage:              61376Mi\n  gpu.intel.com/i915:             1\n  gpu.intel.com/i915_monitoring:  1\n  hugepages-1Gi:                  0\n  hugepages-2Mi:                  0\n  memory:                         32484920Ki\n  pods:                           110\nAllocatable:\n  cpu:                            4\n  ephemeral-storage:              57921660423\n  gpu.intel.com/i915:             1\n  gpu.intel.com/i915_monitoring:  1\n  hugepages-1Gi:                  0\n  hugepages-2Mi:                  0\n  memory:                         32382520Ki\n  pods:                           110\n...\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            655m (16%)  100m (2%)\n  memory                         219Mi (0%)  1102Mi (3%)\n  ephemeral-storage              0 (0%)      0 (0%)\n  hugepages-1Gi                  0 (0%)      0 (0%)\n  hugepages-2Mi                  0 (0%)      0 (0%)\n  gpu.intel.com/i915             0           0\n  gpu.intel.com/i915_monitoring  0           0\n</code></pre>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#logs-reader-helper","title":"Logs reader helper","text":"<p>Troubleshooting pods and services often involves reading or watching the logs, which involves combining two <code>kubectl</code> commands to find the relevant pod/service and requesting the logs. To make this easier, save the following script as <code>~/bin/klogs</code> (and add <code>~/bin/</code> to the <code>$PATH</code>):</p> bin/klogs<pre><code>#!/bin/bash\n#\n# Watch logs from Kubernetes pod/service.\n#\n# Usage: klogs &lt;namespace&gt; &lt;pod/service&gt;\n\nns=$1\npd=$2\nif [[ \"$pd\" == \"\" ]]; then pd=\"$ns\"; fi\nkubectl logs -n $ns \\\n  $(kubectl get pods -n $ns | grep $pd | cut -f1 -d' ') -f\n</code></pre>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#headlamp","title":"Headlamp","text":"<p>Kubernetes Dashboard was deprecated and archived in January 2026, and is no longer maintained due to lack of active maintainers and contributors. Headlamp is the suggested replacement and was already deployed in <code>octavo</code> using its Helm chart.</p>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#metrics-server","title":"Metrics Server","text":"<p>Skooner was briefly installed and it relied heavily on the Kubernetes Metrics Server to display real-time cluster metrics. Although Skooner was later removed (it has been abandoned for 5 years), <code>metrics-server</code> turned out necessary also for many of the Grafana dashboards installed previously to monitor the cluster for vulnerabilities.</p> <p>When installed using its Helm chart, it uses by default a self-signed certificate which is generated during startup and the <code>APIservice</code> resource is registered with <code>.spec.insecureSkipTLSVerify</code> set to <code>true</code>. Although ideally <code>metrics-server</code> can be kept more secure by using the <code>cert-manager</code> that is available in the cluster, in a homelab environment this is typically avoided.</p> <p><code>metrics-server-values.yaml</code></p> <pre><code>apiService:\n  insecureSkipTLSVerify: false\ndefaultArgs:\n  - --kubelet-insecure-tls\n  - --kubelet-preferred-address-types=InternalIP\nreplicas: 2\n</code></pre> Skipping TLS verification is the standard setup for homelabs, not production! <p>In a homelab environment without its own DNS server, nodes hostnames are not resolvable via DNS, making the <code>InternalIP</code> the only way Metrics Server can reach the nodes. However, <code>kubeadm</code> generates self-signed certificates for Kubelets that do not include IP SANs by default, which causes the TLS verification failure when trying to use certificates; metric-server pods would fail with:</p> <pre><code>$ kubectl -n kube-system logs metrics-server-fd5dc6448-f2fkm\n...\nE0204 22:02:20.935565       1 scraper.go:149] \"Failed to scrape node\" err=\"Get \\\"https://192.168.0.6:10250/metrics/resource\\\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.6 because it doesn't contain any IP SANs\" node=\"lexicon\"\nE0204 22:02:20.943097       1 scraper.go:149] \"Failed to scrape node\" err=\"Get \\\"https://192.168.0.8:10250/metrics/resource\\\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.8 because it doesn't contain any IP SANs\" node=\"octavo\"\n</code></pre> <p>The most common solution in environments without internal DNS is to tell Metrics Server to skip TLS verification when connecting to the Kubelet; this is what the <code>defaultArgs</code> flags above do: disable CA verification for the node certificate and force Metrics Server to use the IP directly instead of DNS.</p> <p>To enable secure between the Metrics Server and the kubelets, these need to have Server Certificate Bootstrapping enabled. This allows Kubelets to request certificates signed by the cluster CA that include the correct IP SANs:</p> <ol> <li> <p>Enable Server TLS Bootstrapping: update <code>/var/lib/kubelet/config.yaml</code> on     each node to include <code>serverTLSBootstrap: true</code> and then restart the service     with <code>systemctl restart kubelet</code>.</p> </li> <li> <p>Approve the CSRs: Kubelet will generate a Certificate Signing Request on     each node and these must be approved manually:</p> <pre><code>$ kubectl get csr\n$ kubectl certificate approve &lt;csr-name&gt;\n</code></pre> </li> <li> <p>Update Metrics Server: Once nodes have valid certificates, point Metrics     Server to the cluster's CA:</p> <pre><code>apiService:\n  insecureSkipTLSVerify: false\nreplicas: 2\nargs:\n  - --kubelet-certificate-authority=/etc/kubernetes/pki/ca.crt\n  - --kubelet-preferred-address-types=InternalIP\n</code></pre> </li> </ol> <p>Install the Helm chart and then install <code>metrics-server</code> with the above values:</p> <pre><code>$ helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/\n\n$ helm repo update\nHang tight while we grab the latest from your chart repositories...\n...\n...Successfully got an update from the \"metrics-server\" chart repository\n...\nUpdate Complete. \u2388Happy Helming!\u2388\n\n$ helm upgrade --install \\\n  metrics-server metrics-server/metrics-server \\\n  --namespace=kube-system \\\n  --values=metrics-server-values.yaml\nRelease \"metrics-server\" does not exist. Installing it now.\nNAME: metrics-server\nLAST DEPLOYED: Wed Feb  4 23:00:32 2026\nNAMESPACE: kube-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\n***********************************************************************\n* Metrics Server                                                      *\n***********************************************************************\n  Chart version: 3.13.0\n  App version:   0.8.0\n  Image tag:     registry.k8s.io/metrics-server/metrics-server:v0.8.0\n***********************************************************************\n</code></pre>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#node-affinity","title":"Node Affinity","text":"<p>To ensure specific workloads remain on the primary node unless it goes down, use Node Affinity with a preferred rule based on node labels. First, label the nodes according to the relevant properties for the node affinity preferences; in this case start with a \"performance level\":</p> <pre><code>$ kubectl label node lexicon node-type=low-perf\nnode/lexicon labeled\n$ kubectl label node octavo node-type=high-perf\nnode/octavo labeled\n</code></pre> <p>To apply a preferred affinity, use <code>preferredDuringSchedulingIgnoredDuringExecution</code> in each <code>Deployment</code> manifest to set the preference for the node with the required label value (e.g. <code>node-type=high-perf</code>). The scheduler will then always place the pod on the high-perf node if it is available. If the high-perf node goes down, the scheduler will allow the pod to be scheduled on other nodes because the rule is \"preferred\" rather than \"required\". </p> <p>Adding the <code>affinity</code> block to a Deployment's <code>spec.template.spec</code> can be done as a Hard Rule (Required) or as a Soft Rule (Preferred).</p> <p>Jellyfin must run on <code>octavo</code>, not only to use the Core i7 CPU but also to use the 4TB SATA SSD. If <code>octavo</code> is down, the pod will stay in a <code>Pending</code> state and will not start on <code>lexicon</code> or other nodes without their a local replica of the relevant volumes:</p> <p><code>jellyfin.yaml</code></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jellyfin\n  namespace: media-center\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jellyfin\n  template:\n    metadata:\n      labels:\n        app: jellyfin\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-type\n                operator: In\n                values:\n                - high-perf\n      containers:\n      - image: jellyfin/jellyfin\n        ...\n</code></pre> <p>Audiobookshelf should run preferably on <code>octavo</code> for better performance, but in <code>octavo</code> goes down then it should run on <code>lexicon</code> since it has its own replicas of all the relevant volumes:</p> <p><code>audiobookshelf.yaml</code></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: audiobookshelf\n  name: audiobookshelf\n  namespace: audiobookshelf\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  selector:\n    matchLabels:\n      app: audiobookshelf\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: audiobookshelf\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            preference:\n              matchExpressions:\n              - key: node-type\n                operator: In\n                values:\n                - high-perf\n      containers:\n        - image: ghcr.io/advplyr/audiobookshelf:latest\n          ...\n</code></pre>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#zero-downtime-reboots","title":"Zero-Downtime Reboots","text":"<p>Even though the cluster now has two nodes, most deployments are running only one pod (replica) that may be in either node (unless biased by Node Affinity).</p> <p>Rebooting the control plane node (<code>octavo</code>) will still cause a total outage on <code>lexicon</code> because three fundamental architectural dependencies are not yet correctly setup for High Availability:</p> <ol> <li>Flannel: when <code>octavo</code> goes down, the Flannel pods on <code>lexicon</code> can lose the     ability to communicate with the API server. If <code>lexicon</code>'s networking logic \"hangs\"     because it cannot reach the control plane to verify routing tables, all cross-pod     and ingress traffic stops.</li> <li>MetalLB Controller: MetalLB has two parts: the Speaker (<code>DaemonSet</code>, runs on     both nodes) and the Controller (<code>Deployment</code>, runs on only 1 replica by default).     If the <code>metallb-controller</code> is running on <code>octavo</code> when it reboots, there is no     \"brain\" to assign or refresh IPs. Even though the speaker on <code>lexicon</code> is alive, it     may stop announcing the IP if it loses its lease or a network \"hiccup\" occurs during     the control plane's absence.</li> <li>Pomerium (<code>Ingress</code>): all apps are exposed through Pomerium Ingress. If the     Pomerium pods were not explicitly scaled to 2 replicas and spread across both     nodes, they will likely remain on <code>octavo</code>. Even if some services' pods are running     fine on lexicon, the \"Front Door\" (Pomerium) is shut down when it's (only) in the     rebooting node. However, Pomerium cannot be scaled beyond a single replica when     using file-based persistant storage.</li> </ol> <p>To prevent downtine when a node is rebooted, these components must be made High Availability across both NUCs:</p> <ol> <li> <p>Scale MetalLB: the <code>metallb-controller</code> controller must be scaled manually     because the Helm chart does not support <code>spec.replicaCount</code> (or similar) at all:</p> <pre><code>$ kubectl scale deployment metallb-controller -n metallb-system --replicas=2\ndeployment.apps/metallb-controller scaled\n</code></pre> <p>After a few seconds there should be one replica <code>READY</code> on each node:</p> <pre><code>$ kubectl get pods -n metallb-system -l app.kubernetes.io/component=controller -o wide\nNAME                                  READY   STATUS    RESTARTS   AGE     IP             NODE      NOMINATED NODE   READINESS GATES\nmetallb-controller-764cb589cc-fgk56   1/1     Running   0          5m18s   10.244.1.237   lexicon   &lt;none&gt;           &lt;none&gt;\nmetallb-controller-764cb589cc-hzgbq   1/1     Running   0          29s     10.244.0.31    octavo    &lt;none&gt;           &lt;none&gt;\n</code></pre> </li> <li> <p>Actually Don't Scale Pomerium: even after migrating Pomerium to a Longhorn     <code>ReadWriteMany</code> volume, so that multiple pods can read/write the volume, the     database inside the volume can only be used (locked) by a single pod. This is why,     when scaling Pomerium to 2 replicas, 1 will stay <code>Running</code> but not healthy:</p> <pre><code>$ kubectl scale deployment pomerium --replicas=2\n\n$ kubectl get pods -n pomerium -o wide \nNAME                        READY   STATUS    RESTARTS        AGE   IP             NODE      NOMINATED NODE   READINESS GATES\npomerium-6c7f5448b9-bvw64   0/1     Running   6 (8m19s ago)   40m   10.244.1.13    lexicon   &lt;none&gt;           &lt;none&gt;\npomerium-6c7f5448b9-np27p   1/1     Running   1 (13m ago)     34m   10.244.0.101   octavo    &lt;none&gt;           &lt;none&gt;\n</code></pre> </li> <li> <p>Handle the Control Plane \"Unreachable\" Taints: </p> <p>By default, when the control plane goes down, Kubernetes waits 300 seconds before deciding to failover. This should be reduced in Deployment manifests beforehand:</p> <pre><code>spec:\n  template:\n    spec:\n      tolerations:\n      - key: \"management-only\"\n        operator: \"Equal\"\n        value: \"true\"\n        effect: \"NoSchedule\"\n      - key: \"node.kubernetes.io/unreachable\"\n        operator: \"Exists\"\n        effect: \"NoExecute\"\n        tolerationSeconds: 10\n      - key: \"node.kubernetes.io/not-ready\"\n        operator: \"Exists\"\n        effect: \"NoExecute\"\n        tolerationSeconds: 10\n</code></pre> </li> <li> <p>Longhorn \"First Node\" Settings: </p> <p>Longhorn's UI and some managers often default to the first node. Before rebooting <code>octavo</code>, check the Longhorn UI to ensure all volumes are Healthy on both nodes. If a volume is only on octavo, the pod on <code>lexicon</code> will crash.</p> <pre><code>$ kubectl get pods -A -o wide | grep -E 'longhorn-manager|longhorn-ui'\nlonghorn-system            longhorn-manager-7dh85                                   2/2     Running     4 (11m ago)    5d10h   10.244.1.26    lexicon   &lt;none&gt;           &lt;none&gt;\nlonghorn-system            longhorn-manager-pbl4t                                   2/2     Running     4 (34h ago)    8d      10.244.0.73    octavo    &lt;none&gt;           &lt;none&gt;\nlonghorn-system            longhorn-ui-7fc9b4667f-h22gm                             1/1     Running     4 (34h ago)    8d      10.244.0.68    octavo    &lt;none&gt;           &lt;none&gt;\nlonghorn-system            longhorn-ui-7fc9b4667f-xqdvw                             1/1     Running     4 (34h ago)    8d      10.244.0.58    octavo    &lt;none&gt;           &lt;none&gt;\n</code></pre> </li> <li> <p>Longhorn \"Pod Deletion Policy\": In the Longhorn UI, under Settings &gt; General     set Pod Deletion Policy When Node is Down to     <code>delete-both-statefulset-and-deployment-pod</code> so that when a node is down, Longhorn     will force-delete the pods. This breaks the \"Volume Lock\" and allows the pods to     start on the remaining healthy node (octavo) immediately.</p> </li> </ol>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#tolerations-on-helm-charts","title":"Tolerations on Helm Charts","text":"<p>To apply the above management-only tolerations to those applications deployed using Helm charts, add the <code>tolerations</code> section under the following specific keys in the YAML values for each chart. The simplest charts will take the <code>tolerations</code> at the top level: Headlamp, metrics-server, cert-manager-webhook-porkbun, tailscale.</p> <p>Trivy Operator and Prometheus for the Trivy Operator Dashboard in Grafana only accept the <code>tolerations</code> under specifc keys: under <code>trivyOperator</code> in <code>prometheus/trivy-values.yaml</code> and under <code>prometheus.prometheusSpec</code> in <code>prometheus/values.yaml</code>.</p> <p>Longhorn takes <code>tolerations</code> under multiple keys, but adding it simply under <code>global</code> will make it effective for every <code>Deployment</code> and <code>DaemonSet</code> (<code>longhorn-driver-deployer</code>, <code>longhorn-manager</code>, <code>longhorn-ui</code>); and the The NFS CSI driver takes <code>controller.tolerations</code> and <code>node.tolerations</code> in <code>nfs-csi-values.yaml</code>.</p>"},{"location":"blog/2026/02/01/upgrading-a-single-node-kubernetes-cluster-to-zero-downtime-maintenance/#high-availability-ingress","title":"High Availability Ingress","text":"<p>Incoming requests are routed to the <code>LoadBalancer</code> (virtual) IP of Pomerium; when it goes down in one node and starts on the other one, MetalLB moves the virtual IP to the node now running Pomerium. Even though Pomerium cannot be scaled up beyond one replica, MetalLB takes care of moving requests to the correct node where Pomerium is running.</p> <p>Moreover, when traffic hits the Virtual IP on any node, the Kubernetes internal network (<code>kube-proxy</code>) automatically load balances that request to any available Pomerium pod, regardless of which node it sits on.</p> <p>In Layer 2 mode, MetalLB operates as a failover rather than a load balancer. When adding the second node, the MetalLB speaker pod will automatically deploy to it as part of its <code>DaemonSet</code>. The two speakers will communicate; one node will be \"elected\" the leader and will handle the ARP requests for the Pomerium IP. If the current leader goes offline, the other speaker will detect the loss of its peer and automatically begin announcing that same IP address to the router.</p> <p>Adding a second node to a cluster already running MetalLB in Layer 2 mode provides a \"semi-automatic\" path to high availability. While MetalLB will technically handle the failover, the current setup requires two manual checks to ensure it actually works when a node goes down:</p> <ol> <li> <p>External Traffic Policy: <code>Cluster</code> is the default mode that allows any node to     be the leader, and that is requiried for high availability ingress, so that even if     the Pomerium pod isn't on that specific node. The leader node will catch the traffic     and forward it internally to wherever the Pomerium pod is running. This can be     checked with the following command:</p> <code>kubectl get svc -A</code> <pre><code>$ kubectl get svc -A -o \\\n  custom-columns=\"NAME:.metadata.name,NAMESPACE:.spec.externalTrafficPolicy\" \\\n  | grep -E 'Cluster|Local'\nfirefly-iii-mysql-svc                                   Cluster\nfirefly-iii-svc                                         Cluster\nkomga-svc                                               Cluster\ngrafana-svc                                             Cluster\ninfluxdb-svc                                            Cluster\nnavidrome-svc                                           Cluster\npomerium-proxy                                          Cluster\npostgres-svc                                            Cluster\nryot-svc                                                Cluster\nmongo-svc                                               Cluster\nunifi-tcp                                               Cluster\nunifi-tcp-1                                             Cluster\nunifi-udp                                               Cluster\n</code></pre> </li> <li> <p>L2 Advertisement Scope: must be unrestricted so that the <code>L2Advertisement</code>     resource advertises to all nodes. If restricted via a <code>nodeSelector</code>, it must be     updated it to include all the relevant nodes.</p> </li> </ol> <p>As it happens, MetalLB was already setup using the default <code>Cluster</code> polic and the <code>L2Advertisement</code> resources were not restricted, so it was essentially ready from the beginning.</p>"},{"location":"blog/2026/02/10/self-hosted-personal-dis-organizer-with-vikunja/","title":"Self-hosted Personal Dis-Organizer with Vikunja","text":"<p>There is always too much to do and too little time, it never gets all done so the real challenge is to get the right things done at the right time, just to avoid Unforseen Consequences...</p> <p>In one more attempt at getting better organized than having lots of browser tabs always open, emails unread in the inbox and, worst of all, uncountable ideas floating in the brain with nowhere to rest, lets try Vikunja, the fluffy, open-source, self-hostable to-do app.</p>"},{"location":"blog/2026/02/10/self-hosted-personal-dis-organizer-with-vikunja/#installation","title":"Installation","text":"<p>Installing instructions include a basic example to run with docker and links to other setups. Vikunja Helm Chart includes are more complete setup, but for simplicity this time round Vikunja was deployed with this manifest:</p> <code>vikunja.yaml</code> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: vikunja\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: vikunja-data\n  namespace: vikunja\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: longhorn-nvme\n  resources:\n    requests:\n      storage: 5Gi\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vikunja\n  namespace: vikunja\nspec:\n  replicas: 1\n  revisionHistoryLimit: 0\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: vikunja\n  template:\n    metadata:\n      labels:\n        app: vikunja\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n      containers:\n      - name: vikunja\n        image: vikunja/vikunja:1.0.0\n        ports:\n        - containerPort: 3456\n        env:\n        - name: VIKUNJA_SERVICE_JWTSECRET\n          value: \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n        - name: VIKUNJA_DATABASE_TYPE\n          value: \"sqlite\"\n        - name: VIKUNJA_DATABASE_PATH\n          value: \"/app/vikunja/files/vikunja.db\"\n        - name: VIKUNJA_SERVICE_PUBLICURL\n          value: \"https://vikunja.very-very-dark-gray.top\"\n        volumeMounts:\n        - name: data\n          mountPath: /app/vikunja/files\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: vikunja-data\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vikunja\n  namespace: vikunja\nspec:\n  selector:\n    app: vikunja\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 3456\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: vikunja-ingress-tailscale\n  namespace: vikunja\nspec:\n  ingressClassName: tailscale\n  defaultBackend:\n    service:\n      name: vikunja\n      port:\n        number: 80\n  tls:\n    - hosts:\n        - vikunja\n</code></pre> <p>This deployment includes an <code>Ingress</code> using the Tailscale kubernetes operator because the mobile app seems to be unable to have its requests authenticated through Pomerium, even after singing in using its embeded Chrome browser. To keep access to the full web UI secured behind Pomerium, this ingress is added to Pomerium Kustomize setup:</p> <code>pomerium/pomerium-ingress/vikunja.yaml</code> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: vikunja-pomerium-ingress\n  namespace: vikunja\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ingress.pomerium.io/allow_websockets: true\n    ingress.pomerium.io/idle_timeout: 0s\n    ingress.pomerium.io/pass_identity_headers: true\n    ingress.pomerium.io/preserve_host_header: true\n    ingress.pomerium.io/timeout: 0s\n    ingress.pomerium.io/set_request_headers: |\n      X-Forwarded-Proto: \"https\"\n      X-Forwarded-Host: \"vikunja.very-very-dark-gray.top\"\nspec:\n  ingressClassName: pomerium\n  rules:\n    - host: vikunja.very-very-dark-gray.top\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: vikunja\n                port:\n                  number: 80\n  tls:\n    - secretName: tls-secret\n      hosts:\n        - vikunja.very-very-dark-gray.top\n</code></pre> <p>Applying this manifest should have the service up and running within the minute:</p> <pre><code>$ kubectl apply -f vikunja.yaml \nnamespace/vikunja created\npersistentvolumeclaim/vikunja-data created\ndeployment.apps/vikunja created\nservice/vikunja created\n\n$ kubectl -n vikunja get all\nNAME                          READY   STATUS    RESTARTS   AGE\npod/vikunja-75c7776d7-f7njz   1/1     Running   0          33s\n\nNAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\nservice/vikunja   ClusterIP   10.101.73.47   &lt;none&gt;        80/TCP    19m\n\nNAME                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/vikunja   1/1     1            1           19m\n\nNAME                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/vikunja-75c7776d7   1         1         1       33s\n</code></pre> <p>Applying the Pomerium Ingress needs a couple of minutes for the DNS challenge to be done:</p> <pre><code>$ kubectl apply -k pomerium/pomerium-ingress\n...\ningress.networking.k8s.io/vikunja-pomerium-ingress created\n\n$ kubectl get certificate -n vikunja -w\nNAME         READY   SECRET       AGE\ntls-secret   False   tls-secret   9s\ntls-secret   False   tls-secret   97s\n</code></pre> <p>Once this is ready, the web UI is ready at https://vikunja.very-very-dark-gray.top</p>"},{"location":"blog/2026/02/10/self-hosted-personal-dis-organizer-with-vikunja/#android-app","title":"Android app","text":"<p>The Vikunja Android app is easy to install from the Play Store but not so easy to sing in with, when the web UI is secured behind SSO. There is not a separate path for the application to talk to the API, so to enable direct HTTPS access from the app it would be necessary to disable SSO ACLs on the web UI; not necessarily desirable.</p> <p>Instead, the app can connect to the server via a Tailscale Ingress.</p> <p>Moreover, the latest version of the mobile app is compatible only with version 1.0.0 of the server, which is why the above manifests deploys that old version instead of the latest (1.1.0).</p> <p>On the other hand, the web UI can be accessed with a mobile browser (e.g. Firefox); time will tell whether the mobile app makes itself worthy all the above workarounds.</p>"},{"location":"projects/","title":"Projects","text":""},{"location":"projects/#projects","title":"Projects","text":"<p>Stuff that coalesced into something somewhat useful:</p> <ul> <li>Continuous Monitoring is a small collection of ad-hoc     Bash and Python scripts to collect monitoring metrics and push them     to an InfluxDB 1.x server.</li> <li>Self-hosting is an ever-changing summary of all     the applications I host and manage, or used or, or tried to, or     would like to, maybe some day, on my own server(s).</li> </ul>"},{"location":"projects/conmon/","title":"Continuous Monitoring","text":""},{"location":"projects/conmon/#about-continuous-monitoring","title":"About Continuous Monitoring","text":"<p>This is the latest, most complete version of the scripts for Detailed system and process monitoring.</p> <p>To run in slow systems such as Raspberry Pi computers, including any one from the  Zero W (v1) to the  4 model B, the single-thread script (<code>conmon-st</code>) is preferred, with a sampling delay based on each Raspberry CPU to avoid interfering with other processes. To run in faster systems such as servers and desktop PCs, the multi-thread script (<code>conmon-mt</code>) is preferred, with much smaller sampling delays to capture fast changes in resource utilization more accurately.</p> <p>In addition to the main <code>conmon</code> process, there are a few scripts that usually only make sense to run on a single host in the network:</p> <ul> <li><code>conmon-speedtest</code> reports the    download and upload speeds (Mbps) and ping (ms)    as reported by    sivel/speedtest-cli</li> <li><code>conmon-mystrom</code> reports the telemetry    obtained via the Get report method in the    myStrom REST API from one or    more smart plug/switch devices that report energy usage.</li> <li><code>conmon-tapo.py</code> reports temperature, humidity,    power consumption and other monitoring data from TAPO devices.</li> <li><code>tapo.yaml</code> is a minimal configuration file for       <code>conmon-tapo.py</code>.</li> </ul>"},{"location":"projects/conmon/#kubernetes-setup","title":"Kubernetes Setup","text":"<p>This setup has been  migrated to run on a single-node Kubernetes cluster and the scripts have been updated to support dual-targeting InfluxDB over HTTP without auth and/or HTTPS with Basic Auth.</p>"},{"location":"projects/conmon/#install-influxdb","title":"Install InfluxDB","text":"<p>Install InfluxDB OSS or simply</p> <pre><code># apt install influxdb influxdb-client -y\n</code></pre> <p>Set it up and test  test writing data.</p> <p>Create a <code>monitoring</code> database (name can be different, then update the <code>DBNAME</code> variable in the <code>conmon</code> scripts).</p>"},{"location":"projects/conmon/#install-conmon","title":"Install Conmon","text":"<p>The monitoring script can be installed in any system to report metrics back to the InfluxDB server. A few common tools are required which are not always installed by default:</p> <pre><code># apt install -y curl jq iotop-c lm-sensors \n</code></pre> <p>To gather metrics from GPUs, install also</p> <ul> <li><code>intel-gpu-tools</code> for Intel GPUs.</li> <li>The latest <code>nvidia-utils-xxx</code> for NVidia GPUs     (check with <code>apt-cache search nvidia-smi</code>).</li> </ul> <p>If the target InfluxDB server  requires HTTP authenticatoin, copy or create the credentials into <code>/etc/conmon/influxdb-auth</code>(and <code>chmod 400</code> it).</p> <p>Then choose a version of the <code>conmon</code> script and install it as <code>/usr/local/bin/conmon</code> and run it as a service by creating <code>/etc/systemd/system/conmon.service</code> as follows:</p> <pre><code>[Unit]\nDescription=Continuous Monitoring\nAfter=influxd.service\nWants=influxd.service\n\n[Service]\nExecStart=/usr/local/bin/conmon\nRestart=on-failure\nStandardOutput=null\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Then enable and start the services in <code>systemd</code>:</p> <pre><code># systemctl enable conmon.service\n# systemctl daemon-reload\n# systemctl start conmon.service\n# systemctl status conmon.service\n</code></pre> <p>Once all this is working, the monitoring script can be updated with  <code>deploy-to-pcs</code> or <code>deploy-to-rpis</code>.</p> <p>After a minute or so, there should be enough metrics already in InfluxDB to create a dashboard in Grafana to display them. A good starting point can be cloning an existing dashboard from a similar system, then tweaking the <code>tag::host</code> filter in all queries and some of the <code>Max</code> values.</p>"},{"location":"projects/conmon/#the-future-of-flux","title":"The future of Flux","text":"<p>Flux is going into maintenance mode. You can continue using it as you currently are without any changes to your code.</p> <p>Flux is going into maintenance mode and will not be supported in InfluxDB 3.0. This was a decision based on the broad demand for SQL and the continued growth and adoption of InfluxQL. We are continuing to support Flux for users in 1.x and 2.x so you can continue using it with no changes to your code. If you are interested in transitioning to InfluxDB 3.0 and want to future-proof your code, we suggest using InfluxQL.</p> <p>For information about the future of Flux, see the following:</p> <ul> <li>The plan for InfluxDB 3.0 Open Source</li> <li>InfluxDB 3.0 benchmarks</li> </ul>"},{"location":"projects/conmon/#install-grafana","title":"Install Grafana","text":"<p>To visualize monitoring in this setup Grafana is used.</p> <p>Follow the steps to install Grafana on Ubuntu, start the server with systemd and reset its <code>admin</code> password:</p> <pre><code># grafana-cli admin reset-admin-password \\\n  PLEASE_CHOOSE_A_SENSIBLE_PASSWORD\nINFO[03-20|15:02:11] Connecting to DB                         logger=sqlstore dbtype=sqlite3\nINFO[03-20|15:02:11] Starting DB migrations                   logger=migrator\nAdmin password changed successfully \u2714\n</code></pre> <p>Add your InfluxDB data source to Grafana, create a new Dashboard and Add &gt; Visualization for each measurement.</p> <p>Finally, enable anonymous authentication. by tweaking <code>/etc/grafana/grafana.ini</code> as follows:</p> <pre><code>#################################### Anonymous Auth ######################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\norg_role = Viewer\n\n# systemctl restart grafana-server.service\n</code></pre>"},{"location":"projects/conmon/#install-conmon_1","title":"Install Conmon","text":""},{"location":"projects/conmon/#single-thread","title":"Single-thread","text":""},{"location":"projects/conmon/#deploy-to-rpis","title":"<code>deploy-to-rpis</code>","text":"<p>In environments with multiple Raspberry Pi computers, it may be useful to use this script (<code>deploy-to-rpis</code>) to deploy the latest version of the script to all computers at once.</p> deploy-to-rpis<pre><code>#!/bin/bash\n#\n# Deplay conmon-st to Raspberry Pi hosts.\n\nfor host in alfred pi-f1 pi-z1 pi-z2 pi3a; do\n  if nc 2&gt;&amp;1 -zv ${host} 22 | grep -q succeeded; then\n    echo \"Deploying to ${host} ...\"\n    scp 2&gt;/dev/null \\\n      -qr \\\n      ../conmon \\\n      pi@${host}:src/\n    ssh 2&gt;/dev/null \\\n      pi@${host} \\\n      \"sudo cp /home/pi/src/conmon/conmon-st /usr/local/bin/conmon\"\n    ssh 2&gt;/dev/null \\\n      pi@${host} \\\n      \"sudo systemctl restart conmon.service\"\n    etc_influxdb_auth=/etc/conmon/influxdb-auth\n    for src_influxdb_auth in /etc/conmon/influxdb-auth \"${HOME}/.conmon-influxdb-auth\"; do\n      if [ -f $src_influxdb_auth ]; then\n        auth=$(cat $src_influxdb_auth 2&gt;/dev/null)\n        ssh 2&gt;/dev/null \\\n          pi@${host} \\\n          \"sudo mkdir -p $(dirname $etc_influxdb_auth)\"\n        ssh 2&gt;&amp;1 &gt;/dev/null \\\n          pi@${host} \\\n          \"echo '${auth}' | sudo tee $etc_influxdb_auth\"\n        ssh 2&gt;/dev/null \\\n          pi@${host} \\\n          \"sudo chmod 400 $etc_influxdb_auth\"\n      fi\n    done\n  fi\ndone\n</code></pre>"},{"location":"projects/conmon/#conmon-st","title":"<code>conmon-st</code>","text":"conmon-st<pre><code>#!/bin/bash\n#\n# Export system monitoring metrics to influxdb.\n\n# InfluxDB target.\nDBNAME=monitoring\nTARGET_HTTP='' # Leave empty to skip.\nTARGET_HTTPS='http://octavo:30086'\n\n# Default delay between each POST request to InfluxDB.\nDELAY_POST=4\n\n# Data file for batch POST.\nDDIR=\"/dev/shm/$$\"\nDATA=\"${DDIR}/DATA.txt\"\nmkdir -p \"${DDIR}\"\n\nhost=$(hostname)\n\ntimestamp_ns() {\n  date +'%s%N'\n}\n\nstore_line() {\n  # Write a line of data to the temporary in-memory file.\n  # Exit immediately if this fails.\n  echo $1 &gt;&gt;\"${DATA}\" || exit 1\n}\n\nreport_top_per_process() {\n  # Per-process CPU(%) &amp; MEM(bytes) usage.\n  # Re-use the ptop file created by report_top().\n  ts=$1\n  ptop=$2\n  awk '{print $4}' \"${ptop}\" | sort -u | while read cmd; do\n    user=$(grep \" ${cmd}\\$\" ${ptop} | cut -f1 -d' ' | sort -u | head -1)\n    # CPU per proccess, only when &gt; 0\n    cpu=$(grep \" ${cmd}\\$\" ${ptop} | cut -f2 -d' ' | grep -v '^0\\.0$' | tr '\\n' '+' | sed 's/+$/\\n/' | bc -ql)\n    if [[ ! -z \"${cpu}\" ]]; then\n      store_line \"top_cpu,host=${host},user=${user},command=${cmd} value=${cpu} ${ts}\"\n    fi\n    # Memory per proccess, only when &gt; 1%\n    mem=$(grep \" ${cmd}\\$\" ${ptop} | cut -f3 -d' ' | grep -v '^0\\.0$' | tr '\\n' '+' | sed 's/+$/\\n/' | bc -ql | sed 's/^\\./0./')\n    # Multiply %mem by total system RAM.\n    tot=$(free -b | grep 'Mem:' | awk '{print $2}')\n    ram=$(bc -ql &lt;&lt;&lt;\"${tot} * ${mem} / 100\")\n    if [[ ! -z \"${ram}\" ]]; then\n      store_line \"top_mem,host=${host},user=${user},command=${cmd} value=${ram} ${ts}\"\n    fi\n  done\n  rm -f \"${ptop}\"\n}\n\nreport_top() {\n  # Stats from top: CPU (overall and per process) and RAM (per process).\n  # Depends on: top, free.\n  top_cmd=$(command -v top)\n  top=\"${DDIR}/top\"\n  ts=$(timestamp_ns)\n  ${top_cmd} -b -c -n 1 -w 512 |\n    grep -vE ' 0\\..   0\\..|^top|^Tasks|^%|^MiB|^$|[[:blank:]]*PID USER' |\n    awk '{print $2,$9,$10,$12,$13,$14,$15,$16,$17,$18,$19,$20}' |\n    tr '\\\\' '/' |\n    sed 's/\\.minecraft\\/bin\\/[0-9a-f]\\+.*/minecraft/' |\n    sed 's/[C-Z]:\\/.*\\/\\([a-zA-Z0-9 _-]\\+\\.[a-z][a-z][a-z]\\).*/\\1/' |\n    sed 's/\\/[^ ]*\\///' | sed 's/\\(bash\\|sh\\|python\\|python3\\) .*\\///' |\n    tr -d '[' |\n    tr -d ']' |\n    awk '{print $1,$2,$3,$4}' &gt;\"${top}\"\n  # Total CPU(%) usage.\n  cpu_load=$(awk '{print $2}' ${top} | grep -v '^0\\.0$' | tr '\\n' '+' | sed 's/+$/\\n/' | bc -ql)\n  store_line \"top,host=${host} value=${cpu_load} ${ts}\"\n  # Launch the slower per-process metrics in the background.\n  ptop=\"${top}.$RANDOM\"\n  mv \"${top}\" \"${ptop}\"\n  report_top_per_process \"${ts}\" \"${ptop}\" &amp;\n}\n\nreport_vcgencmd_clock() {\n  # Raspberry Pi CPU clock frequency.\n  # Depends on: vcgencmd.\n  vcgencmd=$(command -v vcgencmd)\n  if [ -z \"${vcgencmd}\" ] || [ ! -f \"${vcgencmd}\" ]; then\n    return\n  fi\n  ts=$(timestamp_ns)\n  cpu_clock=$(echo $(echo \"scale=2; $(vcgencmd measure_clock arm | cut -d '=' -f 2) / 1000000\" | bc))\n  store_line \"vcgencmd,metric=clock,host=${host} value=${cpu_clock} ${ts}\"\n}\n\nreport_vcgencmd_temp() {\n  # Raspberry Pi CPU temperature.\n  # Depends on: vcgencmd.\n  vcgencmd=$(command -v vcgencmd)\n  if [ -z \"${vcgencmd}\" ] || [ ! -f \"${vcgencmd}\" ]; then\n    return\n  fi\n  ts=$(timestamp_ns)\n  cpu_temp=$(${vcgencmd} measure_temp | cut -f2 -d= | cut -f1 -d\"'\")\n  store_line \"vcgencmd,metric=temp,host=${host} value=${cpu_temp} ${ts}\"\n  if grep -q 'Pi 4' /proc/device-tree/model; then\n    ts=$(timestamp_ns)\n    pmic_temp=$(${vcgencmd} measure_temp pmic | cut -f2 -d= | cut -f1 -d\"'\")\n    store_line \"vcgencmd,metric=temp_pmic,host=${host} value=${pmic_temp} ${ts}\"\n  fi\n}\n\nreport_sensors() {\n  # CPU, SSD, NVMe temperatures and other sensors (if available).\n  # Depends on: jq, sensors.\n  jq=$(command -v jq)\n  if [ -z \"${jq}\" ] || [ ! -f \"${jq}\" ]; then\n    return\n  fi\n  sensors=$(command -v sensors)\n  if [ -z \"${sensors}\" ] || [ ! -f \"${sensors}\" ]; then\n    return\n  fi\n  sensors_json=\"${DDIR}/sensors\"\n  ts=$(timestamp_ns)\n  \"${sensors}\" -j &gt;\"${sensors_json}\"\n  $jq 'keys' \"${sensors_json}\" | grep '^  \"' | cut -f2 -d'\"' | while read adapter; do\n    echo \"adapter: $adapter\"\n    $jq \".\\\"${adapter}\\\"\" \"${sensors_json}\" | $jq 'keys' | grep '^  \"' | grep -v '\"Adapter\"' | cut -f2 -d'\"' | while read name; do\n      key=$($jq \".\\\"${adapter}\\\".\\\"${name}\\\"\" \"${sensors_json}\" | $jq 'keys' | grep '^  \"' | grep '_input\"' | cut -f2 -d'\"')\n      value=$($jq \".\\\"${adapter}\\\".\\\"${name}\\\".\\\"${key}\\\"\" \"${sensors_json}\")\n      store_line \"sensors,host=${host},adapter=${adapter},name=${name/ /_} value=${value} ${ts}\"\n    done\n  done\n}\n\nreport_intel_gpu_top() {\n  # Intel GPU.\n  # Depends on: intel_gpu_top (as root).\n  intel_gpu_top=$(command -v intel_gpu_top)\n  if [ -z $intel_gpu_top ] || [ ! -f $intel_gpu_top ]; then\n    return\n  fi\n  ts=$(timestamp_ns)\n  # TODO: find out why this works only when running in a console.\n  timeout 1s sudo ${intel_gpu_top} -c | head -2 &gt;${intel_gpu_csv}\n  for N in $(seq 18); do\n    metric=$(awk -v cn=$N -F',' '{print $cn}' ${intel_gpu_csv} | head -1)\n    value=$(awk -v cn=$N -F',' '{print $cn}' ${intel_gpu_csv} | tail -1)\n    if echo \"$value\" | egrep -q '^0\\.0+$'; then continue; fi\n    echo \"intel_gpu,host=${host},metric=${metric/ /_} value=${value} ${ts}\"\n    store_line \"intel_gpu,host=${host},metric=${metric/ /_} value=${value} ${ts}\"\n  done\n}\n\nreport_nvidia_smi() {\n  # NVidia GPU.\n  # Depends on: nvidia-smi\n  nvidia_smi=$(command -v nvidia-smi)\n  if [ -z \"${nvidia_smi}\" ] || [ ! -f \"${nvidia_smi}\" ]; then\n    return\n  fi\n  ts=$(timestamp_ns)\n  temp=$(${nvidia_smi} -i 0 --query-gpu=temperature.gpu --format=csv,noheader)\n  util=$(${nvidia_smi} -i 0 --query-gpu=utilization.gpu --format=csv,noheader | cut -f1 -d' ')\n  vram=$(${nvidia_smi} -i 0 --query-gpu=memory.used --format=csv,noheader | cut -f1 -d' ')\n  draw=$(${nvidia_smi} -i 0 --query-gpu=power.draw --format=csv,noheader | cut -f1 -d' ')\n  fans=$(${nvidia_smi} -i 0 --query-gpu=fan.speed --format=csv,noheader | cut -f1 -d' ')\n  store_line \"nvidia_smi,host=${host},metric=temperature value=${temp} ${ts}\"\n  store_line \"nvidia_smi,host=${host},metric=utilization value=${util} ${ts}\"\n  store_line \"nvidia_smi,host=${host},metric=memory value=${vram} ${ts}\"\n  store_line \"nvidia_smi,host=${host},metric=power value=${draw} ${ts}\"\n  store_line \"nvidia_smi,host=${host},metric=fan value=${fans} ${ts}\"\n}\n\nreport_free() {\n  # Stats from free: RAM used, buffered/cached, free.\n  # Depends on: free.\n  ts=$(timestamp_ns)\n  mem_used=$(free -b | grep 'Mem:' | awk '{print $3}')\n  mem_free=$(free -b | grep 'Mem:' | awk '{print $4}')\n  mem_buff=$(free -b | grep 'Mem:' | awk '{print $6}')\n  store_line \"free,host=${host},metric=used value=${mem_used} ${ts}\"\n  store_line \"free,host=${host},metric=free value=${mem_free} ${ts}\"\n  store_line \"free,host=${host},metric=buff_cache value=${mem_buff} ${ts}\"\n}\n\nreport_df() {\n  # Stats from df: used/free space per file system.\n  # Depends on: df.\n  ts=$(timestamp_ns)\n  df -k | egrep -v 'udev|loop|/sys|/run|/dev$' |\n    grep ' /' | awk '{print $4,$5,$6}' |\n    while read line; do\n      # Note free space is given in 1KB blocks.\n      fs_path=$(echo \"${line}\" | cut -f3 -d' ')\n      fs_used=$(echo \"${line}\" | cut -f2 -d' ' | cut -f1 -d'%')\n      fs_free=$(echo \"${line}\" | cut -f1 -d' ')\n      fs_free=$((1024 * fs_free))\n      store_line \"df,host=${host},path=${fs_path},metric=tot_free value=${fs_free} ${ts}\"\n      store_line \"df,host=${host},path=${fs_path},metric=pct_used value=${fs_used} ${ts}\"\n    done\n}\n\nreport_diskstats() {\n  # Disk I/O stats from /proc/diskstats.\n  # Depends on: /proc/diskstats.\n  ts=$(timestamp_ns)\n  # The /proc/diskstats file displays the I/O statistics of block devices.\n  # Each line contains the following fields (and more, omitted here):\n  #  1  major number\n  #  2  minor mumber\n  #  3  device name\n  #  4  reads completed successfully\n  #  5  reads merged\n  #  6  sectors read\n  #  7  time spent reading (ms)\n  #  8  writes completed\n  #  9  writes merged\n  # 10  sectors written\n  # 11  time spent writing (ms)\n  # Source: https://www.kernel.org/doc/Documentation/ABI/testing/procfs-diskstats\n  grep -v ' 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0' /proc/diskstats | egrep -v 'loop|[hs]d[a-z][1-9]|k0p[0-9]' | awk '{print $3,$4,$6,$7,$8,$10,$11}' | sort &gt;$DDIR/disk1\n  if [[ -f \"${DDIR}/disk0\" ]]; then\n    # Columns used: disk reads rsect rtime writs wsect wtime\n    # disk:   3  device name\n    # reads:  4  reads completed successfully\n    # rsect:  6  sectors read\n    # rtime:  7  time spent reading (ms)\n    # writs:  8  writes completed\n    # wsect: 10  sectors written\n    # wtime: 11  time spent writing (ms)\n    paste \"${DDIR}/disk0\" \"${DDIR}/disk1\" |\n      while read \\\n        disk0 reads0 rsect0 rtime0 writs0 wsect0 wtime0 \\\n        disk1 reads1 rsect1 rtime1 writs1 wsect1 wtime1; do\n        # Skip momentary inconsistencies when disks are added or removed.\n        if [ \"${disk0}\" == \"${disk1}\" ]; then\n          disk=$disk0\n          # Assume 512-byte sectors.\n          rsect=$((512 * (rsect1 - rsect0)))\n          wsect=$((512 * (wsect1 - wsect0)))\n          store_line \"diskstats,host=${host},device=${disk},metric=bytes_read    value=${rsect} ${ts}\"\n          store_line \"diskstats,host=${host},device=${disk},metric=bytes_written value=${wsect} ${ts}\"\n          # Successful I/O ops.\n          reads=$((reads1 - reads0))\n          writs=$((writs1 - writs0))\n          store_line \"diskstats,host=${host},device=${disk},metric=successful_reads  value=${reads} ${ts}\"\n          store_line \"diskstats,host=${host},device=${disk},metric=successful_writes value=${writs} ${ts}\"\n          # Time spent.\n          read_ms=$((rtime1 - rtime0))\n          writ_ms=$((wtime1 - wtime0))\n          store_line \"diskstats,host=${host},device=${disk},metric=time_spent_reads  value=${read_ms} ${ts}\"\n          store_line \"diskstats,host=${host},device=${disk},metric=time_spent_writes value=${writ_ms} ${ts}\"\n        fi\n      done\n  fi\n  mv \"${DDIR}/disk1\" \"${DDIR}/disk0\"\n}\n\nbytes_from_value_and_units() {\n  value=$1\n  units=$2\n  factor=1\n  case \"${units}\" in\n  \"K/s\")\n    factor=1024\n    ;;\n  \"M/s\")\n    factor=1024*1024\n    ;;\n  \"G/s\")\n    factor=1024*1024*1024\n    ;;\n  \"T/s\")\n    factor=1024*1024*1024*1024\n    ;;\n  esac\n  bc -ql &lt;&lt;&lt;\"${factor} * ${value}\"\n}\n\nreport_iotop() {\n  # I/O stats from iotop (per process).\n  # Depends on: iotop (as root).\n  iotop=$(command -v iotop)\n  if [ -z $iotop ] || [ ! -f $iotop ]; then\n    return\n  fi\n  ts=$(timestamp_ns)\n  iotop_stats=$DDIR/iotop\n  # The Linux kernel interfaces that iotop relies on now require root\n  # privileges or the NET_ADMIN capability. This change occurred because a\n  # security issue (CVE-2011-2494) was found that allows leakage of sensitive\n  # data across user boundaries. If you require the ability to run iotop as a\n  # non-root user, please configure sudo to allow you to run iotop as root.\n  # WARNING: using -n 1 results in only zero values all the time.\n  sudo $iotop -b -P -n 2 |\n    grep -Ev 'task_delayacct|locale|DISK READ|0.00 B/s    0.00 B/s' |\n    tr -d '[' |\n    tr -d ']' |\n    sed 's/kworker\\///' |\n    sed 's/u64:[0-9]\\+-//' |\n    awk '{print $4,$5,$6,$7,$9}' \\\n      &gt;\"${iotop_stats}\"\n  # Return if iotop could not be run successfully.\n  # This is typically caused by 'Netlink error: Operation not permitted'.\n  if [[ $? -gt 0 ]]; then\n    return\n  fi\n  # Report I/O stats for each process.\n  # Note: this aggregates I/O per process name (basename), across all PIDs.\n  cat \"${iotop_stats}\" |\n    cut -f5 -d' ' |\n    sort -u |\n    while read -r process; do\n      process_read=0\n      process_write=0\n      while read -r line; do\n        read_value=$(echo \"${line}\" | cut -f1 -d' ')\n        read_units=$(echo \"${line}\" | cut -f2 -d' ')\n        read_bytes=$(bytes_from_value_and_units \"${read_value}\" \"${read_units}\")\n        process_read=$(bc -ql &lt;&lt;&lt;\"${process_read}+${read_bytes}\")\n        write_value=$(echo \"${line}\" | cut -f3 -d' ')\n        write_units=$(echo \"${line}\" | cut -f4 -d' ')\n        write_bytes=$(bytes_from_value_and_units \"${write_value}\" \"${write_units}\")\n        process_write=$(bc -ql &lt;&lt;&lt;\"${process_write}+${write_bytes}\")\n      done &lt; &lt;(grep \" ${process}\\$\" \"${iotop_stats}\")\n      if [[ $(echo \"${process_read}\" | sed 's/\\..*//') -gt 0 ]]; then\n        store_line \"iotop,host=${host},command=${process},metric=read value=${process_read} ${ts}\"\n      fi\n      if [[ \"$(echo \"${process_write}\" | sed 's/\\..*//')\" -gt 0 ]]; then\n        store_line \"iotop,host=${host},command=${process},metric=write value=${process_write} ${ts}\"\n      fi\n    done\n}\n\nreport_net_dev() {\n  # Network I/O stats from /proc/net/dev.\n  # Depends on: /proc/net/dev.\n  ts=$(timestamp_ns)\n  grep -Ev 'Inter|face' /proc/net/dev | tr -d ':' | awk '{print $1,$2,$10}' | sort &gt;\"${DDIR}/net1\"\n  if [[ -f \"${DDIR}/net0\" ]]; then\n    ts0=$(cat \"${DDIR}/net0-ts\")\n    echo \"${ts}\" &gt;\"${DDIR}/net1-ts\"\n    paste \"${DDIR}/net0\" \"${DDIR}/net1\" | while read -r dev0 rx0 tx0 dev1 rx1 tx1; do\n      # Skip momentary inconsistencies when devs are added or removed.\n      if [[ \"${dev0}\" == \"${dev1}\" ]]; then\n        # Compute rx/tx bytes / sec (ts is in nanoseconds).\n        rx=$(bc -ql &lt;&lt;&lt;\"1000000000 * (${rx1} - ${rx0}) / (${ts} - ${ts0})\")\n        tx=$(bc -ql &lt;&lt;&lt;\"1000000000 * (${tx1} - ${tx0}) / (${ts} - ${ts0})\")\n        store_line \"net_dev,host=${host},device=${dev0},metric=rx value=${rx} ${ts}\"\n        store_line \"net_dev,host=${host},device=${dev0},metric=tx value=${tx} ${ts}\"\n      fi\n    done\n  fi\n  mv \"${DDIR}/net1\" \"${DDIR}/net0\"\n  mv \"${DDIR}/net1-ts\" \"${DDIR}/net0-ts\"\n}\n\nreport_du() {\n  # Disk usage stats from du.\n  # Files and directories to monitor in /etc/conmon/du\n  # Depends on du (as root).\n  if [ -z /etc/conmon/du ] || [ ! -f $/etc/conmon/du ]; then\n    return\n  fi\n  ts=$(timestamp_ns)\n  while read -r path; do\n    kbytes=$(sudo du -s \"${path}\" | awk '{print $1}')\n    bytes=$((1024 * kbytes))\n    store_line \"du,host=${host},path=${path} value=${bytes} ${ts}\"\n  done &lt;/etc/conmon/du\n}\n\npost_lines_to_influxdb() {\n  # POST data to InfluxDB in batch, when target is available.\n  # Depends on: nc.\n  # All other tasks write data to the file in append mode (&gt;&gt;).\n  # This task reads everything at once and immediately deletes the file.\n  # This makes all the other tasks write to the same file, created anew.\n  sleep ${DELAY_POST}\n  mv -f \"${DATA}\" \"${DATA}.POST\"\n  # Post over HTTP without auth.\n  if [ -n \"$TARGET_HTTP\" ]; then\n    host_and_port=$(echo \"$TARGET_HTTP\" | sed 's/.*\\///' | tr : ' ')\n    if nc 2&gt;&amp;1 -zv $host_and_port | grep -q succeeded; then\n      curl &gt;/dev/null 2&gt;/dev/null \\\n        -i -XPOST \"${TARGET_HTTP}/write?db=${DBNAME}\" \\\n        --data-binary @\"${DATA}.POST\"\n    fi\n  fi\n  # Post over HTTPS with Basic Auth, provided credentials are\n  # found in /etc/conmon/influxdb-auth\n  if [ -n \"$TARGET_HTTPS\" ]; then\n    influxdb_auth=/etc/conmon/influxdb-auth\n    if [ -z $influxdb_auth ] || [ ! -f $influxdb_auth ]; then\n      return\n    fi\n    host_and_port=$(echo \"$TARGET_HTTPS\" | sed 's/.*\\///' | tr : ' ')\n    if nc 2&gt;&amp;1 -zv $host_and_port | grep -q succeeded; then\n      curl &gt;/dev/null 2&gt;/dev/null \\\n        -u $(sudo cat $influxdb_auth) \\\n        -i -XPOST \"${TARGET_HTTPS}/write?db=${DBNAME}\" \\\n        --data-binary @\"${DATA}.POST\"\n    fi\n  fi\n}\n\npause_depending_on_rpi_model() {\n  # Pause for a few seconds depending on which model of Raspberry Pi.\n  # The delay depends also on the system load as reported by top.\n  # Depends on: grep, /proc/cpu_info.\n  top_cmd=$(command -v top)\n  load=$(${top_cmd} -b -n 1 | head -1 | sed 's/.*load average: //' | cut -f1 -d, | tr -d '.' | sed 's/^0//')\n  declare -A cpu_mhz_per_model=(\n    [\"Raspberry_Pi_Zero_W_Rev_1_1\"]=1000         # 1x1000 MHz\n    [\"Raspberry_Pi_Zero_2_W_Rev_1_0\"]=4000       # 4x1000 MHz\n    [\"Raspberry_Pi_3_Model_A_Plus_Rev_1_0\"]=1400 # 1x1400 MHz\n    [\"Raspberry_Pi_3_Model_B_Rev_1_2\"]=4800      # 4x1200 MHz\n    [\"Raspberry_Pi_3_Model_B_Rev_1_3\"]=4800      # 4x1200 MHz\n    [\"Raspberry_Pi_4_Model_B_Rev_1_4\"]=6000      # 4x1500 MHz\n    [\"Raspberry_Pi_5_Model_B_Rev_1_0\"]=9600      # 4x2400 MHz\n  )\n  model=$(grep Model /proc/cpuinfo | sed 's/.*: //' | tr ' ' '_' | tr '.' '_')\n  mhz=${cpu_mhz_per_model[\"${model}\"]}\n  delay=$((250 * load / mhz))\n  sleep ${delay}\n}\n\n# Run all the above tasks in a loop.\n# Each task is responsible of its own checks.\nwhile true; do\n  report_df\n  report_du\n  report_top\n  report_free\n  report_iotop\n  report_sensors\n  report_net_dev\n  report_vcgencmd_clock\n  report_vcgencmd_temp\n  report_diskstats\n  report_nvidia_smi\n  report_intel_gpu_top\n  post_lines_to_influxdb\n  pause_depending_on_rpi_model\ndone\n</code></pre>"},{"location":"projects/conmon/#multi-thread","title":"Multi-thread","text":"<p>To run in slow systems such as Raspberry Pi computers, including any one from the  Zero W (v1) to the  4 model B, the single-thread version below (<code>conmon-st</code>) is preferred, with a sampling delay based on each Raspberry CPU to avoid interfering with other processes.</p>"},{"location":"projects/conmon/#deploy-to-pcs","title":"<code>deploy-to-pcs</code>","text":"<p>In environments with multiple (desktop / server) computers, it may be useful to use this script (<code>deploy-to-pcs</code>) to deploy the latest version of the script to all computers.</p> deploy-to-pcs<pre><code>#!/bin/bash\n#\n# Deplay conmon-mt to PC hosts.\n\nfor host in octavo cubito super-tuna computer smart-computer lexicon rapture; do\n  if nc 2&gt;&amp;1 -zv ${host} 22 | grep -q succeeded; then\n    echo \"Deploying to ${host} ...\"\n    scp 2&gt;/dev/null \\\n      -qr \\\n      ../conmon \\\n      root@${host}:\n    ssh 2&gt;/dev/null \\\n      root@${host} \\\n      \"cp /root/conmon/conmon-mt /usr/local/bin/conmon\"\n    ssh 2&gt;/dev/null \\\n      root@${host} \\\n      \"systemctl restart conmon.service\"\n    etc_influxdb_auth=/etc/conmon/influxdb-auth\n    for src_influxdb_auth in /etc/conmon/influxdb-auth \"${HOME}/.conmon-influxdb-auth\"; do\n      if [ -f $src_influxdb_auth ]; then\n        ssh 2&gt;/dev/null \\\n          root@${host} \\\n          \"mkdir -p $(dirname $etc_influxdb_auth)\"\n        scp 2&gt;/dev/null \\\n          -qr \\\n          $src_influxdb_auth \\\n          root@${host}:$etc_influxdb_auth\n        ssh 2&gt;/dev/null \\\n          root@${host} \\\n          \"chmod 400 $etc_influxdb_auth\"\n      fi\n    done\n  fi\ndone\n</code></pre>"},{"location":"projects/conmon/#conmon-mt","title":"<code>conmon-mt</code>","text":"conmon-mt<pre><code>#!/bin/bash\n#\n# Export system monitoring metrics to influxdb.\n\n# InfluxDB target.\nDBNAME=monitoring\nTARGET_HTTP='' # Leave empty to skip.\nTARGET_HTTPS='http://octavo:30086'\n\n# Default delay between each POST request to InfluxDB.\nDELAY_POST=4\n\n# Default delay between each round of \"fast\" metrics.\nDELAY_FAST=2\n\n# Default delay between each round of top (neither fast nor slow).\nDELAY_TOP=2\n\n# Default delay between each round of slow metrics (e.g. I/O bound).\nDELAY_SLOW=300\n\n# Data file for batch POST.\nDDIR=\"/dev/shm/$$\"\nDATA=\"${DDIR}/DATA.txt\"\nmkdir -p \"${DDIR}\"\n\nhost=$(hostname)\n\ntimestamp_ns() {\n  date +'%s%N'\n}\n\nstore_line() {\n  # Write a line of data to the temporary in-memory file.\n  # Exit immediately if this fails.\n  echo $1 &gt;&gt;\"${DATA}\" || exit 1\n}\n\nreport_top_per_process() {\n  # Per-process CPU(%) &amp; MEM(bytes) usage.\n  # Re-use the ptop file created by report_top().\n  ts=$1\n  ptop=$2\n  awk '{print $4}' \"${ptop}\" | sort -u | while read cmd; do\n    user=$(grep \" ${cmd}\\$\" ${ptop} | cut -f1 -d' ' | sort -u | head -1)\n    # CPU per proccess, only when &gt; 0\n    cpu=$(grep \" ${cmd}\\$\" ${ptop} | cut -f2 -d' ' | grep -v '^0\\.0$' | tr '\\n' '+' | sed 's/+$/\\n/' | bc -ql)\n    if [[ ! -z \"${cpu}\" ]]; then\n      store_line \"top_cpu,host=${host},user=${user},command=${cmd} value=${cpu} ${ts}\"\n    fi\n    # Memory per proccess, only when &gt; 1%\n    mem=$(grep \" ${cmd}\\$\" ${ptop} | cut -f3 -d' ' | grep -v '^0\\.0$' | tr '\\n' '+' | sed 's/+$/\\n/' | bc -ql | sed 's/^\\./0./')\n    # Multiply %mem by total system RAM.\n    tot=$(free -b | grep 'Mem:' | awk '{print $2}')\n    ram=$(bc -ql &lt;&lt;&lt;\"${tot} * ${mem} / 100\")\n    if [[ ! -z \"${ram}\" ]]; then\n      store_line \"top_mem,host=${host},user=${user},command=${cmd} value=${ram} ${ts}\"\n    fi\n  done\n  rm -f \"${ptop}\"\n}\n\nreport_cpufreq() {\n  # CPU frequency.\n  # Depends on: bc.\n  while true; do\n    top=\"${DDIR}/cpufreq\"\n    ts=$(timestamp_ns)\n    ncpus=$(grep -c 'cpu MHz' /proc/cpuinfo)\n    cpufreq=$(echo \"($(grep 'cpu MHz' /proc/cpuinfo | sed 's/.*: //' | tr '\\n' +)0)/$ncpus\" | bc -ql)\n    store_line \"cpufreq,host=${host} value=${cpufreq} ${ts}\"\n    sleep ${DELAY_FAST}\n  done\n}\n\nreport_top() {\n  # Stats from top: CPU (overall and per process) and RAM (per process).\n  # Depends on: top, free.\n  while true; do\n    top_cmd=$(command -v top)\n    top=\"${DDIR}/top\"\n    ts=$(timestamp_ns)\n    ${top_cmd} -b -c -n 1 -w 512 |\n      grep -vE ' 0\\..   0\\..|^top|^Tasks|^%|^MiB|^$|[[:blank:]]*PID USER' |\n      awk '{print $2,$9,$10,$12,$13,$14,$15,$16,$17,$18,$19,$20}' |\n      tr '\\\\' '/' |\n      sed 's/\\.minecraft\\/bin\\/[0-9a-f]\\+.*/minecraft/' |\n      sed 's/[C-Z]:\\/.*\\/\\([a-zA-Z0-9 _-]\\+\\.[a-z][a-z][a-z]\\).*/\\1/' |\n      sed 's/\\/[^ ]*\\///' | sed 's/\\(bash\\|sh\\|python\\|python3\\) .*\\///' |\n      tr -d '[' |\n      tr -d ']' |\n      awk '{print $1,$2,$3,$4}' |\n      tr ',' '.' &gt;\"${top}\"\n    # Total CPU(%) usage.\n    cpu_load=$(awk '{print $2}' ${top} | grep -v '^0\\.0$' | tr '\\n' '+' | sed 's/+$/\\n/' | bc -ql)\n    store_line \"top,host=${host} value=${cpu_load} ${ts}\"\n    # Launch the slower per-process metrics in the background.\n    ptop=\"${top}.$RANDOM\"\n    mv \"${top}\" \"${ptop}\"\n    report_top_per_process \"${ts}\" \"${ptop}\" &amp;\n    sleep ${DELAY_TOP}\n  done\n}\n\nreport_vcgencmd() {\n  # Raspberry Pi CPU temperature.\n  # Depends on: vcgencmd.\n  vcgencmd=$(command -v vcgencmd)\n  if [ -z \"${vcgencmd}\" ] || [ ! -f \"${vcgencmd}\" ]; then\n    return\n  fi\n  while true; do\n    ts=$(timestamp_ns)\n    cpu_temp=$(${vcgencmd} measure_temp | cut -f2 -d= | cut -f1 -d\"'\")\n    store_line \"vcgencmd,metric=temp,host=${host} value=${cpu_temp} ${ts}\"\n    if grep -q 'Pi 4' /proc/device-tree/model; then\n      ts=$(timestamp_ns)\n      pmic_temp=$(${vcgencmd} measure_temp pmic | cut -f2 -d= | cut -f1 -d\"'\")\n      store_line \"vcgencmd,metric=temp_pmic,host=${host} value=${pmic_temp} ${ts}\"\n    fi\n    cat \"${DATA}\" | cut -f1 -d, | sort | uniq -c\n    sleep ${DELAY_FAST}\n  done\n}\n\nreport_sensors() {\n  # CPU, SSD, NVMe temperatures and other sensors (if available).\n  # Depends on: jq, sensors.\n  jq=$(command -v jq)\n  if [ -z \"${jq}\" ] || [ ! -f \"${jq}\" ]; then\n    return\n  fi\n  sensors=$(command -v sensors)\n  if [ -z \"${sensors}\" ] || [ ! -f \"${sensors}\" ]; then\n    return\n  fi\n  sensors_json=\"${DDIR}/sensors\"\n  while true; do\n    ts=$(timestamp_ns)\n    \"${sensors}\" -j &gt;\"${sensors_json}\"\n    $jq 'keys' \"${sensors_json}\" | grep '^  \"' | cut -f2 -d'\"' | while read adapter; do\n      echo \"adapter: $adapter\"\n      $jq \".\\\"${adapter}\\\"\" \"${sensors_json}\" | $jq 'keys' | grep '^  \"' | grep -v '\"Adapter\"' | cut -f2 -d'\"' | while read name; do\n        key=$($jq \".\\\"${adapter}\\\".\\\"${name}\\\"\" \"${sensors_json}\" | $jq 'keys' | grep '^  \"' | grep '_input\"' | cut -f2 -d'\"')\n        value=$($jq \".\\\"${adapter}\\\".\\\"${name}\\\".\\\"${key}\\\"\" \"${sensors_json}\")\n        store_line \"sensors,host=${host},adapter=${adapter},name=${name/ /_} value=${value} ${ts}\"\n      done\n    done\n    sleep ${DELAY_FAST}\n  done\n}\n\nreport_intel_gpu_top() {\n  # Intel GPU.\n  # Depends on: intel_gpu_top (as root).\n  intel_gpu_top=$(command -v intel_gpu_top)\n  if [ -z $intel_gpu_top ] || [ ! -f $intel_gpu_top ]; then\n    return\n  fi\n  while true; do\n    ts=$(timestamp_ns)\n    # TODO: find out why this works only when running in a console.\n    timeout 1s sudo ${intel_gpu_top} -c | head -2 &gt;${intel_gpu_csv}\n    for N in $(seq 18); do\n      metric=$(awk -v cn=$N -F',' '{print $cn}' ${intel_gpu_csv} | head -1)\n      value=$(awk -v cn=$N -F',' '{print $cn}' ${intel_gpu_csv} | tail -1)\n      if echo \"$value\" | egrep -q '^0\\.0+$'; then continue; fi\n      echo \"intel_gpu,host=${host},metric=${metric/ /_} value=${value} ${ts}\"\n      store_line \"intel_gpu,host=${host},metric=${metric/ /_} value=${value} ${ts}\"\n    done\n    sleep ${DELAY_FAST}\n  done\n}\n\nreport_nvidia_smi() {\n  # NVidia GPU.\n  # Depends on: nvidia-smi\n  nvidia_smi=$(command -v nvidia-smi)\n  if [ -z \"${nvidia_smi}\" ] || [ ! -f \"${nvidia_smi}\" ]; then\n    return\n  fi\n  while true; do\n    ts=$(timestamp_ns)\n    temp=$(${nvidia_smi} -i 0 --query-gpu=temperature.gpu --format=csv,noheader)\n    util=$(${nvidia_smi} -i 0 --query-gpu=utilization.gpu --format=csv,noheader | cut -f1 -d' ')\n    vram=$(${nvidia_smi} -i 0 --query-gpu=memory.used --format=csv,noheader | cut -f1 -d' ')\n    draw=$(${nvidia_smi} -i 0 --query-gpu=power.draw --format=csv,noheader | cut -f1 -d' ')\n    fans=$(${nvidia_smi} -i 0 --query-gpu=fan.speed --format=csv,noheader | cut -f1 -d' ')\n    store_line \"nvidia_smi,host=${host},metric=temperature value=${temp} ${ts}\"\n    store_line \"nvidia_smi,host=${host},metric=utilization value=${util} ${ts}\"\n    store_line \"nvidia_smi,host=${host},metric=memory value=${vram} ${ts}\"\n    store_line \"nvidia_smi,host=${host},metric=power value=${draw} ${ts}\"\n    store_line \"nvidia_smi,host=${host},metric=fan value=${fans} ${ts}\"\n    sleep ${DELAY_FAST}\n  done\n}\n\nreport_free() {\n  # Stats from free: RAM used, buffered/cached, free.\n  # Depends on: free.\n  while true; do\n    ts=$(timestamp_ns)\n    mem_used=$(free -b | grep 'Mem:' | awk '{print $3}')\n    mem_free=$(free -b | grep 'Mem:' | awk '{print $4}')\n    mem_buff=$(free -b | grep 'Mem:' | awk '{print $6}')\n    store_line \"free,host=${host},metric=used value=${mem_used} ${ts}\"\n    store_line \"free,host=${host},metric=free value=${mem_free} ${ts}\"\n    store_line \"free,host=${host},metric=buff_cache value=${mem_buff} ${ts}\"\n    sleep ${DELAY_FAST}\n  done\n}\n\nreport_df() {\n  # Stats from df: used/free space per file system.\n  # Depends on: df.\n  while true; do\n    ts=$(timestamp_ns)\n    df -k | egrep -v '/run|/pods|udev|loop|/sys|/run|/dev$' |\n      grep ' /' | awk '{print $4,$5,$6}' |\n      while read line; do\n        # Note free space is given in 1KB blocks.\n        fs_path=$(echo \"${line}\" | cut -f3 -d' ')\n        fs_used=$(echo \"${line}\" | cut -f2 -d' ' | cut -f1 -d'%')\n        fs_free=$(echo \"${line}\" | cut -f1 -d' ')\n        fs_free=$((1024 * fs_free))\n        store_line \"df,host=${host},path=${fs_path},metric=tot_free value=${fs_free} ${ts}\"\n        store_line \"df,host=${host},path=${fs_path},metric=pct_used value=${fs_used} ${ts}\"\n      done\n    sleep ${DELAY_FAST}\n  done\n}\n\nreport_diskstats() {\n  # Disk I/O stats from /proc/diskstats.\n  # Depends on: /proc/diskstats.\n  while true; do\n    ts=$(timestamp_ns)\n    # The /proc/diskstats file displays the I/O statistics of block devices.\n    # Each line contains the following fields (and more, omitted here):\n    #  1  major number\n    #  2  minor mumber\n    #  3  device name\n    #  4  reads completed successfully\n    #  5  reads merged\n    #  6  sectors read\n    #  7  time spent reading (ms)\n    #  8  writes completed\n    #  9  writes merged\n    # 10  sectors written\n    # 11  time spent writing (ms)\n    # Source: https://www.kernel.org/doc/Documentation/ABI/testing/procfs-diskstats\n    grep -v ' 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0' /proc/diskstats | egrep -v 'loop|[hs]d[a-z][1-9]|k0p[0-9]' | awk '{print $3,$4,$6,$7,$8,$10,$11}' | sort &gt;$DDIR/disk1\n    if [[ -f \"${DDIR}/disk0\" ]]; then\n      # Columns used: disk reads rsect rtime writs wsect wtime\n      # disk:   3  device name\n      # reads:  4  reads completed successfully\n      # rsect:  6  sectors read\n      # rtime:  7  time spent reading (ms)\n      # writs:  8  writes completed\n      # wsect: 10  sectors written\n      # wtime: 11  time spent writing (ms)\n      paste \"${DDIR}/disk0\" \"${DDIR}/disk1\" |\n        while read \\\n          disk0 reads0 rsect0 rtime0 writs0 wsect0 wtime0 \\\n          disk1 reads1 rsect1 rtime1 writs1 wsect1 wtime1; do\n          # Skip momentary inconsistencies when disks are added or removed.\n          if [ \"${disk0}\" == \"${disk1}\" ]; then\n            disk=$disk0\n            # Assume 512-byte sectors.\n            rsect=$((512 * (rsect1 - rsect0)))\n            wsect=$((512 * (wsect1 - wsect0)))\n            store_line \"diskstats,host=${host},device=${disk},metric=bytes_read    value=${rsect} ${ts}\"\n            store_line \"diskstats,host=${host},device=${disk},metric=bytes_written value=${wsect} ${ts}\"\n            # Successful I/O ops.\n            reads=$((reads1 - reads0))\n            writs=$((writs1 - writs0))\n            store_line \"diskstats,host=${host},device=${disk},metric=successful_reads  value=${reads} ${ts}\"\n            store_line \"diskstats,host=${host},device=${disk},metric=successful_writes value=${writs} ${ts}\"\n            # Time spent.\n            read_ms=$((rtime1 - rtime0))\n            writ_ms=$((wtime1 - wtime0))\n            store_line \"diskstats,host=${host},device=${disk},metric=time_spent_reads  value=${read_ms} ${ts}\"\n            store_line \"diskstats,host=${host},device=${disk},metric=time_spent_writes value=${writ_ms} ${ts}\"\n          fi\n        done\n    fi\n    mv \"${DDIR}/disk1\" \"${DDIR}/disk0\"\n    sleep ${DELAY_FAST}\n  done\n}\n\nbytes_from_value_and_units() {\n  value=$1\n  units=$2\n  factor=1\n  case \"${units}\" in\n  \"K/s\")\n    factor=1024\n    ;;\n  \"M/s\")\n    factor=1024*1024\n    ;;\n  \"G/s\")\n    factor=1024*1024*1024\n    ;;\n  \"T/s\")\n    factor=1024*1024*1024*1024\n    ;;\n  esac\n  bc -ql &lt;&lt;&lt;\"${factor} * ${value}\"\n}\n\nreport_iotop() {\n  # I/O stats from iotop (per process).\n  # Depends on: iotop (as root).\n  iotop=$(command -v iotop)\n  if [ -z $iotop ] || [ ! -f $iotop ]; then\n    return\n  fi\n  while true; do\n    ts=$(timestamp_ns)\n    iotop_stats=$DDIR/iotop\n    # The Linux kernel interfaces that iotop relies on now require root\n    # privileges or the NET_ADMIN capability. This change occurred because a\n    # security issue (CVE-2011-2494) was found that allows leakage of sensitive\n    # data across user boundaries. If you require the ability to run iotop as a\n    # non-root user, please configure sudo to allow you to run iotop as root.\n    # WARNING: using -n 1 results in only zero values all the time.\n    sudo $iotop -b -P -n 2 |\n      grep -Ev 'task_delayacct|locale|DISK READ|0.00 B/s    0.00 B/s' |\n      tr -d '[' |\n      tr -d ']' |\n      sed 's/ %//g' |\n      sed 's/kworker\\///' |\n      sed 's/u64:[0-9]\\+-//' |\n      awk '{print $4,$5,$6,$7,$10}' \\\n        &gt;\"${iotop_stats}\"\n    # Return if iotop could not be run successfully.\n    # This is typically caused by 'Netlink error: Operation not permitted'.\n    if [[ $? -gt 0 ]]; then\n      return\n    fi\n    # Report I/O stats for each process.\n    # Note: this aggregates I/O per process name (basename), across all PIDs.\n    cat \"${iotop_stats}\" |\n      cut -f5 -d' ' |\n      sort -u |\n      while read -r process; do\n        process_read=0\n        process_write=0\n        while read -r line; do\n          read_value=$(echo \"${line}\" | cut -f1 -d' ')\n          read_units=$(echo \"${line}\" | cut -f2 -d' ')\n          read_bytes=$(bytes_from_value_and_units \"${read_value}\" \"${read_units}\")\n          process_read=$(bc -ql &lt;&lt;&lt;\"${process_read}+${read_bytes}\")\n          write_value=$(echo \"${line}\" | cut -f3 -d' ')\n          write_units=$(echo \"${line}\" | cut -f4 -d' ')\n          write_bytes=$(bytes_from_value_and_units \"${write_value}\" \"${write_units}\")\n          process_write=$(bc -ql &lt;&lt;&lt;\"${process_write}+${write_bytes}\")\n        done &lt; &lt;(grep \" ${process}\\$\" \"${iotop_stats}\")\n        if [[ $(echo \"${process_read}\" | sed 's/\\..*//') -gt 0 ]]; then\n          store_line \"iotop,host=${host},command=${process},metric=read value=${process_read} ${ts}\"\n        fi\n        if [[ \"$(echo \"${process_write}\" | sed 's/\\..*//')\" -gt 0 ]]; then\n          store_line \"iotop,host=${host},command=${process},metric=write value=${process_write} ${ts}\"\n        fi\n      done\n    sleep ${DELAY_FAST}\n  done\n}\n\nreport_net_dev() {\n  # Network I/O stats from /proc/net/dev.\n  # Depends on: /proc/net/dev.\n  while true; do\n    ts=$(timestamp_ns)\n    grep -Ev 'Inter|face' /proc/net/dev | tr -d ':' | awk '{print $1,$2,$10}' | sort &gt;\"${DDIR}/net1\"\n    if [[ -f \"${DDIR}/net0\" ]]; then\n      ts0=$(cat \"${DDIR}/net0-ts\")\n      echo \"${ts}\" &gt;\"${DDIR}/net1-ts\"\n      paste \"${DDIR}/net0\" \"${DDIR}/net1\" | while read -r dev0 rx0 tx0 dev1 rx1 tx1; do\n        # Skip momentary inconsistencies when devs are added or removed.\n        if [[ \"${dev0}\" == \"${dev1}\" ]]; then\n          # Compute rx/tx bytes / sec (ts is in nanoseconds).\n          rx=$(bc -ql &lt;&lt;&lt;\"1000000000 * (${rx1} - ${rx0}) / (${ts} - ${ts0})\")\n          tx=$(bc -ql &lt;&lt;&lt;\"1000000000 * (${tx1} - ${tx0}) / (${ts} - ${ts0})\")\n          store_line \"net_dev,host=${host},device=${dev0},metric=rx value=${rx} ${ts}\"\n          store_line \"net_dev,host=${host},device=${dev0},metric=tx value=${tx} ${ts}\"\n        fi\n      done\n    fi\n    mv \"${DDIR}/net1\" \"${DDIR}/net0\"\n    mv \"${DDIR}/net1-ts\" \"${DDIR}/net0-ts\"\n    sleep ${DELAY_FAST}\n  done\n}\n\nreport_du() {\n  # Disk usage stats from du.\n  # Files and directories to monitor in /etc/conmon/du\n  # Depends on du (as root).\n  if [ -z /etc/conmon/du ] || [ ! -f $/etc/conmon/du ]; then\n    return\n  fi\n  while true; do\n    ts=$(timestamp_ns)\n    while read -r path; do\n      kbytes=$(sudo du -s \"${path}\" | awk '{print $1}')\n      bytes=$((1024 * kbytes))\n      store_line \"du,host=${host},path=${path} value=${bytes} ${ts}\"\n    done &lt;/etc/conmon/du\n    sleep ${DELAY_SLOW}\n  done\n}\n\npost_lines_to_influxdb() {\n  # POST data to InfluxDB in batch, when targets are available.\n  # Depends on: curl, nc.\n  # All other tasks write data to the file in append mode (&gt;&gt;).\n  # This task reads everything at once and immediately deletes the file.\n  # This makes all the other tasks write to the same file, created anew.\n  # To avoid losing data between reading and delete the file, rename it,\n  # wait a little (DELAY_FAST) for the writes of tasks that already had\n  # it open, then other tasks will have to write to a new file.\n  while true; do\n    sleep ${DELAY_POST}\n    mv -f \"${DATA}\" \"${DATA}.POST\"\n    # Post over HTTP without auth.\n    if [ -n \"$TARGET_HTTP\" ]; then\n      host_and_port=$(echo \"$TARGET_HTTP\" | sed 's/.*\\///' | tr : ' ')\n      if nc 2&gt;&amp;1 -zv $host_and_port | grep -q succeeded; then\n        curl &gt;/dev/null 2&gt;/dev/null \\\n          -i -XPOST \"${TARGET_HTTP}/write?db=${DBNAME}\" \\\n          --data-binary @\"${DATA}.POST\"\n      fi\n    fi\n    # Post over HTTPS with Basic Auth, provided credentials are\n    # found in /etc/conmon/influxdb-auth\n    if [ -n \"$TARGET_HTTPS\" ]; then\n      influxdb_auth=/etc/conmon/influxdb-auth\n      if [ -z $influxdb_auth ] || [ ! -f $influxdb_auth ]; then\n        return\n      fi\n      host_and_port=$(echo \"$TARGET_HTTPS\" | sed 's/.*\\///' | tr : ' ')\n      if nc 2&gt;&amp;1 -zv $host_and_port | grep -q succeeded; then\n        curl &gt;/dev/null 2&gt;/dev/null \\\n          -u $(sudo cat $influxdb_auth) \\\n          -i -XPOST \"${TARGET_HTTPS}/write?db=${DBNAME}\" \\\n          --data-binary @\"${DATA}.POST\"\n      fi\n    fi\n  done\n}\n\n# Run all the above tasks in parallel.\n# Each task is responsible of its own checks and sampling ratio / delay.\nreport_df &amp;\nreport_du &amp;\nreport_top &amp;\nreport_free &amp;\nreport_iotop &amp;\nreport_cpufreq &amp;\nreport_sensors &amp;\nreport_net_dev &amp;\nreport_vcgencmd &amp;\nreport_diskstats &amp;\nreport_nvidia_smi &amp;\nreport_intel_gpu_top &amp;\npost_lines_to_influxdb &amp;\n\n# HACK: sleep for a while before leaving all the above tasks running in the\n# background. This is so that this can be ended with Ctrl+C.\nsleep 10000000000\n</code></pre>"},{"location":"projects/conmon/#additional-scripts","title":"Additional scripts","text":""},{"location":"projects/conmon/#conmon-speedtest","title":"<code>conmon-speedtest</code>","text":"conmon-speedtest<pre><code>#!/bin/bash\n\n# InfluxDB target.\nDBNAME=monitoring\nTARGET_HTTP='' # Leave empty to skip.\nTARGET_HTTPS='http://octavo:30086'\n\nhost=$(hostname)\n\n# Data file for batch POST.\nDATA=\"/dev/shm/$$.txt\"\n\ntmp=/dev/shm/speedtest\nspeedtest-cli --secure &gt;$tmp\nnetspd_up=$(grep -E 'Upload' $tmp | cut -f2 -d: | egrep -o '[0-9]+\\.[0-9]')\nnetspd_down=$(cat ${tmp} | egrep 'Download' | cut -f2 -d: | egrep -o '[0-9]+\\.[0-9]')\nnetspd_ping=$(cat ${tmp} | egrep 'ms$' | cut -f2 -d: | egrep -o '[0-9]+\\.[0-9]')\n\necho \"inet_up,host=${host} value=${netspd_up}\" &gt;&gt;\"$DATA\"\necho \"inet_down,host=${host} value=${netspd_down}\" &gt;&gt;\"$DATA\"\necho \"inet_ping,host=${host} value=${netspd_ping}\" &gt;&gt;\"$DATA\"\n\n# POST all data points in one batch request.\n# Post over HTTP without auth.\nif [ -n \"$TARGET_HTTP\" ]; then\n    host_and_port=$(echo $TARGET_HTTP | sed 's/.*\\///' | tr : ' ')\n    if nc 2&gt;&amp;1 -zv $host_and_port | grep -q succeeded; then\n        curl &gt;/dev/null 2&gt;/dev/null \\\n            -i -XPOST \"${TARGET_HTTP}/write?db=${DBNAME}\" \\\n            --data-binary @\"${DATA}\"\n    fi\nfi\n# Post over HTTPS with Basic Auth, provided credentials are\n# found in /etc/conmon/influxdb-auth\nif [ -n \"$TARGET_HTTPS\" ]; then\n    influxdb_auth=/etc/conmon/influxdb-auth\n    if [ ! -f $influxdb_auth ]; then\n        exit 1\n    fi\n    host_and_port=$(echo $TARGET_HTTPS | sed 's/.*\\///' | tr : ' ')\n    if nc 2&gt;&amp;1 -zv $host_and_port | grep -q succeeded; then\n        curl &gt;/dev/null 2&gt;/dev/null \\\n            -u $(cat $influxdb_auth) \\\n            -i -XPOST \"${TARGET_HTTPS}/write?db=${DBNAME}\" \\\n            --data-binary @\"${DATA}\"\n    fi\nfi\nrm -f \"${DATA}\"\n</code></pre>"},{"location":"projects/conmon/#conmon-mystrom","title":"<code>conmon-mystrom</code>","text":"conmon-mystrom<pre><code>#!/bin/bash\n#\n# Export system monitoring metrics to influxdb.\n\n# InfluxDB target.\nDBNAME=monitoring\nTARGET_HTTP='' # Leave empty to skip.\nTARGET_HTTPS='http://octavo:30086'\n\n# MyStrom switches.\ndeclare -A switches=(\n    [\"office\"]=\"192.168.0.191\"\n)\n\n# Default delay between each POST request to InfluxDB.\nDELAY_POST=4\n\n# Data file for batch POST.\nDDIR=\"/dev/shm/$$\"\nDATA=\"${DDIR}/DATA.txt\"\nmkdir -p \"${DDIR}\"\n\nhost=$(hostname)\n\ntimestamp_ns() {\n    date +'%s%N'\n}\n\nstore_line() {\n    # Write a line of data to the temporary in-memory file.\n    # Exit immediately if this fails.\n    echo $1 &gt;&gt;\"${DATA}\" || exit 1\n}\n\npost_lines_to_influxdb() {\n    # POST data to InfluxDB in batch, when target is available.\n    # Depends on: nc.\n    # All other tasks write data to the file in append mode (&gt;&gt;).\n    # This task reads everything at once and immediately deletes the file.\n    # This makes all the other tasks write to the same file, created anew.\n    # To avoid losing data between reading and delete the file, rename it,\n    # wait a little (DELAY_FAST) for the writes of tasks that already had\n    # it open, then other tasks will have to write to a new file.\n    # Post over HTTP without auth.\n    sleep ${DELAY_POST}\n    if [ -n \"$TARGET_HTTP\" ]; then\n        host_and_port=$(echo $TARGET_HTTP | sed 's/.*\\///' | tr : ' ')\n        if nc 2&gt;&amp;1 -zv $host_and_port | grep -q succeeded; then\n            curl &gt;/dev/null 2&gt;/dev/null \\\n                -i -XPOST \"${TARGET_HTTP}/write?db=${DBNAME}\" \\\n                --data-binary @\"${DATA}\"\n        fi\n    fi\n    # Post over HTTPS with Basic Auth, provided credentials are\n    # found in /etc/conmon/influxdb-auth\n    if [ -n \"$TARGET_HTTPS\" ]; then\n        influxdb_auth=/etc/conmon/influxdb-auth\n        if [ -z $influxdb_auth ] || [ ! -f $influxdb_auth ]; then\n            return\n        fi\n        host_and_port=$(echo $TARGET_HTTPS | sed 's/.*\\///' | tr : ' ')\n        if nc 2&gt;&amp;1 -zv $host_and_port | grep -q succeeded; then\n            curl &gt;/dev/null 2&gt;/dev/null \\\n                -u $(cat $influxdb_auth) \\\n                -i -XPOST \"${TARGET_HTTPS}/write?db=${DBNAME}\" \\\n                --data-binary @\"${DATA}\"\n        fi\n    fi\n}\n\n# myStrom WiFi switch.\n# API: https://api.mystrom.ch/\n# Depends on: curl.\nwhile true; do\n    for name in \"${!switches[@]}\"; do\n        ts=$(timestamp_ns)\n        switch_ip=\"${switches[$name]}\"\n        json=$(curl 2&gt;/dev/null http://${switch_ip}/report)\n        for metric in Ws power temperature; do\n            value=$(echo ${json} | grep -o \".${metric}.:[^,}]*\" | cut -f2 -d:)\n            store_line \"mystrom,switch=${name},metric=${metric} value=${value} ${ts}\"\n        done\n    done\n    post_lines_to_influxdb\ndone\n</code></pre>"},{"location":"projects/conmon/#conmon-tapopy","title":"<code>conmon-tapo.py</code>","text":"<p>Continuous Monitoring for TP-Link Tapo devices explains this script and its dependencies in more details.</p> <pre><code>#!/usr/bin/env python3\n\n\"\"\"Script to poll TAPO devices for monitoring data and post it to InfluxDB.\n\nRun conmon-tapo --help to see all available options.\n\nUsage:\n  conmon-tapo [options]\n\nRequires:\n  absl-py\n  https://github.com/abseil/abseil-py\n\n  tapo\n  https://github.com/mihai-dinculescu/tapo/\n\"\"\"\n\nimport asyncio\nimport json\nimport os\nimport socket\nimport sys\nimport time\nimport yaml\n\nfrom absl import app, flags\nfrom datetime import datetime\nfrom influxdb import InfluxDBClient\n\nfrom tapo import ApiClient\nfrom tapo.requests import EnergyDataInterval\nfrom tapo.responses import T31XResult\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\n    \"config\",\n    \"/etc/conmon/tapo.yaml\",\n    \"Configuration file with settings for InfluxDB and Tapo devices.\",\n    short_name=\"c\"\n)\n\n\ndef load_config(filepath):\n    with open(filepath) as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)\n    # Override values from environment variables.\n    for section in (\"influxdb\", \"tapo_auth\"):\n        for variable in config[section]:\n            if variable[-8:] in (\"username\", \"password\"):\n                env_value = os.getenv(config[section][variable])\n                if env_value:\n                    config[section][variable] = env_value\n    return config\n\n\nasync def fetch_reports(config):\n    client = ApiClient(**config[\"tapo_auth\"])\n    reports = []\n    for device in config[\"devices\"]:\n        model = device[\"model\"]\n        report=dict(model=model)\n        try:\n            if model == \"H100\":\n                device_conn = await client.h100(device[\"ip\"])\n                device_info = await device_conn.get_device_info()\n                report=dict(\n                    model=device_info.to_dict().get(\"model\"),\n                    nickname=device_info.to_dict().get(\"nickname\")\n                )\n                children=[]\n                child_device_list = await device_conn.get_child_device_list()\n                for child in child_device_list:\n                    if isinstance(child, T31XResult):\n                        t315 = await device_conn.t315(device_id=child.device_id)\n                        children.append(dict(\n                            model=\"T315\",\n                            nickname=child.nickname,\n                            humidity=child.current_humidity,\n                            temperature=child.current_temperature\n                        ))\n                report.update(dict(children=children))\n            elif model in (\"P110\", \"P115\"):\n                device_conn = await client.p110(device[\"ip\"])\n                device_info = await device_conn.get_device_info()\n                energy_usage = await device_conn.get_energy_usage()\n                report=dict(\n                    model=device_info.to_dict().get(\"model\"),\n                    nickname=device_info.to_dict().get(\"nickname\"),\n                    current_power=float(energy_usage.to_dict().get(\"current_power\")/1000)\n                )\n        except:\n            print(\"Could not get data from %s on %s \" % (\n                device[\"model\"], device[\"ip\"]))\n        else:\n            reports.append(report)\n    return reports\n\n\ndef always_on_reports(config):\n    reports = []\n    if \"always_on\" not in config:\n      return reports\n    for device in config[\"always_on\"]:\n        reports.append(dict(\n            model=\"P115\",\n            nickname=device[\"name\"],\n            current_power=float(device[\"power\"])\n        ))\n    return reports\n\n\ndef json_body_point(measurement, value, ts, tags):\n    tags.update(dict(host=socket.gethostname()))\n    return {\n      \"measurement\": measurement,\n      \"tags\": tags,\n      \"time\": ts,\n      \"fields\": {\n        \"value\": value\n      }\n    }\n\n\ndef json_body_points_from_report(report, ts):\n    json_body = []\n    tags = dict(model=report[\"model\"], nickname=report[\"nickname\"])\n    for field in report:\n        if field in (\"model\", \"nickname\"): continue\n        json_body.append(json_body_point(\"tapo_%s\" % field, report[field], ts, tags))\n    return json_body\n\n\ndef post_reports(config, reports):\n    client = InfluxDBClient(**config[\"influxdb\"])\n    json_body = []\n    ts = int(1000000000 * time.mktime(time.localtime()))\n    for report in reports:\n        if \"children\" in report:\n            for child in report[\"children\"]:\n                json_body.extend(json_body_points_from_report(child, ts))\n        else:\n            json_body.extend(json_body_points_from_report(report, ts))\n    client.write_points(json_body)\n\n\ndef main(argv):\n    config = load_config(FLAGS.config)\n    reports = asyncio.run(fetch_reports(config))\n    reports.extend(always_on_reports(config))\n    post_reports(config, reports)\n\n\nif __name__ == \"__main__\":\n    app.run(main)\n</code></pre>"},{"location":"projects/conmon/#tapoyaml","title":"<code>tapo.yaml</code>","text":"<pre><code>always_on:\n  - name: \"Dehumidifier\"\n    power: \"250\"\n  - name: \"PC\"\n    power: \"150\"\ndevices:\n  - ip: \"192.168.0.115\"\n    model: \"P115\"\n  - ip: \"192.168.0.100\"\n    model: \"H100\"\ninfluxdb:\n  host: inf.ssl.uu.am\n  port: 443\n  database: \"home\"\n  username: \"INFLUXDB_USERNAME\"\n  password: \"INFLUXDB_PASSWORD\"\n  ssl: True\n  verify_ssl: True\ntapo_auth:\n  tapo_username: \"TAPO_USERNAME\"\n  tapo_password: \"TAPO_PASSWORD\"\n</code></pre>"},{"location":"projects/conmon/#python-dependencies","title":"Python dependencies","text":"<p>The <code>absl</code> and <code>influxdb</code> modules can be installed with<code>apt</code>:</p> <pre><code># apt install python3-absl python3-influxdb\n</code></pre> <p>The <code>tapo</code> module is more complicated to install. The recommended approach is to use virtualenv:</p> <code># apt install python3-venv -y</code> <pre><code># apt install python3-venv -y\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  python3-pip-whl python3-setuptools-whl python3.12-venv\nThe following NEW packages will be installed:\n  python3-pip-whl python3-setuptools-whl python3-venv python3.12-venv\n0 upgraded, 4 newly installed, 0 to remove and 3 not upgraded.\nNeed to get 2,425 kB of archives.\nAfter this operation, 2,777 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3-pip-whl all 24.0+dfsg-1ubuntu1.1 [1,703 kB]\nGet:2 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3-setuptools-whl all 68.1.2-2ubuntu1.1 [716 kB]\nGet:3 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3.12-venv amd64 3.12.3-1ubuntu0.3 [5,678 B]\nGet:4 http://archive.ubuntu.com/ubuntu noble-updates/universe amd64 python3-venv amd64 3.12.3-0ubuntu2 [1,034 B]\nFetched 2,425 kB in 1s (1,736 kB/s)      \nSelecting previously unselected package python3-pip-whl.\n(Reading database ... 425834 files and directories currently installed.)\nPreparing to unpack .../python3-pip-whl_24.0+dfsg-1ubuntu1.1_all.deb ...\nUnpacking python3-pip-whl (24.0+dfsg-1ubuntu1.1) ...\nSelecting previously unselected package python3-setuptools-whl.\nPreparing to unpack .../python3-setuptools-whl_68.1.2-2ubuntu1.1_all.deb ...\nUnpacking python3-setuptools-whl (68.1.2-2ubuntu1.1) ...\nSelecting previously unselected package python3.12-venv.\nPreparing to unpack .../python3.12-venv_3.12.3-1ubuntu0.3_amd64.deb ...\nUnpacking python3.12-venv (3.12.3-1ubuntu0.3) ...\nSelecting previously unselected package python3-venv.\nPreparing to unpack .../python3-venv_3.12.3-0ubuntu2_amd64.deb ...\nUnpacking python3-venv (3.12.3-0ubuntu2) ...\nSetting up python3-setuptools-whl (68.1.2-2ubuntu1.1) ...\nSetting up python3-pip-whl (24.0+dfsg-1ubuntu1.1) ...\nSetting up python3.12-venv (3.12.3-1ubuntu0.3) ...\nSetting up python3-venv (3.12.3-0ubuntu2) ...\n</code></pre> <pre><code>$ mkdir -p ~/.venvs\n$ python3 -m venv --system-site-packages ~/.venvs/tapo\n$ ~/.venvs/tapo/bin/python -m pip install tapo\nCollecting tapo\n  Using cached tapo-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nUsing cached tapo-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\nInstalling collected packages: tapo\nSuccessfully installed tapo-0.8.0\n\n$ ~/.venvs/tapo/bin/python ./conmon-tapo.py\n</code></pre> <p>The <code>tapo</code> module can also be installed the not recommended way: </p> <pre><code># pip install --break-system-packages tapo\nCollecting tapo\n  Using cached tapo-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nUsing cached tapo-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\nInstalling collected packages: tapo\nSuccessfully installed tapo-0.8.0\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n</code></pre>"},{"location":"projects/conmon/#device-ip-dependencies","title":"Device IP dependencies","text":"<p>The \"<code>conmon-tapo.py</code> script was initially writen on the assumption that Tapo devices would keep their IP address constant long-term, so if they change IP addresses the script crashes when trying to read the wrong fields, or simply not being able to connect to one of them:</p> Trying to reach a device at nobody's IP address <pre><code>$ ~/.venvs/tapo/bin/python ./conmon-tapo.py \nTraceback (most recent call last):\n  File \"/home/coder/src/conmon/./conmon-tapo.py\", line 138, in &lt;module&gt;\n    app.run(main)\n  File \"/usr/lib/python3/dist-packages/absl/app.py\", line 308, in run\n    _run_main(main, args)\n  File \"/usr/lib/python3/dist-packages/absl/app.py\", line 254, in _run_main\n    sys.exit(main(argv))\n            ^^^^^^^^^^\n  File \"/home/coder/src/conmon/./conmon-tapo.py\", line 132, in main\n    reports = asyncio.run(fetch_reports(config))\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n          ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/home/coder/src/conmon/./conmon-tapo.py\", line 83, in fetch_reports\n    device_conn = await client.p110(device[\"ip\"])\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nException: Http(reqwest::Error { kind: Request, url: \"http://192.168.0.41/app\", source: hyper_util::client::legacy::Error(Connect, ConnectError(\"tcp connect error\", Os { code: 113, kind: HostUnreachable, message: \"No route to host\" })) })\n</code></pre> Trying to reach a device at somebody else's IP address <pre><code>$ ~/.venvs/tapo/bin/python ./conmon-tapo.py \nTraceback (most recent call last):\n  File \"/home/coder/src/conmon/./conmon-tapo.py\", line 138, in &lt;module&gt;\n    app.run(main)\n  File \"/usr/lib/python3/dist-packages/absl/app.py\", line 308, in run\n    _run_main(main, args)\n  File \"/usr/lib/python3/dist-packages/absl/app.py\", line 254, in _run_main\n    sys.exit(main(argv))\n            ^^^^^^^^^^\n  File \"/home/coder/src/conmon/./conmon-tapo.py\", line 132, in main\n    reports = asyncio.run(fetch_reports(config))\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n          ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/home/coder/src/conmon/./conmon-tapo.py\", line 83, in fetch_reports\n    device_conn = await client.p110(device[\"ip\"])\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nException: Http(reqwest::Error { kind: Request, url: \"http://192.168.0.32/app\", source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(Io, Os { code: 104, kind: ConnectionReset, message: \"Connection reset by peer\" })) })\n</code></pre> Trying to read a P115 like it was an H100 <pre><code>$ ~/.venvs/tapo/bin/python ./conmon-tapo.py\nTraceback (most recent call last):\n  File \"/home/coder/src/conmon/./conmon-tapo.py\", line 138, in &lt;module&gt;\n    app.run(main)\n  File \"/usr/lib/python3/dist-packages/absl/app.py\", line 308, in run\n    _run_main(main, args)\n  File \"/usr/lib/python3/dist-packages/absl/app.py\", line 254, in _run_main\n    sys.exit(main(argv))\n            ^^^^^^^^^^\n  File \"/home/coder/src/conmon/./conmon-tapo.py\", line 132, in main\n    reports = asyncio.run(fetch_reports(config))\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n          ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n          ^^^^^^^^^^^^^^^\n  File \"/home/coder/src/conmon/./conmon-tapo.py\", line 65, in fetch_reports\n    device_info = await device_conn.get_device_info()\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nException: Serde(Error(\"missing field `in_alarm_source`\", line: 1, column: 822))\n</code></pre>"},{"location":"projects/self-hosting/","title":"Self-hosting","text":"<p>Self-hosting is the practice of hosting and managing applications on your own server(s).</p> <p>Source: awesome-selfhosted.</p>"},{"location":"projects/self-hosting/#why-because-i-can","title":"Why? Because I Can!","text":"<p>Self-Hosting is a choice. It is primarily about choice and control.</p> <p>Hosted services are still valuable as they provide access to content and tools that would otherwise be unavailable. Self-hosted services are not necessarily are replacement to those, most of the times they are an extension or addition.</p> <p>But sometimes it is nice to replace a hosted service with a self-hosted alternative; I find listening to audiobooks and podcasts substantially more enjoyable since I started using Audiobookshelf and eventually removed the Audible app since all I was getting out of it was annoying push notifications.</p>"},{"location":"projects/self-hosting/#not-a-question-of-price","title":"Not a Question of Price","text":"<p>Self-Hosting is a hobby, not a way to save money, or a business plan. Like every hobby, it is practiced for the joy of it within the limits allowed by restrictions impossed by the environment.</p> <p>The Cost of Self-Hosting is always a consideration, and it's not just the monetary cost; a significant amount of time goes into learning how to get things to work, and then later into dealing with things breaking down, because every thing breaks down eventually. For now, I am still happy to take that as an investment because I enjoy the learning process.</p> <p>Last but not least, Self-Hosting Isn't a Solution; It's A Patch.</p>"},{"location":"projects/self-hosting/#applications-installed","title":"Applications Installed","text":"<p>These are the applications that have been installed and used, at least enough to determine whether they are a good match for my intended purpose/s.</p>"},{"location":"projects/self-hosting/#used-often","title":"Used Often","text":"<p>These are the applications I find myself using often, most of them on a daily basis, otherwise at least once or twice a week.</p>"},{"location":"projects/self-hosting/#audiobookshelf","title":"Audiobookshelf","text":"<p>Audiobookshelf on Kubernetes may be the one application I use every single day, to listen to podcasts during the day and to audiobooks in the evening, sometimes also offline while traveling.</p>"},{"location":"projects/self-hosting/#continuous-monitoring","title":"Continuous Monitoring","text":"<p>Continuous Monitoring is also used very nearly on a daily basis. It started in early 2020 as an ad-hoc implementation of detailed system and process monitoring and 4 years later remained my preferred setup for monitoring with InfluxDB and Grafana on Kubernetes.</p> <p>Continuous Monitoring for TP-Link Tapo devices made this application a mission critical tool that I used all the time, every single day, for a few weeks every year.</p>"},{"location":"projects/self-hosting/#komga","title":"Komga","text":"<p>Self-hosted eBook library with Komga has made it substantially easier, and thus more likely, for me to read digital books. Not only me either, since some of the books were really purchased for the kids, having a central library we all can use, from any and every device, is a lot easier than sharing files in an inevitably more disorganized fashion.</p>"},{"location":"projects/self-hosting/#longhorn","title":"Longhorn","text":"<p>Longhorn is not an application but instead a storage drivers to support future expansion to an Active-Passive High Availability cluster.</p>"},{"location":"projects/self-hosting/#navidrome","title":"Navidrome","text":"<p>Self-hosted music streaming with Navidrome may not be perfect but it works quite well enough for listening to music while working or chilling out. I like to listen to the same music again and again anyway, I only buy albums from my favorite artists at Bandcamp and listen to them on an infinite loop.</p>"},{"location":"projects/self-hosting/#unifi-network-server","title":"UniFi Network Server","text":"<p>Migrating UniFi Controller to Kubernetes means no longer having to manually update the UniFi Network Server plus its dependencies (MongoDB and Java). The alternatives to self-hosting are pricey, starting at $15/month or $29/month depending on the provider.</p>"},{"location":"projects/self-hosting/#visual-studio-code-server","title":"Visual Studio Code Server","text":"<p>Running Visual Studio Code Server on Kubernetes was the first self-hosted service on the first single-node Kubernetes cluster on Ubuntu Server (lexicon). It remains frequently used for the ability to edit Kubernetes deployment files directly on the server, even after installing  Visual Studio Code on desktop PCs which does work better for developing for desktop PCs.</p>"},{"location":"projects/self-hosting/#used-occasionally","title":"Used Occasionally","text":"<p>These are the applications I still consider in use, even though use is less frequent. Most of them still get used on a weekly basis, otherwise at least once or twice per month.</p>"},{"location":"projects/self-hosting/#activitywatch","title":"ActivityWatch","text":"<p>Self-hosted time tracking with ActivityWatch is limited to the one machine where it is deployed, by design. This has made its usefulness somewhat limited but not as much as how hard it really is to categorize and aggregate \"activities\" into groups to represent real-life activities.</p>"},{"location":"projects/self-hosting/#home-assistant","title":"Home Assistant","text":"<p>Home Assistant on a Raspberry Pi 5 (<code>alfred</code>) should become a good replacement for the Continuous Monitoring for TP-Link Tapo devices, although that may yet take some more work.</p>"},{"location":"projects/self-hosting/#homepage","title":"Homepage","text":"<p>Homepage is a modern, highly customizable application dashboard that could be useful to have a big picture view of all services in one place, should there ever be too many of them. It was easy enough to install and setup in a couple of days during holidays, time will yet have to tell how much this get actually used.</p>"},{"location":"projects/self-hosting/#kubernetes-dashboard","title":"Kubernetes Dashboard","text":"<p>The built-in Kubernetes Dashboard was deployed as part of the  single-node Kubernetes cluster on Ubuntu Server (lexicon) and is a nice UI to see how the cluster is doing, although when it comes to root-causing problems for real it doesn't seem to provide quite enough details.</p>"},{"location":"projects/self-hosting/#jellyfin","title":"Jellyfin","text":"<p>Jellyfin on Kubernetes with Intel GPU turned out even better than expected, even though it required a fair bit of troubleshooting to get everything to work.</p> <p>It handles private videos, such as family videos and purchased video lectures, none of which would be found in a public database like IMDB, better than the (retired) Plex Media Server.</p>"},{"location":"projects/self-hosting/#not-really-used","title":"Not Really Used","text":"<p>These are applications I used for a bit or tried out, but then did not become frequently used.</p>"},{"location":"projects/self-hosting/#firefly-iii","title":"Firefly III","text":"<p>Self-hosted accountancy with Firefly III works well and feels agile enough to use, yet the most important ingredient to keep using such an application is perseverance; that's what I don't have.</p>"},{"location":"projects/self-hosting/#headless-steam","title":"Headless Steam","text":"<p>Headless Steam Service was installed and proved somewhat useful to casually play Steam games in the browser but is not really used for two reasons: 1. there is only one GPU in the cluster and Jellyfin needs it (and uses it); 2. games audio is not available outside of the LAN, which for me pretty much defeats the purpose of the whole thing.</p>"},{"location":"projects/self-hosting/#homebox","title":"Homebox","text":"<p>Self-hosted inventory with Homebox looks promising and easy enough to use, yet again without a good motivation to invest the hours to fill it in, there is only so much you can do with it. It will probably make more sense after establishing a criteria for what goes in, because it hardly makes sense to try and get it all in.</p>"},{"location":"projects/self-hosting/#minecraft-server","title":"Minecraft Server","text":"<p>Running Minecraft Java Server for Bedrock clients on Kubernetes is a convenient method to keep the Minecraft Java Edition server up to date and make it available to multiple kids, including friends playing remotely.</p> <p>Sometimes docker images are released several days later than the original server, which leads to a temporary version mismatch between the server and the clients, but when the server is lagging one version behind, (or, rarely, down) the kids will just use one of their own PCs as the secondary server and play on that one until the primary server is fixed.</p> <p>Eventually the kids seem to have grown out of it and are no longer so interested in playing Minecraft, so the server has been archived for potential future use.</p>"},{"location":"projects/self-hosting/#not-really-useful","title":"Not Really Useful","text":"<p>These applications turned out to be not really useful for the intended purpose/s. This may be due to open issues (bugs) or their design and intended behavior not matching intended purpose/s.</p>"},{"location":"projects/self-hosting/#kavita","title":"Kavita","text":"<p>Kavita looked promising and was installed during the process that lead to self-hosted eBook library with Komga. What kept me from using this one long-term was that it really is built for comic series and not so much for individual books.</p>"},{"location":"projects/self-hosting/#photoprism","title":"PhotoPrism\u00ae","text":"<p>Self-hosted photo albums with PhotoPrism\u00ae was very promising with its use of the latest technologies to tag and find pictures automatically without getting in your way, but turned out to present one Big Problem that kept it from being really useful (and that was not the only one).</p> <p>There may be solutions for those problems but, even then, the whole navigation and UI experience was not entirely satisfactory. The current plan is to try Immich next.</p>"},{"location":"projects/self-hosting/#applications-considered","title":"Applications Considered","text":"<p>These applications have not been installed yet.</p>"},{"location":"projects/self-hosting/#desired","title":"Desired","text":"<p>These applications have been briefly evaluated and look like they may be a good match for my intended purpose/s.</p>"},{"location":"projects/self-hosting/#adguardhome","title":"AdGuardHome","text":"<p>AdGuardHome may be a good tool to mitigate phishing and malware attacks on the web and possibly leverage blocking of adult domains; provided it works better than Pi-hole\u00ae.</p>"},{"location":"projects/self-hosting/#affine","title":"AFFiNE","text":"<p>AFFiNE is a workspace with fully merged docs, whiteboards and databases, a privacy-focused, local-first, open-source, and ready-to-use alternative for Notion &amp; Miro.</p> <p>To self-host AFFiNE in a Kubernetes cluster, a deployment including AFFiNE and its dependencies can be created from their example <code>compose.yaml</code>.</p>"},{"location":"projects/self-hosting/#blocky","title":"Blocky","text":"<p>Blocky may be a better alternative to Pi-hole\u00ae than AdGuardHome. Some people report having unexplained latency issues with AdGuard and/or some users experiencing broken websites, while these issues do not seem to occur with Blocky. There is no frontend, but the configuration lives in a single YAML file which is easy to track.</p>"},{"location":"projects/self-hosting/#fail2ban","title":"Fail2Ban","text":"<p>Fail2Ban scans log files and bans IP addresses conducting too many failed login attempts.</p> <p>This is already setup in the host OS but is limited to the SSH service, which is has password authentication disabled. The next step would be to set it up to ban IPs that fail to authenticate through Authentik.</p>"},{"location":"projects/self-hosting/#freshrss","title":"FreshRSS","text":"<p>FreshRSS is a self-hosted RSS feed aggregator, much like NewsBlur, but perhaps the plus of having integration with Homepage.</p>"},{"location":"projects/self-hosting/#forgejo","title":"Forgejo","text":"<p>Forgejo is a self-hosted lightweight software forge. There is a Helm Chart and other goodies:</p> <ul> <li>Integrating Forgejo Actions on my Self-Hosted Instance</li> <li>Delightful Forgejo</li> </ul>"},{"location":"projects/self-hosting/#gatus","title":"Gatus","text":"<p>Gatus is a developer-oriented health dashboard to monitor your services, evaluate the result of queries based on conditions and health checks can be paired with alerting, e.g. via  Ntfy alerts to push notifications to your phone via ntfy.</p>"},{"location":"projects/self-hosting/#ghostfolio","title":"Ghostfolio","text":"<p>Ghostfolio is an open source wealth management software built with web technology. The application empowers busy people to keep track of stocks, ETFs or cryptocurrencies and make solid, data-driven investment decisions. The software is designed for personal use in continuous operation.</p>"},{"location":"projects/self-hosting/#gitea","title":"Gitea","text":"<p>Gitea is a painless, self-hosted Git service, although the last (first) time I tried it, it was painfully finicky to use, setting up Nginx ingress with HTTPs took me a while and I could never figure out how to use it as the (only) remote repository when working from Visual Studio Code Server.</p>"},{"location":"projects/self-hosting/#heimdall","title":"Heimdall","text":"<p>Heimdall Application Dashboard is a dashboard for all your web applications and links to anything else which seems more versatile than a applications-only dashboard like Homepage.</p>"},{"location":"projects/self-hosting/#immich","title":"Immich","text":"<p>Immich is a self-hosted photo and video management solution that should make it easy to browse, search and organize photos and videos with ease, without sacrificing privacy.</p> <p>Immich can be deployed on Kubernetes using the official Helm charts, which requires a suitable postgres instance with the vectorchord extension. It is recommended to use cloudnative-pg with the tensorchord/cloudnative-vectorchord container image. An example cluster manifest can be found here. There are also examples of how other people run Immich on Kubernetes, using the official chart or otherwise.</p> <p>Besides deployment, leveraging Immich for a rich multi-user experience will require much fine-tuning of multiple features:</p> <ul> <li>External Libraries to track assets   stored in the filesystem outside of Immich, since there are multiple personal   libraries with &gt;100,000 photos going back &gt;20 years.</li> <li>Folder View to navigate through the   folders and files in those external librarires, which are already organized.</li> <li>Storage Template to set   the uploaded filename patterns for each user.</li> <li>User Management because   each familiy member should have their own library.</li> <li>Partner Sharing for each user to   let other see all their photos.</li> <li>Immich Folder Album Creator   to automatically create albums from (some) directories.</li> </ul>"},{"location":"projects/self-hosting/#jellystat","title":"Jellystat","text":"<p>Jellystat is a free and open source Statistics App for Jellyfin.</p>"},{"location":"projects/self-hosting/#leantime","title":"Leantime","text":"<p>Leantime is an open source project management system for non-project managers. We combine strategy, planning and execution while making it easy for everyone on the team to use. Built with ADHD, dyslexia and autism in mind. \ud83e\udde0</p> <p>Sounds likely a better fit for me than Kendo Manager.</p>"},{"location":"projects/self-hosting/#linkwarden","title":"Linkwarden","text":"<p>Linkwarden is a self-hosted, open-source collaborative bookmark manager to collect, read, annotate, and fully preserve what matters, all in one place. There are only installation steps for Docker and <code>compose</code>, but should be easy to run in Kubernetes. It also supports  Google (OIDC).</p>"},{"location":"projects/self-hosting/#netdata","title":"Netdata","text":"<p>Netdata could replace Continuous Monitoring, at the cost of $90 billed yearly (Homelab pricing), probably a significant amount of time to set it up in all hosts and sending all telemetry off-site to Netdata Cloud, where it can only be visualized in the closed-source Netdata UI.</p> <p>Otherwise, the free plan is limited to Max 5 Active Connected Nodes (in total), not enough to monitor all the active hosts in our home network. It may be enough to monitor the most active hosts, to get a sense of how much more desirable an upgrade may be.</p> <p>In addition to the limitation on the number of hosts, metrics are aggregated past 14 days so it still requires an external database for long-term storage. Export metrics to external time-series databases supports InfluxDB via Graphite and VictoriaMetrics via Prometheus Remote Write.</p> <p>That said, Netdata offers superior monitoring functionalities, with Top Monitoring (Netdata Functions) including customizable Applications CPU Utilization and Aggregating CPU Consumption Across Process Trees, better than other console based tools which is what is used under the hood by Continuous Monitoring. Also, there are hundreds of integrations, including HDD temperature, Intel GPU, Linux Sensors, Nvidia GPU and even TP-Link P110.</p>"},{"location":"projects/self-hosting/#newsblur","title":"NewsBlur","text":"<p>NewsBlur is a personal news reader bringing people together to talk about the world, something I've been missing since Google Reader shut down on July 1, 2013.</p>"},{"location":"projects/self-hosting/#ollama","title":"Ollama","text":"<p>Ollama allows you to get up and running with large language models, including Llama 3.3, Phi 3, Mistral, Gemma 2, and others. Whether this can actually be useful or fun, that is to be determined; it should be at least some fun for things with object / audio detection.</p> <p>There is a Helm chart in otwld/ollama-helm and a user-friendly self-hosted WebUI at open-webui/open-webui. You can even install both Ollama and Open WebUI using Helm.</p>"},{"location":"projects/self-hosting/#pi-hole","title":"Pi-hole\u00ae","text":"<p>Pi-hole\u00ae is a renowned Network-wide Ad Blocking and is very simple to run. However, blocking ads is not the main concern, but instead blocking phishing and malware domains. This requires using custom blocklists manually, like tweedge/emerging-threats-pihole.</p>"},{"location":"projects/self-hosting/#pterodactyl","title":"Pterodactyl\u00ae","text":"<p>Pterodactyl\u00ae is a free, open-source game server management panel designed with security in mind, which runs  all game servers in isolated Docker containers while exposing a beautiful and intuitive UI to end users.</p> <p>There is an example <code>docker-compose.yml</code> here. The full list of supported games is split between pelican-eggs/games-standalone and pelican-eggs/games-steamcmd.</p> <ul> <li>SteamCMD</li> <li>Sven Co-op : Running a server</li> <li>How to start a Sven Coop server?</li> <li>Sven Co-op server hosting</li> <li>Sven Co-op: How do I turn off Survival Mode?</li> </ul>"},{"location":"projects/self-hosting/#scrutiny","title":"Scrutiny","text":"<p>scrutiny is a WebUI for smartd S.M.A.R.T monitoring that includes a collector that can run on a  Hub &amp; Spoke model, with multiple Hosts. The Hub host needs to run all 3 images, as illustrated in the example <code>docker-compose.yml</code>.</p>"},{"location":"projects/self-hosting/#tp-link-omada-controller","title":"TP-Link Omada Controller","text":"<p>TP-Link Omada Hardware works well enough with each access point managing its own wireless network, but the TP-Link Omada Controller may offer additiona (desirable) features to adjust their behavior and have the same wireless network/s across the building.</p> <p>mbentley/docker-omada-controller includes a few examples files for Kubernetes deployments, and TP-Link Omada Controller on Kubernetes contains a first-hand account of deploying the service in a small cluster.</p>"},{"location":"projects/self-hosting/#unifi-poller","title":"UniFi Poller","text":"<p>UniFi Poller allows you to collect data from your UniFi network controller, save it to a database, and then display it on pre-supplied attractive and data-rich Grafana dashboards and you can also re-use existing database or Grafana installations.</p>"},{"location":"projects/self-hosting/#victoriametrics","title":"VictoriaMetrics","text":"<p>Migrating Continuous Monitoring, from InfluxDB 1.x to InfluxDB 2.7 may be too much trouble, it may turn out to be easier to replace InfluxDB with VictoriaMetrics. VictoriaMetrics is a fast, cost-saving, and scalable solution for monitoring and managing time series data.</p>"},{"location":"projects/self-hosting/#vikunja","title":"Vikunja","text":"<p>Vikunja is a TODO app, like a notebook where you can have all your things to keep track of.</p>"},{"location":"projects/self-hosting/#discarded","title":"Discarded","text":"<p>These applications were evaluated based on their documentation and/or live demos, and deemed not a good match for my intended purpose/s.</p>"},{"location":"projects/self-hosting/#authentik","title":"Authentik","text":"<p>Authentik allows restricting access to a specific set of users based on their email addresses, so that each applications can only be accessed by their legit users and their authentication is enforced by their respective identity providers.</p> <p>In particular, it is clearly documented that the rather popular <code>@gmail.com</code> addresses are supported by the Google identity provider (see also  authentik/discussions/1776).</p> <p>replacing Ingress-NGINX with Pomerium superseded Authentik because Pomerium natively supports restricting access to a specific set of users based on their email addresses, just <code>@gmail.com</code> addresses.</p>"},{"location":"projects/self-hosting/#mediatracker","title":"MediaTracker","text":"<p>MediaTracker is a self hosted platform for tracking movies, tv shows, video games, books and audiobooks, which would make it more interesting than Yamtrack if only it would allow you to add media manually.</p>"},{"location":"projects/self-hosting/#nginx-proxy-manager","title":"Nginx Proxy Manager","text":"<p>Nginx Proxy Manager would be nice to have a GUI, but the current setup with ingress Nginx. paired with ACME cert manager deployment already provides the same functionality.</p> <p>IP restrictions and other advanced settings can be deployed by making use of <code>nginx.ingress.kubernetes.io</code> annotation snippets.</p>"},{"location":"projects/self-hosting/#outline","title":"Outline","text":"<p>Outline is a blazing fast editor with markdown support. Discarded in favor of Material for MkDocs hosted on GitHub Pages because it looks like a better fit teams rather than one individual. Previously, Jekyll on GitHub pages filled the same role.</p> <p>It may still be an interesting learning exercise, to create a Kubernetes deployment based on their recommended method to self-host with Docker Compose.</p>"},{"location":"projects/self-hosting/#plex-media-server","title":"Plex Media Server","text":"<p>Plex Media Server was very convenient to let Kubernetes take care of updating the software and it served me well for a few years, while I enjoyed catching up with old (sometimes very old) podcasts. Since then, I've moved entirely to Audiobookshelf has already replaced it for audiobooks and podcasts (and found a way to catch up with old ones).</p> <p>Immich is yet to be tested, but even without it the need for a Plex server has not been felt since the migration of services to the new NUC server (octavo). Since then, Jellyfin has replaced Plex for watching videos and Navidrome has replaced it for music.</p>"},{"location":"projects/self-hosting/#project-management","title":"Project Management","text":"<p>OpenProject (Helm chart), Taiga and WeKan (Helm chart) all seem very promising applications for task and project management, but it the prospects of having any time to manage are not all that promising; a handful of post-its already reflects the very limited success so far in 2025.</p>"},{"location":"projects/self-hosting/#ryot","title":"Ryot","text":"<p>Ryot turned out work well only for Audiobookshelf; all other imports and integrations failed.</p>"},{"location":"projects/self-hosting/#yamtrack","title":"Yamtrack","text":"<p>Yamtrack would add no more (useful to me) features compared to Ryot, it would seem easier (or, at least, a better idea) to implement Generic JSON imports and integrations than to try importing using Yamtrack CSV import format.</p>"},{"location":"blog/archive/2026/","title":"2026","text":""},{"location":"blog/archive/2026/#2026","title":"2026","text":""},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2025/#2025","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2024/#2024","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2023/#2023","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""},{"location":"blog/archive/2022/#2022","title":"2022","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/archive/2020/#2020","title":"2020","text":""},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/archive/2019/#2019","title":"2019","text":""},{"location":"blog/category/linux/","title":"linux","text":""},{"location":"blog/category/linux/#linux","title":"linux","text":""},{"location":"blog/category/kubernetes/","title":"kubernetes","text":""},{"location":"blog/category/kubernetes/#kubernetes","title":"kubernetes","text":""},{"location":"blog/category/self-hosted/","title":"self-hosted","text":""},{"location":"blog/category/self-hosted/#self-hosted","title":"self-hosted","text":""},{"location":"blog/category/organizer/","title":"organizer","text":""},{"location":"blog/category/organizer/#organizer","title":"organizer","text":""},{"location":"blog/category/ubuntu/","title":"Ubuntu","text":""},{"location":"blog/category/ubuntu/#ubuntu","title":"Ubuntu","text":""},{"location":"blog/category/server/","title":"Server","text":""},{"location":"blog/category/server/#server","title":"Server","text":""},{"location":"blog/category/intel-nuc/","title":"Intel NUC","text":""},{"location":"blog/category/intel-nuc/#intel-nuc","title":"Intel NUC","text":""},{"location":"blog/category/docker/","title":"docker","text":""},{"location":"blog/category/docker/#docker","title":"docker","text":""},{"location":"blog/category/security/","title":"security","text":""},{"location":"blog/category/security/#security","title":"security","text":""},{"location":"blog/category/desktop/","title":"desktop","text":""},{"location":"blog/category/desktop/#desktop","title":"desktop","text":""},{"location":"blog/category/foss/","title":"foss","text":""},{"location":"blog/category/foss/#foss","title":"foss","text":""},{"location":"blog/category/photography/","title":"photography","text":""},{"location":"blog/category/photography/#photography","title":"photography","text":""},{"location":"blog/category/raspberrypi/","title":"raspberrypi","text":""},{"location":"blog/category/raspberrypi/#raspberrypi","title":"raspberrypi","text":""},{"location":"blog/category/gaming/","title":"gaming","text":""},{"location":"blog/category/gaming/#gaming","title":"gaming","text":""},{"location":"blog/category/streaming/","title":"streaming","text":""},{"location":"blog/category/streaming/#streaming","title":"streaming","text":""},{"location":"blog/category/steam/","title":"steam","text":""},{"location":"blog/category/steam/#steam","title":"steam","text":""},{"location":"blog/category/time-tracking/","title":"time-tracking","text":""},{"location":"blog/category/time-tracking/#time-tracking","title":"time-tracking","text":""},{"location":"blog/category/media-tracking/","title":"media-tracking","text":""},{"location":"blog/category/media-tracking/#media-tracking","title":"media-tracking","text":""},{"location":"blog/category/moonlight/","title":"moonlight","text":""},{"location":"blog/category/moonlight/#moonlight","title":"moonlight","text":""},{"location":"blog/category/sunshine/","title":"sunshine","text":""},{"location":"blog/category/sunshine/#sunshine","title":"sunshine","text":""},{"location":"blog/category/setup/","title":"setup","text":""},{"location":"blog/category/setup/#setup","title":"setup","text":""},{"location":"blog/category/upgrade/","title":"upgrade","text":""},{"location":"blog/category/upgrade/#upgrade","title":"upgrade","text":""},{"location":"blog/category/hardware/","title":"Hardware","text":""},{"location":"blog/category/hardware/#hardware","title":"Hardware","text":""},{"location":"blog/category/jellyfin/","title":"Jellyfin","text":""},{"location":"blog/category/jellyfin/#jellyfin","title":"Jellyfin","text":""},{"location":"blog/category/intelgpu/","title":"IntelGPU","text":""},{"location":"blog/category/intelgpu/#intelgpu","title":"IntelGPU","text":""},{"location":"blog/category/media/","title":"Media","text":""},{"location":"blog/category/media/#media","title":"Media","text":""},{"location":"blog/category/btrfs/","title":"Btrfs","text":""},{"location":"blog/category/btrfs/#btrfs","title":"Btrfs","text":""},{"location":"blog/category/synology/","title":"Synology","text":""},{"location":"blog/category/synology/#synology","title":"Synology","text":""},{"location":"blog/category/nas/","title":"NAS","text":""},{"location":"blog/category/nas/#nas","title":"NAS","text":""},{"location":"blog/category/privacy/","title":"privacy","text":""},{"location":"blog/category/privacy/#privacy","title":"privacy","text":""},{"location":"blog/category/cloudflare/","title":"cloudflare","text":""},{"location":"blog/category/cloudflare/#cloudflare","title":"cloudflare","text":""},{"location":"blog/category/tailscale/","title":"tailscale","text":""},{"location":"blog/category/tailscale/#tailscale","title":"tailscale","text":""},{"location":"blog/category/tunnels/","title":"tunnels","text":""},{"location":"blog/category/tunnels/#tunnels","title":"tunnels","text":""},{"location":"blog/category/firefox/","title":"firefox","text":""},{"location":"blog/category/firefox/#firefox","title":"firefox","text":""},{"location":"blog/category/home-assistant/","title":"home assistant","text":""},{"location":"blog/category/home-assistant/#home-assistant","title":"home assistant","text":""},{"location":"blog/category/unifi/","title":"unifi","text":""},{"location":"blog/category/unifi/#unifi","title":"unifi","text":""},{"location":"blog/category/monitoring/","title":"monitoring","text":""},{"location":"blog/category/monitoring/#monitoring","title":"monitoring","text":""},{"location":"blog/category/python/","title":"python","text":""},{"location":"blog/category/python/#python","title":"python","text":""},{"location":"blog/category/tapo/","title":"tapo","text":""},{"location":"blog/category/tapo/#tapo","title":"tapo","text":""},{"location":"blog/category/installation/","title":"installation","text":""},{"location":"blog/category/installation/#installation","title":"installation","text":""},{"location":"blog/category/github/","title":"github","text":""},{"location":"blog/category/github/#github","title":"github","text":""},{"location":"blog/category/markdown/","title":"markdown","text":""},{"location":"blog/category/markdown/#markdown","title":"markdown","text":""},{"location":"blog/category/blog/","title":"blog","text":""},{"location":"blog/category/blog/#blog","title":"blog","text":""},{"location":"blog/category/migration/","title":"migration","text":""},{"location":"blog/category/migration/#migration","title":"migration","text":""},{"location":"blog/category/mkdocs/","title":"mkdocs","text":""},{"location":"blog/category/mkdocs/#mkdocs","title":"mkdocs","text":""},{"location":"blog/category/material/","title":"material","text":""},{"location":"blog/category/material/#material","title":"material","text":""},{"location":"blog/category/intel/","title":"intel","text":""},{"location":"blog/category/intel/#intel","title":"intel","text":""},{"location":"blog/category/nuc/","title":"nuc","text":""},{"location":"blog/category/nuc/#nuc","title":"nuc","text":""},{"location":"blog/category/music/","title":"music","text":""},{"location":"blog/category/music/#music","title":"music","text":""},{"location":"blog/category/navidrome/","title":"navidrome","text":""},{"location":"blog/category/navidrome/#navidrome","title":"navidrome","text":""},{"location":"blog/category/dns/","title":"dns","text":""},{"location":"blog/category/dns/#dns","title":"dns","text":""},{"location":"blog/category/photo-albums/","title":"photo-albums","text":""},{"location":"blog/category/photo-albums/#photo-albums","title":"photo-albums","text":""},{"location":"blog/category/photoprism/","title":"photoprism","text":""},{"location":"blog/category/photoprism/#photoprism","title":"photoprism","text":""},{"location":"blog/category/homelab/","title":"homelab","text":""},{"location":"blog/category/homelab/#homelab","title":"homelab","text":""},{"location":"blog/category/homebox/","title":"homebox","text":""},{"location":"blog/category/homebox/#homebox","title":"homebox","text":""},{"location":"blog/category/inventory/","title":"inventory","text":""},{"location":"blog/category/inventory/#inventory","title":"inventory","text":""},{"location":"blog/category/activitywatch/","title":"ActivityWatch","text":""},{"location":"blog/category/activitywatch/#activitywatch","title":"ActivityWatch","text":""},{"location":"blog/category/ebook/","title":"ebook","text":""},{"location":"blog/category/ebook/#ebook","title":"ebook","text":""},{"location":"blog/category/komga/","title":"komga","text":""},{"location":"blog/category/komga/#komga","title":"komga","text":""},{"location":"blog/category/firefly-iii/","title":"firefly-iii","text":""},{"location":"blog/category/firefly-iii/#firefly-iii","title":"firefly-iii","text":""},{"location":"blog/category/studio/","title":"studio","text":""},{"location":"blog/category/studio/#studio","title":"studio","text":""},{"location":"blog/category/proton/","title":"proton","text":""},{"location":"blog/category/proton/#proton","title":"proton","text":""},{"location":"blog/category/videogames/","title":"videogames","text":""},{"location":"blog/category/videogames/#videogames","title":"videogames","text":""},{"location":"blog/category/bethesda/","title":"bethesda","text":""},{"location":"blog/category/bethesda/#bethesda","title":"bethesda","text":""},{"location":"blog/category/fallout/","title":"fallout","text":""},{"location":"blog/category/fallout/#fallout","title":"fallout","text":""},{"location":"blog/category/influxdb/","title":"influxdb","text":""},{"location":"blog/category/influxdb/#influxdb","title":"influxdb","text":""},{"location":"blog/category/grafana/","title":"grafana","text":""},{"location":"blog/category/grafana/#grafana","title":"grafana","text":""},{"location":"blog/category/minecraft/","title":"minecraft","text":""},{"location":"blog/category/minecraft/#minecraft","title":"minecraft","text":""},{"location":"blog/category/java/","title":"java","text":""},{"location":"blog/category/java/#java","title":"java","text":""},{"location":"blog/category/bedrock/","title":"bedrock","text":""},{"location":"blog/category/bedrock/#bedrock","title":"bedrock","text":""},{"location":"blog/category/audiobookshelf/","title":"audiobookshelf","text":""},{"location":"blog/category/audiobookshelf/#audiobookshelf","title":"audiobookshelf","text":""},{"location":"blog/category/audiobooks/","title":"audiobooks","text":""},{"location":"blog/category/audiobooks/#audiobooks","title":"audiobooks","text":""},{"location":"blog/category/encryption/","title":"encryption","text":""},{"location":"blog/category/encryption/#encryption","title":"encryption","text":""},{"location":"blog/category/veracrypt/","title":"veracrypt","text":""},{"location":"blog/category/veracrypt/#veracrypt","title":"veracrypt","text":""},{"location":"blog/category/luks/","title":"luks","text":""},{"location":"blog/category/luks/#luks","title":"luks","text":""},{"location":"blog/category/cryptsetup/","title":"cryptsetup","text":""},{"location":"blog/category/cryptsetup/#cryptsetup","title":"cryptsetup","text":""},{"location":"blog/category/displaycal/","title":"displaycal","text":""},{"location":"blog/category/displaycal/#displaycal","title":"displaycal","text":""},{"location":"blog/category/skyrim/","title":"skyrim","text":""},{"location":"blog/category/skyrim/#skyrim","title":"skyrim","text":""},{"location":"blog/category/mods/","title":"mods","text":""},{"location":"blog/category/mods/#mods","title":"mods","text":""},{"location":"blog/category/audio/","title":"audio","text":""},{"location":"blog/category/audio/#audio","title":"audio","text":""},{"location":"blog/category/recording/","title":"recording","text":""},{"location":"blog/category/recording/#recording","title":"recording","text":""},{"location":"blog/category/alsa/","title":"alsa","text":""},{"location":"blog/category/alsa/#alsa","title":"alsa","text":""},{"location":"blog/category/headphones/","title":"headphones","text":""},{"location":"blog/category/headphones/#headphones","title":"headphones","text":""},{"location":"blog/category/ea/","title":"ea","text":""},{"location":"blog/category/ea/#ea","title":"ea","text":""},{"location":"blog/category/bioware/","title":"bioware","text":""},{"location":"blog/category/bioware/#bioware","title":"bioware","text":""},{"location":"blog/category/jekyll/","title":"jekyll","text":""},{"location":"blog/category/jekyll/#jekyll","title":"jekyll","text":""},{"location":"blog/category/plex/","title":"plex","text":""},{"location":"blog/category/plex/#plex","title":"plex","text":""},{"location":"blog/category/xhci/","title":"xhci","text":""},{"location":"blog/category/xhci/#xhci","title":"xhci","text":""},{"location":"blog/category/usb/","title":"usb","text":""},{"location":"blog/category/usb/#usb","title":"usb","text":""},{"location":"blog/category/failure/","title":"failure","text":""},{"location":"blog/category/failure/#failure","title":"failure","text":""},{"location":"blog/category/workaround/","title":"workaround","text":""},{"location":"blog/category/workaround/#workaround","title":"workaround","text":""},{"location":"blog/category/asus/","title":"asus","text":""},{"location":"blog/category/asus/#asus","title":"asus","text":""},{"location":"blog/category/mediatek/","title":"mediatek","text":""},{"location":"blog/category/mediatek/#mediatek","title":"mediatek","text":""},{"location":"blog/category/glitch/","title":"glitch","text":""},{"location":"blog/category/glitch/#glitch","title":"glitch","text":""},{"location":"blog/category/timeout/","title":"timeout","text":""},{"location":"blog/category/timeout/#timeout","title":"timeout","text":""},{"location":"blog/category/keyboard/","title":"keyboard","text":""},{"location":"blog/category/keyboard/#keyboard","title":"keyboard","text":""},{"location":"blog/category/macropad/","title":"macropad","text":""},{"location":"blog/category/macropad/#macropad","title":"macropad","text":""},{"location":"blog/category/qmk/","title":"qmk","text":""},{"location":"blog/category/qmk/#qmk","title":"qmk","text":""},{"location":"blog/category/firmware/","title":"firmware","text":""},{"location":"blog/category/firmware/#firmware","title":"firmware","text":""},{"location":"blog/category/vscode/","title":"vscode","text":""},{"location":"blog/category/vscode/#vscode","title":"vscode","text":""},{"location":"blog/category/nvidia/","title":"nvidia","text":""},{"location":"blog/category/nvidia/#nvidia","title":"nvidia","text":""},{"location":"blog/category/troubleshooting/","title":"troubleshooting","text":""},{"location":"blog/category/troubleshooting/#troubleshooting","title":"troubleshooting","text":""},{"location":"blog/category/gpuburn/","title":"gpuburn","text":""},{"location":"blog/category/gpuburn/#gpuburn","title":"gpuburn","text":""},{"location":"blog/category/cuda/","title":"cuda","text":""},{"location":"blog/category/cuda/#cuda","title":"cuda","text":""},{"location":"blog/category/crucial/","title":"crucial","text":""},{"location":"blog/category/crucial/#crucial","title":"crucial","text":""},{"location":"blog/category/ssd/","title":"ssd","text":""},{"location":"blog/category/ssd/#ssd","title":"ssd","text":""},{"location":"blog/category/locales/","title":"locales","text":""},{"location":"blog/category/locales/#locales","title":"locales","text":""},{"location":"blog/category/chrome/","title":"chrome","text":""},{"location":"blog/category/chrome/#chrome","title":"chrome","text":""},{"location":"blog/category/raid/","title":"RAID","text":""},{"location":"blog/category/raid/#raid","title":"RAID","text":""},{"location":"blog/category/audible/","title":"Audible","text":""},{"location":"blog/category/audible/#audible","title":"Audible","text":""},{"location":"blog/category/bash/","title":"Bash","text":""},{"location":"blog/category/bash/#bash","title":"Bash","text":""},{"location":"blog/category/mp3/","title":"MP3","text":""},{"location":"blog/category/mp3/#mp3","title":"MP3","text":""},{"location":"blog/page/2/","title":"Unexpected Linux Adventures","text":""},{"location":"blog/page/2/#unexpected-linux-adventures","title":"Unexpected Linux Adventures","text":""},{"location":"blog/page/3/","title":"Unexpected Linux Adventures","text":""},{"location":"blog/page/3/#unexpected-linux-adventures","title":"Unexpected Linux Adventures","text":""},{"location":"blog/page/4/","title":"Unexpected Linux Adventures","text":""},{"location":"blog/page/4/#unexpected-linux-adventures","title":"Unexpected Linux Adventures","text":""},{"location":"blog/page/5/","title":"Unexpected Linux Adventures","text":""},{"location":"blog/page/5/#unexpected-linux-adventures","title":"Unexpected Linux Adventures","text":""},{"location":"blog/page/6/","title":"Unexpected Linux Adventures","text":""},{"location":"blog/page/6/#unexpected-linux-adventures","title":"Unexpected Linux Adventures","text":""},{"location":"blog/page/7/","title":"Unexpected Linux Adventures","text":""},{"location":"blog/page/7/#unexpected-linux-adventures","title":"Unexpected Linux Adventures","text":""},{"location":"blog/archive/2025/page/2/","title":"2025","text":""},{"location":"blog/archive/2025/page/2/#2025","title":"2025","text":""},{"location":"blog/archive/2024/page/2/","title":"2024","text":""},{"location":"blog/archive/2024/page/2/#2024","title":"2024","text":""},{"location":"blog/archive/2024/page/3/","title":"2024","text":""},{"location":"blog/archive/2024/page/3/#2024","title":"2024","text":""},{"location":"blog/archive/2023/page/2/","title":"2023","text":""},{"location":"blog/archive/2023/page/2/#2023","title":"2023","text":""},{"location":"blog/category/server/page/2/","title":"Server","text":""},{"location":"blog/category/server/page/2/#server","title":"Server","text":""},{"location":"blog/category/server/page/3/","title":"Server","text":""},{"location":"blog/category/server/page/3/#server","title":"Server","text":""},{"location":"blog/category/server/page/4/","title":"Server","text":""},{"location":"blog/category/server/page/4/#server","title":"Server","text":""},{"location":"blog/category/ubuntu/page/2/","title":"Ubuntu","text":""},{"location":"blog/category/ubuntu/page/2/#ubuntu","title":"Ubuntu","text":""},{"location":"blog/category/ubuntu/page/3/","title":"Ubuntu","text":""},{"location":"blog/category/ubuntu/page/3/#ubuntu","title":"Ubuntu","text":""},{"location":"blog/category/ubuntu/page/4/","title":"Ubuntu","text":""},{"location":"blog/category/ubuntu/page/4/#ubuntu","title":"Ubuntu","text":""},{"location":"blog/category/docker/page/2/","title":"docker","text":""},{"location":"blog/category/docker/page/2/#docker","title":"docker","text":""},{"location":"blog/category/docker/page/3/","title":"docker","text":""},{"location":"blog/category/docker/page/3/#docker","title":"docker","text":""},{"location":"blog/category/kubernetes/page/2/","title":"kubernetes","text":""},{"location":"blog/category/kubernetes/page/2/#kubernetes","title":"kubernetes","text":""},{"location":"blog/category/kubernetes/page/3/","title":"kubernetes","text":""},{"location":"blog/category/kubernetes/page/3/#kubernetes","title":"kubernetes","text":""},{"location":"blog/category/kubernetes/page/4/","title":"kubernetes","text":""},{"location":"blog/category/kubernetes/page/4/#kubernetes","title":"kubernetes","text":""},{"location":"blog/category/linux/page/2/","title":"linux","text":""},{"location":"blog/category/linux/page/2/#linux","title":"linux","text":""},{"location":"blog/category/linux/page/3/","title":"linux","text":""},{"location":"blog/category/linux/page/3/#linux","title":"linux","text":""},{"location":"blog/category/linux/page/4/","title":"linux","text":""},{"location":"blog/category/linux/page/4/#linux","title":"linux","text":""},{"location":"blog/category/linux/page/5/","title":"linux","text":""},{"location":"blog/category/linux/page/5/#linux","title":"linux","text":""},{"location":"blog/category/self-hosted/page/2/","title":"self-hosted","text":""},{"location":"blog/category/self-hosted/page/2/#self-hosted","title":"self-hosted","text":""}]}